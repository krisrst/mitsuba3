diff --git a/resources/ptx/optix_rt.ptx b/resources/ptx/optix_rt.ptx
index 06f63634..2c5c6f60 100644
--- a/resources/ptx/optix_rt.ptx
+++ b/resources/ptx/optix_rt.ptx
@@ -1,12 +1,12 @@
 //
 // Generated by NVIDIA NVVM Compiler
 //
-// Compiler Build ID: CL-26907403
-// Cuda compilation tools, release 10.1, V10.1.243
-// Based on LLVM 3.4svn
+// Compiler Build ID: CL-31294372
+// Cuda compilation tools, release 11.7, V11.7.64
+// Based on NVVM 7.0.1
 //
 
-.version 6.4
+.version 7.7
 .target sm_61
 .address_size 64
 
@@ -17,35799 +17,31847 @@
 	.param .b64 vprintf_param_1
 )
 ;
-.func  (.param .b64 func_retval0) __internal_accurate_pow
-(
-	.param .b64 __internal_accurate_pow_param_0
-)
-;
 .const .align 8 .b8 params[368];
 .global .align 1 .b8 $str[36] = {79, 80, 84, 73, 88, 95, 69, 88, 67, 69, 80, 84, 73, 79, 78, 95, 67, 79, 68, 69, 95, 83, 84, 65, 67, 75, 95, 79, 86, 69, 82, 70, 76, 79, 87, 0};
-.global .align 1 .b8 $str1[42] = {79, 80, 84, 73, 88, 95, 69, 88, 67, 69, 80, 84, 73, 79, 78, 95, 67, 79, 68, 69, 95, 84, 82, 65, 67, 69, 95, 68, 69, 80, 84, 72, 95, 69, 88, 67, 69, 69, 68, 69, 68, 0};
-.global .align 1 .b8 $str2[46] = {79, 80, 84, 73, 88, 95, 69, 88, 67, 69, 80, 84, 73, 79, 78, 95, 67, 79, 68, 69, 95, 84, 82, 65, 86, 69, 82, 83, 65, 76, 95, 68, 69, 80, 84, 72, 95, 69, 88, 67, 69, 69, 68, 69, 68, 0};
-.global .align 1 .b8 $str3[51] = {79, 80, 84, 73, 88, 95, 69, 88, 67, 69, 80, 84, 73, 79, 78, 95, 67, 79, 68, 69, 95, 84, 82, 65, 86, 69, 82, 83, 65, 76, 95, 73, 78, 86, 65, 76, 73, 68, 95, 84, 82, 65, 86, 69, 82, 83, 65, 66, 76, 69, 0};
-.global .align 1 .b8 $str4[48] = {79, 80, 84, 73, 88, 95, 69, 88, 67, 69, 80, 84, 73, 79, 78, 95, 67, 79, 68, 69, 95, 84, 82, 65, 86, 69, 82, 83, 65, 76, 95, 73, 78, 86, 65, 76, 73, 68, 95, 77, 73, 83, 83, 95, 83, 66, 84, 0};
-.global .align 1 .b8 $str5[47] = {79, 80, 84, 73, 88, 95, 69, 88, 67, 69, 80, 84, 73, 79, 78, 95, 67, 79, 68, 69, 95, 84, 82, 65, 86, 69, 82, 83, 65, 76, 95, 73, 78, 86, 65, 76, 73, 68, 95, 72, 73, 84, 95, 83, 66, 84, 0};
-.const .align 8 .u64 exceptions[12] = {4294967295, generic($str), 4294967294, generic($str1), 4294967293, generic($str2), 4294967291, generic($str3), 4294967290, generic($str4), 4294967289, generic($str5)};
-.global .align 1 .b8 $str6[24] = {79, 112, 116, 105, 120, 32, 69, 120, 99, 101, 112, 116, 105, 111, 110, 32, 37, 117, 58, 32, 37, 115, 10, 0};
-
-.visible .entry __intersection__cylinder(
-
-)
+.global .align 1 .b8 $str$1[42] = {79, 80, 84, 73, 88, 95, 69, 88, 67, 69, 80, 84, 73, 79, 78, 95, 67, 79, 68, 69, 95, 84, 82, 65, 67, 69, 95, 68, 69, 80, 84, 72, 95, 69, 88, 67, 69, 69, 68, 69, 68, 0};
+.global .align 1 .b8 $str$2[46] = {79, 80, 84, 73, 88, 95, 69, 88, 67, 69, 80, 84, 73, 79, 78, 95, 67, 79, 68, 69, 95, 84, 82, 65, 86, 69, 82, 83, 65, 76, 95, 68, 69, 80, 84, 72, 95, 69, 88, 67, 69, 69, 68, 69, 68, 0};
+.global .align 1 .b8 $str$3[51] = {79, 80, 84, 73, 88, 95, 69, 88, 67, 69, 80, 84, 73, 79, 78, 95, 67, 79, 68, 69, 95, 84, 82, 65, 86, 69, 82, 83, 65, 76, 95, 73, 78, 86, 65, 76, 73, 68, 95, 84, 82, 65, 86, 69, 82, 83, 65, 66, 76, 69, 0};
+.global .align 1 .b8 $str$4[48] = {79, 80, 84, 73, 88, 95, 69, 88, 67, 69, 80, 84, 73, 79, 78, 95, 67, 79, 68, 69, 95, 84, 82, 65, 86, 69, 82, 83, 65, 76, 95, 73, 78, 86, 65, 76, 73, 68, 95, 77, 73, 83, 83, 95, 83, 66, 84, 0};
+.global .align 1 .b8 $str$5[47] = {79, 80, 84, 73, 88, 95, 69, 88, 67, 69, 80, 84, 73, 79, 78, 95, 67, 79, 68, 69, 95, 84, 82, 65, 86, 69, 82, 83, 65, 76, 95, 73, 78, 86, 65, 76, 73, 68, 95, 72, 73, 84, 95, 83, 66, 84, 0};
+.const .align 8 .u64 exceptions[12] = {4294967295, generic($str), 4294967294, generic($str$1), 4294967293, generic($str$2), 4294967291, generic($str$3), 4294967290, generic($str$4), 4294967289, generic($str$5)};
+.global .align 1 .b8 $str$6[24] = {79, 112, 116, 105, 120, 32, 69, 120, 99, 101, 112, 116, 105, 111, 110, 32, 37, 117, 58, 32, 37, 115, 10, 0};
+
+.visible .entry __intersection__cylinder()
 {
 	.reg .pred 	%p<54>;
-	.reg .b16 	%rs<14>;
-	.reg .f32 	%f<965>;
-	.reg .b32 	%r<320>;
-	.reg .b64 	%rd<266>;
-
-
-	// inline asm
-	call (%rd19), _optix_get_sbt_data_ptr_64, ();
-	// inline asm
-	ld.u64 	%rd1, [%rd19+8];
-	// inline asm
-	call (%f330), _optix_get_world_ray_origin_x, ();
-	// inline asm
-	// inline asm
-	call (%f331), _optix_get_world_ray_origin_y, ();
-	// inline asm
-	// inline asm
-	call (%f910), _optix_get_world_ray_origin_z, ();
-	// inline asm
-	// inline asm
-	call (%r8), _optix_get_transform_list_size, ();
-	// inline asm
-	setp.eq.s32	%p5, %r8, 0;
-	@%p5 bra 	BB0_1;
-
-	mov.u32 	%r318, 0;
-	// inline asm
-	call (%f333), _optix_get_ray_time, ();
-	// inline asm
-
-BB0_3:
+	.reg .b16 	%rs<8>;
+	.reg .f32 	%f<1009>;
+	.reg .b32 	%r<323>;
+	.reg .b64 	%rd<259>;
+
+
+	// begin inline asm
+	call (%rd17), _optix_get_sbt_data_ptr_64, ();
+	// end inline asm
+	ld.u64 	%rd1, [%rd17+8];
+	// begin inline asm
+	call (%f945), _optix_get_world_ray_origin_x, ();
+	// end inline asm
+	// begin inline asm
+	call (%f946), _optix_get_world_ray_origin_y, ();
+	// end inline asm
+	// begin inline asm
+	call (%f947), _optix_get_world_ray_origin_z, ();
+	// end inline asm
+	// begin inline asm
+	call (%r9), _optix_get_transform_list_size, ();
+	// end inline asm
+	setp.eq.s32 	%p2, %r9, 0;
+	@%p2 bra 	$L__BB0_21;
+
+	// begin inline asm
+	call (%r10), _optix_get_transform_list_size, ();
+	// end inline asm
+	// begin inline asm
+	call (%f359), _optix_get_ray_time, ();
+	// end inline asm
+	setp.eq.s32 	%p3, %r10, 0;
+	@%p3 bra 	$L__BB0_19;
+
+	mov.u32 	%r321, 0;
+
+$L__BB0_3:
 	.pragma "nounroll";
-	// inline asm
-	call (%rd20), _optix_get_transform_list_handle, (%r318);
-	// inline asm
-	// inline asm
-	call (%r11), _optix_get_transform_type_from_handle, (%rd20);
-	// inline asm
-	and.b32  	%r12, %r11, -2;
-	setp.eq.s32	%p6, %r12, 2;
-	@%p6 bra 	BB0_9;
-	bra.uni 	BB0_4;
-
-BB0_9:
-	setp.eq.s32	%p9, %r11, 2;
-	@%p9 bra 	BB0_13;
-	bra.uni 	BB0_10;
-
-BB0_13:
-	// inline asm
-	call (%rd94), _optix_get_matrix_motion_transform_from_handle, (%rd20);
-	// inline asm
-	// inline asm
-	cvta.to.global.u64 %rd96, %rd94;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r100,%r101,%r102,%r103}, [%rd96];
-	// inline asm
-	mov.b32	{%rs4, %rs5}, %r102;
-	add.s64 	%rd100, %rd94, 16;
-	// inline asm
-	cvta.to.global.u64 %rd99, %rd100;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r104,%r105,%r106,%r107}, [%rd99];
-	// inline asm
-	add.s64 	%rd103, %rd94, 32;
-	// inline asm
-	cvta.to.global.u64 %rd102, %rd103;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r108,%r109,%r110,%r111}, [%rd102];
-	// inline asm
-	add.s64 	%rd106, %rd94, 48;
-	// inline asm
-	cvta.to.global.u64 %rd105, %rd106;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r112,%r113,%r114,%r115}, [%rd105];
-	// inline asm
-	add.s64 	%rd109, %rd94, 64;
-	// inline asm
-	cvta.to.global.u64 %rd108, %rd109;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r116,%r117,%r118,%r119}, [%rd108];
-	// inline asm
-	add.s64 	%rd112, %rd94, 80;
-	// inline asm
-	cvta.to.global.u64 %rd111, %rd112;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r120,%r121,%r122,%r123}, [%rd111];
-	// inline asm
-	add.s64 	%rd115, %rd94, 96;
-	// inline asm
-	cvta.to.global.u64 %rd114, %rd115;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r124,%r125,%r126,%r127}, [%rd114];
-	// inline asm
-	add.s64 	%rd118, %rd94, 112;
-	// inline asm
-	cvta.to.global.u64 %rd117, %rd118;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r128,%r129,%r130,%r131}, [%rd117];
-	// inline asm
-	mov.b32 	 %f460, %r103;
-	mov.b32 	 %f461, %r104;
-	cvt.u32.u16	%r144, %rs4;
-	add.s32 	%r145, %r144, -1;
-	cvt.rn.f32.s32	%f462, %r145;
-	sub.f32 	%f463, %f333, %f460;
-	mul.f32 	%f464, %f463, %f462;
-	sub.f32 	%f465, %f461, %f460;
-	div.rn.f32 	%f466, %f464, %f465;
-	min.f32 	%f467, %f462, %f466;
-	mov.f32 	%f468, 0f00000000;
-	max.f32 	%f469, %f468, %f467;
-	cvt.rmi.f32.f32	%f470, %f469;
-	cvt.rzi.s32.f32	%r146, %f470;
-	mul.wide.s32 	%rd129, %r146, 48;
-	add.s64 	%rd121, %rd103, %rd129;
-	// inline asm
-	cvta.to.global.u64 %rd120, %rd121;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r132,%r133,%r134,%r135}, [%rd120];
-	// inline asm
-	mov.b32 	 %f882, %r132;
-	mov.b32 	 %f883, %r133;
-	mov.b32 	 %f884, %r134;
-	mov.b32 	 %f885, %r135;
-	add.s64 	%rd124, %rd121, 16;
-	// inline asm
-	cvta.to.global.u64 %rd123, %rd124;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r136,%r137,%r138,%r139}, [%rd123];
-	// inline asm
-	mov.b32 	 %f878, %r136;
-	mov.b32 	 %f879, %r137;
-	mov.b32 	 %f880, %r138;
-	mov.b32 	 %f881, %r139;
-	add.s64 	%rd127, %rd121, 32;
-	// inline asm
+	// begin inline asm
+	call (%rd18), _optix_get_transform_list_handle, (%r321);
+	// end inline asm
+	// begin inline asm
+	call (%r13), _optix_get_transform_type_from_handle, (%rd18);
+	// end inline asm
+	or.b32  	%r14, %r13, 1;
+	setp.eq.s32 	%p4, %r14, 3;
+	@%p4 bra 	$L__BB0_9;
+	bra.uni 	$L__BB0_4;
+
+$L__BB0_9:
+	setp.eq.s32 	%p7, %r13, 2;
+	@%p7 bra 	$L__BB0_13;
+	bra.uni 	$L__BB0_10;
+
+$L__BB0_13:
+	// begin inline asm
+	call (%rd90), _optix_get_matrix_motion_transform_from_handle, (%rd18);
+	// end inline asm
+	// begin inline asm
+	cvta.to.global.u64 %rd92, %rd90;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r102,%r103,%r104,%r105}, [%rd92];
+	// end inline asm
+	add.s64 	%rd96, %rd90, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd95, %rd96;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r106,%r107,%r108,%r109}, [%rd95];
+	// end inline asm
+	add.s64 	%rd99, %rd90, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd98, %rd99;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r110,%r111,%r112,%r113}, [%rd98];
+	// end inline asm
+	add.s64 	%rd102, %rd90, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd101, %rd102;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r114,%r115,%r116,%r117}, [%rd101];
+	// end inline asm
+	add.s64 	%rd105, %rd90, 64;
+	// begin inline asm
+	cvta.to.global.u64 %rd104, %rd105;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r118,%r119,%r120,%r121}, [%rd104];
+	// end inline asm
+	add.s64 	%rd108, %rd90, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd107, %rd108;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r122,%r123,%r124,%r125}, [%rd107];
+	// end inline asm
+	add.s64 	%rd111, %rd90, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd110, %rd111;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r126,%r127,%r128,%r129}, [%rd110];
+	// end inline asm
+	add.s64 	%rd114, %rd90, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd113, %rd114;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r130,%r131,%r132,%r133}, [%rd113];
+	// end inline asm
+	mov.b32 	%f487, %r105;
+	mov.b32 	%f488, %r106;
+	and.b32  	%r146, %r104, 65535;
+	add.s32 	%r147, %r146, -1;
+	cvt.rn.f32.s32 	%f489, %r147;
+	sub.f32 	%f490, %f359, %f487;
+	mul.f32 	%f491, %f490, %f489;
+	sub.f32 	%f492, %f488, %f487;
+	div.rn.f32 	%f493, %f491, %f492;
+	min.f32 	%f494, %f489, %f493;
+	mov.f32 	%f495, 0f00000000;
+	max.f32 	%f496, %f495, %f494;
+	cvt.rmi.f32.f32 	%f497, %f496;
+	sub.f32 	%f90, %f496, %f497;
+	cvt.rzi.s32.f32 	%r148, %f497;
+	mul.wide.s32 	%rd125, %r148, 48;
+	add.s64 	%rd117, %rd99, %rd125;
+	// begin inline asm
+	cvta.to.global.u64 %rd116, %rd117;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r134,%r135,%r136,%r137}, [%rd116];
+	// end inline asm
+	mov.b32 	%f900, %r134;
+	mov.b32 	%f899, %r135;
+	mov.b32 	%f898, %r136;
+	mov.b32 	%f897, %r137;
+	add.s64 	%rd120, %rd117, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd119, %rd120;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r138,%r139,%r140,%r141}, [%rd119];
+	// end inline asm
+	mov.b32 	%f904, %r138;
+	mov.b32 	%f903, %r139;
+	mov.b32 	%f902, %r140;
+	mov.b32 	%f901, %r141;
+	add.s64 	%rd123, %rd117, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd122, %rd123;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r142,%r143,%r144,%r145}, [%rd122];
+	// end inline asm
+	mov.b32 	%f908, %r142;
+	mov.b32 	%f907, %r143;
+	mov.b32 	%f906, %r144;
+	mov.b32 	%f905, %r145;
+	setp.leu.f32 	%p9, %f90, 0f00000000;
+	@%p9 bra 	$L__BB0_15;
+
+	cvt.rmi.f32.f32 	%f868, %f496;
+	cvt.rzi.s32.f32 	%r320, %f868;
+	cvt.s64.s32 	%rd256, %r320;
+	mov.f32 	%f498, 0f3F800000;
+	sub.f32 	%f499, %f498, %f90;
+	mul.lo.s64 	%rd135, %rd256, 48;
+	add.s64 	%rd136, %rd90, %rd135;
+	add.s64 	%rd127, %rd136, 80;
+	// begin inline asm
 	cvta.to.global.u64 %rd126, %rd127;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r140,%r141,%r142,%r143}, [%rd126];
-	// inline asm
-	sub.f32 	%f98, %f469, %f470;
-	mov.b32 	 %f874, %r140;
-	mov.b32 	 %f875, %r141;
-	mov.b32 	 %f876, %r142;
-	mov.b32 	 %f877, %r143;
-	setp.leu.f32	%p11, %f98, 0f00000000;
-	@%p11 bra 	BB0_15;
-
-	cvt.rmi.f32.f32	%f845, %f469;
-	cvt.rzi.s32.f32	%r317, %f845;
-	cvt.s64.s32	%rd263, %r317;
-	mul.lo.s64 	%rd139, %rd263, 48;
-	add.s64 	%rd140, %rd94, %rd139;
-	add.s64 	%rd131, %rd140, 80;
-	// inline asm
-	cvta.to.global.u64 %rd130, %rd131;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r147,%r148,%r149,%r150}, [%rd130];
-	// inline asm
-	mov.b32 	 %f471, %r147;
-	mov.b32 	 %f472, %r148;
-	mov.b32 	 %f473, %r149;
-	mov.b32 	 %f474, %r150;
-	add.s64 	%rd134, %rd140, 96;
-	// inline asm
-	cvta.to.global.u64 %rd133, %rd134;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r151,%r152,%r153,%r154}, [%rd133];
-	// inline asm
-	mov.b32 	 %f475, %r151;
-	mov.b32 	 %f476, %r152;
-	mov.b32 	 %f477, %r153;
-	mov.b32 	 %f478, %r154;
-	add.s64 	%rd137, %rd140, 112;
-	// inline asm
-	cvta.to.global.u64 %rd136, %rd137;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r155,%r156,%r157,%r158}, [%rd136];
-	// inline asm
-	mov.f32 	%f479, 0f3F800000;
-	sub.f32 	%f480, %f479, %f98;
-	mul.f32 	%f481, %f98, %f471;
-	mul.f32 	%f482, %f98, %f472;
-	mul.f32 	%f483, %f98, %f473;
-	mul.f32 	%f484, %f98, %f474;
-	fma.rn.f32 	%f882, %f480, %f882, %f481;
-	fma.rn.f32 	%f883, %f480, %f883, %f482;
-	fma.rn.f32 	%f884, %f480, %f884, %f483;
-	fma.rn.f32 	%f885, %f480, %f885, %f484;
-	mul.f32 	%f485, %f98, %f475;
-	mul.f32 	%f486, %f98, %f476;
-	mul.f32 	%f487, %f98, %f477;
-	mul.f32 	%f488, %f98, %f478;
-	fma.rn.f32 	%f878, %f480, %f878, %f485;
-	fma.rn.f32 	%f879, %f480, %f879, %f486;
-	fma.rn.f32 	%f880, %f480, %f880, %f487;
-	fma.rn.f32 	%f881, %f480, %f881, %f488;
-	mov.b32 	 %f489, %r155;
-	mov.b32 	 %f490, %r156;
-	mov.b32 	 %f491, %r157;
-	mov.b32 	 %f492, %r158;
-	mul.f32 	%f493, %f98, %f489;
-	mul.f32 	%f494, %f98, %f490;
-	mul.f32 	%f495, %f98, %f491;
-	mul.f32 	%f496, %f98, %f492;
-	fma.rn.f32 	%f874, %f480, %f874, %f493;
-	fma.rn.f32 	%f875, %f480, %f875, %f494;
-	fma.rn.f32 	%f876, %f480, %f876, %f495;
-	fma.rn.f32 	%f877, %f480, %f877, %f496;
-	bra.uni 	BB0_15;
-
-BB0_4:
-	mov.f32 	%f886, 0f00000000;
-	mov.f32 	%f888, 0f3F800000;
-	setp.eq.s32	%p7, %r11, 4;
-	@%p7 bra 	BB0_7;
-	bra.uni 	BB0_5;
-
-BB0_7:
-	// inline asm
-	call (%rd264), _optix_get_instance_inverse_transform_from_handle, (%rd20);
-	// inline asm
-	bra.uni 	BB0_8;
-
-BB0_10:
-	// inline asm
-	call (%rd35), _optix_get_srt_motion_transform_from_handle, (%rd20);
-	// inline asm
-	// inline asm
-	cvta.to.global.u64 %rd37, %rd35;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r25,%r26,%r27,%r28}, [%rd37];
-	// inline asm
-	mov.b32	{%rs2, %rs3}, %r27;
-	add.s64 	%rd41, %rd35, 16;
-	// inline asm
-	cvta.to.global.u64 %rd40, %rd41;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r29,%r30,%r31,%r32}, [%rd40];
-	// inline asm
-	add.s64 	%rd44, %rd35, 32;
-	// inline asm
-	cvta.to.global.u64 %rd43, %rd44;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r33,%r34,%r35,%r36}, [%rd43];
-	// inline asm
-	add.s64 	%rd47, %rd35, 48;
-	// inline asm
-	cvta.to.global.u64 %rd46, %rd47;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r37,%r38,%r39,%r40}, [%rd46];
-	// inline asm
-	add.s64 	%rd50, %rd35, 64;
-	// inline asm
-	cvta.to.global.u64 %rd49, %rd50;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r41,%r42,%r43,%r44}, [%rd49];
-	// inline asm
-	add.s64 	%rd53, %rd35, 80;
-	// inline asm
-	cvta.to.global.u64 %rd52, %rd53;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r45,%r46,%r47,%r48}, [%rd52];
-	// inline asm
-	add.s64 	%rd56, %rd35, 96;
-	// inline asm
-	cvta.to.global.u64 %rd55, %rd56;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r49,%r50,%r51,%r52}, [%rd55];
-	// inline asm
-	add.s64 	%rd59, %rd35, 112;
-	// inline asm
-	cvta.to.global.u64 %rd58, %rd59;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r53,%r54,%r55,%r56}, [%rd58];
-	// inline asm
-	add.s64 	%rd62, %rd35, 128;
-	// inline asm
-	cvta.to.global.u64 %rd61, %rd62;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r57,%r58,%r59,%r60}, [%rd61];
-	// inline asm
-	add.s64 	%rd65, %rd35, 144;
-	// inline asm
-	cvta.to.global.u64 %rd64, %rd65;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r61,%r62,%r63,%r64}, [%rd64];
-	// inline asm
-	mov.b32 	 %f347, %r28;
-	mov.b32 	 %f348, %r29;
-	cvt.u32.u16	%r81, %rs2;
-	add.s32 	%r82, %r81, -1;
-	cvt.rn.f32.s32	%f349, %r82;
-	sub.f32 	%f350, %f333, %f347;
-	mul.f32 	%f351, %f350, %f349;
-	sub.f32 	%f352, %f348, %f347;
-	div.rn.f32 	%f353, %f351, %f352;
-	min.f32 	%f354, %f349, %f353;
-	mov.f32 	%f355, 0f00000000;
-	max.f32 	%f356, %f355, %f354;
-	cvt.rmi.f32.f32	%f357, %f356;
-	cvt.rzi.s32.f32	%r83, %f357;
-	mul.wide.s32 	%rd79, %r83, 64;
-	add.s64 	%rd68, %rd44, %rd79;
-	// inline asm
-	cvta.to.global.u64 %rd67, %rd68;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r65,%r66,%r67,%r68}, [%rd67];
-	// inline asm
-	mov.b32 	 %f858, %r65;
-	mov.b32 	 %f859, %r66;
-	mov.b32 	 %f860, %r67;
-	mov.b32 	 %f861, %r68;
-	add.s64 	%rd71, %rd68, 16;
-	// inline asm
-	cvta.to.global.u64 %rd70, %rd71;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r69,%r70,%r71,%r72}, [%rd70];
-	// inline asm
-	mov.b32 	 %f862, %r69;
-	mov.b32 	 %f863, %r70;
-	mov.b32 	 %f864, %r71;
-	mov.b32 	 %f865, %r72;
-	add.s64 	%rd74, %rd68, 32;
-	// inline asm
-	cvta.to.global.u64 %rd73, %rd74;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r73,%r74,%r75,%r76}, [%rd73];
-	// inline asm
-	sub.f32 	%f37, %f356, %f357;
-	mov.b32 	 %f866, %r73;
-	mov.b32 	 %f867, %r74;
-	mov.b32 	 %f868, %r75;
-	mov.b32 	 %f869, %r76;
-	add.s64 	%rd77, %rd68, 48;
-	// inline asm
-	cvta.to.global.u64 %rd76, %rd77;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r77,%r78,%r79,%r80}, [%rd76];
-	// inline asm
-	mov.b32 	 %f870, %r77;
-	mov.b32 	 %f871, %r78;
-	mov.b32 	 %f872, %r79;
-	mov.b32 	 %f873, %r80;
-	setp.leu.f32	%p10, %f37, 0f00000000;
-	@%p10 bra 	BB0_12;
-
-	cvt.rmi.f32.f32	%f844, %f356;
-	cvt.rzi.s32.f32	%r316, %f844;
-	cvt.s64.s32	%rd262, %r316;
-	shl.b64 	%rd92, %rd262, 6;
-	add.s64 	%rd93, %rd92, %rd35;
-	add.s64 	%rd81, %rd93, 96;
-	// inline asm
-	cvta.to.global.u64 %rd80, %rd81;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r84,%r85,%r86,%r87}, [%rd80];
-	// inline asm
-	mov.b32 	 %f358, %r84;
-	mov.b32 	 %f359, %r85;
-	mov.b32 	 %f360, %r86;
-	mov.b32 	 %f361, %r87;
-	add.s64 	%rd84, %rd93, 112;
-	// inline asm
-	cvta.to.global.u64 %rd83, %rd84;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r88,%r89,%r90,%r91}, [%rd83];
-	// inline asm
-	mov.b32 	 %f362, %r88;
-	mov.b32 	 %f363, %r89;
-	mov.b32 	 %f364, %r90;
-	mov.b32 	 %f365, %r91;
-	add.s64 	%rd87, %rd93, 128;
-	// inline asm
-	cvta.to.global.u64 %rd86, %rd87;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r92,%r93,%r94,%r95}, [%rd86];
-	// inline asm
-	mov.b32 	 %f366, %r92;
-	mov.b32 	 %f367, %r93;
-	mov.b32 	 %f368, %r94;
-	mov.b32 	 %f369, %r95;
-	add.s64 	%rd90, %rd93, 144;
-	// inline asm
-	cvta.to.global.u64 %rd89, %rd90;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r96,%r97,%r98,%r99}, [%rd89];
-	// inline asm
-	mov.f32 	%f370, 0f3F800000;
-	sub.f32 	%f371, %f370, %f37;
-	mul.f32 	%f372, %f37, %f358;
-	mul.f32 	%f373, %f37, %f359;
-	mul.f32 	%f374, %f37, %f360;
-	mul.f32 	%f375, %f37, %f361;
-	fma.rn.f32 	%f858, %f371, %f858, %f372;
-	fma.rn.f32 	%f859, %f371, %f859, %f373;
-	fma.rn.f32 	%f860, %f371, %f860, %f374;
-	fma.rn.f32 	%f861, %f371, %f861, %f375;
-	mul.f32 	%f376, %f37, %f362;
-	mul.f32 	%f377, %f37, %f363;
-	mul.f32 	%f378, %f37, %f364;
-	mul.f32 	%f379, %f37, %f365;
-	fma.rn.f32 	%f862, %f371, %f862, %f376;
-	fma.rn.f32 	%f863, %f371, %f863, %f377;
-	fma.rn.f32 	%f864, %f371, %f864, %f378;
-	fma.rn.f32 	%f865, %f371, %f865, %f379;
-	mul.f32 	%f380, %f37, %f366;
-	mul.f32 	%f381, %f37, %f367;
-	mul.f32 	%f382, %f37, %f368;
-	mul.f32 	%f383, %f37, %f369;
-	fma.rn.f32 	%f866, %f371, %f866, %f380;
-	fma.rn.f32 	%f384, %f371, %f867, %f381;
-	fma.rn.f32 	%f385, %f371, %f868, %f382;
-	fma.rn.f32 	%f386, %f371, %f869, %f383;
-	mov.b32 	 %f387, %r96;
-	mov.b32 	 %f388, %r97;
-	mov.b32 	 %f389, %r98;
-	mov.b32 	 %f390, %r99;
-	mul.f32 	%f391, %f37, %f387;
-	mul.f32 	%f392, %f37, %f388;
-	mul.f32 	%f393, %f37, %f389;
-	mul.f32 	%f394, %f37, %f390;
-	fma.rn.f32 	%f395, %f371, %f870, %f391;
-	fma.rn.f32 	%f871, %f371, %f871, %f392;
-	fma.rn.f32 	%f872, %f371, %f872, %f393;
-	fma.rn.f32 	%f873, %f371, %f873, %f394;
-	mul.f32 	%f396, %f385, %f385;
-	fma.rn.f32 	%f397, %f384, %f384, %f396;
-	fma.rn.f32 	%f398, %f386, %f386, %f397;
-	fma.rn.f32 	%f399, %f395, %f395, %f398;
-	sqrt.rn.f32 	%f400, %f399;
-	rcp.rn.f32 	%f401, %f400;
-	mul.f32 	%f867, %f384, %f401;
-	mul.f32 	%f868, %f385, %f401;
-	mul.f32 	%f869, %f386, %f401;
-	mul.f32 	%f870, %f395, %f401;
-
-BB0_12:
-	mul.f32 	%f402, %f868, %f868;
-	fma.rn.f32 	%f403, %f867, %f867, %f402;
-	fma.rn.f32 	%f404, %f869, %f869, %f403;
-	fma.rn.f32 	%f405, %f870, %f870, %f404;
-	rcp.rn.f32 	%f406, %f405;
-	mul.f32 	%f407, %f867, %f406;
-	mul.f32 	%f408, %f868, %f406;
-	mul.f32 	%f409, %f869, %f406;
-	mul.f32 	%f410, %f870, %f406;
-	mul.f32 	%f411, %f867, %f407;
-	mul.f32 	%f412, %f868, %f408;
-	mul.f32 	%f413, %f869, %f409;
-	mul.f32 	%f414, %f867, %f408;
-	mul.f32 	%f415, %f869, %f410;
-	mul.f32 	%f416, %f867, %f409;
-	mul.f32 	%f417, %f868, %f410;
-	mul.f32 	%f418, %f868, %f409;
-	mul.f32 	%f419, %f867, %f410;
-	sub.f32 	%f420, %f411, %f412;
-	sub.f32 	%f421, %f420, %f413;
-	fma.rn.f32 	%f422, %f870, %f410, %f421;
-	sub.f32 	%f423, %f414, %f415;
-	add.f32 	%f424, %f423, %f423;
-	add.f32 	%f425, %f416, %f417;
-	add.f32 	%f426, %f425, %f425;
-	add.f32 	%f427, %f414, %f415;
-	add.f32 	%f428, %f427, %f427;
-	sub.f32 	%f429, %f412, %f411;
-	sub.f32 	%f430, %f429, %f413;
-	fma.rn.f32 	%f431, %f870, %f410, %f430;
-	sub.f32 	%f432, %f418, %f419;
-	add.f32 	%f433, %f432, %f432;
-	sub.f32 	%f434, %f416, %f417;
-	add.f32 	%f435, %f434, %f434;
-	add.f32 	%f436, %f418, %f419;
-	add.f32 	%f437, %f436, %f436;
-	neg.f32 	%f438, %f411;
-	sub.f32 	%f439, %f438, %f412;
-	add.f32 	%f440, %f413, %f439;
-	fma.rn.f32 	%f441, %f870, %f410, %f440;
-	mul.f32 	%f442, %f861, %f422;
-	fma.rn.f32 	%f443, %f864, %f424, %f442;
-	fma.rn.f32 	%f444, %f866, %f426, %f443;
-	sub.f32 	%f885, %f871, %f444;
-	mul.f32 	%f445, %f864, %f431;
-	fma.rn.f32 	%f446, %f861, %f428, %f445;
-	fma.rn.f32 	%f447, %f866, %f433, %f446;
-	sub.f32 	%f881, %f872, %f447;
-	mul.f32 	%f448, %f864, %f437;
-	fma.rn.f32 	%f449, %f861, %f435, %f448;
-	fma.rn.f32 	%f450, %f866, %f441, %f449;
-	sub.f32 	%f877, %f873, %f450;
-	mul.f32 	%f451, %f860, %f422;
-	fma.rn.f32 	%f452, %f863, %f424, %f451;
-	fma.rn.f32 	%f884, %f865, %f426, %f452;
-	mul.f32 	%f453, %f863, %f431;
-	fma.rn.f32 	%f454, %f860, %f428, %f453;
-	fma.rn.f32 	%f880, %f865, %f433, %f454;
-	mul.f32 	%f455, %f863, %f437;
-	fma.rn.f32 	%f456, %f860, %f435, %f455;
-	fma.rn.f32 	%f876, %f865, %f441, %f456;
-	mul.f32 	%f457, %f859, %f422;
-	fma.rn.f32 	%f883, %f862, %f424, %f457;
-	mul.f32 	%f458, %f862, %f431;
-	fma.rn.f32 	%f879, %f859, %f428, %f458;
-	mul.f32 	%f459, %f862, %f437;
-	fma.rn.f32 	%f875, %f859, %f435, %f459;
-	mul.f32 	%f882, %f858, %f422;
-	mul.f32 	%f878, %f858, %f428;
-	mul.f32 	%f874, %f858, %f435;
-
-BB0_15:
-	mul.f32 	%f497, %f875, %f880;
-	mul.f32 	%f498, %f876, %f879;
-	sub.f32 	%f499, %f498, %f497;
-	mul.f32 	%f500, %f882, %f499;
-	mul.f32 	%f501, %f874, %f880;
-	mul.f32 	%f502, %f876, %f878;
-	sub.f32 	%f503, %f502, %f501;
-	mul.f32 	%f504, %f503, %f883;
-	sub.f32 	%f505, %f500, %f504;
-	mul.f32 	%f506, %f874, %f879;
-	mul.f32 	%f507, %f875, %f878;
-	sub.f32 	%f508, %f507, %f506;
-	fma.rn.f32 	%f509, %f508, %f884, %f505;
-	rcp.rn.f32 	%f510, %f509;
-	mul.f32 	%f894, %f499, %f510;
-	mul.f32 	%f511, %f876, %f883;
-	mul.f32 	%f512, %f875, %f884;
-	sub.f32 	%f513, %f512, %f511;
-	mul.f32 	%f895, %f510, %f513;
-	mul.f32 	%f514, %f879, %f884;
-	mul.f32 	%f515, %f880, %f883;
-	sub.f32 	%f516, %f515, %f514;
-	mul.f32 	%f896, %f510, %f516;
-	sub.f32 	%f517, %f501, %f502;
-	mul.f32 	%f890, %f517, %f510;
-	mul.f32 	%f518, %f874, %f884;
-	mul.f32 	%f519, %f876, %f882;
-	sub.f32 	%f520, %f519, %f518;
-	mul.f32 	%f891, %f510, %f520;
-	mul.f32 	%f521, %f880, %f882;
-	mul.f32 	%f522, %f878, %f884;
-	sub.f32 	%f523, %f522, %f521;
-	mul.f32 	%f892, %f510, %f523;
-	mul.f32 	%f886, %f508, %f510;
-	mul.f32 	%f524, %f875, %f882;
-	mul.f32 	%f525, %f874, %f883;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r149,%r150,%r151,%r152}, [%rd126];
+	// end inline asm
+	mov.b32 	%f500, %r149;
+	mov.b32 	%f501, %r150;
+	mov.b32 	%f502, %r151;
+	mov.b32 	%f503, %r152;
+	mul.f32 	%f504, %f90, %f500;
+	mul.f32 	%f505, %f90, %f501;
+	mul.f32 	%f506, %f90, %f502;
+	mul.f32 	%f507, %f90, %f503;
+	fma.rn.f32 	%f900, %f499, %f900, %f504;
+	fma.rn.f32 	%f899, %f499, %f899, %f505;
+	fma.rn.f32 	%f898, %f499, %f898, %f506;
+	fma.rn.f32 	%f897, %f499, %f897, %f507;
+	add.s64 	%rd130, %rd136, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd129, %rd130;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r153,%r154,%r155,%r156}, [%rd129];
+	// end inline asm
+	mov.b32 	%f508, %r153;
+	mov.b32 	%f509, %r154;
+	mov.b32 	%f510, %r155;
+	mov.b32 	%f511, %r156;
+	mul.f32 	%f512, %f90, %f508;
+	mul.f32 	%f513, %f90, %f509;
+	mul.f32 	%f514, %f90, %f510;
+	mul.f32 	%f515, %f90, %f511;
+	fma.rn.f32 	%f904, %f499, %f904, %f512;
+	fma.rn.f32 	%f903, %f499, %f903, %f513;
+	fma.rn.f32 	%f902, %f499, %f902, %f514;
+	fma.rn.f32 	%f901, %f499, %f901, %f515;
+	add.s64 	%rd133, %rd136, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd132, %rd133;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r157,%r158,%r159,%r160}, [%rd132];
+	// end inline asm
+	mov.b32 	%f516, %r157;
+	mov.b32 	%f517, %r158;
+	mov.b32 	%f518, %r159;
+	mov.b32 	%f519, %r160;
+	mul.f32 	%f520, %f90, %f516;
+	mul.f32 	%f521, %f90, %f517;
+	mul.f32 	%f522, %f90, %f518;
+	mul.f32 	%f523, %f90, %f519;
+	fma.rn.f32 	%f908, %f499, %f908, %f520;
+	fma.rn.f32 	%f907, %f499, %f907, %f521;
+	fma.rn.f32 	%f906, %f499, %f906, %f522;
+	fma.rn.f32 	%f905, %f499, %f905, %f523;
+	bra.uni 	$L__BB0_15;
+
+$L__BB0_4:
+	mov.f32 	%f909, 0f00000000;
+	mov.f32 	%f912, 0f3F800000;
+	setp.eq.s32 	%p5, %r13, 4;
+	@%p5 bra 	$L__BB0_7;
+
+	setp.ne.s32 	%p6, %r13, 1;
+	mov.f32 	%f910, %f909;
+	mov.f32 	%f911, %f909;
+	mov.f32 	%f913, %f909;
+	mov.f32 	%f914, %f909;
+	mov.f32 	%f915, %f912;
+	mov.f32 	%f916, %f909;
+	mov.f32 	%f917, %f909;
+	mov.f32 	%f918, %f912;
+	mov.f32 	%f919, %f909;
+	mov.f32 	%f920, %f909;
+	@%p6 bra 	$L__BB0_16;
+
+	// begin inline asm
+	call (%rd20), _optix_get_static_transform_from_handle, (%rd18);
+	// end inline asm
+	add.s64 	%rd257, %rd20, 64;
+	bra.uni 	$L__BB0_8;
+
+$L__BB0_10:
+	// begin inline asm
+	call (%rd33), _optix_get_srt_motion_transform_from_handle, (%rd18);
+	// end inline asm
+	// begin inline asm
+	cvta.to.global.u64 %rd35, %rd33;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r27,%r28,%r29,%r30}, [%rd35];
+	// end inline asm
+	add.s64 	%rd39, %rd33, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd38, %rd39;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r31,%r32,%r33,%r34}, [%rd38];
+	// end inline asm
+	add.s64 	%rd42, %rd33, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd41, %rd42;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r35,%r36,%r37,%r38}, [%rd41];
+	// end inline asm
+	add.s64 	%rd45, %rd33, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd44, %rd45;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r39,%r40,%r41,%r42}, [%rd44];
+	// end inline asm
+	add.s64 	%rd48, %rd33, 64;
+	// begin inline asm
+	cvta.to.global.u64 %rd47, %rd48;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r43,%r44,%r45,%r46}, [%rd47];
+	// end inline asm
+	add.s64 	%rd51, %rd33, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd50, %rd51;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r47,%r48,%r49,%r50}, [%rd50];
+	// end inline asm
+	add.s64 	%rd54, %rd33, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd53, %rd54;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r51,%r52,%r53,%r54}, [%rd53];
+	// end inline asm
+	add.s64 	%rd57, %rd33, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd56, %rd57;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r55,%r56,%r57,%r58}, [%rd56];
+	// end inline asm
+	add.s64 	%rd60, %rd33, 128;
+	// begin inline asm
+	cvta.to.global.u64 %rd59, %rd60;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r59,%r60,%r61,%r62}, [%rd59];
+	// end inline asm
+	add.s64 	%rd63, %rd33, 144;
+	// begin inline asm
+	cvta.to.global.u64 %rd62, %rd63;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r63,%r64,%r65,%r66}, [%rd62];
+	// end inline asm
+	mov.b32 	%f374, %r30;
+	mov.b32 	%f375, %r31;
+	and.b32  	%r83, %r29, 65535;
+	add.s32 	%r84, %r83, -1;
+	cvt.rn.f32.s32 	%f376, %r84;
+	sub.f32 	%f377, %f359, %f374;
+	mul.f32 	%f378, %f377, %f376;
+	sub.f32 	%f379, %f375, %f374;
+	div.rn.f32 	%f380, %f378, %f379;
+	min.f32 	%f381, %f376, %f380;
+	mov.f32 	%f382, 0f00000000;
+	max.f32 	%f383, %f382, %f381;
+	cvt.rmi.f32.f32 	%f384, %f383;
+	sub.f32 	%f29, %f383, %f384;
+	cvt.rzi.s32.f32 	%r85, %f384;
+	mul.wide.s32 	%rd77, %r85, 64;
+	add.s64 	%rd66, %rd42, %rd77;
+	// begin inline asm
+	cvta.to.global.u64 %rd65, %rd66;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r67,%r68,%r69,%r70}, [%rd65];
+	// end inline asm
+	mov.b32 	%f881, %r67;
+	mov.b32 	%f882, %r68;
+	mov.b32 	%f883, %r69;
+	mov.b32 	%f884, %r70;
+	add.s64 	%rd69, %rd66, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd68, %rd69;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r71,%r72,%r73,%r74}, [%rd68];
+	// end inline asm
+	mov.b32 	%f885, %r71;
+	mov.b32 	%f886, %r72;
+	mov.b32 	%f887, %r73;
+	mov.b32 	%f888, %r74;
+	add.s64 	%rd72, %rd66, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd71, %rd72;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r75,%r76,%r77,%r78}, [%rd71];
+	// end inline asm
+	mov.b32 	%f889, %r75;
+	mov.b32 	%f890, %r76;
+	mov.b32 	%f891, %r77;
+	mov.b32 	%f892, %r78;
+	add.s64 	%rd75, %rd66, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd74, %rd75;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r79,%r80,%r81,%r82}, [%rd74];
+	// end inline asm
+	mov.b32 	%f893, %r79;
+	mov.b32 	%f894, %r80;
+	mov.b32 	%f895, %r81;
+	mov.b32 	%f896, %r82;
+	setp.leu.f32 	%p8, %f29, 0f00000000;
+	@%p8 bra 	$L__BB0_12;
+
+	mov.f32 	%f385, 0f3F800000;
+	sub.f32 	%f386, %f385, %f29;
+	add.s64 	%rd79, %rd66, 64;
+	// begin inline asm
+	cvta.to.global.u64 %rd78, %rd79;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r86,%r87,%r88,%r89}, [%rd78];
+	// end inline asm
+	mov.b32 	%f387, %r86;
+	mov.b32 	%f388, %r87;
+	mov.b32 	%f389, %r88;
+	mov.b32 	%f390, %r89;
+	mul.f32 	%f391, %f29, %f387;
+	mul.f32 	%f392, %f29, %f388;
+	mul.f32 	%f393, %f29, %f389;
+	mul.f32 	%f394, %f29, %f390;
+	fma.rn.f32 	%f881, %f386, %f881, %f391;
+	fma.rn.f32 	%f882, %f386, %f882, %f392;
+	fma.rn.f32 	%f883, %f386, %f883, %f393;
+	fma.rn.f32 	%f884, %f386, %f884, %f394;
+	add.s64 	%rd82, %rd66, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd81, %rd82;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r90,%r91,%r92,%r93}, [%rd81];
+	// end inline asm
+	mov.b32 	%f395, %r90;
+	mov.b32 	%f396, %r91;
+	mov.b32 	%f397, %r92;
+	mov.b32 	%f398, %r93;
+	mul.f32 	%f399, %f29, %f395;
+	mul.f32 	%f400, %f29, %f396;
+	mul.f32 	%f401, %f29, %f397;
+	mul.f32 	%f402, %f29, %f398;
+	fma.rn.f32 	%f885, %f386, %f885, %f399;
+	fma.rn.f32 	%f886, %f386, %f886, %f400;
+	fma.rn.f32 	%f887, %f386, %f887, %f401;
+	fma.rn.f32 	%f888, %f386, %f888, %f402;
+	add.s64 	%rd85, %rd66, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd84, %rd85;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r94,%r95,%r96,%r97}, [%rd84];
+	// end inline asm
+	mov.b32 	%f403, %r94;
+	mov.b32 	%f404, %r95;
+	mov.b32 	%f405, %r96;
+	mov.b32 	%f406, %r97;
+	mul.f32 	%f407, %f29, %f403;
+	mul.f32 	%f408, %f29, %f404;
+	mul.f32 	%f409, %f29, %f405;
+	mul.f32 	%f410, %f29, %f406;
+	fma.rn.f32 	%f889, %f386, %f889, %f407;
+	fma.rn.f32 	%f411, %f386, %f890, %f408;
+	fma.rn.f32 	%f412, %f386, %f891, %f409;
+	fma.rn.f32 	%f413, %f386, %f892, %f410;
+	add.s64 	%rd88, %rd66, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd87, %rd88;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r98,%r99,%r100,%r101}, [%rd87];
+	// end inline asm
+	mov.b32 	%f414, %r98;
+	mov.b32 	%f415, %r99;
+	mov.b32 	%f416, %r100;
+	mov.b32 	%f417, %r101;
+	mul.f32 	%f418, %f29, %f414;
+	mul.f32 	%f419, %f29, %f415;
+	mul.f32 	%f420, %f29, %f416;
+	mul.f32 	%f421, %f29, %f417;
+	fma.rn.f32 	%f422, %f386, %f893, %f418;
+	fma.rn.f32 	%f894, %f386, %f894, %f419;
+	fma.rn.f32 	%f895, %f386, %f895, %f420;
+	fma.rn.f32 	%f896, %f386, %f896, %f421;
+	mul.f32 	%f423, %f412, %f412;
+	fma.rn.f32 	%f424, %f411, %f411, %f423;
+	fma.rn.f32 	%f425, %f413, %f413, %f424;
+	fma.rn.f32 	%f426, %f422, %f422, %f425;
+	sqrt.rn.f32 	%f427, %f426;
+	rcp.rn.f32 	%f428, %f427;
+	mul.f32 	%f890, %f411, %f428;
+	mul.f32 	%f891, %f412, %f428;
+	mul.f32 	%f892, %f413, %f428;
+	mul.f32 	%f893, %f428, %f422;
+
+$L__BB0_12:
+	mul.f32 	%f429, %f891, %f891;
+	fma.rn.f32 	%f430, %f890, %f890, %f429;
+	fma.rn.f32 	%f431, %f892, %f892, %f430;
+	fma.rn.f32 	%f432, %f893, %f893, %f431;
+	rcp.rn.f32 	%f433, %f432;
+	mul.f32 	%f434, %f890, %f433;
+	mul.f32 	%f435, %f891, %f433;
+	mul.f32 	%f436, %f892, %f433;
+	mul.f32 	%f437, %f893, %f433;
+	mul.f32 	%f438, %f890, %f434;
+	mul.f32 	%f439, %f891, %f435;
+	mul.f32 	%f440, %f892, %f436;
+	mul.f32 	%f441, %f890, %f435;
+	mul.f32 	%f442, %f892, %f437;
+	mul.f32 	%f443, %f890, %f436;
+	mul.f32 	%f444, %f891, %f437;
+	mul.f32 	%f445, %f891, %f436;
+	mul.f32 	%f446, %f890, %f437;
+	sub.f32 	%f447, %f438, %f439;
+	sub.f32 	%f448, %f447, %f440;
+	fma.rn.f32 	%f449, %f893, %f437, %f448;
+	sub.f32 	%f450, %f441, %f442;
+	add.f32 	%f451, %f450, %f450;
+	add.f32 	%f452, %f443, %f444;
+	add.f32 	%f453, %f452, %f452;
+	add.f32 	%f454, %f441, %f442;
+	add.f32 	%f455, %f454, %f454;
+	sub.f32 	%f456, %f439, %f438;
+	sub.f32 	%f457, %f456, %f440;
+	fma.rn.f32 	%f458, %f893, %f437, %f457;
+	sub.f32 	%f459, %f445, %f446;
+	add.f32 	%f460, %f459, %f459;
+	sub.f32 	%f461, %f443, %f444;
+	add.f32 	%f462, %f461, %f461;
+	add.f32 	%f463, %f445, %f446;
+	add.f32 	%f464, %f463, %f463;
+	neg.f32 	%f465, %f438;
+	sub.f32 	%f466, %f465, %f439;
+	add.f32 	%f467, %f440, %f466;
+	fma.rn.f32 	%f468, %f893, %f437, %f467;
+	mul.f32 	%f469, %f884, %f449;
+	fma.rn.f32 	%f470, %f887, %f451, %f469;
+	fma.rn.f32 	%f471, %f889, %f453, %f470;
+	sub.f32 	%f897, %f894, %f471;
+	mul.f32 	%f472, %f887, %f458;
+	fma.rn.f32 	%f473, %f884, %f455, %f472;
+	fma.rn.f32 	%f474, %f889, %f460, %f473;
+	sub.f32 	%f901, %f895, %f474;
+	mul.f32 	%f475, %f887, %f464;
+	fma.rn.f32 	%f476, %f884, %f462, %f475;
+	fma.rn.f32 	%f477, %f889, %f468, %f476;
+	sub.f32 	%f905, %f896, %f477;
+	mul.f32 	%f478, %f883, %f449;
+	fma.rn.f32 	%f479, %f886, %f451, %f478;
+	fma.rn.f32 	%f898, %f888, %f453, %f479;
+	mul.f32 	%f480, %f886, %f458;
+	fma.rn.f32 	%f481, %f883, %f455, %f480;
+	fma.rn.f32 	%f902, %f888, %f460, %f481;
+	mul.f32 	%f482, %f886, %f464;
+	fma.rn.f32 	%f483, %f883, %f462, %f482;
+	fma.rn.f32 	%f906, %f888, %f468, %f483;
+	mul.f32 	%f484, %f882, %f449;
+	fma.rn.f32 	%f899, %f885, %f451, %f484;
+	mul.f32 	%f485, %f885, %f458;
+	fma.rn.f32 	%f903, %f882, %f455, %f485;
+	mul.f32 	%f486, %f885, %f464;
+	fma.rn.f32 	%f907, %f882, %f462, %f486;
+	mul.f32 	%f900, %f881, %f449;
+	mul.f32 	%f904, %f881, %f455;
+	mul.f32 	%f908, %f881, %f462;
+
+$L__BB0_15:
+	mul.f32 	%f524, %f902, %f907;
+	mul.f32 	%f525, %f903, %f906;
 	sub.f32 	%f526, %f525, %f524;
-	mul.f32 	%f887, %f526, %f510;
-	mul.f32 	%f527, %f878, %f883;
-	mul.f32 	%f528, %f879, %f882;
-	sub.f32 	%f529, %f528, %f527;
-	mul.f32 	%f888, %f529, %f510;
-	mul.f32 	%f530, %f885, %f894;
-	neg.f32 	%f531, %f530;
-	mul.f32 	%f532, %f881, %f895;
-	sub.f32 	%f533, %f531, %f532;
-	mul.f32 	%f534, %f877, %f896;
-	sub.f32 	%f897, %f533, %f534;
-	mul.f32 	%f535, %f885, %f890;
-	neg.f32 	%f536, %f535;
-	mul.f32 	%f537, %f881, %f891;
-	sub.f32 	%f538, %f536, %f537;
-	mul.f32 	%f539, %f877, %f892;
-	sub.f32 	%f893, %f538, %f539;
-	mul.f32 	%f540, %f885, %f886;
-	neg.f32 	%f541, %f540;
-	mul.f32 	%f542, %f881, %f887;
-	sub.f32 	%f543, %f541, %f542;
-	mul.f32 	%f544, %f877, %f888;
-	sub.f32 	%f889, %f543, %f544;
-	bra.uni 	BB0_16;
-
-BB0_5:
-	setp.ne.s32	%p8, %r11, 1;
-	mov.f32 	%f887, %f886;
-	mov.f32 	%f889, %f886;
-	mov.f32 	%f890, %f886;
-	mov.f32 	%f891, %f888;
-	mov.f32 	%f892, %f886;
-	mov.f32 	%f893, %f886;
-	mov.f32 	%f894, %f888;
-	mov.f32 	%f895, %f886;
-	mov.f32 	%f896, %f886;
-	mov.f32 	%f897, %f886;
-	@%p8 bra 	BB0_16;
-
-	// inline asm
-	call (%rd22), _optix_get_static_transform_from_handle, (%rd20);
-	// inline asm
-	add.s64 	%rd264, %rd22, 64;
-
-BB0_8:
-	// inline asm
-	cvta.to.global.u64 %rd26, %rd264;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r13,%r14,%r15,%r16}, [%rd26];
-	// inline asm
-	mov.b32 	 %f894, %r13;
-	mov.b32 	 %f895, %r14;
-	mov.b32 	 %f896, %r15;
-	mov.b32 	 %f897, %r16;
-	add.s64 	%rd30, %rd264, 16;
-	// inline asm
-	cvta.to.global.u64 %rd29, %rd30;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r17,%r18,%r19,%r20}, [%rd29];
-	// inline asm
-	mov.b32 	 %f890, %r17;
-	mov.b32 	 %f891, %r18;
-	mov.b32 	 %f892, %r19;
-	mov.b32 	 %f893, %r20;
-	add.s64 	%rd33, %rd264, 32;
-	// inline asm
-	cvta.to.global.u64 %rd32, %rd33;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r21,%r22,%r23,%r24}, [%rd32];
-	// inline asm
-	mov.b32 	 %f886, %r21;
-	mov.b32 	 %f887, %r22;
-	mov.b32 	 %f888, %r23;
-	mov.b32 	 %f889, %r24;
-
-BB0_16:
-	setp.eq.s32	%p12, %r318, 0;
-	@%p12 bra 	BB0_17;
-	bra.uni 	BB0_18;
-
-BB0_17:
-	mov.f32 	%f857, %f897;
-	mov.f32 	%f856, %f896;
-	mov.f32 	%f855, %f895;
-	mov.f32 	%f854, %f894;
-	mov.f32 	%f853, %f893;
-	mov.f32 	%f852, %f892;
-	mov.f32 	%f851, %f891;
-	mov.f32 	%f850, %f890;
-	mov.f32 	%f849, %f889;
-	mov.f32 	%f848, %f888;
-	mov.f32 	%f847, %f887;
-	mov.f32 	%f846, %f886;
-	bra.uni 	BB0_19;
-
-BB0_18:
-	mul.f32 	%f545, %f850, %f895;
-	fma.rn.f32 	%f546, %f854, %f894, %f545;
-	fma.rn.f32 	%f151, %f846, %f896, %f546;
-	mul.f32 	%f547, %f851, %f895;
-	fma.rn.f32 	%f548, %f855, %f894, %f547;
-	fma.rn.f32 	%f152, %f847, %f896, %f548;
-	mul.f32 	%f549, %f852, %f895;
-	fma.rn.f32 	%f550, %f856, %f894, %f549;
-	fma.rn.f32 	%f153, %f848, %f896, %f550;
-	mul.f32 	%f551, %f853, %f895;
-	fma.rn.f32 	%f552, %f857, %f894, %f551;
-	fma.rn.f32 	%f553, %f849, %f896, %f552;
-	add.f32 	%f154, %f897, %f553;
-	mul.f32 	%f554, %f850, %f891;
-	fma.rn.f32 	%f555, %f854, %f890, %f554;
-	fma.rn.f32 	%f155, %f846, %f892, %f555;
-	mul.f32 	%f556, %f851, %f891;
-	fma.rn.f32 	%f557, %f855, %f890, %f556;
-	fma.rn.f32 	%f156, %f847, %f892, %f557;
-	mul.f32 	%f558, %f852, %f891;
-	fma.rn.f32 	%f559, %f856, %f890, %f558;
-	fma.rn.f32 	%f157, %f848, %f892, %f559;
-	mul.f32 	%f560, %f853, %f891;
-	fma.rn.f32 	%f561, %f857, %f890, %f560;
-	fma.rn.f32 	%f562, %f849, %f892, %f561;
-	add.f32 	%f158, %f893, %f562;
-	mul.f32 	%f563, %f850, %f887;
-	fma.rn.f32 	%f564, %f854, %f886, %f563;
-	fma.rn.f32 	%f846, %f846, %f888, %f564;
-	mul.f32 	%f565, %f851, %f887;
-	fma.rn.f32 	%f566, %f855, %f886, %f565;
-	fma.rn.f32 	%f847, %f847, %f888, %f566;
-	mul.f32 	%f567, %f852, %f887;
-	fma.rn.f32 	%f568, %f856, %f886, %f567;
-	fma.rn.f32 	%f848, %f848, %f888, %f568;
-	mul.f32 	%f569, %f853, %f887;
-	fma.rn.f32 	%f570, %f857, %f886, %f569;
-	fma.rn.f32 	%f571, %f849, %f888, %f570;
-	add.f32 	%f849, %f889, %f571;
-	mov.f32 	%f857, %f154;
-	mov.f32 	%f856, %f153;
-	mov.f32 	%f855, %f152;
-	mov.f32 	%f854, %f151;
-	mov.f32 	%f853, %f158;
-	mov.f32 	%f852, %f157;
-	mov.f32 	%f851, %f156;
-	mov.f32 	%f850, %f155;
-
-BB0_19:
-	add.s32 	%r318, %r318, 1;
-	setp.lt.u32	%p13, %r318, %r8;
-	@%p13 bra 	BB0_3;
-
-	mul.f32 	%f572, %f330, %f854;
-	fma.rn.f32 	%f573, %f331, %f855, %f572;
-	fma.rn.f32 	%f574, %f910, %f856, %f573;
-	add.f32 	%f912, %f857, %f574;
-	mul.f32 	%f575, %f330, %f850;
-	fma.rn.f32 	%f576, %f331, %f851, %f575;
-	fma.rn.f32 	%f577, %f910, %f852, %f576;
-	add.f32 	%f911, %f853, %f577;
-	mul.f32 	%f578, %f330, %f846;
-	fma.rn.f32 	%f579, %f331, %f847, %f578;
-	fma.rn.f32 	%f580, %f910, %f848, %f579;
-	add.f32 	%f910, %f849, %f580;
-	bra.uni 	BB0_21;
-
-BB0_1:
-	mov.f32 	%f911, %f331;
-	mov.f32 	%f912, %f330;
-
-BB0_21:
-	setp.eq.s32	%p51, %r8, 0;
-	// inline asm
-	call (%f581), _optix_get_world_ray_direction_x, ();
-	// inline asm
-	// inline asm
-	call (%f582), _optix_get_world_ray_direction_y, ();
-	// inline asm
-	// inline asm
-	call (%f961), _optix_get_world_ray_direction_z, ();
-	// inline asm
-	// inline asm
-	call (%f584), _optix_get_ray_time, ();
-	// inline asm
-	mov.u32 	%r319, 0;
-	@%p51 bra 	BB0_22;
-
-BB0_23:
+	mul.f32 	%f527, %f900, %f526;
+	mul.f32 	%f528, %f902, %f908;
+	mul.f32 	%f529, %f904, %f906;
+	sub.f32 	%f530, %f529, %f528;
+	mul.f32 	%f531, %f899, %f530;
+	sub.f32 	%f532, %f527, %f531;
+	mul.f32 	%f533, %f903, %f908;
+	mul.f32 	%f534, %f904, %f907;
+	sub.f32 	%f535, %f534, %f533;
+	fma.rn.f32 	%f536, %f898, %f535, %f532;
+	rcp.rn.f32 	%f537, %f536;
+	mul.f32 	%f912, %f526, %f537;
+	mul.f32 	%f538, %f899, %f906;
+	mul.f32 	%f539, %f898, %f907;
+	sub.f32 	%f540, %f539, %f538;
+	mul.f32 	%f911, %f540, %f537;
+	mul.f32 	%f541, %f898, %f903;
+	mul.f32 	%f542, %f899, %f902;
+	sub.f32 	%f543, %f542, %f541;
+	mul.f32 	%f910, %f543, %f537;
+	sub.f32 	%f544, %f528, %f529;
+	mul.f32 	%f916, %f544, %f537;
+	mul.f32 	%f545, %f898, %f908;
+	mul.f32 	%f546, %f900, %f906;
+	sub.f32 	%f547, %f546, %f545;
+	mul.f32 	%f915, %f547, %f537;
+	mul.f32 	%f548, %f900, %f902;
+	mul.f32 	%f549, %f898, %f904;
+	sub.f32 	%f550, %f549, %f548;
+	mul.f32 	%f914, %f550, %f537;
+	mul.f32 	%f920, %f535, %f537;
+	mul.f32 	%f551, %f900, %f907;
+	mul.f32 	%f552, %f899, %f908;
+	sub.f32 	%f553, %f552, %f551;
+	mul.f32 	%f919, %f553, %f537;
+	mul.f32 	%f554, %f899, %f904;
+	mul.f32 	%f555, %f900, %f903;
+	sub.f32 	%f556, %f555, %f554;
+	mul.f32 	%f918, %f556, %f537;
+	mul.f32 	%f557, %f897, %f912;
+	neg.f32 	%f558, %f557;
+	mul.f32 	%f559, %f901, %f911;
+	sub.f32 	%f560, %f558, %f559;
+	mul.f32 	%f561, %f905, %f910;
+	sub.f32 	%f909, %f560, %f561;
+	mul.f32 	%f562, %f897, %f916;
+	neg.f32 	%f563, %f562;
+	mul.f32 	%f564, %f901, %f915;
+	sub.f32 	%f565, %f563, %f564;
+	mul.f32 	%f566, %f905, %f914;
+	sub.f32 	%f913, %f565, %f566;
+	mul.f32 	%f567, %f897, %f920;
+	neg.f32 	%f568, %f567;
+	mul.f32 	%f569, %f901, %f919;
+	sub.f32 	%f570, %f568, %f569;
+	mul.f32 	%f571, %f905, %f918;
+	sub.f32 	%f917, %f570, %f571;
+	bra.uni 	$L__BB0_16;
+
+$L__BB0_7:
+	// begin inline asm
+	call (%rd257), _optix_get_instance_inverse_transform_from_handle, (%rd18);
+	// end inline asm
+
+$L__BB0_8:
+	// begin inline asm
+	cvta.to.global.u64 %rd24, %rd257;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r15,%r16,%r17,%r18}, [%rd24];
+	// end inline asm
+	mov.b32 	%f912, %r15;
+	mov.b32 	%f911, %r16;
+	mov.b32 	%f910, %r17;
+	mov.b32 	%f909, %r18;
+	add.s64 	%rd28, %rd257, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd27, %rd28;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r19,%r20,%r21,%r22}, [%rd27];
+	// end inline asm
+	mov.b32 	%f916, %r19;
+	mov.b32 	%f915, %r20;
+	mov.b32 	%f914, %r21;
+	mov.b32 	%f913, %r22;
+	add.s64 	%rd31, %rd257, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd30, %rd31;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r23,%r24,%r25,%r26}, [%rd30];
+	// end inline asm
+	mov.b32 	%f920, %r23;
+	mov.b32 	%f919, %r24;
+	mov.b32 	%f918, %r25;
+	mov.b32 	%f917, %r26;
+
+$L__BB0_16:
+	setp.eq.s32 	%p10, %r321, 0;
+	@%p10 bra 	$L__BB0_18;
+
+	mul.f32 	%f572, %f877, %f912;
+	fma.rn.f32 	%f573, %f873, %f911, %f572;
+	fma.rn.f32 	%f151, %f869, %f910, %f573;
+	mul.f32 	%f574, %f878, %f912;
+	fma.rn.f32 	%f575, %f874, %f911, %f574;
+	fma.rn.f32 	%f152, %f870, %f910, %f575;
+	mul.f32 	%f576, %f879, %f912;
+	fma.rn.f32 	%f577, %f875, %f911, %f576;
+	fma.rn.f32 	%f153, %f871, %f910, %f577;
+	mul.f32 	%f578, %f880, %f912;
+	fma.rn.f32 	%f579, %f876, %f911, %f578;
+	fma.rn.f32 	%f580, %f872, %f910, %f579;
+	add.f32 	%f909, %f909, %f580;
+	mul.f32 	%f581, %f877, %f916;
+	fma.rn.f32 	%f582, %f873, %f915, %f581;
+	fma.rn.f32 	%f155, %f869, %f914, %f582;
+	mul.f32 	%f583, %f878, %f916;
+	fma.rn.f32 	%f584, %f874, %f915, %f583;
+	fma.rn.f32 	%f156, %f870, %f914, %f584;
+	mul.f32 	%f585, %f879, %f916;
+	fma.rn.f32 	%f586, %f875, %f915, %f585;
+	fma.rn.f32 	%f157, %f871, %f914, %f586;
+	mul.f32 	%f587, %f880, %f916;
+	fma.rn.f32 	%f588, %f876, %f915, %f587;
+	fma.rn.f32 	%f589, %f872, %f914, %f588;
+	add.f32 	%f913, %f913, %f589;
+	mul.f32 	%f590, %f877, %f920;
+	fma.rn.f32 	%f591, %f873, %f919, %f590;
+	fma.rn.f32 	%f159, %f869, %f918, %f591;
+	mul.f32 	%f592, %f878, %f920;
+	fma.rn.f32 	%f593, %f874, %f919, %f592;
+	fma.rn.f32 	%f160, %f870, %f918, %f593;
+	mul.f32 	%f594, %f879, %f920;
+	fma.rn.f32 	%f595, %f875, %f919, %f594;
+	fma.rn.f32 	%f161, %f871, %f918, %f595;
+	mul.f32 	%f596, %f880, %f920;
+	fma.rn.f32 	%f597, %f876, %f919, %f596;
+	fma.rn.f32 	%f598, %f872, %f918, %f597;
+	add.f32 	%f917, %f917, %f598;
+	mov.f32 	%f910, %f153;
+	mov.f32 	%f911, %f152;
+	mov.f32 	%f912, %f151;
+	mov.f32 	%f914, %f157;
+	mov.f32 	%f915, %f156;
+	mov.f32 	%f916, %f155;
+	mov.f32 	%f918, %f161;
+	mov.f32 	%f919, %f160;
+	mov.f32 	%f920, %f159;
+
+$L__BB0_18:
+	add.s32 	%r321, %r321, 1;
+	setp.lt.u32 	%p11, %r321, %r10;
+	mov.f32 	%f869, %f920;
+	mov.f32 	%f870, %f919;
+	mov.f32 	%f871, %f918;
+	mov.f32 	%f872, %f917;
+	mov.f32 	%f873, %f916;
+	mov.f32 	%f874, %f915;
+	mov.f32 	%f875, %f914;
+	mov.f32 	%f876, %f913;
+	mov.f32 	%f877, %f912;
+	mov.f32 	%f878, %f911;
+	mov.f32 	%f879, %f910;
+	mov.f32 	%f880, %f909;
+	@%p11 bra 	$L__BB0_3;
+
+$L__BB0_19:
+	mul.f32 	%f599, %f945, %f912;
+	fma.rn.f32 	%f600, %f946, %f911, %f599;
+	fma.rn.f32 	%f601, %f947, %f910, %f600;
+	mul.f32 	%f602, %f945, %f916;
+	fma.rn.f32 	%f603, %f946, %f915, %f602;
+	fma.rn.f32 	%f604, %f947, %f914, %f603;
+	mul.f32 	%f605, %f945, %f920;
+	fma.rn.f32 	%f606, %f946, %f919, %f605;
+	fma.rn.f32 	%f607, %f947, %f918, %f606;
+	add.f32 	%f947, %f917, %f607;
+	add.f32 	%f946, %f913, %f604;
+	add.f32 	%f945, %f909, %f601;
+
+$L__BB0_21:
+	// begin inline asm
+	call (%f1003), _optix_get_world_ray_direction_x, ();
+	// end inline asm
+	// begin inline asm
+	call (%f1004), _optix_get_world_ray_direction_y, ();
+	// end inline asm
+	// begin inline asm
+	call (%f610), _optix_get_world_ray_direction_z, ();
+	// end inline asm
+	// begin inline asm
+	call (%r161), _optix_get_transform_list_size, ();
+	// end inline asm
+	setp.eq.s32 	%p12, %r161, 0;
+	@%p12 bra 	$L__BB0_41;
+
+	// begin inline asm
+	call (%r162), _optix_get_transform_list_size, ();
+	// end inline asm
+	// begin inline asm
+	call (%f611), _optix_get_ray_time, ();
+	// end inline asm
+	setp.eq.s32 	%p13, %r162, 0;
+	@%p13 bra 	$L__BB0_40;
+
+	mov.u32 	%r322, 0;
+
+$L__BB0_24:
 	.pragma "nounroll";
-	// inline asm
-	call (%rd141), _optix_get_transform_list_handle, (%r319);
-	// inline asm
-	// inline asm
-	call (%r161), _optix_get_transform_type_from_handle, (%rd141);
-	// inline asm
-	and.b32  	%r162, %r161, -2;
-	setp.eq.s32	%p15, %r162, 2;
-	@%p15 bra 	BB0_29;
-	bra.uni 	BB0_24;
-
-BB0_29:
-	setp.eq.s32	%p18, %r161, 2;
-	@%p18 bra 	BB0_33;
-	bra.uni 	BB0_30;
-
-BB0_33:
-	// inline asm
-	call (%rd215), _optix_get_matrix_motion_transform_from_handle, (%rd141);
-	// inline asm
-	// inline asm
-	cvta.to.global.u64 %rd217, %rd215;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r250,%r251,%r252,%r253}, [%rd217];
-	// inline asm
-	mov.b32	{%rs8, %rs9}, %r252;
-	add.s64 	%rd221, %rd215, 16;
-	// inline asm
+	// begin inline asm
+	call (%rd137), _optix_get_transform_list_handle, (%r322);
+	// end inline asm
+	// begin inline asm
+	call (%r165), _optix_get_transform_type_from_handle, (%rd137);
+	// end inline asm
+	or.b32  	%r166, %r165, 1;
+	setp.eq.s32 	%p14, %r166, 3;
+	@%p14 bra 	$L__BB0_30;
+	bra.uni 	$L__BB0_25;
+
+$L__BB0_30:
+	setp.eq.s32 	%p17, %r165, 2;
+	@%p17 bra 	$L__BB0_34;
+	bra.uni 	$L__BB0_31;
+
+$L__BB0_34:
+	// begin inline asm
+	call (%rd209), _optix_get_matrix_motion_transform_from_handle, (%rd137);
+	// end inline asm
+	// begin inline asm
+	cvta.to.global.u64 %rd211, %rd209;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r254,%r255,%r256,%r257}, [%rd211];
+	// end inline asm
+	add.s64 	%rd215, %rd209, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd214, %rd215;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r258,%r259,%r260,%r261}, [%rd214];
+	// end inline asm
+	add.s64 	%rd218, %rd209, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd217, %rd218;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r262,%r263,%r264,%r265}, [%rd217];
+	// end inline asm
+	add.s64 	%rd221, %rd209, 48;
+	// begin inline asm
 	cvta.to.global.u64 %rd220, %rd221;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r254,%r255,%r256,%r257}, [%rd220];
-	// inline asm
-	add.s64 	%rd224, %rd215, 32;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r266,%r267,%r268,%r269}, [%rd220];
+	// end inline asm
+	add.s64 	%rd224, %rd209, 64;
+	// begin inline asm
 	cvta.to.global.u64 %rd223, %rd224;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r258,%r259,%r260,%r261}, [%rd223];
-	// inline asm
-	add.s64 	%rd227, %rd215, 48;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r270,%r271,%r272,%r273}, [%rd223];
+	// end inline asm
+	add.s64 	%rd227, %rd209, 80;
+	// begin inline asm
 	cvta.to.global.u64 %rd226, %rd227;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r262,%r263,%r264,%r265}, [%rd226];
-	// inline asm
-	add.s64 	%rd230, %rd215, 64;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r274,%r275,%r276,%r277}, [%rd226];
+	// end inline asm
+	add.s64 	%rd230, %rd209, 96;
+	// begin inline asm
 	cvta.to.global.u64 %rd229, %rd230;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r266,%r267,%r268,%r269}, [%rd229];
-	// inline asm
-	add.s64 	%rd233, %rd215, 80;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r278,%r279,%r280,%r281}, [%rd229];
+	// end inline asm
+	add.s64 	%rd233, %rd209, 112;
+	// begin inline asm
 	cvta.to.global.u64 %rd232, %rd233;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r270,%r271,%r272,%r273}, [%rd232];
-	// inline asm
-	add.s64 	%rd236, %rd215, 96;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r282,%r283,%r284,%r285}, [%rd232];
+	// end inline asm
+	mov.b32 	%f715, %r257;
+	mov.b32 	%f716, %r258;
+	and.b32  	%r298, %r256, 65535;
+	add.s32 	%r299, %r298, -1;
+	cvt.rn.f32.s32 	%f717, %r299;
+	sub.f32 	%f718, %f611, %f715;
+	mul.f32 	%f719, %f718, %f717;
+	sub.f32 	%f720, %f716, %f715;
+	div.rn.f32 	%f721, %f719, %f720;
+	min.f32 	%f722, %f717, %f721;
+	mov.f32 	%f723, 0f00000000;
+	max.f32 	%f724, %f723, %f722;
+	cvt.rmi.f32.f32 	%f725, %f724;
+	sub.f32 	%f258, %f724, %f725;
+	cvt.rzi.s32.f32 	%r300, %f725;
+	cvt.s64.s32 	%rd15, %r300;
+	mul.wide.s32 	%rd244, %r300, 48;
+	add.s64 	%rd236, %rd218, %rd244;
+	// begin inline asm
 	cvta.to.global.u64 %rd235, %rd236;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r274,%r275,%r276,%r277}, [%rd235];
-	// inline asm
-	add.s64 	%rd239, %rd215, 112;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r286,%r287,%r288,%r289}, [%rd235];
+	// end inline asm
+	mov.b32 	%f973, %r286;
+	mov.b32 	%f974, %r287;
+	mov.b32 	%f975, %r288;
+	add.s64 	%rd239, %rd236, 16;
+	// begin inline asm
 	cvta.to.global.u64 %rd238, %rd239;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r278,%r279,%r280,%r281}, [%rd238];
-	// inline asm
-	mov.b32 	 %f687, %r253;
-	mov.b32 	 %f688, %r254;
-	cvt.u32.u16	%r294, %rs8;
-	add.s32 	%r295, %r294, -1;
-	cvt.rn.f32.s32	%f689, %r295;
-	sub.f32 	%f690, %f584, %f687;
-	mul.f32 	%f691, %f690, %f689;
-	sub.f32 	%f692, %f688, %f687;
-	div.rn.f32 	%f693, %f691, %f692;
-	min.f32 	%f694, %f689, %f693;
-	mov.f32 	%f695, 0f00000000;
-	max.f32 	%f696, %f695, %f694;
-	cvt.rmi.f32.f32	%f697, %f696;
-	cvt.rzi.s32.f32	%r296, %f697;
-	cvt.s64.s32	%rd17, %r296;
-	mul.wide.s32 	%rd250, %r296, 48;
-	add.s64 	%rd242, %rd224, %rd250;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r290,%r291,%r292,%r293}, [%rd238];
+	// end inline asm
+	mov.b32 	%f970, %r290;
+	mov.b32 	%f971, %r291;
+	mov.b32 	%f972, %r292;
+	add.s64 	%rd242, %rd236, 32;
+	// begin inline asm
 	cvta.to.global.u64 %rd241, %rd242;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r282,%r283,%r284,%r285}, [%rd241];
-	// inline asm
-	mov.b32 	 %f938, %r282;
-	mov.b32 	 %f939, %r283;
-	mov.b32 	 %f940, %r284;
-	add.s64 	%rd245, %rd242, 16;
-	// inline asm
-	cvta.to.global.u64 %rd244, %rd245;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r286,%r287,%r288,%r289}, [%rd244];
-	// inline asm
-	mov.b32 	 %f935, %r286;
-	mov.b32 	 %f936, %r287;
-	mov.b32 	 %f937, %r288;
-	add.s64 	%rd248, %rd242, 32;
-	// inline asm
-	cvta.to.global.u64 %rd247, %rd248;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r290,%r291,%r292,%r293}, [%rd247];
-	// inline asm
-	sub.f32 	%f249, %f696, %f697;
-	mov.b32 	 %f932, %r290;
-	mov.b32 	 %f933, %r291;
-	mov.b32 	 %f934, %r292;
-	setp.leu.f32	%p20, %f249, 0f00000000;
-	@%p20 bra 	BB0_35;
-
-	mul.lo.s64 	%rd260, %rd17, 48;
-	add.s64 	%rd261, %rd215, %rd260;
-	add.s64 	%rd252, %rd261, 80;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r294,%r295,%r296,%r297}, [%rd241];
+	// end inline asm
+	mov.b32 	%f967, %r294;
+	mov.b32 	%f968, %r295;
+	mov.b32 	%f969, %r296;
+	setp.leu.f32 	%p19, %f258, 0f00000000;
+	@%p19 bra 	$L__BB0_36;
+
+	mov.f32 	%f726, 0f3F800000;
+	sub.f32 	%f727, %f726, %f258;
+	mul.lo.s64 	%rd254, %rd15, 48;
+	add.s64 	%rd255, %rd209, %rd254;
+	add.s64 	%rd246, %rd255, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd245, %rd246;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r301,%r302,%r303,%r304}, [%rd245];
+	// end inline asm
+	mov.b32 	%f728, %r301;
+	mov.b32 	%f729, %r302;
+	mov.b32 	%f730, %r303;
+	mul.f32 	%f731, %f258, %f728;
+	mul.f32 	%f732, %f258, %f729;
+	mul.f32 	%f733, %f258, %f730;
+	fma.rn.f32 	%f973, %f727, %f973, %f731;
+	fma.rn.f32 	%f974, %f727, %f974, %f732;
+	fma.rn.f32 	%f975, %f727, %f975, %f733;
+	add.s64 	%rd249, %rd255, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd248, %rd249;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r305,%r306,%r307,%r308}, [%rd248];
+	// end inline asm
+	mov.b32 	%f734, %r305;
+	mov.b32 	%f735, %r306;
+	mov.b32 	%f736, %r307;
+	mul.f32 	%f737, %f258, %f734;
+	mul.f32 	%f738, %f258, %f735;
+	mul.f32 	%f739, %f258, %f736;
+	fma.rn.f32 	%f970, %f727, %f970, %f737;
+	fma.rn.f32 	%f971, %f727, %f971, %f738;
+	fma.rn.f32 	%f972, %f727, %f972, %f739;
+	add.s64 	%rd252, %rd255, 112;
+	// begin inline asm
 	cvta.to.global.u64 %rd251, %rd252;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r297,%r298,%r299,%r300}, [%rd251];
-	// inline asm
-	mov.b32 	 %f698, %r297;
-	mov.b32 	 %f699, %r298;
-	mov.b32 	 %f700, %r299;
-	add.s64 	%rd255, %rd261, 96;
-	// inline asm
-	cvta.to.global.u64 %rd254, %rd255;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r301,%r302,%r303,%r304}, [%rd254];
-	// inline asm
-	mov.b32 	 %f701, %r301;
-	mov.b32 	 %f702, %r302;
-	mov.b32 	 %f703, %r303;
-	add.s64 	%rd258, %rd261, 112;
-	// inline asm
-	cvta.to.global.u64 %rd257, %rd258;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r305,%r306,%r307,%r308}, [%rd257];
-	// inline asm
-	mov.f32 	%f704, 0f3F800000;
-	sub.f32 	%f705, %f704, %f249;
-	mul.f32 	%f706, %f249, %f698;
-	mul.f32 	%f707, %f249, %f699;
-	mul.f32 	%f708, %f249, %f700;
-	fma.rn.f32 	%f938, %f705, %f938, %f706;
-	fma.rn.f32 	%f939, %f705, %f939, %f707;
-	fma.rn.f32 	%f940, %f705, %f940, %f708;
-	mul.f32 	%f709, %f249, %f701;
-	mul.f32 	%f710, %f249, %f702;
-	mul.f32 	%f711, %f249, %f703;
-	fma.rn.f32 	%f935, %f705, %f935, %f709;
-	fma.rn.f32 	%f936, %f705, %f936, %f710;
-	fma.rn.f32 	%f937, %f705, %f937, %f711;
-	mov.b32 	 %f712, %r305;
-	mov.b32 	 %f713, %r306;
-	mov.b32 	 %f714, %r307;
-	mul.f32 	%f715, %f249, %f712;
-	mul.f32 	%f716, %f249, %f713;
-	mul.f32 	%f717, %f249, %f714;
-	fma.rn.f32 	%f932, %f705, %f932, %f715;
-	fma.rn.f32 	%f933, %f705, %f933, %f716;
-	fma.rn.f32 	%f934, %f705, %f934, %f717;
-	bra.uni 	BB0_35;
-
-BB0_24:
-	mov.f32 	%f941, 0f00000000;
-	mov.f32 	%f943, 0f3F800000;
-	setp.eq.s32	%p16, %r161, 4;
-	@%p16 bra 	BB0_27;
-	bra.uni 	BB0_25;
-
-BB0_27:
-	// inline asm
-	call (%rd265), _optix_get_instance_inverse_transform_from_handle, (%rd141);
-	// inline asm
-	bra.uni 	BB0_28;
-
-BB0_30:
-	// inline asm
-	call (%rd156), _optix_get_srt_motion_transform_from_handle, (%rd141);
-	// inline asm
-	// inline asm
-	cvta.to.global.u64 %rd158, %rd156;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r175,%r176,%r177,%r178}, [%rd158];
-	// inline asm
-	mov.b32	{%rs6, %rs7}, %r177;
-	add.s64 	%rd162, %rd156, 16;
-	// inline asm
-	cvta.to.global.u64 %rd161, %rd162;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r179,%r180,%r181,%r182}, [%rd161];
-	// inline asm
-	add.s64 	%rd165, %rd156, 32;
-	// inline asm
-	cvta.to.global.u64 %rd164, %rd165;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r183,%r184,%r185,%r186}, [%rd164];
-	// inline asm
-	add.s64 	%rd168, %rd156, 48;
-	// inline asm
-	cvta.to.global.u64 %rd167, %rd168;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r187,%r188,%r189,%r190}, [%rd167];
-	// inline asm
-	add.s64 	%rd171, %rd156, 64;
-	// inline asm
-	cvta.to.global.u64 %rd170, %rd171;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r191,%r192,%r193,%r194}, [%rd170];
-	// inline asm
-	add.s64 	%rd174, %rd156, 80;
-	// inline asm
-	cvta.to.global.u64 %rd173, %rd174;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r195,%r196,%r197,%r198}, [%rd173];
-	// inline asm
-	add.s64 	%rd177, %rd156, 96;
-	// inline asm
-	cvta.to.global.u64 %rd176, %rd177;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r199,%r200,%r201,%r202}, [%rd176];
-	// inline asm
-	add.s64 	%rd180, %rd156, 112;
-	// inline asm
-	cvta.to.global.u64 %rd179, %rd180;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r203,%r204,%r205,%r206}, [%rd179];
-	// inline asm
-	add.s64 	%rd183, %rd156, 128;
-	// inline asm
-	cvta.to.global.u64 %rd182, %rd183;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r207,%r208,%r209,%r210}, [%rd182];
-	// inline asm
-	add.s64 	%rd186, %rd156, 144;
-	// inline asm
-	cvta.to.global.u64 %rd185, %rd186;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r211,%r212,%r213,%r214}, [%rd185];
-	// inline asm
-	mov.b32 	 %f595, %r178;
-	mov.b32 	 %f596, %r179;
-	cvt.u32.u16	%r231, %rs6;
-	add.s32 	%r232, %r231, -1;
-	cvt.rn.f32.s32	%f597, %r232;
-	sub.f32 	%f598, %f584, %f595;
-	mul.f32 	%f599, %f598, %f597;
-	sub.f32 	%f600, %f596, %f595;
-	div.rn.f32 	%f601, %f599, %f600;
-	min.f32 	%f602, %f597, %f601;
-	mov.f32 	%f603, 0f00000000;
-	max.f32 	%f604, %f603, %f602;
-	cvt.rmi.f32.f32	%f605, %f604;
-	cvt.rzi.s32.f32	%r233, %f605;
-	cvt.s64.s32	%rd15, %r233;
-	mul.wide.s32 	%rd200, %r233, 64;
-	add.s64 	%rd189, %rd165, %rd200;
-	// inline asm
-	cvta.to.global.u64 %rd188, %rd189;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r215,%r216,%r217,%r218}, [%rd188];
-	// inline asm
-	mov.b32 	 %f922, %r215;
-	mov.b32 	 %f923, %r216;
-	mov.b32 	 %f924, %r217;
-	add.s64 	%rd192, %rd189, 16;
-	// inline asm
-	cvta.to.global.u64 %rd191, %rd192;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r219,%r220,%r221,%r222}, [%rd191];
-	// inline asm
-	mov.b32 	 %f925, %r219;
-	mov.b32 	 %f926, %r220;
-	mov.b32 	 %f927, %r222;
-	add.s64 	%rd195, %rd189, 32;
-	// inline asm
-	cvta.to.global.u64 %rd194, %rd195;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r223,%r224,%r225,%r226}, [%rd194];
-	// inline asm
-	sub.f32 	%f209, %f604, %f605;
-	mov.b32 	 %f928, %r224;
-	mov.b32 	 %f929, %r225;
-	mov.b32 	 %f930, %r226;
-	add.s64 	%rd198, %rd189, 48;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r309,%r310,%r311,%r312}, [%rd251];
+	// end inline asm
+	mov.b32 	%f740, %r309;
+	mov.b32 	%f741, %r310;
+	mov.b32 	%f742, %r311;
+	mul.f32 	%f743, %f258, %f740;
+	mul.f32 	%f744, %f258, %f741;
+	mul.f32 	%f745, %f258, %f742;
+	fma.rn.f32 	%f967, %f727, %f967, %f743;
+	fma.rn.f32 	%f968, %f727, %f968, %f744;
+	fma.rn.f32 	%f969, %f727, %f969, %f745;
+	bra.uni 	$L__BB0_36;
+
+$L__BB0_25:
+	mov.f32 	%f976, 0f00000000;
+	mov.f32 	%f978, 0f3F800000;
+	setp.eq.s32 	%p15, %r165, 4;
+	@%p15 bra 	$L__BB0_28;
+
+	setp.ne.s32 	%p16, %r165, 1;
+	mov.f32 	%f977, %f976;
+	mov.f32 	%f979, %f976;
+	mov.f32 	%f980, %f978;
+	mov.f32 	%f981, %f976;
+	mov.f32 	%f982, %f978;
+	mov.f32 	%f983, %f976;
+	mov.f32 	%f984, %f976;
+	@%p16 bra 	$L__BB0_37;
+
+	// begin inline asm
+	call (%rd139), _optix_get_static_transform_from_handle, (%rd137);
+	// end inline asm
+	add.s64 	%rd258, %rd139, 64;
+	bra.uni 	$L__BB0_29;
+
+$L__BB0_31:
+	// begin inline asm
+	call (%rd152), _optix_get_srt_motion_transform_from_handle, (%rd137);
+	// end inline asm
+	// begin inline asm
+	cvta.to.global.u64 %rd154, %rd152;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r179,%r180,%r181,%r182}, [%rd154];
+	// end inline asm
+	add.s64 	%rd158, %rd152, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd157, %rd158;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r183,%r184,%r185,%r186}, [%rd157];
+	// end inline asm
+	add.s64 	%rd161, %rd152, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd160, %rd161;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r187,%r188,%r189,%r190}, [%rd160];
+	// end inline asm
+	add.s64 	%rd164, %rd152, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd163, %rd164;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r191,%r192,%r193,%r194}, [%rd163];
+	// end inline asm
+	add.s64 	%rd167, %rd152, 64;
+	// begin inline asm
+	cvta.to.global.u64 %rd166, %rd167;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r195,%r196,%r197,%r198}, [%rd166];
+	// end inline asm
+	add.s64 	%rd170, %rd152, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd169, %rd170;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r199,%r200,%r201,%r202}, [%rd169];
+	// end inline asm
+	add.s64 	%rd173, %rd152, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd172, %rd173;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r203,%r204,%r205,%r206}, [%rd172];
+	// end inline asm
+	add.s64 	%rd176, %rd152, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd175, %rd176;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r207,%r208,%r209,%r210}, [%rd175];
+	// end inline asm
+	add.s64 	%rd179, %rd152, 128;
+	// begin inline asm
+	cvta.to.global.u64 %rd178, %rd179;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r211,%r212,%r213,%r214}, [%rd178];
+	// end inline asm
+	add.s64 	%rd182, %rd152, 144;
+	// begin inline asm
+	cvta.to.global.u64 %rd181, %rd182;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r215,%r216,%r217,%r218}, [%rd181];
+	// end inline asm
+	mov.b32 	%f623, %r182;
+	mov.b32 	%f624, %r183;
+	and.b32  	%r235, %r181, 65535;
+	add.s32 	%r236, %r235, -1;
+	cvt.rn.f32.s32 	%f625, %r236;
+	sub.f32 	%f626, %f611, %f623;
+	mul.f32 	%f627, %f626, %f625;
+	sub.f32 	%f628, %f624, %f623;
+	div.rn.f32 	%f629, %f627, %f628;
+	min.f32 	%f630, %f625, %f629;
+	mov.f32 	%f631, 0f00000000;
+	max.f32 	%f632, %f631, %f630;
+	cvt.rmi.f32.f32 	%f633, %f632;
+	sub.f32 	%f218, %f632, %f633;
+	cvt.rzi.s32.f32 	%r237, %f633;
+	mul.wide.s32 	%rd196, %r237, 64;
+	add.s64 	%rd185, %rd161, %rd196;
+	// begin inline asm
+	cvta.to.global.u64 %rd184, %rd185;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r219,%r220,%r221,%r222}, [%rd184];
+	// end inline asm
+	mov.b32 	%f957, %r219;
+	mov.b32 	%f958, %r220;
+	mov.b32 	%f959, %r221;
+	add.s64 	%rd188, %rd185, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd187, %rd188;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r223,%r224,%r225,%r226}, [%rd187];
+	// end inline asm
+	mov.b32 	%f960, %r223;
+	mov.b32 	%f961, %r224;
+	mov.b32 	%f962, %r226;
+	add.s64 	%rd191, %rd185, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd190, %rd191;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r227,%r228,%r229,%r230}, [%rd190];
+	// end inline asm
+	mov.b32 	%f963, %r228;
+	mov.b32 	%f964, %r229;
+	mov.b32 	%f965, %r230;
+	add.s64 	%rd194, %rd185, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd193, %rd194;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r231,%r232,%r233,%r234}, [%rd193];
+	// end inline asm
+	mov.b32 	%f966, %r231;
+	setp.leu.f32 	%p18, %f218, 0f00000000;
+	@%p18 bra 	$L__BB0_33;
+
+	mov.f32 	%f634, 0f3F800000;
+	sub.f32 	%f635, %f634, %f218;
+	add.s64 	%rd198, %rd185, 64;
+	// begin inline asm
 	cvta.to.global.u64 %rd197, %rd198;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r227,%r228,%r229,%r230}, [%rd197];
-	// inline asm
-	mov.b32 	 %f931, %r227;
-	setp.leu.f32	%p19, %f209, 0f00000000;
-	@%p19 bra 	BB0_32;
-
-	shl.b64 	%rd213, %rd15, 6;
-	add.s64 	%rd214, %rd213, %rd156;
-	add.s64 	%rd202, %rd214, 96;
-	// inline asm
-	cvta.to.global.u64 %rd201, %rd202;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r234,%r235,%r236,%r237}, [%rd201];
-	// inline asm
-	mov.b32 	 %f606, %r234;
-	mov.b32 	 %f607, %r235;
-	mov.b32 	 %f608, %r236;
-	add.s64 	%rd205, %rd214, 112;
-	// inline asm
-	cvta.to.global.u64 %rd204, %rd205;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r238,%r239,%r240,%r241}, [%rd204];
-	// inline asm
-	mov.b32 	 %f609, %r238;
-	mov.b32 	 %f610, %r239;
-	mov.b32 	 %f611, %r241;
-	add.s64 	%rd208, %rd214, 128;
-	// inline asm
-	cvta.to.global.u64 %rd207, %rd208;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r242,%r243,%r244,%r245}, [%rd207];
-	// inline asm
-	mov.b32 	 %f612, %r243;
-	mov.b32 	 %f613, %r244;
-	mov.b32 	 %f614, %r245;
-	add.s64 	%rd211, %rd214, 144;
-	// inline asm
-	cvta.to.global.u64 %rd210, %rd211;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r246,%r247,%r248,%r249}, [%rd210];
-	// inline asm
-	mov.f32 	%f615, 0f3F800000;
-	sub.f32 	%f616, %f615, %f209;
-	mul.f32 	%f617, %f209, %f606;
-	mul.f32 	%f618, %f209, %f607;
-	mul.f32 	%f619, %f209, %f608;
-	fma.rn.f32 	%f922, %f616, %f922, %f617;
-	fma.rn.f32 	%f923, %f616, %f923, %f618;
-	fma.rn.f32 	%f924, %f616, %f924, %f619;
-	mul.f32 	%f620, %f209, %f609;
-	mul.f32 	%f621, %f209, %f610;
-	mul.f32 	%f622, %f209, %f611;
-	fma.rn.f32 	%f925, %f616, %f925, %f620;
-	fma.rn.f32 	%f926, %f616, %f926, %f621;
-	fma.rn.f32 	%f927, %f616, %f927, %f622;
-	mul.f32 	%f623, %f209, %f612;
-	mul.f32 	%f624, %f209, %f613;
-	mul.f32 	%f625, %f209, %f614;
-	fma.rn.f32 	%f626, %f616, %f928, %f623;
-	fma.rn.f32 	%f627, %f616, %f929, %f624;
-	fma.rn.f32 	%f628, %f616, %f930, %f625;
-	mov.b32 	 %f629, %r246;
-	mul.f32 	%f630, %f209, %f629;
-	fma.rn.f32 	%f631, %f616, %f931, %f630;
-	mul.f32 	%f632, %f627, %f627;
-	fma.rn.f32 	%f633, %f626, %f626, %f632;
-	fma.rn.f32 	%f634, %f628, %f628, %f633;
-	fma.rn.f32 	%f635, %f631, %f631, %f634;
-	sqrt.rn.f32 	%f636, %f635;
-	rcp.rn.f32 	%f637, %f636;
-	mul.f32 	%f928, %f626, %f637;
-	mul.f32 	%f929, %f627, %f637;
-	mul.f32 	%f930, %f628, %f637;
-	mul.f32 	%f931, %f631, %f637;
-
-BB0_32:
-	mul.f32 	%f638, %f929, %f929;
-	fma.rn.f32 	%f639, %f928, %f928, %f638;
-	fma.rn.f32 	%f640, %f930, %f930, %f639;
-	fma.rn.f32 	%f641, %f931, %f931, %f640;
-	rcp.rn.f32 	%f642, %f641;
-	mul.f32 	%f643, %f928, %f642;
-	mul.f32 	%f644, %f929, %f642;
-	mul.f32 	%f645, %f930, %f642;
-	mul.f32 	%f646, %f931, %f642;
-	mul.f32 	%f647, %f928, %f643;
-	mul.f32 	%f648, %f929, %f644;
-	mul.f32 	%f649, %f930, %f645;
-	mul.f32 	%f650, %f928, %f644;
-	mul.f32 	%f651, %f930, %f646;
-	mul.f32 	%f652, %f928, %f645;
-	mul.f32 	%f653, %f929, %f646;
-	mul.f32 	%f654, %f929, %f645;
-	mul.f32 	%f655, %f928, %f646;
-	sub.f32 	%f656, %f647, %f648;
-	sub.f32 	%f657, %f656, %f649;
-	fma.rn.f32 	%f658, %f931, %f646, %f657;
-	sub.f32 	%f659, %f650, %f651;
-	add.f32 	%f660, %f659, %f659;
-	add.f32 	%f661, %f652, %f653;
-	add.f32 	%f662, %f661, %f661;
-	add.f32 	%f663, %f650, %f651;
-	add.f32 	%f664, %f663, %f663;
-	sub.f32 	%f665, %f648, %f647;
-	sub.f32 	%f666, %f665, %f649;
-	fma.rn.f32 	%f667, %f931, %f646, %f666;
-	sub.f32 	%f668, %f654, %f655;
-	add.f32 	%f669, %f668, %f668;
-	sub.f32 	%f670, %f652, %f653;
-	add.f32 	%f671, %f670, %f670;
-	add.f32 	%f672, %f654, %f655;
-	add.f32 	%f673, %f672, %f672;
-	neg.f32 	%f674, %f647;
-	sub.f32 	%f675, %f674, %f648;
-	add.f32 	%f676, %f649, %f675;
-	fma.rn.f32 	%f677, %f931, %f646, %f676;
-	mul.f32 	%f678, %f924, %f658;
-	fma.rn.f32 	%f679, %f926, %f660, %f678;
-	fma.rn.f32 	%f940, %f927, %f662, %f679;
-	mul.f32 	%f680, %f926, %f667;
-	fma.rn.f32 	%f681, %f924, %f664, %f680;
-	fma.rn.f32 	%f937, %f927, %f669, %f681;
-	mul.f32 	%f682, %f926, %f673;
-	fma.rn.f32 	%f683, %f924, %f671, %f682;
-	fma.rn.f32 	%f934, %f927, %f677, %f683;
-	mul.f32 	%f684, %f923, %f658;
-	fma.rn.f32 	%f939, %f925, %f660, %f684;
-	mul.f32 	%f685, %f925, %f667;
-	fma.rn.f32 	%f936, %f923, %f664, %f685;
-	mul.f32 	%f686, %f925, %f673;
-	fma.rn.f32 	%f933, %f923, %f671, %f686;
-	mul.f32 	%f938, %f922, %f658;
-	mul.f32 	%f935, %f922, %f664;
-	mul.f32 	%f932, %f922, %f671;
-
-BB0_35:
-	mul.f32 	%f718, %f933, %f937;
-	mul.f32 	%f719, %f934, %f936;
-	sub.f32 	%f720, %f719, %f718;
-	mul.f32 	%f721, %f938, %f720;
-	mul.f32 	%f722, %f932, %f937;
-	mul.f32 	%f723, %f934, %f935;
-	sub.f32 	%f724, %f723, %f722;
-	mul.f32 	%f725, %f724, %f939;
-	sub.f32 	%f726, %f721, %f725;
-	mul.f32 	%f727, %f932, %f936;
-	mul.f32 	%f728, %f933, %f935;
-	sub.f32 	%f729, %f728, %f727;
-	fma.rn.f32 	%f730, %f729, %f940, %f726;
-	rcp.rn.f32 	%f731, %f730;
-	mul.f32 	%f947, %f720, %f731;
-	mul.f32 	%f732, %f934, %f939;
-	mul.f32 	%f733, %f933, %f940;
-	sub.f32 	%f734, %f733, %f732;
-	mul.f32 	%f948, %f731, %f734;
-	mul.f32 	%f735, %f936, %f940;
-	mul.f32 	%f736, %f937, %f939;
-	sub.f32 	%f737, %f736, %f735;
-	mul.f32 	%f949, %f731, %f737;
-	sub.f32 	%f738, %f722, %f723;
-	mul.f32 	%f944, %f738, %f731;
-	mul.f32 	%f739, %f932, %f940;
-	mul.f32 	%f740, %f934, %f938;
-	sub.f32 	%f741, %f740, %f739;
-	mul.f32 	%f945, %f731, %f741;
-	mul.f32 	%f742, %f937, %f938;
-	mul.f32 	%f743, %f935, %f940;
-	sub.f32 	%f744, %f743, %f742;
-	mul.f32 	%f946, %f731, %f744;
-	mul.f32 	%f941, %f729, %f731;
-	mul.f32 	%f745, %f933, %f938;
-	mul.f32 	%f746, %f932, %f939;
-	sub.f32 	%f747, %f746, %f745;
-	mul.f32 	%f942, %f747, %f731;
-	mul.f32 	%f748, %f935, %f939;
-	mul.f32 	%f749, %f936, %f938;
-	sub.f32 	%f750, %f749, %f748;
-	mul.f32 	%f943, %f750, %f731;
-	bra.uni 	BB0_36;
-
-BB0_25:
-	setp.ne.s32	%p17, %r161, 1;
-	mov.f32 	%f942, %f941;
-	mov.f32 	%f944, %f941;
-	mov.f32 	%f945, %f943;
-	mov.f32 	%f946, %f941;
-	mov.f32 	%f947, %f943;
-	mov.f32 	%f948, %f941;
-	mov.f32 	%f949, %f941;
-	@%p17 bra 	BB0_36;
-
-	// inline asm
-	call (%rd143), _optix_get_static_transform_from_handle, (%rd141);
-	// inline asm
-	add.s64 	%rd265, %rd143, 64;
-
-BB0_28:
-	// inline asm
-	cvta.to.global.u64 %rd147, %rd265;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r163,%r164,%r165,%r166}, [%rd147];
-	// inline asm
-	mov.b32 	 %f947, %r163;
-	mov.b32 	 %f948, %r164;
-	mov.b32 	 %f949, %r165;
-	add.s64 	%rd151, %rd265, 16;
-	// inline asm
-	cvta.to.global.u64 %rd150, %rd151;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r167,%r168,%r169,%r170}, [%rd150];
-	// inline asm
-	mov.b32 	 %f944, %r167;
-	mov.b32 	 %f945, %r168;
-	mov.b32 	 %f946, %r169;
-	add.s64 	%rd154, %rd265, 32;
-	// inline asm
-	cvta.to.global.u64 %rd153, %rd154;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r171,%r172,%r173,%r174}, [%rd153];
-	// inline asm
-	mov.b32 	 %f941, %r171;
-	mov.b32 	 %f942, %r172;
-	mov.b32 	 %f943, %r173;
-
-BB0_36:
-	setp.eq.s32	%p21, %r319, 0;
-	@%p21 bra 	BB0_37;
-	bra.uni 	BB0_38;
-
-BB0_37:
-	mov.f32 	%f921, %f941;
-	mov.f32 	%f920, %f942;
-	mov.f32 	%f919, %f943;
-	mov.f32 	%f918, %f944;
-	mov.f32 	%f917, %f945;
-	mov.f32 	%f916, %f946;
-	mov.f32 	%f915, %f947;
-	mov.f32 	%f914, %f948;
-	mov.f32 	%f913, %f949;
-	bra.uni 	BB0_39;
-
-BB0_38:
-	mul.f32 	%f751, %f918, %f948;
-	fma.rn.f32 	%f752, %f915, %f947, %f751;
-	fma.rn.f32 	%f289, %f921, %f949, %f752;
-	mul.f32 	%f753, %f917, %f948;
-	fma.rn.f32 	%f754, %f914, %f947, %f753;
-	fma.rn.f32 	%f290, %f920, %f949, %f754;
-	mul.f32 	%f755, %f916, %f948;
-	fma.rn.f32 	%f756, %f913, %f947, %f755;
-	fma.rn.f32 	%f291, %f919, %f949, %f756;
-	mul.f32 	%f757, %f918, %f945;
-	fma.rn.f32 	%f758, %f915, %f944, %f757;
-	fma.rn.f32 	%f292, %f921, %f946, %f758;
-	mul.f32 	%f759, %f917, %f945;
-	fma.rn.f32 	%f760, %f914, %f944, %f759;
-	fma.rn.f32 	%f293, %f920, %f946, %f760;
-	mul.f32 	%f761, %f916, %f945;
-	fma.rn.f32 	%f762, %f913, %f944, %f761;
-	fma.rn.f32 	%f294, %f919, %f946, %f762;
-	mul.f32 	%f763, %f918, %f942;
-	fma.rn.f32 	%f764, %f915, %f941, %f763;
-	fma.rn.f32 	%f921, %f921, %f943, %f764;
-	mul.f32 	%f765, %f917, %f942;
-	fma.rn.f32 	%f766, %f914, %f941, %f765;
-	fma.rn.f32 	%f920, %f920, %f943, %f766;
-	mul.f32 	%f767, %f916, %f942;
-	fma.rn.f32 	%f768, %f913, %f941, %f767;
-	fma.rn.f32 	%f919, %f919, %f943, %f768;
-	mov.f32 	%f918, %f292;
-	mov.f32 	%f917, %f293;
-	mov.f32 	%f916, %f294;
-	mov.f32 	%f915, %f289;
-	mov.f32 	%f914, %f290;
-	mov.f32 	%f913, %f291;
-
-BB0_39:
-	add.s32 	%r319, %r319, 1;
-	setp.lt.u32	%p22, %r319, %r8;
-	@%p22 bra 	BB0_23;
-
-	mul.f32 	%f769, %f582, %f914;
-	fma.rn.f32 	%f770, %f581, %f915, %f769;
-	fma.rn.f32 	%f959, %f961, %f913, %f770;
-	mul.f32 	%f771, %f582, %f917;
-	fma.rn.f32 	%f772, %f581, %f918, %f771;
-	fma.rn.f32 	%f960, %f961, %f916, %f772;
-	mul.f32 	%f773, %f582, %f920;
-	fma.rn.f32 	%f774, %f581, %f921, %f773;
-	fma.rn.f32 	%f961, %f961, %f919, %f774;
-	bra.uni 	BB0_41;
-
-BB0_22:
-	mov.f32 	%f959, %f581;
-	mov.f32 	%f960, %f582;
-
-BB0_41:
-	// inline asm
-	call (%f775), _optix_get_ray_tmin, ();
-	// inline asm
-	// inline asm
-	call (%f776), _optix_get_ray_tmax, ();
-	// inline asm
-	ld.v4.f32 	{%f778, %f779, %f780, %f781}, [%rd1+208];
-	ld.v4.f32 	{%f785, %f786, %f787, %f788}, [%rd1+160];
-	fma.rn.f32 	%f790, %f912, %f785, %f778;
-	fma.rn.f32 	%f792, %f912, %f786, %f779;
-	fma.rn.f32 	%f794, %f912, %f787, %f780;
-	ld.v4.f32 	{%f795, %f796, %f797, %f798}, [%rd1+176];
-	fma.rn.f32 	%f800, %f911, %f795, %f790;
-	fma.rn.f32 	%f802, %f911, %f796, %f792;
-	fma.rn.f32 	%f804, %f911, %f797, %f794;
-	ld.v4.f32 	{%f805, %f806, %f807, %f808}, [%rd1+192];
-	fma.rn.f32 	%f810, %f910, %f805, %f800;
-	fma.rn.f32 	%f812, %f910, %f806, %f802;
-	fma.rn.f32 	%f315, %f910, %f807, %f804;
-	mul.f32 	%f814, %f959, %f785;
-	mul.f32 	%f815, %f959, %f786;
-	mul.f32 	%f816, %f959, %f787;
-	fma.rn.f32 	%f817, %f960, %f795, %f814;
-	fma.rn.f32 	%f818, %f960, %f796, %f815;
-	fma.rn.f32 	%f819, %f960, %f797, %f816;
-	fma.rn.f32 	%f820, %f961, %f805, %f817;
-	fma.rn.f32 	%f821, %f961, %f806, %f818;
-	fma.rn.f32 	%f316, %f961, %f807, %f819;
-	mul.f32 	%f822, %f821, %f821;
-	fma.rn.f32 	%f317, %f820, %f820, %f822;
-	mul.f32 	%f823, %f812, %f821;
-	fma.rn.f32 	%f824, %f810, %f820, %f823;
-	add.f32 	%f318, %f824, %f824;
-	mul.f32 	%f825, %f812, %f812;
-	fma.rn.f32 	%f826, %f810, %f810, %f825;
-	ld.f32 	%f827, [%rd1+292];
-	mul.f32 	%f828, %f827, %f827;
-	sub.f32 	%f319, %f826, %f828;
-	setp.eq.f32	%p23, %f317, 0f00000000;
-	setp.eq.f32	%p24, %f318, 0f00000000;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r238,%r239,%r240,%r241}, [%rd197];
+	// end inline asm
+	mov.b32 	%f636, %r238;
+	mov.b32 	%f637, %r239;
+	mov.b32 	%f638, %r240;
+	mul.f32 	%f639, %f218, %f636;
+	mul.f32 	%f640, %f218, %f637;
+	mul.f32 	%f641, %f218, %f638;
+	fma.rn.f32 	%f957, %f635, %f957, %f639;
+	fma.rn.f32 	%f958, %f635, %f958, %f640;
+	fma.rn.f32 	%f959, %f635, %f959, %f641;
+	add.s64 	%rd201, %rd185, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd200, %rd201;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r242,%r243,%r244,%r245}, [%rd200];
+	// end inline asm
+	mov.b32 	%f642, %r242;
+	mov.b32 	%f643, %r243;
+	mov.b32 	%f644, %r245;
+	mul.f32 	%f645, %f218, %f642;
+	mul.f32 	%f646, %f218, %f643;
+	mul.f32 	%f647, %f218, %f644;
+	fma.rn.f32 	%f960, %f635, %f960, %f645;
+	fma.rn.f32 	%f961, %f635, %f961, %f646;
+	fma.rn.f32 	%f962, %f635, %f962, %f647;
+	add.s64 	%rd204, %rd185, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd203, %rd204;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r246,%r247,%r248,%r249}, [%rd203];
+	// end inline asm
+	mov.b32 	%f648, %r247;
+	mov.b32 	%f649, %r248;
+	mov.b32 	%f650, %r249;
+	mul.f32 	%f651, %f218, %f648;
+	mul.f32 	%f652, %f218, %f649;
+	mul.f32 	%f653, %f218, %f650;
+	fma.rn.f32 	%f654, %f635, %f963, %f651;
+	fma.rn.f32 	%f655, %f635, %f964, %f652;
+	fma.rn.f32 	%f656, %f635, %f965, %f653;
+	add.s64 	%rd207, %rd185, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd206, %rd207;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r250,%r251,%r252,%r253}, [%rd206];
+	// end inline asm
+	mov.b32 	%f657, %r250;
+	mul.f32 	%f658, %f218, %f657;
+	fma.rn.f32 	%f659, %f635, %f966, %f658;
+	mul.f32 	%f660, %f655, %f655;
+	fma.rn.f32 	%f661, %f654, %f654, %f660;
+	fma.rn.f32 	%f662, %f656, %f656, %f661;
+	fma.rn.f32 	%f663, %f659, %f659, %f662;
+	sqrt.rn.f32 	%f664, %f663;
+	rcp.rn.f32 	%f665, %f664;
+	mul.f32 	%f963, %f654, %f665;
+	mul.f32 	%f964, %f655, %f665;
+	mul.f32 	%f965, %f656, %f665;
+	mul.f32 	%f966, %f665, %f659;
+
+$L__BB0_33:
+	mul.f32 	%f666, %f964, %f964;
+	fma.rn.f32 	%f667, %f963, %f963, %f666;
+	fma.rn.f32 	%f668, %f965, %f965, %f667;
+	fma.rn.f32 	%f669, %f966, %f966, %f668;
+	rcp.rn.f32 	%f670, %f669;
+	mul.f32 	%f671, %f963, %f670;
+	mul.f32 	%f672, %f964, %f670;
+	mul.f32 	%f673, %f965, %f670;
+	mul.f32 	%f674, %f966, %f670;
+	mul.f32 	%f675, %f963, %f671;
+	mul.f32 	%f676, %f964, %f672;
+	mul.f32 	%f677, %f965, %f673;
+	mul.f32 	%f678, %f963, %f672;
+	mul.f32 	%f679, %f965, %f674;
+	mul.f32 	%f680, %f963, %f673;
+	mul.f32 	%f681, %f964, %f674;
+	mul.f32 	%f682, %f964, %f673;
+	mul.f32 	%f683, %f963, %f674;
+	sub.f32 	%f684, %f675, %f676;
+	sub.f32 	%f685, %f684, %f677;
+	fma.rn.f32 	%f686, %f966, %f674, %f685;
+	sub.f32 	%f687, %f678, %f679;
+	add.f32 	%f688, %f687, %f687;
+	add.f32 	%f689, %f680, %f681;
+	add.f32 	%f690, %f689, %f689;
+	add.f32 	%f691, %f678, %f679;
+	add.f32 	%f692, %f691, %f691;
+	sub.f32 	%f693, %f676, %f675;
+	sub.f32 	%f694, %f693, %f677;
+	fma.rn.f32 	%f695, %f966, %f674, %f694;
+	sub.f32 	%f696, %f682, %f683;
+	add.f32 	%f697, %f696, %f696;
+	sub.f32 	%f698, %f680, %f681;
+	add.f32 	%f699, %f698, %f698;
+	add.f32 	%f700, %f682, %f683;
+	add.f32 	%f701, %f700, %f700;
+	neg.f32 	%f702, %f675;
+	sub.f32 	%f703, %f702, %f676;
+	add.f32 	%f704, %f677, %f703;
+	fma.rn.f32 	%f705, %f966, %f674, %f704;
+	mul.f32 	%f706, %f959, %f686;
+	fma.rn.f32 	%f707, %f961, %f688, %f706;
+	fma.rn.f32 	%f975, %f962, %f690, %f707;
+	mul.f32 	%f708, %f961, %f695;
+	fma.rn.f32 	%f709, %f959, %f692, %f708;
+	fma.rn.f32 	%f972, %f962, %f697, %f709;
+	mul.f32 	%f710, %f961, %f701;
+	fma.rn.f32 	%f711, %f959, %f699, %f710;
+	fma.rn.f32 	%f969, %f962, %f705, %f711;
+	mul.f32 	%f712, %f958, %f686;
+	fma.rn.f32 	%f974, %f960, %f688, %f712;
+	mul.f32 	%f713, %f960, %f695;
+	fma.rn.f32 	%f971, %f958, %f692, %f713;
+	mul.f32 	%f714, %f960, %f701;
+	fma.rn.f32 	%f968, %f958, %f699, %f714;
+	mul.f32 	%f973, %f957, %f686;
+	mul.f32 	%f970, %f957, %f692;
+	mul.f32 	%f967, %f957, %f699;
+
+$L__BB0_36:
+	mul.f32 	%f746, %f968, %f972;
+	mul.f32 	%f747, %f969, %f971;
+	sub.f32 	%f748, %f747, %f746;
+	mul.f32 	%f749, %f973, %f748;
+	mul.f32 	%f750, %f967, %f972;
+	mul.f32 	%f751, %f969, %f970;
+	sub.f32 	%f752, %f751, %f750;
+	mul.f32 	%f753, %f752, %f974;
+	sub.f32 	%f754, %f749, %f753;
+	mul.f32 	%f755, %f967, %f971;
+	mul.f32 	%f756, %f968, %f970;
+	sub.f32 	%f757, %f756, %f755;
+	fma.rn.f32 	%f758, %f757, %f975, %f754;
+	rcp.rn.f32 	%f759, %f758;
+	mul.f32 	%f982, %f748, %f759;
+	mul.f32 	%f760, %f969, %f974;
+	mul.f32 	%f761, %f968, %f975;
+	sub.f32 	%f762, %f761, %f760;
+	mul.f32 	%f983, %f762, %f759;
+	mul.f32 	%f763, %f971, %f975;
+	mul.f32 	%f764, %f972, %f974;
+	sub.f32 	%f765, %f764, %f763;
+	mul.f32 	%f984, %f765, %f759;
+	sub.f32 	%f766, %f750, %f751;
+	mul.f32 	%f979, %f766, %f759;
+	mul.f32 	%f767, %f967, %f975;
+	mul.f32 	%f768, %f969, %f973;
+	sub.f32 	%f769, %f768, %f767;
+	mul.f32 	%f980, %f769, %f759;
+	mul.f32 	%f770, %f972, %f973;
+	mul.f32 	%f771, %f970, %f975;
+	sub.f32 	%f772, %f771, %f770;
+	mul.f32 	%f981, %f772, %f759;
+	mul.f32 	%f976, %f757, %f759;
+	mul.f32 	%f773, %f968, %f973;
+	mul.f32 	%f774, %f967, %f974;
+	sub.f32 	%f775, %f774, %f773;
+	mul.f32 	%f977, %f775, %f759;
+	mul.f32 	%f776, %f970, %f974;
+	mul.f32 	%f777, %f971, %f973;
+	sub.f32 	%f778, %f777, %f776;
+	mul.f32 	%f978, %f778, %f759;
+	bra.uni 	$L__BB0_37;
+
+$L__BB0_28:
+	// begin inline asm
+	call (%rd258), _optix_get_instance_inverse_transform_from_handle, (%rd137);
+	// end inline asm
+
+$L__BB0_29:
+	// begin inline asm
+	cvta.to.global.u64 %rd143, %rd258;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r167,%r168,%r169,%r170}, [%rd143];
+	// end inline asm
+	mov.b32 	%f982, %r167;
+	mov.b32 	%f983, %r168;
+	mov.b32 	%f984, %r169;
+	add.s64 	%rd147, %rd258, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd146, %rd147;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r171,%r172,%r173,%r174}, [%rd146];
+	// end inline asm
+	mov.b32 	%f979, %r171;
+	mov.b32 	%f980, %r172;
+	mov.b32 	%f981, %r173;
+	add.s64 	%rd150, %rd258, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd149, %rd150;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r175,%r176,%r177,%r178}, [%rd149];
+	// end inline asm
+	mov.b32 	%f976, %r175;
+	mov.b32 	%f977, %r176;
+	mov.b32 	%f978, %r177;
+
+$L__BB0_37:
+	setp.eq.s32 	%p20, %r322, 0;
+	@%p20 bra 	$L__BB0_39;
+
+	mul.f32 	%f779, %f953, %f983;
+	fma.rn.f32 	%f780, %f950, %f982, %f779;
+	fma.rn.f32 	%f304, %f956, %f984, %f780;
+	mul.f32 	%f781, %f952, %f983;
+	fma.rn.f32 	%f782, %f949, %f982, %f781;
+	fma.rn.f32 	%f305, %f955, %f984, %f782;
+	mul.f32 	%f783, %f951, %f983;
+	fma.rn.f32 	%f784, %f948, %f982, %f783;
+	fma.rn.f32 	%f984, %f954, %f984, %f784;
+	mul.f32 	%f785, %f953, %f980;
+	fma.rn.f32 	%f786, %f950, %f979, %f785;
+	fma.rn.f32 	%f307, %f956, %f981, %f786;
+	mul.f32 	%f787, %f952, %f980;
+	fma.rn.f32 	%f788, %f949, %f979, %f787;
+	fma.rn.f32 	%f308, %f955, %f981, %f788;
+	mul.f32 	%f789, %f951, %f980;
+	fma.rn.f32 	%f790, %f948, %f979, %f789;
+	fma.rn.f32 	%f981, %f954, %f981, %f790;
+	mul.f32 	%f791, %f953, %f977;
+	fma.rn.f32 	%f792, %f950, %f976, %f791;
+	fma.rn.f32 	%f310, %f956, %f978, %f792;
+	mul.f32 	%f793, %f952, %f977;
+	fma.rn.f32 	%f794, %f949, %f976, %f793;
+	fma.rn.f32 	%f311, %f955, %f978, %f794;
+	mul.f32 	%f795, %f951, %f977;
+	fma.rn.f32 	%f796, %f948, %f976, %f795;
+	fma.rn.f32 	%f978, %f954, %f978, %f796;
+	mov.f32 	%f976, %f310;
+	mov.f32 	%f977, %f311;
+	mov.f32 	%f979, %f307;
+	mov.f32 	%f980, %f308;
+	mov.f32 	%f982, %f304;
+	mov.f32 	%f983, %f305;
+
+$L__BB0_39:
+	add.s32 	%r322, %r322, 1;
+	setp.lt.u32 	%p21, %r322, %r162;
+	mov.f32 	%f948, %f984;
+	mov.f32 	%f949, %f983;
+	mov.f32 	%f950, %f982;
+	mov.f32 	%f951, %f981;
+	mov.f32 	%f952, %f980;
+	mov.f32 	%f953, %f979;
+	mov.f32 	%f954, %f978;
+	mov.f32 	%f955, %f977;
+	mov.f32 	%f956, %f976;
+	@%p21 bra 	$L__BB0_24;
+
+$L__BB0_40:
+	mul.f32 	%f797, %f1004, %f983;
+	fma.rn.f32 	%f798, %f1003, %f982, %f797;
+	mul.f32 	%f799, %f1004, %f980;
+	fma.rn.f32 	%f800, %f1003, %f979, %f799;
+	mul.f32 	%f801, %f1004, %f977;
+	fma.rn.f32 	%f802, %f1003, %f976, %f801;
+	fma.rn.f32 	%f1005, %f610, %f978, %f802;
+	fma.rn.f32 	%f1004, %f610, %f981, %f800;
+	fma.rn.f32 	%f1003, %f610, %f984, %f798;
+	bra.uni 	$L__BB0_42;
+
+$L__BB0_41:
+	mov.f32 	%f1005, %f610;
+
+$L__BB0_42:
+	// begin inline asm
+	call (%f803), _optix_get_ray_tmin, ();
+	// end inline asm
+	// begin inline asm
+	call (%f804), _optix_get_ray_tmax, ();
+	// end inline asm
+	add.s64 	%rd16, %rd1, 208;
+	ld.v4.f32 	{%f807, %f808, %f809, %f810}, [%rd1+208];
+	ld.f32 	%f814, [%rd1+160];
+	fma.rn.f32 	%f815, %f945, %f814, %f807;
+	ld.f32 	%f816, [%rd1+164];
+	fma.rn.f32 	%f817, %f945, %f816, %f808;
+	ld.f32 	%f818, [%rd1+168];
+	fma.rn.f32 	%f819, %f945, %f818, %f809;
+	ld.f32 	%f820, [%rd1+176];
+	fma.rn.f32 	%f821, %f946, %f820, %f815;
+	ld.f32 	%f822, [%rd1+180];
+	fma.rn.f32 	%f823, %f946, %f822, %f817;
+	ld.f32 	%f824, [%rd1+184];
+	fma.rn.f32 	%f825, %f946, %f824, %f819;
+	ld.f32 	%f826, [%rd1+192];
+	fma.rn.f32 	%f827, %f947, %f826, %f821;
+	ld.f32 	%f828, [%rd1+196];
+	fma.rn.f32 	%f829, %f947, %f828, %f823;
+	ld.f32 	%f830, [%rd1+200];
+	fma.rn.f32 	%f342, %f947, %f830, %f825;
+	ld.v4.f32 	{%f831, %f832, %f833, %f834}, [%rd1+160];
+	mul.f32 	%f838, %f1003, %f831;
+	mul.f32 	%f839, %f1003, %f832;
+	mul.f32 	%f840, %f1003, %f833;
+	fma.rn.f32 	%f841, %f1004, %f820, %f838;
+	fma.rn.f32 	%f842, %f1004, %f822, %f839;
+	fma.rn.f32 	%f843, %f1004, %f824, %f840;
+	fma.rn.f32 	%f844, %f1005, %f826, %f841;
+	fma.rn.f32 	%f845, %f1005, %f828, %f842;
+	fma.rn.f32 	%f343, %f1005, %f830, %f843;
+	mul.f32 	%f846, %f845, %f845;
+	fma.rn.f32 	%f344, %f844, %f844, %f846;
+	mul.f32 	%f847, %f829, %f845;
+	fma.rn.f32 	%f848, %f827, %f844, %f847;
+	add.f32 	%f345, %f848, %f848;
+	mul.f32 	%f849, %f829, %f829;
+	fma.rn.f32 	%f850, %f827, %f827, %f849;
+	ld.f32 	%f851, [%rd1+292];
+	mul.f32 	%f852, %f851, %f851;
+	sub.f32 	%f346, %f850, %f852;
+	setp.eq.f32 	%p23, %f344, 0f00000000;
+	setp.eq.f32 	%p24, %f345, 0f00000000;
 	and.pred  	%p25, %p23, %p24;
-	mov.u16 	%rs13, 0;
-	@%p25 bra 	BB0_45;
-
-	neg.f32 	%f829, %f319;
-	div.rn.f32 	%f963, %f829, %f318;
-	mul.f32 	%f830, %f317, 0fC0800000;
-	mul.f32 	%f831, %f830, %f319;
-	fma.rn.f32 	%f321, %f318, %f318, %f831;
-	setp.lt.f32	%p26, %f321, 0f00000000;
-	setp.neu.f32	%p27, %f317, 0f00000000;
-	and.pred  	%p28, %p26, %p27;
-	@%p28 bra 	BB0_43;
-	bra.uni 	BB0_44;
-
-BB0_43:
-	mov.f32 	%f962, %f963;
-	bra.uni 	BB0_45;
-
-BB0_44:
-	mov.b32 	 %r309, %f318;
-	and.b32  	%r310, %r309, -2147483648;
-	sqrt.rn.f32 	%f832, %f321;
-	mov.b32 	 %r311, %f832;
-	and.b32  	%r312, %r311, 2147483647;
-	or.b32  	%r313, %r312, %r310;
-	mov.b32 	 %f833, %r313;
-	add.f32 	%f834, %f318, %f833;
-	mul.f32 	%f835, %f834, 0fBF000000;
-	div.rn.f32 	%f836, %f835, %f317;
-	div.rn.f32 	%f837, %f319, %f835;
-	min.f32 	%f838, %f836, %f837;
-	max.f32 	%f839, %f836, %f837;
-	selp.f32	%f962, %f963, %f838, %p23;
-	selp.f32	%f963, %f963, %f839, %p23;
-	mov.u16 	%rs13, 1;
-
-BB0_45:
-	setp.gtu.f32	%p31, %f962, %f776;
-	mov.pred 	%p30, 0;
-	mov.pred 	%p52, %p30;
-	@%p31 bra 	BB0_47;
-
-	setp.ge.f32	%p52, %f963, %f775;
-
-BB0_47:
-	fma.rn.f32 	%f326, %f316, %f962, %f315;
-	fma.rn.f32 	%f327, %f316, %f963, %f315;
-	setp.geu.f32	%p33, %f962, %f775;
-	setp.leu.f32	%p34, %f963, %f776;
-	or.pred  	%p35, %p33, %p34;
-	setp.ne.s16	%p36, %rs13, 0;
-	and.pred  	%p37, %p52, %p36;
-	and.pred  	%p38, %p37, %p35;
-	mov.pred 	%p53, %p30;
-	@!%p38 bra 	BB0_54;
-	bra.uni 	BB0_48;
-
-BB0_48:
-	setp.ltu.f32	%p39, %f326, 0f00000000;
-	@%p39 bra 	BB0_50;
-
-	ld.f32 	%f840, [%rd1+288];
-	setp.le.f32	%p41, %f326, %f840;
-	setp.ge.f32	%p42, %f962, %f775;
+	mov.pred 	%p53, 0;
+	@%p25 bra 	$L__BB0_45;
+
+	neg.f32 	%f853, %f346;
+	div.rn.f32 	%f1006, %f853, %f345;
+	mul.f32 	%f854, %f344, 0fC0800000;
+	mul.f32 	%f855, %f854, %f346;
+	fma.rn.f32 	%f348, %f345, %f345, %f855;
+	setp.neu.f32 	%p27, %f344, 0f00000000;
+	setp.lt.f32 	%p28, %f348, 0f00000000;
+	and.pred  	%p29, %p28, %p27;
+	mov.f32 	%f1007, %f1006;
+	@%p29 bra 	$L__BB0_45;
+
+	mov.b32 	%r313, %f345;
+	and.b32  	%r314, %r313, -2147483648;
+	sqrt.rn.f32 	%f856, %f348;
+	mov.b32 	%r315, %f856;
+	and.b32  	%r316, %r315, 2147483647;
+	or.b32  	%r317, %r316, %r314;
+	mov.b32 	%f857, %r317;
+	add.f32 	%f858, %f345, %f857;
+	mul.f32 	%f859, %f858, 0fBF000000;
+	div.rn.f32 	%f860, %f859, %f344;
+	div.rn.f32 	%f861, %f346, %f859;
+	min.f32 	%f862, %f860, %f861;
+	max.f32 	%f863, %f860, %f861;
+	selp.f32 	%f349, %f1006, %f862, %p23;
+	selp.f32 	%f1007, %f1006, %f863, %p23;
 	mov.pred 	%p53, -1;
-	and.pred  	%p43, %p41, %p42;
-	@%p43 bra 	BB0_54;
-
-BB0_50:
-	setp.ltu.f32	%p45, %f327, 0f00000000;
-	@%p45 bra 	BB0_51;
-	bra.uni 	BB0_52;
-
-BB0_51:
-	mov.pred 	%p53, %p30;
-	bra.uni 	BB0_54;
-
-BB0_52:
-	ld.f32 	%f841, [%rd1+288];
-	setp.gtu.f32	%p47, %f327, %f841;
-	mov.pred 	%p53, %p30;
-	@%p47 bra 	BB0_54;
-
-	setp.le.f32	%p53, %f963, %f776;
-
-BB0_54:
-	setp.ltu.f32	%p48, %f326, 0f00000000;
-	@%p48 bra 	BB0_57;
-
-	ld.f32 	%f842, [%rd1+288];
-	setp.gtu.f32	%p49, %f326, %f842;
-	@%p49 bra 	BB0_57;
-
-	setp.ge.f32	%p50, %f962, %f775;
-	selp.f32	%f963, %f962, %f963, %p50;
-
-BB0_57:
-	@!%p53 bra 	BB0_59;
-	bra.uni 	BB0_58;
-
-BB0_58:
-	mov.u32 	%r315, 254;
-	// inline asm
-	call (%r314), _optix_report_intersection_0, (%f963, %r315);
-	// inline asm
-
-BB0_59:
+	mov.f32 	%f1006, %f349;
+
+$L__BB0_45:
+	fma.rn.f32 	%f353, %f343, %f1006, %f342;
+	fma.rn.f32 	%f354, %f343, %f1007, %f342;
+	setp.ge.f32 	%p32, %f1007, %f803;
+	setp.le.f32 	%p33, %f1006, %f804;
+	and.pred  	%p34, %p33, %p32;
+	and.pred  	%p35, %p53, %p34;
+	setp.leu.f32 	%p36, %f1007, %f804;
+	setp.geu.f32 	%p37, %f1006, %f803;
+	or.pred  	%p38, %p37, %p36;
+	and.pred  	%p39, %p35, %p38;
+	mov.u16 	%rs3, 0;
+	not.pred 	%p40, %p39;
+	mov.u16 	%rs7, %rs3;
+	@%p40 bra 	$L__BB0_51;
+
+	setp.ltu.f32 	%p41, %f353, 0f00000000;
+	@%p41 bra 	$L__BB0_48;
+
+	ld.f32 	%f864, [%rd16+80];
+	setp.le.f32 	%p42, %f353, %f864;
+	setp.ge.f32 	%p43, %f1006, %f803;
+	mov.u16 	%rs7, 1;
+	and.pred  	%p44, %p42, %p43;
+	@%p44 bra 	$L__BB0_51;
+
+$L__BB0_48:
+	setp.ltu.f32 	%p45, %f354, 0f00000000;
+	mov.u16 	%rs7, %rs3;
+	@%p45 bra 	$L__BB0_51;
+
+	ld.f32 	%f865, [%rd16+80];
+	setp.gtu.f32 	%p46, %f354, %f865;
+	mov.u16 	%rs7, %rs3;
+	@%p46 bra 	$L__BB0_51;
+
+	setp.le.f32 	%p47, %f1007, %f804;
+	selp.u16 	%rs7, 1, 0, %p47;
+
+$L__BB0_51:
+	setp.ltu.f32 	%p48, %f353, 0f00000000;
+	@%p48 bra 	$L__BB0_53;
+
+	ld.f32 	%f866, [%rd16+80];
+	setp.le.f32 	%p49, %f353, %f866;
+	setp.ge.f32 	%p50, %f1006, %f803;
+	and.pred  	%p51, %p49, %p50;
+	@%p51 bra 	$L__BB0_54;
+
+$L__BB0_53:
+	mov.f32 	%f1006, %f1007;
+
+$L__BB0_54:
+	setp.eq.s16 	%p52, %rs7, 0;
+	@%p52 bra 	$L__BB0_56;
+
+	mov.u32 	%r319, 254;
+	// begin inline asm
+	call (%r318), _optix_report_intersection_0, (%f1006, %r319);
+	// end inline asm
+
+$L__BB0_56:
 	ret;
-}
 
+}
 	// .globl	__closesthit__cylinder
-.visible .entry __closesthit__cylinder(
-
-)
+.visible .entry __closesthit__cylinder()
 {
-	.reg .pred 	%p<65>;
-	.reg .b16 	%rs<19>;
-	.reg .f32 	%f<2060>;
-	.reg .b32 	%r<650>;
-	.reg .b64 	%rd<665>;
-
-
-	// inline asm
-	call (%r23), _optix_get_launch_dimension_x, ();
-	// inline asm
-	// inline asm
-	call (%r24), _optix_get_launch_dimension_y, ();
-	// inline asm
-	// inline asm
-	call (%r26), _optix_get_launch_index_x, ();
-	// inline asm
-	// inline asm
-	call (%r27), _optix_get_launch_index_y, ();
-	// inline asm
-	// inline asm
-	call (%r28), _optix_get_launch_index_z, ();
-	// inline asm
-	mad.lo.s32 	%r29, %r28, %r24, %r27;
-	mad.lo.s32 	%r1, %r29, %r23, %r26;
+	.reg .pred 	%p<69>;
+	.reg .b16 	%rs<3>;
+	.reg .f32 	%f<2103>;
+	.reg .b32 	%r<660>;
+	.reg .b64 	%rd<652>;
+
+
+	// begin inline asm
+	call (%r27), _optix_get_launch_dimension_x, ();
+	// end inline asm
+	// begin inline asm
+	call (%r28), _optix_get_launch_dimension_y, ();
+	// end inline asm
+	// begin inline asm
+	call (%r30), _optix_get_launch_index_x, ();
+	// end inline asm
+	// begin inline asm
+	call (%r31), _optix_get_launch_index_y, ();
+	// end inline asm
+	// begin inline asm
+	call (%r32), _optix_get_launch_index_z, ();
+	// end inline asm
+	mad.lo.s32 	%r33, %r32, %r28, %r31;
+	mad.lo.s32 	%r1, %r33, %r27, %r30;
 	ld.const.u64 	%rd1, [params+352];
-	setp.eq.s64	%p1, %rd1, 0;
-	@%p1 bra 	BB1_2;
+	setp.eq.s64 	%p1, %rd1, 0;
+	@%p1 bra 	$L__BB1_2;
 
-	cvta.to.global.u64 	%rd48, %rd1;
-	cvt.u64.u32	%rd49, %r1;
-	add.s64 	%rd50, %rd48, %rd49;
+	cvta.to.global.u64 	%rd44, %rd1;
+	cvt.u64.u32 	%rd45, %r1;
+	add.s64 	%rd46, %rd44, %rd45;
 	mov.u16 	%rs1, 1;
-	st.global.u8 	[%rd50], %rs1;
-	bra.uni 	BB1_114;
-
-BB1_2:
-	// inline asm
-	call (%rd51), _optix_get_sbt_data_ptr_64, ();
-	// inline asm
-	ld.u64 	%rd3, [%rd51+8];
-	// inline asm
-	call (%f692), _optix_get_world_ray_origin_x, ();
-	// inline asm
-	// inline asm
-	call (%f693), _optix_get_world_ray_origin_y, ();
-	// inline asm
-	// inline asm
-	call (%f1849), _optix_get_world_ray_origin_z, ();
-	// inline asm
-	// inline asm
-	call (%r30), _optix_get_transform_list_size, ();
-	// inline asm
-	setp.eq.s32	%p2, %r30, 0;
-	@%p2 bra 	BB1_3;
-
-	mov.u32 	%r646, 0;
-	// inline asm
-	call (%f695), _optix_get_ray_time, ();
-	// inline asm
-
-BB1_5:
+	st.global.u8 	[%rd46], %rs1;
+	bra.uni 	$L__BB1_113;
+
+$L__BB1_2:
+	// begin inline asm
+	call (%rd47), _optix_get_sbt_data_ptr_64, ();
+	// end inline asm
+	ld.u64 	%rd3, [%rd47+8];
+	// begin inline asm
+	call (%f1883), _optix_get_world_ray_origin_x, ();
+	// end inline asm
+	// begin inline asm
+	call (%f1884), _optix_get_world_ray_origin_y, ();
+	// end inline asm
+	// begin inline asm
+	call (%f1885), _optix_get_world_ray_origin_z, ();
+	// end inline asm
+	// begin inline asm
+	call (%r34), _optix_get_transform_list_size, ();
+	// end inline asm
+	setp.eq.s32 	%p2, %r34, 0;
+	@%p2 bra 	$L__BB1_22;
+
+	// begin inline asm
+	call (%r35), _optix_get_transform_list_size, ();
+	// end inline asm
+	// begin inline asm
+	call (%f724), _optix_get_ray_time, ();
+	// end inline asm
+	setp.eq.s32 	%p3, %r35, 0;
+	@%p3 bra 	$L__BB1_21;
+
+	mov.u32 	%r655, 0;
+
+$L__BB1_5:
 	.pragma "nounroll";
-	// inline asm
-	call (%rd52), _optix_get_transform_list_handle, (%r646);
-	// inline asm
-	// inline asm
-	call (%r33), _optix_get_transform_type_from_handle, (%rd52);
-	// inline asm
-	and.b32  	%r34, %r33, -2;
-	setp.eq.s32	%p3, %r34, 2;
-	@%p3 bra 	BB1_11;
-	bra.uni 	BB1_6;
-
-BB1_11:
-	setp.eq.s32	%p6, %r33, 2;
-	@%p6 bra 	BB1_15;
-	bra.uni 	BB1_12;
-
-BB1_15:
-	// inline asm
-	call (%rd126), _optix_get_matrix_motion_transform_from_handle, (%rd52);
-	// inline asm
-	// inline asm
-	cvta.to.global.u64 %rd128, %rd126;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r122,%r123,%r124,%r125}, [%rd128];
-	// inline asm
-	mov.b32	{%rs4, %rs5}, %r124;
-	add.s64 	%rd132, %rd126, 16;
-	// inline asm
+	// begin inline asm
+	call (%rd48), _optix_get_transform_list_handle, (%r655);
+	// end inline asm
+	// begin inline asm
+	call (%r38), _optix_get_transform_type_from_handle, (%rd48);
+	// end inline asm
+	or.b32  	%r39, %r38, 1;
+	setp.eq.s32 	%p4, %r39, 3;
+	@%p4 bra 	$L__BB1_11;
+	bra.uni 	$L__BB1_6;
+
+$L__BB1_11:
+	setp.eq.s32 	%p7, %r38, 2;
+	@%p7 bra 	$L__BB1_15;
+	bra.uni 	$L__BB1_12;
+
+$L__BB1_15:
+	// begin inline asm
+	call (%rd120), _optix_get_matrix_motion_transform_from_handle, (%rd48);
+	// end inline asm
+	// begin inline asm
+	cvta.to.global.u64 %rd122, %rd120;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r127,%r128,%r129,%r130}, [%rd122];
+	// end inline asm
+	add.s64 	%rd126, %rd120, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd125, %rd126;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r131,%r132,%r133,%r134}, [%rd125];
+	// end inline asm
+	add.s64 	%rd129, %rd120, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd128, %rd129;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r135,%r136,%r137,%r138}, [%rd128];
+	// end inline asm
+	add.s64 	%rd132, %rd120, 48;
+	// begin inline asm
 	cvta.to.global.u64 %rd131, %rd132;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r126,%r127,%r128,%r129}, [%rd131];
-	// inline asm
-	add.s64 	%rd135, %rd126, 32;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r139,%r140,%r141,%r142}, [%rd131];
+	// end inline asm
+	add.s64 	%rd135, %rd120, 64;
+	// begin inline asm
 	cvta.to.global.u64 %rd134, %rd135;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r130,%r131,%r132,%r133}, [%rd134];
-	// inline asm
-	add.s64 	%rd138, %rd126, 48;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r143,%r144,%r145,%r146}, [%rd134];
+	// end inline asm
+	add.s64 	%rd138, %rd120, 80;
+	// begin inline asm
 	cvta.to.global.u64 %rd137, %rd138;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r134,%r135,%r136,%r137}, [%rd137];
-	// inline asm
-	add.s64 	%rd141, %rd126, 64;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r147,%r148,%r149,%r150}, [%rd137];
+	// end inline asm
+	add.s64 	%rd141, %rd120, 96;
+	// begin inline asm
 	cvta.to.global.u64 %rd140, %rd141;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r138,%r139,%r140,%r141}, [%rd140];
-	// inline asm
-	add.s64 	%rd144, %rd126, 80;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r151,%r152,%r153,%r154}, [%rd140];
+	// end inline asm
+	add.s64 	%rd144, %rd120, 112;
+	// begin inline asm
 	cvta.to.global.u64 %rd143, %rd144;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r142,%r143,%r144,%r145}, [%rd143];
-	// inline asm
-	add.s64 	%rd147, %rd126, 96;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r155,%r156,%r157,%r158}, [%rd143];
+	// end inline asm
+	mov.b32 	%f852, %r130;
+	mov.b32 	%f853, %r131;
+	and.b32  	%r171, %r129, 65535;
+	add.s32 	%r172, %r171, -1;
+	cvt.rn.f32.s32 	%f854, %r172;
+	sub.f32 	%f855, %f724, %f852;
+	mul.f32 	%f856, %f855, %f854;
+	sub.f32 	%f857, %f853, %f852;
+	div.rn.f32 	%f858, %f856, %f857;
+	min.f32 	%f859, %f854, %f858;
+	mov.f32 	%f860, 0f00000000;
+	max.f32 	%f861, %f860, %f859;
+	cvt.rmi.f32.f32 	%f862, %f861;
+	sub.f32 	%f90, %f861, %f862;
+	cvt.rzi.s32.f32 	%r173, %f862;
+	mul.wide.s32 	%rd155, %r173, 48;
+	add.s64 	%rd147, %rd129, %rd155;
+	// begin inline asm
 	cvta.to.global.u64 %rd146, %rd147;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r146,%r147,%r148,%r149}, [%rd146];
-	// inline asm
-	add.s64 	%rd150, %rd126, 112;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r159,%r160,%r161,%r162}, [%rd146];
+	// end inline asm
+	mov.b32 	%f1838, %r159;
+	mov.b32 	%f1837, %r160;
+	mov.b32 	%f1836, %r161;
+	mov.b32 	%f1835, %r162;
+	add.s64 	%rd150, %rd147, 16;
+	// begin inline asm
 	cvta.to.global.u64 %rd149, %rd150;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r150,%r151,%r152,%r153}, [%rd149];
-	// inline asm
-	mov.b32 	 %f822, %r125;
-	mov.b32 	 %f823, %r126;
-	cvt.u32.u16	%r166, %rs4;
-	add.s32 	%r167, %r166, -1;
-	cvt.rn.f32.s32	%f824, %r167;
-	sub.f32 	%f825, %f695, %f822;
-	mul.f32 	%f826, %f825, %f824;
-	sub.f32 	%f827, %f823, %f822;
-	div.rn.f32 	%f828, %f826, %f827;
-	min.f32 	%f829, %f824, %f828;
-	mov.f32 	%f830, 0f00000000;
-	max.f32 	%f831, %f830, %f829;
-	cvt.rmi.f32.f32	%f832, %f831;
-	cvt.rzi.s32.f32	%r168, %f832;
-	mul.wide.s32 	%rd161, %r168, 48;
-	add.s64 	%rd153, %rd135, %rd161;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r163,%r164,%r165,%r166}, [%rd149];
+	// end inline asm
+	mov.b32 	%f1842, %r163;
+	mov.b32 	%f1841, %r164;
+	mov.b32 	%f1840, %r165;
+	mov.b32 	%f1839, %r166;
+	add.s64 	%rd153, %rd147, 32;
+	// begin inline asm
 	cvta.to.global.u64 %rd152, %rd153;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r154,%r155,%r156,%r157}, [%rd152];
-	// inline asm
-	mov.b32 	 %f1816, %r154;
-	mov.b32 	 %f1815, %r155;
-	mov.b32 	 %f1814, %r156;
-	mov.b32 	 %f1813, %r157;
-	add.s64 	%rd156, %rd153, 16;
-	// inline asm
-	cvta.to.global.u64 %rd155, %rd156;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r158,%r159,%r160,%r161}, [%rd155];
-	// inline asm
-	mov.b32 	 %f1820, %r158;
-	mov.b32 	 %f1819, %r159;
-	mov.b32 	 %f1818, %r160;
-	mov.b32 	 %f1817, %r161;
-	add.s64 	%rd159, %rd153, 32;
-	// inline asm
-	cvta.to.global.u64 %rd158, %rd159;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r162,%r163,%r164,%r165}, [%rd158];
-	// inline asm
-	sub.f32 	%f98, %f831, %f832;
-	mov.b32 	 %f1824, %r162;
-	mov.b32 	 %f1823, %r163;
-	mov.b32 	 %f1822, %r164;
-	mov.b32 	 %f1821, %r165;
-	setp.leu.f32	%p8, %f98, 0f00000000;
-	@%p8 bra 	BB1_17;
-
-	cvt.rmi.f32.f32	%f1784, %f831;
-	cvt.rzi.s32.f32	%r645, %f1784;
-	cvt.s64.s32	%rd660, %r645;
-	mul.lo.s64 	%rd171, %rd660, 48;
-	add.s64 	%rd172, %rd126, %rd171;
-	add.s64 	%rd163, %rd172, 80;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r167,%r168,%r169,%r170}, [%rd152];
+	// end inline asm
+	mov.b32 	%f1846, %r167;
+	mov.b32 	%f1845, %r168;
+	mov.b32 	%f1844, %r169;
+	mov.b32 	%f1843, %r170;
+	setp.leu.f32 	%p9, %f90, 0f00000000;
+	@%p9 bra 	$L__BB1_17;
+
+	cvt.rmi.f32.f32 	%f1806, %f861;
+	cvt.rzi.s32.f32 	%r654, %f1806;
+	cvt.s64.s32 	%rd647, %r654;
+	mov.f32 	%f863, 0f3F800000;
+	sub.f32 	%f864, %f863, %f90;
+	mul.lo.s64 	%rd165, %rd647, 48;
+	add.s64 	%rd166, %rd120, %rd165;
+	add.s64 	%rd157, %rd166, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd156, %rd157;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r174,%r175,%r176,%r177}, [%rd156];
+	// end inline asm
+	mov.b32 	%f865, %r174;
+	mov.b32 	%f866, %r175;
+	mov.b32 	%f867, %r176;
+	mov.b32 	%f868, %r177;
+	mul.f32 	%f869, %f90, %f865;
+	mul.f32 	%f870, %f90, %f866;
+	mul.f32 	%f871, %f90, %f867;
+	mul.f32 	%f872, %f90, %f868;
+	fma.rn.f32 	%f1838, %f864, %f1838, %f869;
+	fma.rn.f32 	%f1837, %f864, %f1837, %f870;
+	fma.rn.f32 	%f1836, %f864, %f1836, %f871;
+	fma.rn.f32 	%f1835, %f864, %f1835, %f872;
+	add.s64 	%rd160, %rd166, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd159, %rd160;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r178,%r179,%r180,%r181}, [%rd159];
+	// end inline asm
+	mov.b32 	%f873, %r178;
+	mov.b32 	%f874, %r179;
+	mov.b32 	%f875, %r180;
+	mov.b32 	%f876, %r181;
+	mul.f32 	%f877, %f90, %f873;
+	mul.f32 	%f878, %f90, %f874;
+	mul.f32 	%f879, %f90, %f875;
+	mul.f32 	%f880, %f90, %f876;
+	fma.rn.f32 	%f1842, %f864, %f1842, %f877;
+	fma.rn.f32 	%f1841, %f864, %f1841, %f878;
+	fma.rn.f32 	%f1840, %f864, %f1840, %f879;
+	fma.rn.f32 	%f1839, %f864, %f1839, %f880;
+	add.s64 	%rd163, %rd166, 112;
+	// begin inline asm
 	cvta.to.global.u64 %rd162, %rd163;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r169,%r170,%r171,%r172}, [%rd162];
-	// inline asm
-	mov.b32 	 %f833, %r169;
-	mov.b32 	 %f834, %r170;
-	mov.b32 	 %f835, %r171;
-	mov.b32 	 %f836, %r172;
-	add.s64 	%rd166, %rd172, 96;
-	// inline asm
-	cvta.to.global.u64 %rd165, %rd166;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r173,%r174,%r175,%r176}, [%rd165];
-	// inline asm
-	mov.b32 	 %f837, %r173;
-	mov.b32 	 %f838, %r174;
-	mov.b32 	 %f839, %r175;
-	mov.b32 	 %f840, %r176;
-	add.s64 	%rd169, %rd172, 112;
-	// inline asm
-	cvta.to.global.u64 %rd168, %rd169;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r177,%r178,%r179,%r180}, [%rd168];
-	// inline asm
-	mov.f32 	%f841, 0f3F800000;
-	sub.f32 	%f842, %f841, %f98;
-	mul.f32 	%f843, %f98, %f833;
-	mul.f32 	%f844, %f98, %f834;
-	mul.f32 	%f845, %f98, %f835;
-	mul.f32 	%f846, %f98, %f836;
-	fma.rn.f32 	%f1816, %f842, %f1816, %f843;
-	fma.rn.f32 	%f1815, %f842, %f1815, %f844;
-	fma.rn.f32 	%f1814, %f842, %f1814, %f845;
-	fma.rn.f32 	%f1813, %f842, %f1813, %f846;
-	mul.f32 	%f847, %f98, %f837;
-	mul.f32 	%f848, %f98, %f838;
-	mul.f32 	%f849, %f98, %f839;
-	mul.f32 	%f850, %f98, %f840;
-	fma.rn.f32 	%f1820, %f842, %f1820, %f847;
-	fma.rn.f32 	%f1819, %f842, %f1819, %f848;
-	fma.rn.f32 	%f1818, %f842, %f1818, %f849;
-	fma.rn.f32 	%f1817, %f842, %f1817, %f850;
-	mov.b32 	 %f851, %r177;
-	mov.b32 	 %f852, %r178;
-	mov.b32 	 %f853, %r179;
-	mov.b32 	 %f854, %r180;
-	mul.f32 	%f855, %f98, %f851;
-	mul.f32 	%f856, %f98, %f852;
-	mul.f32 	%f857, %f98, %f853;
-	mul.f32 	%f858, %f98, %f854;
-	fma.rn.f32 	%f1824, %f842, %f1824, %f855;
-	fma.rn.f32 	%f1823, %f842, %f1823, %f856;
-	fma.rn.f32 	%f1822, %f842, %f1822, %f857;
-	fma.rn.f32 	%f1821, %f842, %f1821, %f858;
-	bra.uni 	BB1_17;
-
-BB1_6:
-	mov.f32 	%f1825, 0f00000000;
-	mov.f32 	%f1828, 0f3F800000;
-	setp.eq.s32	%p4, %r33, 4;
-	@%p4 bra 	BB1_9;
-	bra.uni 	BB1_7;
-
-BB1_9:
-	// inline asm
-	call (%rd661), _optix_get_instance_inverse_transform_from_handle, (%rd52);
-	// inline asm
-	bra.uni 	BB1_10;
-
-BB1_12:
-	// inline asm
-	call (%rd67), _optix_get_srt_motion_transform_from_handle, (%rd52);
-	// inline asm
-	// inline asm
-	cvta.to.global.u64 %rd69, %rd67;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r47,%r48,%r49,%r50}, [%rd69];
-	// inline asm
-	mov.b32	{%rs2, %rs3}, %r49;
-	add.s64 	%rd73, %rd67, 16;
-	// inline asm
-	cvta.to.global.u64 %rd72, %rd73;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r51,%r52,%r53,%r54}, [%rd72];
-	// inline asm
-	add.s64 	%rd76, %rd67, 32;
-	// inline asm
-	cvta.to.global.u64 %rd75, %rd76;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r55,%r56,%r57,%r58}, [%rd75];
-	// inline asm
-	add.s64 	%rd79, %rd67, 48;
-	// inline asm
-	cvta.to.global.u64 %rd78, %rd79;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r59,%r60,%r61,%r62}, [%rd78];
-	// inline asm
-	add.s64 	%rd82, %rd67, 64;
-	// inline asm
-	cvta.to.global.u64 %rd81, %rd82;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r63,%r64,%r65,%r66}, [%rd81];
-	// inline asm
-	add.s64 	%rd85, %rd67, 80;
-	// inline asm
-	cvta.to.global.u64 %rd84, %rd85;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r67,%r68,%r69,%r70}, [%rd84];
-	// inline asm
-	add.s64 	%rd88, %rd67, 96;
-	// inline asm
-	cvta.to.global.u64 %rd87, %rd88;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r71,%r72,%r73,%r74}, [%rd87];
-	// inline asm
-	add.s64 	%rd91, %rd67, 112;
-	// inline asm
-	cvta.to.global.u64 %rd90, %rd91;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r75,%r76,%r77,%r78}, [%rd90];
-	// inline asm
-	add.s64 	%rd94, %rd67, 128;
-	// inline asm
-	cvta.to.global.u64 %rd93, %rd94;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r79,%r80,%r81,%r82}, [%rd93];
-	// inline asm
-	add.s64 	%rd97, %rd67, 144;
-	// inline asm
-	cvta.to.global.u64 %rd96, %rd97;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r83,%r84,%r85,%r86}, [%rd96];
-	// inline asm
-	mov.b32 	 %f709, %r50;
-	mov.b32 	 %f710, %r51;
-	cvt.u32.u16	%r103, %rs2;
-	add.s32 	%r104, %r103, -1;
-	cvt.rn.f32.s32	%f711, %r104;
-	sub.f32 	%f712, %f695, %f709;
-	mul.f32 	%f713, %f712, %f711;
-	sub.f32 	%f714, %f710, %f709;
-	div.rn.f32 	%f715, %f713, %f714;
-	min.f32 	%f716, %f711, %f715;
-	mov.f32 	%f717, 0f00000000;
-	max.f32 	%f718, %f717, %f716;
-	cvt.rmi.f32.f32	%f719, %f718;
-	cvt.rzi.s32.f32	%r105, %f719;
-	mul.wide.s32 	%rd111, %r105, 64;
-	add.s64 	%rd100, %rd76, %rd111;
-	// inline asm
-	cvta.to.global.u64 %rd99, %rd100;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r87,%r88,%r89,%r90}, [%rd99];
-	// inline asm
-	mov.b32 	 %f1797, %r87;
-	mov.b32 	 %f1798, %r88;
-	mov.b32 	 %f1799, %r89;
-	mov.b32 	 %f1800, %r90;
-	add.s64 	%rd103, %rd100, 16;
-	// inline asm
-	cvta.to.global.u64 %rd102, %rd103;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r91,%r92,%r93,%r94}, [%rd102];
-	// inline asm
-	mov.b32 	 %f1801, %r91;
-	mov.b32 	 %f1802, %r92;
-	mov.b32 	 %f1803, %r93;
-	mov.b32 	 %f1804, %r94;
-	add.s64 	%rd106, %rd100, 32;
-	// inline asm
-	cvta.to.global.u64 %rd105, %rd106;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r95,%r96,%r97,%r98}, [%rd105];
-	// inline asm
-	sub.f32 	%f37, %f718, %f719;
-	mov.b32 	 %f1805, %r95;
-	mov.b32 	 %f1806, %r96;
-	mov.b32 	 %f1807, %r97;
-	mov.b32 	 %f1808, %r98;
-	add.s64 	%rd109, %rd100, 48;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r182,%r183,%r184,%r185}, [%rd162];
+	// end inline asm
+	mov.b32 	%f881, %r182;
+	mov.b32 	%f882, %r183;
+	mov.b32 	%f883, %r184;
+	mov.b32 	%f884, %r185;
+	mul.f32 	%f885, %f90, %f881;
+	mul.f32 	%f886, %f90, %f882;
+	mul.f32 	%f887, %f90, %f883;
+	mul.f32 	%f888, %f90, %f884;
+	fma.rn.f32 	%f1846, %f864, %f1846, %f885;
+	fma.rn.f32 	%f1845, %f864, %f1845, %f886;
+	fma.rn.f32 	%f1844, %f864, %f1844, %f887;
+	fma.rn.f32 	%f1843, %f864, %f1843, %f888;
+	bra.uni 	$L__BB1_17;
+
+$L__BB1_6:
+	mov.f32 	%f1847, 0f00000000;
+	mov.f32 	%f1850, 0f3F800000;
+	setp.eq.s32 	%p5, %r38, 4;
+	@%p5 bra 	$L__BB1_9;
+
+	setp.ne.s32 	%p6, %r38, 1;
+	mov.f32 	%f1848, %f1847;
+	mov.f32 	%f1849, %f1847;
+	mov.f32 	%f1851, %f1847;
+	mov.f32 	%f1852, %f1847;
+	mov.f32 	%f1853, %f1850;
+	mov.f32 	%f1854, %f1847;
+	mov.f32 	%f1855, %f1847;
+	mov.f32 	%f1856, %f1850;
+	mov.f32 	%f1857, %f1847;
+	mov.f32 	%f1858, %f1847;
+	@%p6 bra 	$L__BB1_18;
+
+	// begin inline asm
+	call (%rd50), _optix_get_static_transform_from_handle, (%rd48);
+	// end inline asm
+	add.s64 	%rd648, %rd50, 64;
+	bra.uni 	$L__BB1_10;
+
+$L__BB1_12:
+	// begin inline asm
+	call (%rd63), _optix_get_srt_motion_transform_from_handle, (%rd48);
+	// end inline asm
+	// begin inline asm
+	cvta.to.global.u64 %rd65, %rd63;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r52,%r53,%r54,%r55}, [%rd65];
+	// end inline asm
+	add.s64 	%rd69, %rd63, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd68, %rd69;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r56,%r57,%r58,%r59}, [%rd68];
+	// end inline asm
+	add.s64 	%rd72, %rd63, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd71, %rd72;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r60,%r61,%r62,%r63}, [%rd71];
+	// end inline asm
+	add.s64 	%rd75, %rd63, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd74, %rd75;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r64,%r65,%r66,%r67}, [%rd74];
+	// end inline asm
+	add.s64 	%rd78, %rd63, 64;
+	// begin inline asm
+	cvta.to.global.u64 %rd77, %rd78;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r68,%r69,%r70,%r71}, [%rd77];
+	// end inline asm
+	add.s64 	%rd81, %rd63, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd80, %rd81;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r72,%r73,%r74,%r75}, [%rd80];
+	// end inline asm
+	add.s64 	%rd84, %rd63, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd83, %rd84;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r76,%r77,%r78,%r79}, [%rd83];
+	// end inline asm
+	add.s64 	%rd87, %rd63, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd86, %rd87;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r80,%r81,%r82,%r83}, [%rd86];
+	// end inline asm
+	add.s64 	%rd90, %rd63, 128;
+	// begin inline asm
+	cvta.to.global.u64 %rd89, %rd90;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r84,%r85,%r86,%r87}, [%rd89];
+	// end inline asm
+	add.s64 	%rd93, %rd63, 144;
+	// begin inline asm
+	cvta.to.global.u64 %rd92, %rd93;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r88,%r89,%r90,%r91}, [%rd92];
+	// end inline asm
+	mov.b32 	%f739, %r55;
+	mov.b32 	%f740, %r56;
+	and.b32  	%r108, %r54, 65535;
+	add.s32 	%r109, %r108, -1;
+	cvt.rn.f32.s32 	%f741, %r109;
+	sub.f32 	%f742, %f724, %f739;
+	mul.f32 	%f743, %f742, %f741;
+	sub.f32 	%f744, %f740, %f739;
+	div.rn.f32 	%f745, %f743, %f744;
+	min.f32 	%f746, %f741, %f745;
+	mov.f32 	%f747, 0f00000000;
+	max.f32 	%f748, %f747, %f746;
+	cvt.rmi.f32.f32 	%f749, %f748;
+	sub.f32 	%f29, %f748, %f749;
+	cvt.rzi.s32.f32 	%r110, %f749;
+	mul.wide.s32 	%rd107, %r110, 64;
+	add.s64 	%rd96, %rd72, %rd107;
+	// begin inline asm
+	cvta.to.global.u64 %rd95, %rd96;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r92,%r93,%r94,%r95}, [%rd95];
+	// end inline asm
+	mov.b32 	%f1819, %r92;
+	mov.b32 	%f1820, %r93;
+	mov.b32 	%f1821, %r94;
+	mov.b32 	%f1822, %r95;
+	add.s64 	%rd99, %rd96, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd98, %rd99;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r96,%r97,%r98,%r99}, [%rd98];
+	// end inline asm
+	mov.b32 	%f1823, %r96;
+	mov.b32 	%f1824, %r97;
+	mov.b32 	%f1825, %r98;
+	mov.b32 	%f1826, %r99;
+	add.s64 	%rd102, %rd96, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd101, %rd102;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r100,%r101,%r102,%r103}, [%rd101];
+	// end inline asm
+	mov.b32 	%f1827, %r100;
+	mov.b32 	%f1828, %r101;
+	mov.b32 	%f1829, %r102;
+	mov.b32 	%f1830, %r103;
+	add.s64 	%rd105, %rd96, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd104, %rd105;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r104,%r105,%r106,%r107}, [%rd104];
+	// end inline asm
+	mov.b32 	%f1831, %r104;
+	mov.b32 	%f1832, %r105;
+	mov.b32 	%f1833, %r106;
+	mov.b32 	%f1834, %r107;
+	setp.leu.f32 	%p8, %f29, 0f00000000;
+	@%p8 bra 	$L__BB1_14;
+
+	mov.f32 	%f750, 0f3F800000;
+	sub.f32 	%f751, %f750, %f29;
+	add.s64 	%rd109, %rd96, 64;
+	// begin inline asm
 	cvta.to.global.u64 %rd108, %rd109;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r99,%r100,%r101,%r102}, [%rd108];
-	// inline asm
-	mov.b32 	 %f1809, %r99;
-	mov.b32 	 %f1810, %r100;
-	mov.b32 	 %f1811, %r101;
-	mov.b32 	 %f1812, %r102;
-	setp.leu.f32	%p7, %f37, 0f00000000;
-	@%p7 bra 	BB1_14;
-
-	cvt.rmi.f32.f32	%f1783, %f718;
-	cvt.rzi.s32.f32	%r644, %f1783;
-	cvt.s64.s32	%rd659, %r644;
-	shl.b64 	%rd124, %rd659, 6;
-	add.s64 	%rd125, %rd124, %rd67;
-	add.s64 	%rd113, %rd125, 96;
-	// inline asm
-	cvta.to.global.u64 %rd112, %rd113;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r106,%r107,%r108,%r109}, [%rd112];
-	// inline asm
-	mov.b32 	 %f720, %r106;
-	mov.b32 	 %f721, %r107;
-	mov.b32 	 %f722, %r108;
-	mov.b32 	 %f723, %r109;
-	add.s64 	%rd116, %rd125, 112;
-	// inline asm
-	cvta.to.global.u64 %rd115, %rd116;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r110,%r111,%r112,%r113}, [%rd115];
-	// inline asm
-	mov.b32 	 %f724, %r110;
-	mov.b32 	 %f725, %r111;
-	mov.b32 	 %f726, %r112;
-	mov.b32 	 %f727, %r113;
-	add.s64 	%rd119, %rd125, 128;
-	// inline asm
-	cvta.to.global.u64 %rd118, %rd119;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r114,%r115,%r116,%r117}, [%rd118];
-	// inline asm
-	mov.b32 	 %f728, %r114;
-	mov.b32 	 %f729, %r115;
-	mov.b32 	 %f730, %r116;
-	mov.b32 	 %f731, %r117;
-	add.s64 	%rd122, %rd125, 144;
-	// inline asm
-	cvta.to.global.u64 %rd121, %rd122;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r118,%r119,%r120,%r121}, [%rd121];
-	// inline asm
-	mov.f32 	%f732, 0f3F800000;
-	sub.f32 	%f733, %f732, %f37;
-	mul.f32 	%f734, %f37, %f720;
-	mul.f32 	%f735, %f37, %f721;
-	mul.f32 	%f736, %f37, %f722;
-	mul.f32 	%f737, %f37, %f723;
-	fma.rn.f32 	%f1797, %f733, %f1797, %f734;
-	fma.rn.f32 	%f1798, %f733, %f1798, %f735;
-	fma.rn.f32 	%f1799, %f733, %f1799, %f736;
-	fma.rn.f32 	%f1800, %f733, %f1800, %f737;
-	mul.f32 	%f738, %f37, %f724;
-	mul.f32 	%f739, %f37, %f725;
-	mul.f32 	%f740, %f37, %f726;
-	mul.f32 	%f741, %f37, %f727;
-	fma.rn.f32 	%f1801, %f733, %f1801, %f738;
-	fma.rn.f32 	%f1802, %f733, %f1802, %f739;
-	fma.rn.f32 	%f1803, %f733, %f1803, %f740;
-	fma.rn.f32 	%f1804, %f733, %f1804, %f741;
-	mul.f32 	%f742, %f37, %f728;
-	mul.f32 	%f743, %f37, %f729;
-	mul.f32 	%f744, %f37, %f730;
-	mul.f32 	%f745, %f37, %f731;
-	fma.rn.f32 	%f1805, %f733, %f1805, %f742;
-	fma.rn.f32 	%f746, %f733, %f1806, %f743;
-	fma.rn.f32 	%f747, %f733, %f1807, %f744;
-	fma.rn.f32 	%f748, %f733, %f1808, %f745;
-	mov.b32 	 %f749, %r118;
-	mov.b32 	 %f750, %r119;
-	mov.b32 	 %f751, %r120;
-	mov.b32 	 %f752, %r121;
-	mul.f32 	%f753, %f37, %f749;
-	mul.f32 	%f754, %f37, %f750;
-	mul.f32 	%f755, %f37, %f751;
-	mul.f32 	%f756, %f37, %f752;
-	fma.rn.f32 	%f757, %f733, %f1809, %f753;
-	fma.rn.f32 	%f1810, %f733, %f1810, %f754;
-	fma.rn.f32 	%f1811, %f733, %f1811, %f755;
-	fma.rn.f32 	%f1812, %f733, %f1812, %f756;
-	mul.f32 	%f758, %f747, %f747;
-	fma.rn.f32 	%f759, %f746, %f746, %f758;
-	fma.rn.f32 	%f760, %f748, %f748, %f759;
-	fma.rn.f32 	%f761, %f757, %f757, %f760;
-	sqrt.rn.f32 	%f762, %f761;
-	rcp.rn.f32 	%f763, %f762;
-	mul.f32 	%f1806, %f746, %f763;
-	mul.f32 	%f1807, %f747, %f763;
-	mul.f32 	%f1808, %f748, %f763;
-	mul.f32 	%f1809, %f757, %f763;
-
-BB1_14:
-	mul.f32 	%f764, %f1807, %f1807;
-	fma.rn.f32 	%f765, %f1806, %f1806, %f764;
-	fma.rn.f32 	%f766, %f1808, %f1808, %f765;
-	fma.rn.f32 	%f767, %f1809, %f1809, %f766;
-	rcp.rn.f32 	%f768, %f767;
-	mul.f32 	%f769, %f1806, %f768;
-	mul.f32 	%f770, %f1807, %f768;
-	mul.f32 	%f771, %f1808, %f768;
-	mul.f32 	%f772, %f1809, %f768;
-	mul.f32 	%f773, %f1806, %f769;
-	mul.f32 	%f774, %f1807, %f770;
-	mul.f32 	%f775, %f1808, %f771;
-	mul.f32 	%f776, %f1806, %f770;
-	mul.f32 	%f777, %f1808, %f772;
-	mul.f32 	%f778, %f1806, %f771;
-	mul.f32 	%f779, %f1807, %f772;
-	mul.f32 	%f780, %f1807, %f771;
-	mul.f32 	%f781, %f1806, %f772;
-	sub.f32 	%f782, %f773, %f774;
-	sub.f32 	%f783, %f782, %f775;
-	fma.rn.f32 	%f784, %f1809, %f772, %f783;
-	sub.f32 	%f785, %f776, %f777;
-	add.f32 	%f786, %f785, %f785;
-	add.f32 	%f787, %f778, %f779;
-	add.f32 	%f788, %f787, %f787;
-	add.f32 	%f789, %f776, %f777;
-	add.f32 	%f790, %f789, %f789;
-	sub.f32 	%f791, %f774, %f773;
-	sub.f32 	%f792, %f791, %f775;
-	fma.rn.f32 	%f793, %f1809, %f772, %f792;
-	sub.f32 	%f794, %f780, %f781;
-	add.f32 	%f795, %f794, %f794;
-	sub.f32 	%f796, %f778, %f779;
-	add.f32 	%f797, %f796, %f796;
-	add.f32 	%f798, %f780, %f781;
-	add.f32 	%f799, %f798, %f798;
-	neg.f32 	%f800, %f773;
-	sub.f32 	%f801, %f800, %f774;
-	add.f32 	%f802, %f775, %f801;
-	fma.rn.f32 	%f803, %f1809, %f772, %f802;
-	mul.f32 	%f804, %f1800, %f784;
-	fma.rn.f32 	%f805, %f1803, %f786, %f804;
-	fma.rn.f32 	%f806, %f1805, %f788, %f805;
-	sub.f32 	%f1813, %f1810, %f806;
-	mul.f32 	%f807, %f1803, %f793;
-	fma.rn.f32 	%f808, %f1800, %f790, %f807;
-	fma.rn.f32 	%f809, %f1805, %f795, %f808;
-	sub.f32 	%f1817, %f1811, %f809;
-	mul.f32 	%f810, %f1803, %f799;
-	fma.rn.f32 	%f811, %f1800, %f797, %f810;
-	fma.rn.f32 	%f812, %f1805, %f803, %f811;
-	sub.f32 	%f1821, %f1812, %f812;
-	mul.f32 	%f813, %f1799, %f784;
-	fma.rn.f32 	%f814, %f1802, %f786, %f813;
-	fma.rn.f32 	%f1814, %f1804, %f788, %f814;
-	mul.f32 	%f815, %f1802, %f793;
-	fma.rn.f32 	%f816, %f1799, %f790, %f815;
-	fma.rn.f32 	%f1818, %f1804, %f795, %f816;
-	mul.f32 	%f817, %f1802, %f799;
-	fma.rn.f32 	%f818, %f1799, %f797, %f817;
-	fma.rn.f32 	%f1822, %f1804, %f803, %f818;
-	mul.f32 	%f819, %f1798, %f784;
-	fma.rn.f32 	%f1815, %f1801, %f786, %f819;
-	mul.f32 	%f820, %f1801, %f793;
-	fma.rn.f32 	%f1819, %f1798, %f790, %f820;
-	mul.f32 	%f821, %f1801, %f799;
-	fma.rn.f32 	%f1823, %f1798, %f797, %f821;
-	mul.f32 	%f1816, %f1797, %f784;
-	mul.f32 	%f1820, %f1797, %f790;
-	mul.f32 	%f1824, %f1797, %f797;
-
-BB1_17:
-	mul.f32 	%f859, %f1818, %f1823;
-	mul.f32 	%f860, %f1819, %f1822;
-	sub.f32 	%f861, %f860, %f859;
-	mul.f32 	%f862, %f1816, %f861;
-	mul.f32 	%f863, %f1818, %f1824;
-	mul.f32 	%f864, %f1820, %f1822;
-	sub.f32 	%f865, %f864, %f863;
-	mul.f32 	%f866, %f1815, %f865;
-	sub.f32 	%f867, %f862, %f866;
-	mul.f32 	%f868, %f1819, %f1824;
-	mul.f32 	%f869, %f1820, %f1823;
-	sub.f32 	%f870, %f869, %f868;
-	fma.rn.f32 	%f871, %f1814, %f870, %f867;
-	rcp.rn.f32 	%f872, %f871;
-	mul.f32 	%f1828, %f872, %f861;
-	mul.f32 	%f873, %f1815, %f1822;
-	mul.f32 	%f874, %f1814, %f1823;
-	sub.f32 	%f875, %f874, %f873;
-	mul.f32 	%f1827, %f872, %f875;
-	mul.f32 	%f876, %f1814, %f1819;
-	mul.f32 	%f877, %f1815, %f1818;
-	sub.f32 	%f878, %f877, %f876;
-	mul.f32 	%f1826, %f878, %f872;
-	sub.f32 	%f879, %f863, %f864;
-	mul.f32 	%f1832, %f872, %f879;
-	mul.f32 	%f880, %f1814, %f1824;
-	mul.f32 	%f881, %f1816, %f1822;
-	sub.f32 	%f882, %f881, %f880;
-	mul.f32 	%f1831, %f872, %f882;
-	mul.f32 	%f883, %f1816, %f1818;
-	mul.f32 	%f884, %f1814, %f1820;
-	sub.f32 	%f885, %f884, %f883;
-	mul.f32 	%f1830, %f885, %f872;
-	mul.f32 	%f1836, %f872, %f870;
-	mul.f32 	%f886, %f1816, %f1823;
-	mul.f32 	%f887, %f1815, %f1824;
-	sub.f32 	%f888, %f887, %f886;
-	mul.f32 	%f1835, %f872, %f888;
-	mul.f32 	%f889, %f1815, %f1820;
-	mul.f32 	%f890, %f1816, %f1819;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r111,%r112,%r113,%r114}, [%rd108];
+	// end inline asm
+	mov.b32 	%f752, %r111;
+	mov.b32 	%f753, %r112;
+	mov.b32 	%f754, %r113;
+	mov.b32 	%f755, %r114;
+	mul.f32 	%f756, %f29, %f752;
+	mul.f32 	%f757, %f29, %f753;
+	mul.f32 	%f758, %f29, %f754;
+	mul.f32 	%f759, %f29, %f755;
+	fma.rn.f32 	%f1819, %f751, %f1819, %f756;
+	fma.rn.f32 	%f1820, %f751, %f1820, %f757;
+	fma.rn.f32 	%f1821, %f751, %f1821, %f758;
+	fma.rn.f32 	%f1822, %f751, %f1822, %f759;
+	add.s64 	%rd112, %rd96, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd111, %rd112;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r115,%r116,%r117,%r118}, [%rd111];
+	// end inline asm
+	mov.b32 	%f760, %r115;
+	mov.b32 	%f761, %r116;
+	mov.b32 	%f762, %r117;
+	mov.b32 	%f763, %r118;
+	mul.f32 	%f764, %f29, %f760;
+	mul.f32 	%f765, %f29, %f761;
+	mul.f32 	%f766, %f29, %f762;
+	mul.f32 	%f767, %f29, %f763;
+	fma.rn.f32 	%f1823, %f751, %f1823, %f764;
+	fma.rn.f32 	%f1824, %f751, %f1824, %f765;
+	fma.rn.f32 	%f1825, %f751, %f1825, %f766;
+	fma.rn.f32 	%f1826, %f751, %f1826, %f767;
+	add.s64 	%rd115, %rd96, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd114, %rd115;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r119,%r120,%r121,%r122}, [%rd114];
+	// end inline asm
+	mov.b32 	%f768, %r119;
+	mov.b32 	%f769, %r120;
+	mov.b32 	%f770, %r121;
+	mov.b32 	%f771, %r122;
+	mul.f32 	%f772, %f29, %f768;
+	mul.f32 	%f773, %f29, %f769;
+	mul.f32 	%f774, %f29, %f770;
+	mul.f32 	%f775, %f29, %f771;
+	fma.rn.f32 	%f1827, %f751, %f1827, %f772;
+	fma.rn.f32 	%f776, %f751, %f1828, %f773;
+	fma.rn.f32 	%f777, %f751, %f1829, %f774;
+	fma.rn.f32 	%f778, %f751, %f1830, %f775;
+	add.s64 	%rd118, %rd96, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd117, %rd118;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r123,%r124,%r125,%r126}, [%rd117];
+	// end inline asm
+	mov.b32 	%f779, %r123;
+	mov.b32 	%f780, %r124;
+	mov.b32 	%f781, %r125;
+	mov.b32 	%f782, %r126;
+	mul.f32 	%f783, %f29, %f779;
+	mul.f32 	%f784, %f29, %f780;
+	mul.f32 	%f785, %f29, %f781;
+	mul.f32 	%f786, %f29, %f782;
+	fma.rn.f32 	%f787, %f751, %f1831, %f783;
+	fma.rn.f32 	%f1832, %f751, %f1832, %f784;
+	fma.rn.f32 	%f1833, %f751, %f1833, %f785;
+	fma.rn.f32 	%f1834, %f751, %f1834, %f786;
+	mul.f32 	%f788, %f777, %f777;
+	fma.rn.f32 	%f789, %f776, %f776, %f788;
+	fma.rn.f32 	%f790, %f778, %f778, %f789;
+	fma.rn.f32 	%f791, %f787, %f787, %f790;
+	sqrt.rn.f32 	%f792, %f791;
+	rcp.rn.f32 	%f793, %f792;
+	mul.f32 	%f1828, %f776, %f793;
+	mul.f32 	%f1829, %f777, %f793;
+	mul.f32 	%f1830, %f778, %f793;
+	mul.f32 	%f1831, %f793, %f787;
+
+$L__BB1_14:
+	mul.f32 	%f794, %f1829, %f1829;
+	fma.rn.f32 	%f795, %f1828, %f1828, %f794;
+	fma.rn.f32 	%f796, %f1830, %f1830, %f795;
+	fma.rn.f32 	%f797, %f1831, %f1831, %f796;
+	rcp.rn.f32 	%f798, %f797;
+	mul.f32 	%f799, %f1828, %f798;
+	mul.f32 	%f800, %f1829, %f798;
+	mul.f32 	%f801, %f1830, %f798;
+	mul.f32 	%f802, %f1831, %f798;
+	mul.f32 	%f803, %f1828, %f799;
+	mul.f32 	%f804, %f1829, %f800;
+	mul.f32 	%f805, %f1830, %f801;
+	mul.f32 	%f806, %f1828, %f800;
+	mul.f32 	%f807, %f1830, %f802;
+	mul.f32 	%f808, %f1828, %f801;
+	mul.f32 	%f809, %f1829, %f802;
+	mul.f32 	%f810, %f1829, %f801;
+	mul.f32 	%f811, %f1828, %f802;
+	sub.f32 	%f812, %f803, %f804;
+	sub.f32 	%f813, %f812, %f805;
+	fma.rn.f32 	%f814, %f1831, %f802, %f813;
+	sub.f32 	%f815, %f806, %f807;
+	add.f32 	%f816, %f815, %f815;
+	add.f32 	%f817, %f808, %f809;
+	add.f32 	%f818, %f817, %f817;
+	add.f32 	%f819, %f806, %f807;
+	add.f32 	%f820, %f819, %f819;
+	sub.f32 	%f821, %f804, %f803;
+	sub.f32 	%f822, %f821, %f805;
+	fma.rn.f32 	%f823, %f1831, %f802, %f822;
+	sub.f32 	%f824, %f810, %f811;
+	add.f32 	%f825, %f824, %f824;
+	sub.f32 	%f826, %f808, %f809;
+	add.f32 	%f827, %f826, %f826;
+	add.f32 	%f828, %f810, %f811;
+	add.f32 	%f829, %f828, %f828;
+	neg.f32 	%f830, %f803;
+	sub.f32 	%f831, %f830, %f804;
+	add.f32 	%f832, %f805, %f831;
+	fma.rn.f32 	%f833, %f1831, %f802, %f832;
+	mul.f32 	%f834, %f1822, %f814;
+	fma.rn.f32 	%f835, %f1825, %f816, %f834;
+	fma.rn.f32 	%f836, %f1827, %f818, %f835;
+	sub.f32 	%f1835, %f1832, %f836;
+	mul.f32 	%f837, %f1825, %f823;
+	fma.rn.f32 	%f838, %f1822, %f820, %f837;
+	fma.rn.f32 	%f839, %f1827, %f825, %f838;
+	sub.f32 	%f1839, %f1833, %f839;
+	mul.f32 	%f840, %f1825, %f829;
+	fma.rn.f32 	%f841, %f1822, %f827, %f840;
+	fma.rn.f32 	%f842, %f1827, %f833, %f841;
+	sub.f32 	%f1843, %f1834, %f842;
+	mul.f32 	%f843, %f1821, %f814;
+	fma.rn.f32 	%f844, %f1824, %f816, %f843;
+	fma.rn.f32 	%f1836, %f1826, %f818, %f844;
+	mul.f32 	%f845, %f1824, %f823;
+	fma.rn.f32 	%f846, %f1821, %f820, %f845;
+	fma.rn.f32 	%f1840, %f1826, %f825, %f846;
+	mul.f32 	%f847, %f1824, %f829;
+	fma.rn.f32 	%f848, %f1821, %f827, %f847;
+	fma.rn.f32 	%f1844, %f1826, %f833, %f848;
+	mul.f32 	%f849, %f1820, %f814;
+	fma.rn.f32 	%f1837, %f1823, %f816, %f849;
+	mul.f32 	%f850, %f1823, %f823;
+	fma.rn.f32 	%f1841, %f1820, %f820, %f850;
+	mul.f32 	%f851, %f1823, %f829;
+	fma.rn.f32 	%f1845, %f1820, %f827, %f851;
+	mul.f32 	%f1838, %f1819, %f814;
+	mul.f32 	%f1842, %f1819, %f820;
+	mul.f32 	%f1846, %f1819, %f827;
+
+$L__BB1_17:
+	mul.f32 	%f889, %f1840, %f1845;
+	mul.f32 	%f890, %f1841, %f1844;
 	sub.f32 	%f891, %f890, %f889;
-	mul.f32 	%f1834, %f891, %f872;
-	mul.f32 	%f892, %f1813, %f1828;
-	neg.f32 	%f893, %f892;
-	mul.f32 	%f894, %f1817, %f1827;
-	sub.f32 	%f895, %f893, %f894;
-	mul.f32 	%f896, %f1821, %f1826;
-	sub.f32 	%f1825, %f895, %f896;
-	mul.f32 	%f897, %f1813, %f1832;
-	neg.f32 	%f898, %f897;
-	mul.f32 	%f899, %f1817, %f1831;
-	sub.f32 	%f900, %f898, %f899;
-	mul.f32 	%f901, %f1821, %f1830;
-	sub.f32 	%f1829, %f900, %f901;
-	mul.f32 	%f902, %f1813, %f1836;
-	neg.f32 	%f903, %f902;
-	mul.f32 	%f904, %f1817, %f1835;
-	sub.f32 	%f905, %f903, %f904;
-	mul.f32 	%f906, %f1821, %f1834;
-	sub.f32 	%f1833, %f905, %f906;
-	bra.uni 	BB1_18;
+	mul.f32 	%f892, %f1838, %f891;
+	mul.f32 	%f893, %f1840, %f1846;
+	mul.f32 	%f894, %f1842, %f1844;
+	sub.f32 	%f895, %f894, %f893;
+	mul.f32 	%f896, %f1837, %f895;
+	sub.f32 	%f897, %f892, %f896;
+	mul.f32 	%f898, %f1841, %f1846;
+	mul.f32 	%f899, %f1842, %f1845;
+	sub.f32 	%f900, %f899, %f898;
+	fma.rn.f32 	%f901, %f1836, %f900, %f897;
+	rcp.rn.f32 	%f902, %f901;
+	mul.f32 	%f1850, %f891, %f902;
+	mul.f32 	%f903, %f1837, %f1844;
+	mul.f32 	%f904, %f1836, %f1845;
+	sub.f32 	%f905, %f904, %f903;
+	mul.f32 	%f1849, %f905, %f902;
+	mul.f32 	%f906, %f1836, %f1841;
+	mul.f32 	%f907, %f1837, %f1840;
+	sub.f32 	%f908, %f907, %f906;
+	mul.f32 	%f1848, %f908, %f902;
+	sub.f32 	%f909, %f893, %f894;
+	mul.f32 	%f1854, %f909, %f902;
+	mul.f32 	%f910, %f1836, %f1846;
+	mul.f32 	%f911, %f1838, %f1844;
+	sub.f32 	%f912, %f911, %f910;
+	mul.f32 	%f1853, %f912, %f902;
+	mul.f32 	%f913, %f1838, %f1840;
+	mul.f32 	%f914, %f1836, %f1842;
+	sub.f32 	%f915, %f914, %f913;
+	mul.f32 	%f1852, %f915, %f902;
+	mul.f32 	%f1858, %f900, %f902;
+	mul.f32 	%f916, %f1838, %f1845;
+	mul.f32 	%f917, %f1837, %f1846;
+	sub.f32 	%f918, %f917, %f916;
+	mul.f32 	%f1857, %f918, %f902;
+	mul.f32 	%f919, %f1837, %f1842;
+	mul.f32 	%f920, %f1838, %f1841;
+	sub.f32 	%f921, %f920, %f919;
+	mul.f32 	%f1856, %f921, %f902;
+	mul.f32 	%f922, %f1835, %f1850;
+	neg.f32 	%f923, %f922;
+	mul.f32 	%f924, %f1839, %f1849;
+	sub.f32 	%f925, %f923, %f924;
+	mul.f32 	%f926, %f1843, %f1848;
+	sub.f32 	%f1847, %f925, %f926;
+	mul.f32 	%f927, %f1835, %f1854;
+	neg.f32 	%f928, %f927;
+	mul.f32 	%f929, %f1839, %f1853;
+	sub.f32 	%f930, %f928, %f929;
+	mul.f32 	%f931, %f1843, %f1852;
+	sub.f32 	%f1851, %f930, %f931;
+	mul.f32 	%f932, %f1835, %f1858;
+	neg.f32 	%f933, %f932;
+	mul.f32 	%f934, %f1839, %f1857;
+	sub.f32 	%f935, %f933, %f934;
+	mul.f32 	%f936, %f1843, %f1856;
+	sub.f32 	%f1855, %f935, %f936;
+	bra.uni 	$L__BB1_18;
+
+$L__BB1_9:
+	// begin inline asm
+	call (%rd648), _optix_get_instance_inverse_transform_from_handle, (%rd48);
+	// end inline asm
+
+$L__BB1_10:
+	// begin inline asm
+	cvta.to.global.u64 %rd54, %rd648;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r40,%r41,%r42,%r43}, [%rd54];
+	// end inline asm
+	mov.b32 	%f1850, %r40;
+	mov.b32 	%f1849, %r41;
+	mov.b32 	%f1848, %r42;
+	mov.b32 	%f1847, %r43;
+	add.s64 	%rd58, %rd648, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd57, %rd58;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r44,%r45,%r46,%r47}, [%rd57];
+	// end inline asm
+	mov.b32 	%f1854, %r44;
+	mov.b32 	%f1853, %r45;
+	mov.b32 	%f1852, %r46;
+	mov.b32 	%f1851, %r47;
+	add.s64 	%rd61, %rd648, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd60, %rd61;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r48,%r49,%r50,%r51}, [%rd60];
+	// end inline asm
+	mov.b32 	%f1858, %r48;
+	mov.b32 	%f1857, %r49;
+	mov.b32 	%f1856, %r50;
+	mov.b32 	%f1855, %r51;
+
+$L__BB1_18:
+	setp.eq.s32 	%p10, %r655, 0;
+	@%p10 bra 	$L__BB1_20;
+
+	mul.f32 	%f937, %f1815, %f1850;
+	fma.rn.f32 	%f938, %f1811, %f1849, %f937;
+	fma.rn.f32 	%f151, %f1807, %f1848, %f938;
+	mul.f32 	%f939, %f1816, %f1850;
+	fma.rn.f32 	%f940, %f1812, %f1849, %f939;
+	fma.rn.f32 	%f152, %f1808, %f1848, %f940;
+	mul.f32 	%f941, %f1817, %f1850;
+	fma.rn.f32 	%f942, %f1813, %f1849, %f941;
+	fma.rn.f32 	%f153, %f1809, %f1848, %f942;
+	mul.f32 	%f943, %f1818, %f1850;
+	fma.rn.f32 	%f944, %f1814, %f1849, %f943;
+	fma.rn.f32 	%f945, %f1810, %f1848, %f944;
+	add.f32 	%f1847, %f1847, %f945;
+	mul.f32 	%f946, %f1815, %f1854;
+	fma.rn.f32 	%f947, %f1811, %f1853, %f946;
+	fma.rn.f32 	%f155, %f1807, %f1852, %f947;
+	mul.f32 	%f948, %f1816, %f1854;
+	fma.rn.f32 	%f949, %f1812, %f1853, %f948;
+	fma.rn.f32 	%f156, %f1808, %f1852, %f949;
+	mul.f32 	%f950, %f1817, %f1854;
+	fma.rn.f32 	%f951, %f1813, %f1853, %f950;
+	fma.rn.f32 	%f157, %f1809, %f1852, %f951;
+	mul.f32 	%f952, %f1818, %f1854;
+	fma.rn.f32 	%f953, %f1814, %f1853, %f952;
+	fma.rn.f32 	%f954, %f1810, %f1852, %f953;
+	add.f32 	%f1851, %f1851, %f954;
+	mul.f32 	%f955, %f1815, %f1858;
+	fma.rn.f32 	%f956, %f1811, %f1857, %f955;
+	fma.rn.f32 	%f159, %f1807, %f1856, %f956;
+	mul.f32 	%f957, %f1816, %f1858;
+	fma.rn.f32 	%f958, %f1812, %f1857, %f957;
+	fma.rn.f32 	%f160, %f1808, %f1856, %f958;
+	mul.f32 	%f959, %f1817, %f1858;
+	fma.rn.f32 	%f960, %f1813, %f1857, %f959;
+	fma.rn.f32 	%f161, %f1809, %f1856, %f960;
+	mul.f32 	%f961, %f1818, %f1858;
+	fma.rn.f32 	%f962, %f1814, %f1857, %f961;
+	fma.rn.f32 	%f963, %f1810, %f1856, %f962;
+	add.f32 	%f1855, %f1855, %f963;
+	mov.f32 	%f1848, %f153;
+	mov.f32 	%f1849, %f152;
+	mov.f32 	%f1850, %f151;
+	mov.f32 	%f1852, %f157;
+	mov.f32 	%f1853, %f156;
+	mov.f32 	%f1854, %f155;
+	mov.f32 	%f1856, %f161;
+	mov.f32 	%f1857, %f160;
+	mov.f32 	%f1858, %f159;
+
+$L__BB1_20:
+	add.s32 	%r655, %r655, 1;
+	setp.lt.u32 	%p11, %r655, %r35;
+	mov.f32 	%f1807, %f1858;
+	mov.f32 	%f1808, %f1857;
+	mov.f32 	%f1809, %f1856;
+	mov.f32 	%f1810, %f1855;
+	mov.f32 	%f1811, %f1854;
+	mov.f32 	%f1812, %f1853;
+	mov.f32 	%f1813, %f1852;
+	mov.f32 	%f1814, %f1851;
+	mov.f32 	%f1815, %f1850;
+	mov.f32 	%f1816, %f1849;
+	mov.f32 	%f1817, %f1848;
+	mov.f32 	%f1818, %f1847;
+	@%p11 bra 	$L__BB1_5;
+
+$L__BB1_21:
+	mul.f32 	%f964, %f1883, %f1850;
+	fma.rn.f32 	%f965, %f1884, %f1849, %f964;
+	fma.rn.f32 	%f966, %f1885, %f1848, %f965;
+	mul.f32 	%f967, %f1883, %f1854;
+	fma.rn.f32 	%f968, %f1884, %f1853, %f967;
+	fma.rn.f32 	%f969, %f1885, %f1852, %f968;
+	mul.f32 	%f970, %f1883, %f1858;
+	fma.rn.f32 	%f971, %f1884, %f1857, %f970;
+	fma.rn.f32 	%f972, %f1885, %f1856, %f971;
+	add.f32 	%f1885, %f1855, %f972;
+	add.f32 	%f1884, %f1851, %f969;
+	add.f32 	%f1883, %f1847, %f966;
+
+$L__BB1_22:
+	// begin inline asm
+	call (%f1941), _optix_get_world_ray_direction_x, ();
+	// end inline asm
+	// begin inline asm
+	call (%f1942), _optix_get_world_ray_direction_y, ();
+	// end inline asm
+	// begin inline asm
+	call (%f975), _optix_get_world_ray_direction_z, ();
+	// end inline asm
+	// begin inline asm
+	call (%r186), _optix_get_transform_list_size, ();
+	// end inline asm
+	setp.eq.s32 	%p12, %r186, 0;
+	@%p12 bra 	$L__BB1_42;
+
+	// begin inline asm
+	call (%r187), _optix_get_transform_list_size, ();
+	// end inline asm
+	// begin inline asm
+	call (%f976), _optix_get_ray_time, ();
+	// end inline asm
+	setp.eq.s32 	%p13, %r187, 0;
+	@%p13 bra 	$L__BB1_41;
 
-BB1_7:
-	setp.ne.s32	%p5, %r33, 1;
-	mov.f32 	%f1826, %f1825;
-	mov.f32 	%f1827, %f1825;
-	mov.f32 	%f1829, %f1825;
-	mov.f32 	%f1830, %f1825;
-	mov.f32 	%f1831, %f1828;
-	mov.f32 	%f1832, %f1825;
-	mov.f32 	%f1833, %f1825;
-	mov.f32 	%f1834, %f1828;
-	mov.f32 	%f1835, %f1825;
-	mov.f32 	%f1836, %f1825;
-	@%p5 bra 	BB1_18;
-
-	// inline asm
-	call (%rd54), _optix_get_static_transform_from_handle, (%rd52);
-	// inline asm
-	add.s64 	%rd661, %rd54, 64;
-
-BB1_10:
-	// inline asm
-	cvta.to.global.u64 %rd58, %rd661;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r35,%r36,%r37,%r38}, [%rd58];
-	// inline asm
-	mov.b32 	 %f1828, %r35;
-	mov.b32 	 %f1827, %r36;
-	mov.b32 	 %f1826, %r37;
-	mov.b32 	 %f1825, %r38;
-	add.s64 	%rd62, %rd661, 16;
-	// inline asm
-	cvta.to.global.u64 %rd61, %rd62;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r39,%r40,%r41,%r42}, [%rd61];
-	// inline asm
-	mov.b32 	 %f1832, %r39;
-	mov.b32 	 %f1831, %r40;
-	mov.b32 	 %f1830, %r41;
-	mov.b32 	 %f1829, %r42;
-	add.s64 	%rd65, %rd661, 32;
-	// inline asm
-	cvta.to.global.u64 %rd64, %rd65;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r43,%r44,%r45,%r46}, [%rd64];
-	// inline asm
-	mov.b32 	 %f1836, %r43;
-	mov.b32 	 %f1835, %r44;
-	mov.b32 	 %f1834, %r45;
-	mov.b32 	 %f1833, %r46;
-
-BB1_18:
-	setp.eq.s32	%p9, %r646, 0;
-	@%p9 bra 	BB1_19;
-	bra.uni 	BB1_20;
-
-BB1_19:
-	mov.f32 	%f1796, %f1825;
-	mov.f32 	%f1795, %f1826;
-	mov.f32 	%f1794, %f1827;
-	mov.f32 	%f1793, %f1828;
-	mov.f32 	%f1792, %f1829;
-	mov.f32 	%f1791, %f1830;
-	mov.f32 	%f1790, %f1831;
-	mov.f32 	%f1789, %f1832;
-	mov.f32 	%f1788, %f1833;
-	mov.f32 	%f1787, %f1834;
-	mov.f32 	%f1786, %f1835;
-	mov.f32 	%f1785, %f1836;
-	bra.uni 	BB1_21;
-
-BB1_20:
-	mul.f32 	%f907, %f1793, %f1828;
-	fma.rn.f32 	%f908, %f1789, %f1827, %f907;
-	fma.rn.f32 	%f151, %f1785, %f1826, %f908;
-	mul.f32 	%f909, %f1794, %f1828;
-	fma.rn.f32 	%f910, %f1790, %f1827, %f909;
-	fma.rn.f32 	%f152, %f1786, %f1826, %f910;
-	mul.f32 	%f911, %f1795, %f1828;
-	fma.rn.f32 	%f912, %f1791, %f1827, %f911;
-	fma.rn.f32 	%f153, %f1787, %f1826, %f912;
-	mul.f32 	%f913, %f1796, %f1828;
-	fma.rn.f32 	%f914, %f1792, %f1827, %f913;
-	fma.rn.f32 	%f915, %f1788, %f1826, %f914;
-	add.f32 	%f154, %f1825, %f915;
-	mul.f32 	%f916, %f1793, %f1832;
-	fma.rn.f32 	%f917, %f1789, %f1831, %f916;
-	fma.rn.f32 	%f155, %f1785, %f1830, %f917;
-	mul.f32 	%f918, %f1794, %f1832;
-	fma.rn.f32 	%f919, %f1790, %f1831, %f918;
-	fma.rn.f32 	%f156, %f1786, %f1830, %f919;
-	mul.f32 	%f920, %f1795, %f1832;
-	fma.rn.f32 	%f921, %f1791, %f1831, %f920;
-	fma.rn.f32 	%f157, %f1787, %f1830, %f921;
-	mul.f32 	%f922, %f1796, %f1832;
-	fma.rn.f32 	%f923, %f1792, %f1831, %f922;
-	fma.rn.f32 	%f924, %f1788, %f1830, %f923;
-	add.f32 	%f158, %f1829, %f924;
-	mul.f32 	%f925, %f1793, %f1836;
-	fma.rn.f32 	%f926, %f1789, %f1835, %f925;
-	fma.rn.f32 	%f1785, %f1785, %f1834, %f926;
-	mul.f32 	%f927, %f1794, %f1836;
-	fma.rn.f32 	%f928, %f1790, %f1835, %f927;
-	fma.rn.f32 	%f1786, %f1786, %f1834, %f928;
-	mul.f32 	%f929, %f1795, %f1836;
-	fma.rn.f32 	%f930, %f1791, %f1835, %f929;
-	fma.rn.f32 	%f1787, %f1787, %f1834, %f930;
-	mul.f32 	%f931, %f1796, %f1836;
-	fma.rn.f32 	%f932, %f1792, %f1835, %f931;
-	fma.rn.f32 	%f933, %f1788, %f1834, %f932;
-	add.f32 	%f1788, %f1833, %f933;
-	mov.f32 	%f1796, %f154;
-	mov.f32 	%f1795, %f153;
-	mov.f32 	%f1794, %f152;
-	mov.f32 	%f1793, %f151;
-	mov.f32 	%f1792, %f158;
-	mov.f32 	%f1791, %f157;
-	mov.f32 	%f1790, %f156;
-	mov.f32 	%f1789, %f155;
-
-BB1_21:
-	add.s32 	%r646, %r646, 1;
-	setp.lt.u32	%p10, %r646, %r30;
-	@%p10 bra 	BB1_5;
-
-	mul.f32 	%f934, %f692, %f1793;
-	fma.rn.f32 	%f935, %f693, %f1794, %f934;
-	fma.rn.f32 	%f936, %f1849, %f1795, %f935;
-	add.f32 	%f1851, %f1796, %f936;
-	mul.f32 	%f937, %f692, %f1789;
-	fma.rn.f32 	%f938, %f693, %f1790, %f937;
-	fma.rn.f32 	%f939, %f1849, %f1791, %f938;
-	add.f32 	%f1850, %f1792, %f939;
-	mul.f32 	%f940, %f692, %f1785;
-	fma.rn.f32 	%f941, %f693, %f1786, %f940;
-	fma.rn.f32 	%f942, %f1849, %f1787, %f941;
-	add.f32 	%f1849, %f1788, %f942;
-	bra.uni 	BB1_23;
-
-BB1_3:
-	mov.f32 	%f1850, %f693;
-	mov.f32 	%f1851, %f692;
-
-BB1_23:
-	// inline asm
-	call (%f943), _optix_get_world_ray_direction_x, ();
-	// inline asm
-	// inline asm
-	call (%f944), _optix_get_world_ray_direction_y, ();
-	// inline asm
-	// inline asm
-	call (%f1900), _optix_get_world_ray_direction_z, ();
-	// inline asm
-	// inline asm
-	call (%f946), _optix_get_ray_time, ();
-	// inline asm
-	mov.u32 	%r647, 0;
-	@%p2 bra 	BB1_24;
-
-BB1_25:
+	mov.u32 	%r656, 0;
+
+$L__BB1_25:
 	.pragma "nounroll";
-	// inline asm
-	call (%rd173), _optix_get_transform_list_handle, (%r647);
-	// inline asm
-	// inline asm
-	call (%r183), _optix_get_transform_type_from_handle, (%rd173);
-	// inline asm
-	and.b32  	%r184, %r183, -2;
-	setp.eq.s32	%p12, %r184, 2;
-	@%p12 bra 	BB1_31;
-	bra.uni 	BB1_26;
-
-BB1_31:
-	setp.eq.s32	%p15, %r183, 2;
-	@%p15 bra 	BB1_35;
-	bra.uni 	BB1_32;
-
-BB1_35:
-	// inline asm
-	call (%rd247), _optix_get_matrix_motion_transform_from_handle, (%rd173);
-	// inline asm
-	// inline asm
-	cvta.to.global.u64 %rd249, %rd247;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r272,%r273,%r274,%r275}, [%rd249];
-	// inline asm
-	mov.b32	{%rs8, %rs9}, %r274;
-	add.s64 	%rd253, %rd247, 16;
-	// inline asm
-	cvta.to.global.u64 %rd252, %rd253;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r276,%r277,%r278,%r279}, [%rd252];
-	// inline asm
-	add.s64 	%rd256, %rd247, 32;
-	// inline asm
-	cvta.to.global.u64 %rd255, %rd256;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r280,%r281,%r282,%r283}, [%rd255];
-	// inline asm
-	add.s64 	%rd259, %rd247, 48;
-	// inline asm
-	cvta.to.global.u64 %rd258, %rd259;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r284,%r285,%r286,%r287}, [%rd258];
-	// inline asm
-	add.s64 	%rd262, %rd247, 64;
-	// inline asm
-	cvta.to.global.u64 %rd261, %rd262;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r288,%r289,%r290,%r291}, [%rd261];
-	// inline asm
-	add.s64 	%rd265, %rd247, 80;
-	// inline asm
-	cvta.to.global.u64 %rd264, %rd265;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r292,%r293,%r294,%r295}, [%rd264];
-	// inline asm
-	add.s64 	%rd268, %rd247, 96;
-	// inline asm
-	cvta.to.global.u64 %rd267, %rd268;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r296,%r297,%r298,%r299}, [%rd267];
-	// inline asm
-	add.s64 	%rd271, %rd247, 112;
-	// inline asm
-	cvta.to.global.u64 %rd270, %rd271;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r300,%r301,%r302,%r303}, [%rd270];
-	// inline asm
-	mov.b32 	 %f1049, %r275;
-	mov.b32 	 %f1050, %r276;
-	cvt.u32.u16	%r316, %rs8;
-	add.s32 	%r317, %r316, -1;
-	cvt.rn.f32.s32	%f1051, %r317;
-	sub.f32 	%f1052, %f946, %f1049;
-	mul.f32 	%f1053, %f1052, %f1051;
-	sub.f32 	%f1054, %f1050, %f1049;
-	div.rn.f32 	%f1055, %f1053, %f1054;
-	min.f32 	%f1056, %f1051, %f1055;
-	mov.f32 	%f1057, 0f00000000;
-	max.f32 	%f1058, %f1057, %f1056;
-	cvt.rmi.f32.f32	%f1059, %f1058;
-	cvt.rzi.s32.f32	%r318, %f1059;
-	cvt.s64.s32	%rd19, %r318;
-	mul.wide.s32 	%rd282, %r318, 48;
-	add.s64 	%rd274, %rd256, %rd282;
-	// inline asm
-	cvta.to.global.u64 %rd273, %rd274;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r304,%r305,%r306,%r307}, [%rd273];
-	// inline asm
-	mov.b32 	 %f1877, %r304;
-	mov.b32 	 %f1878, %r305;
-	mov.b32 	 %f1879, %r306;
-	add.s64 	%rd277, %rd274, 16;
-	// inline asm
-	cvta.to.global.u64 %rd276, %rd277;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r308,%r309,%r310,%r311}, [%rd276];
-	// inline asm
-	mov.b32 	 %f1874, %r308;
-	mov.b32 	 %f1875, %r309;
-	mov.b32 	 %f1876, %r310;
-	add.s64 	%rd280, %rd274, 32;
-	// inline asm
-	cvta.to.global.u64 %rd279, %rd280;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r312,%r313,%r314,%r315}, [%rd279];
-	// inline asm
-	sub.f32 	%f249, %f1058, %f1059;
-	mov.b32 	 %f1871, %r312;
-	mov.b32 	 %f1872, %r313;
-	mov.b32 	 %f1873, %r314;
-	setp.leu.f32	%p17, %f249, 0f00000000;
-	@%p17 bra 	BB1_37;
-
-	mul.lo.s64 	%rd292, %rd19, 48;
-	add.s64 	%rd293, %rd247, %rd292;
-	add.s64 	%rd284, %rd293, 80;
-	// inline asm
-	cvta.to.global.u64 %rd283, %rd284;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r319,%r320,%r321,%r322}, [%rd283];
-	// inline asm
-	mov.b32 	 %f1060, %r319;
-	mov.b32 	 %f1061, %r320;
-	mov.b32 	 %f1062, %r321;
-	add.s64 	%rd287, %rd293, 96;
-	// inline asm
-	cvta.to.global.u64 %rd286, %rd287;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r323,%r324,%r325,%r326}, [%rd286];
-	// inline asm
-	mov.b32 	 %f1063, %r323;
-	mov.b32 	 %f1064, %r324;
-	mov.b32 	 %f1065, %r325;
-	add.s64 	%rd290, %rd293, 112;
-	// inline asm
-	cvta.to.global.u64 %rd289, %rd290;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r327,%r328,%r329,%r330}, [%rd289];
-	// inline asm
-	mov.f32 	%f1066, 0f3F800000;
-	sub.f32 	%f1067, %f1066, %f249;
-	mul.f32 	%f1068, %f249, %f1060;
-	mul.f32 	%f1069, %f249, %f1061;
-	mul.f32 	%f1070, %f249, %f1062;
-	fma.rn.f32 	%f1877, %f1067, %f1877, %f1068;
-	fma.rn.f32 	%f1878, %f1067, %f1878, %f1069;
-	fma.rn.f32 	%f1879, %f1067, %f1879, %f1070;
-	mul.f32 	%f1071, %f249, %f1063;
-	mul.f32 	%f1072, %f249, %f1064;
-	mul.f32 	%f1073, %f249, %f1065;
-	fma.rn.f32 	%f1874, %f1067, %f1874, %f1071;
-	fma.rn.f32 	%f1875, %f1067, %f1875, %f1072;
-	fma.rn.f32 	%f1876, %f1067, %f1876, %f1073;
-	mov.b32 	 %f1074, %r327;
-	mov.b32 	 %f1075, %r328;
-	mov.b32 	 %f1076, %r329;
-	mul.f32 	%f1077, %f249, %f1074;
-	mul.f32 	%f1078, %f249, %f1075;
-	mul.f32 	%f1079, %f249, %f1076;
-	fma.rn.f32 	%f1871, %f1067, %f1871, %f1077;
-	fma.rn.f32 	%f1872, %f1067, %f1872, %f1078;
-	fma.rn.f32 	%f1873, %f1067, %f1873, %f1079;
-	bra.uni 	BB1_37;
-
-BB1_26:
-	mov.f32 	%f1880, 0f00000000;
-	mov.f32 	%f1882, 0f3F800000;
-	setp.eq.s32	%p13, %r183, 4;
-	@%p13 bra 	BB1_29;
-	bra.uni 	BB1_27;
-
-BB1_29:
-	// inline asm
-	call (%rd662), _optix_get_instance_inverse_transform_from_handle, (%rd173);
-	// inline asm
-	bra.uni 	BB1_30;
-
-BB1_32:
-	// inline asm
-	call (%rd188), _optix_get_srt_motion_transform_from_handle, (%rd173);
-	// inline asm
-	// inline asm
-	cvta.to.global.u64 %rd190, %rd188;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r197,%r198,%r199,%r200}, [%rd190];
-	// inline asm
-	mov.b32	{%rs6, %rs7}, %r199;
-	add.s64 	%rd194, %rd188, 16;
-	// inline asm
-	cvta.to.global.u64 %rd193, %rd194;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r201,%r202,%r203,%r204}, [%rd193];
-	// inline asm
-	add.s64 	%rd197, %rd188, 32;
-	// inline asm
-	cvta.to.global.u64 %rd196, %rd197;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r205,%r206,%r207,%r208}, [%rd196];
-	// inline asm
-	add.s64 	%rd200, %rd188, 48;
-	// inline asm
+	// begin inline asm
+	call (%rd167), _optix_get_transform_list_handle, (%r656);
+	// end inline asm
+	// begin inline asm
+	call (%r190), _optix_get_transform_type_from_handle, (%rd167);
+	// end inline asm
+	or.b32  	%r191, %r190, 1;
+	setp.eq.s32 	%p14, %r191, 3;
+	@%p14 bra 	$L__BB1_31;
+	bra.uni 	$L__BB1_26;
+
+$L__BB1_31:
+	setp.eq.s32 	%p17, %r190, 2;
+	@%p17 bra 	$L__BB1_35;
+	bra.uni 	$L__BB1_32;
+
+$L__BB1_35:
+	// begin inline asm
+	call (%rd239), _optix_get_matrix_motion_transform_from_handle, (%rd167);
+	// end inline asm
+	// begin inline asm
+	cvta.to.global.u64 %rd241, %rd239;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r279,%r280,%r281,%r282}, [%rd241];
+	// end inline asm
+	add.s64 	%rd245, %rd239, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd244, %rd245;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r283,%r284,%r285,%r286}, [%rd244];
+	// end inline asm
+	add.s64 	%rd248, %rd239, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd247, %rd248;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r287,%r288,%r289,%r290}, [%rd247];
+	// end inline asm
+	add.s64 	%rd251, %rd239, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd250, %rd251;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r291,%r292,%r293,%r294}, [%rd250];
+	// end inline asm
+	add.s64 	%rd254, %rd239, 64;
+	// begin inline asm
+	cvta.to.global.u64 %rd253, %rd254;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r295,%r296,%r297,%r298}, [%rd253];
+	// end inline asm
+	add.s64 	%rd257, %rd239, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd256, %rd257;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r299,%r300,%r301,%r302}, [%rd256];
+	// end inline asm
+	add.s64 	%rd260, %rd239, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd259, %rd260;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r303,%r304,%r305,%r306}, [%rd259];
+	// end inline asm
+	add.s64 	%rd263, %rd239, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd262, %rd263;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r307,%r308,%r309,%r310}, [%rd262];
+	// end inline asm
+	mov.b32 	%f1080, %r282;
+	mov.b32 	%f1081, %r283;
+	and.b32  	%r323, %r281, 65535;
+	add.s32 	%r324, %r323, -1;
+	cvt.rn.f32.s32 	%f1082, %r324;
+	sub.f32 	%f1083, %f976, %f1080;
+	mul.f32 	%f1084, %f1083, %f1082;
+	sub.f32 	%f1085, %f1081, %f1080;
+	div.rn.f32 	%f1086, %f1084, %f1085;
+	min.f32 	%f1087, %f1082, %f1086;
+	mov.f32 	%f1088, 0f00000000;
+	max.f32 	%f1089, %f1088, %f1087;
+	cvt.rmi.f32.f32 	%f1090, %f1089;
+	sub.f32 	%f258, %f1089, %f1090;
+	cvt.rzi.s32.f32 	%r325, %f1090;
+	cvt.s64.s32 	%rd17, %r325;
+	mul.wide.s32 	%rd274, %r325, 48;
+	add.s64 	%rd266, %rd248, %rd274;
+	// begin inline asm
+	cvta.to.global.u64 %rd265, %rd266;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r311,%r312,%r313,%r314}, [%rd265];
+	// end inline asm
+	mov.b32 	%f1911, %r311;
+	mov.b32 	%f1912, %r312;
+	mov.b32 	%f1913, %r313;
+	add.s64 	%rd269, %rd266, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd268, %rd269;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r315,%r316,%r317,%r318}, [%rd268];
+	// end inline asm
+	mov.b32 	%f1908, %r315;
+	mov.b32 	%f1909, %r316;
+	mov.b32 	%f1910, %r317;
+	add.s64 	%rd272, %rd266, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd271, %rd272;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r319,%r320,%r321,%r322}, [%rd271];
+	// end inline asm
+	mov.b32 	%f1905, %r319;
+	mov.b32 	%f1906, %r320;
+	mov.b32 	%f1907, %r321;
+	setp.leu.f32 	%p19, %f258, 0f00000000;
+	@%p19 bra 	$L__BB1_37;
+
+	mov.f32 	%f1091, 0f3F800000;
+	sub.f32 	%f1092, %f1091, %f258;
+	mul.lo.s64 	%rd284, %rd17, 48;
+	add.s64 	%rd285, %rd239, %rd284;
+	add.s64 	%rd276, %rd285, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd275, %rd276;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r326,%r327,%r328,%r329}, [%rd275];
+	// end inline asm
+	mov.b32 	%f1093, %r326;
+	mov.b32 	%f1094, %r327;
+	mov.b32 	%f1095, %r328;
+	mul.f32 	%f1096, %f258, %f1093;
+	mul.f32 	%f1097, %f258, %f1094;
+	mul.f32 	%f1098, %f258, %f1095;
+	fma.rn.f32 	%f1911, %f1092, %f1911, %f1096;
+	fma.rn.f32 	%f1912, %f1092, %f1912, %f1097;
+	fma.rn.f32 	%f1913, %f1092, %f1913, %f1098;
+	add.s64 	%rd279, %rd285, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd278, %rd279;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r330,%r331,%r332,%r333}, [%rd278];
+	// end inline asm
+	mov.b32 	%f1099, %r330;
+	mov.b32 	%f1100, %r331;
+	mov.b32 	%f1101, %r332;
+	mul.f32 	%f1102, %f258, %f1099;
+	mul.f32 	%f1103, %f258, %f1100;
+	mul.f32 	%f1104, %f258, %f1101;
+	fma.rn.f32 	%f1908, %f1092, %f1908, %f1102;
+	fma.rn.f32 	%f1909, %f1092, %f1909, %f1103;
+	fma.rn.f32 	%f1910, %f1092, %f1910, %f1104;
+	add.s64 	%rd282, %rd285, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd281, %rd282;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r334,%r335,%r336,%r337}, [%rd281];
+	// end inline asm
+	mov.b32 	%f1105, %r334;
+	mov.b32 	%f1106, %r335;
+	mov.b32 	%f1107, %r336;
+	mul.f32 	%f1108, %f258, %f1105;
+	mul.f32 	%f1109, %f258, %f1106;
+	mul.f32 	%f1110, %f258, %f1107;
+	fma.rn.f32 	%f1905, %f1092, %f1905, %f1108;
+	fma.rn.f32 	%f1906, %f1092, %f1906, %f1109;
+	fma.rn.f32 	%f1907, %f1092, %f1907, %f1110;
+	bra.uni 	$L__BB1_37;
+
+$L__BB1_26:
+	mov.f32 	%f1914, 0f00000000;
+	mov.f32 	%f1916, 0f3F800000;
+	setp.eq.s32 	%p15, %r190, 4;
+	@%p15 bra 	$L__BB1_29;
+
+	setp.ne.s32 	%p16, %r190, 1;
+	mov.f32 	%f1915, %f1914;
+	mov.f32 	%f1917, %f1914;
+	mov.f32 	%f1918, %f1916;
+	mov.f32 	%f1919, %f1914;
+	mov.f32 	%f1920, %f1916;
+	mov.f32 	%f1921, %f1914;
+	mov.f32 	%f1922, %f1914;
+	@%p16 bra 	$L__BB1_38;
+
+	// begin inline asm
+	call (%rd169), _optix_get_static_transform_from_handle, (%rd167);
+	// end inline asm
+	add.s64 	%rd649, %rd169, 64;
+	bra.uni 	$L__BB1_30;
+
+$L__BB1_32:
+	// begin inline asm
+	call (%rd182), _optix_get_srt_motion_transform_from_handle, (%rd167);
+	// end inline asm
+	// begin inline asm
+	cvta.to.global.u64 %rd184, %rd182;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r204,%r205,%r206,%r207}, [%rd184];
+	// end inline asm
+	add.s64 	%rd188, %rd182, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd187, %rd188;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r208,%r209,%r210,%r211}, [%rd187];
+	// end inline asm
+	add.s64 	%rd191, %rd182, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd190, %rd191;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r212,%r213,%r214,%r215}, [%rd190];
+	// end inline asm
+	add.s64 	%rd194, %rd182, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd193, %rd194;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r216,%r217,%r218,%r219}, [%rd193];
+	// end inline asm
+	add.s64 	%rd197, %rd182, 64;
+	// begin inline asm
+	cvta.to.global.u64 %rd196, %rd197;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r220,%r221,%r222,%r223}, [%rd196];
+	// end inline asm
+	add.s64 	%rd200, %rd182, 80;
+	// begin inline asm
 	cvta.to.global.u64 %rd199, %rd200;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r209,%r210,%r211,%r212}, [%rd199];
-	// inline asm
-	add.s64 	%rd203, %rd188, 64;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r224,%r225,%r226,%r227}, [%rd199];
+	// end inline asm
+	add.s64 	%rd203, %rd182, 96;
+	// begin inline asm
 	cvta.to.global.u64 %rd202, %rd203;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r213,%r214,%r215,%r216}, [%rd202];
-	// inline asm
-	add.s64 	%rd206, %rd188, 80;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r228,%r229,%r230,%r231}, [%rd202];
+	// end inline asm
+	add.s64 	%rd206, %rd182, 112;
+	// begin inline asm
 	cvta.to.global.u64 %rd205, %rd206;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r217,%r218,%r219,%r220}, [%rd205];
-	// inline asm
-	add.s64 	%rd209, %rd188, 96;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r232,%r233,%r234,%r235}, [%rd205];
+	// end inline asm
+	add.s64 	%rd209, %rd182, 128;
+	// begin inline asm
 	cvta.to.global.u64 %rd208, %rd209;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r221,%r222,%r223,%r224}, [%rd208];
-	// inline asm
-	add.s64 	%rd212, %rd188, 112;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r236,%r237,%r238,%r239}, [%rd208];
+	// end inline asm
+	add.s64 	%rd212, %rd182, 144;
+	// begin inline asm
 	cvta.to.global.u64 %rd211, %rd212;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r225,%r226,%r227,%r228}, [%rd211];
-	// inline asm
-	add.s64 	%rd215, %rd188, 128;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r240,%r241,%r242,%r243}, [%rd211];
+	// end inline asm
+	mov.b32 	%f988, %r207;
+	mov.b32 	%f989, %r208;
+	and.b32  	%r260, %r206, 65535;
+	add.s32 	%r261, %r260, -1;
+	cvt.rn.f32.s32 	%f990, %r261;
+	sub.f32 	%f991, %f976, %f988;
+	mul.f32 	%f992, %f991, %f990;
+	sub.f32 	%f993, %f989, %f988;
+	div.rn.f32 	%f994, %f992, %f993;
+	min.f32 	%f995, %f990, %f994;
+	mov.f32 	%f996, 0f00000000;
+	max.f32 	%f997, %f996, %f995;
+	cvt.rmi.f32.f32 	%f998, %f997;
+	sub.f32 	%f218, %f997, %f998;
+	cvt.rzi.s32.f32 	%r262, %f998;
+	mul.wide.s32 	%rd226, %r262, 64;
+	add.s64 	%rd215, %rd191, %rd226;
+	// begin inline asm
 	cvta.to.global.u64 %rd214, %rd215;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r229,%r230,%r231,%r232}, [%rd214];
-	// inline asm
-	add.s64 	%rd218, %rd188, 144;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r244,%r245,%r246,%r247}, [%rd214];
+	// end inline asm
+	mov.b32 	%f1895, %r244;
+	mov.b32 	%f1896, %r245;
+	mov.b32 	%f1897, %r246;
+	add.s64 	%rd218, %rd215, 16;
+	// begin inline asm
 	cvta.to.global.u64 %rd217, %rd218;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r233,%r234,%r235,%r236}, [%rd217];
-	// inline asm
-	mov.b32 	 %f957, %r200;
-	mov.b32 	 %f958, %r201;
-	cvt.u32.u16	%r253, %rs6;
-	add.s32 	%r254, %r253, -1;
-	cvt.rn.f32.s32	%f959, %r254;
-	sub.f32 	%f960, %f946, %f957;
-	mul.f32 	%f961, %f960, %f959;
-	sub.f32 	%f962, %f958, %f957;
-	div.rn.f32 	%f963, %f961, %f962;
-	min.f32 	%f964, %f959, %f963;
-	mov.f32 	%f965, 0f00000000;
-	max.f32 	%f966, %f965, %f964;
-	cvt.rmi.f32.f32	%f967, %f966;
-	cvt.rzi.s32.f32	%r255, %f967;
-	cvt.s64.s32	%rd17, %r255;
-	mul.wide.s32 	%rd232, %r255, 64;
-	add.s64 	%rd221, %rd197, %rd232;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r248,%r249,%r250,%r251}, [%rd217];
+	// end inline asm
+	mov.b32 	%f1898, %r248;
+	mov.b32 	%f1899, %r249;
+	mov.b32 	%f1900, %r251;
+	add.s64 	%rd221, %rd215, 32;
+	// begin inline asm
 	cvta.to.global.u64 %rd220, %rd221;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r237,%r238,%r239,%r240}, [%rd220];
-	// inline asm
-	mov.b32 	 %f1861, %r237;
-	mov.b32 	 %f1862, %r238;
-	mov.b32 	 %f1863, %r239;
-	add.s64 	%rd224, %rd221, 16;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r252,%r253,%r254,%r255}, [%rd220];
+	// end inline asm
+	mov.b32 	%f1901, %r253;
+	mov.b32 	%f1902, %r254;
+	mov.b32 	%f1903, %r255;
+	add.s64 	%rd224, %rd215, 48;
+	// begin inline asm
 	cvta.to.global.u64 %rd223, %rd224;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r241,%r242,%r243,%r244}, [%rd223];
-	// inline asm
-	mov.b32 	 %f1864, %r241;
-	mov.b32 	 %f1865, %r242;
-	mov.b32 	 %f1866, %r244;
-	add.s64 	%rd227, %rd221, 32;
-	// inline asm
-	cvta.to.global.u64 %rd226, %rd227;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r245,%r246,%r247,%r248}, [%rd226];
-	// inline asm
-	sub.f32 	%f209, %f966, %f967;
-	mov.b32 	 %f1867, %r246;
-	mov.b32 	 %f1868, %r247;
-	mov.b32 	 %f1869, %r248;
-	add.s64 	%rd230, %rd221, 48;
-	// inline asm
-	cvta.to.global.u64 %rd229, %rd230;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r249,%r250,%r251,%r252}, [%rd229];
-	// inline asm
-	mov.b32 	 %f1870, %r249;
-	setp.leu.f32	%p16, %f209, 0f00000000;
-	@%p16 bra 	BB1_34;
-
-	shl.b64 	%rd245, %rd17, 6;
-	add.s64 	%rd246, %rd245, %rd188;
-	add.s64 	%rd234, %rd246, 96;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r256,%r257,%r258,%r259}, [%rd223];
+	// end inline asm
+	mov.b32 	%f1904, %r256;
+	setp.leu.f32 	%p18, %f218, 0f00000000;
+	@%p18 bra 	$L__BB1_34;
+
+	mov.f32 	%f999, 0f3F800000;
+	sub.f32 	%f1000, %f999, %f218;
+	add.s64 	%rd228, %rd215, 64;
+	// begin inline asm
+	cvta.to.global.u64 %rd227, %rd228;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r263,%r264,%r265,%r266}, [%rd227];
+	// end inline asm
+	mov.b32 	%f1001, %r263;
+	mov.b32 	%f1002, %r264;
+	mov.b32 	%f1003, %r265;
+	mul.f32 	%f1004, %f218, %f1001;
+	mul.f32 	%f1005, %f218, %f1002;
+	mul.f32 	%f1006, %f218, %f1003;
+	fma.rn.f32 	%f1895, %f1000, %f1895, %f1004;
+	fma.rn.f32 	%f1896, %f1000, %f1896, %f1005;
+	fma.rn.f32 	%f1897, %f1000, %f1897, %f1006;
+	add.s64 	%rd231, %rd215, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd230, %rd231;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r267,%r268,%r269,%r270}, [%rd230];
+	// end inline asm
+	mov.b32 	%f1007, %r267;
+	mov.b32 	%f1008, %r268;
+	mov.b32 	%f1009, %r270;
+	mul.f32 	%f1010, %f218, %f1007;
+	mul.f32 	%f1011, %f218, %f1008;
+	mul.f32 	%f1012, %f218, %f1009;
+	fma.rn.f32 	%f1898, %f1000, %f1898, %f1010;
+	fma.rn.f32 	%f1899, %f1000, %f1899, %f1011;
+	fma.rn.f32 	%f1900, %f1000, %f1900, %f1012;
+	add.s64 	%rd234, %rd215, 96;
+	// begin inline asm
 	cvta.to.global.u64 %rd233, %rd234;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r256,%r257,%r258,%r259}, [%rd233];
-	// inline asm
-	mov.b32 	 %f968, %r256;
-	mov.b32 	 %f969, %r257;
-	mov.b32 	 %f970, %r258;
-	add.s64 	%rd237, %rd246, 112;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r271,%r272,%r273,%r274}, [%rd233];
+	// end inline asm
+	mov.b32 	%f1013, %r272;
+	mov.b32 	%f1014, %r273;
+	mov.b32 	%f1015, %r274;
+	mul.f32 	%f1016, %f218, %f1013;
+	mul.f32 	%f1017, %f218, %f1014;
+	mul.f32 	%f1018, %f218, %f1015;
+	fma.rn.f32 	%f1019, %f1000, %f1901, %f1016;
+	fma.rn.f32 	%f1020, %f1000, %f1902, %f1017;
+	fma.rn.f32 	%f1021, %f1000, %f1903, %f1018;
+	add.s64 	%rd237, %rd215, 112;
+	// begin inline asm
 	cvta.to.global.u64 %rd236, %rd237;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r260,%r261,%r262,%r263}, [%rd236];
-	// inline asm
-	mov.b32 	 %f971, %r260;
-	mov.b32 	 %f972, %r261;
-	mov.b32 	 %f973, %r263;
-	add.s64 	%rd240, %rd246, 128;
-	// inline asm
-	cvta.to.global.u64 %rd239, %rd240;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r264,%r265,%r266,%r267}, [%rd239];
-	// inline asm
-	mov.b32 	 %f974, %r265;
-	mov.b32 	 %f975, %r266;
-	mov.b32 	 %f976, %r267;
-	add.s64 	%rd243, %rd246, 144;
-	// inline asm
-	cvta.to.global.u64 %rd242, %rd243;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r268,%r269,%r270,%r271}, [%rd242];
-	// inline asm
-	mov.f32 	%f977, 0f3F800000;
-	sub.f32 	%f978, %f977, %f209;
-	mul.f32 	%f979, %f209, %f968;
-	mul.f32 	%f980, %f209, %f969;
-	mul.f32 	%f981, %f209, %f970;
-	fma.rn.f32 	%f1861, %f978, %f1861, %f979;
-	fma.rn.f32 	%f1862, %f978, %f1862, %f980;
-	fma.rn.f32 	%f1863, %f978, %f1863, %f981;
-	mul.f32 	%f982, %f209, %f971;
-	mul.f32 	%f983, %f209, %f972;
-	mul.f32 	%f984, %f209, %f973;
-	fma.rn.f32 	%f1864, %f978, %f1864, %f982;
-	fma.rn.f32 	%f1865, %f978, %f1865, %f983;
-	fma.rn.f32 	%f1866, %f978, %f1866, %f984;
-	mul.f32 	%f985, %f209, %f974;
-	mul.f32 	%f986, %f209, %f975;
-	mul.f32 	%f987, %f209, %f976;
-	fma.rn.f32 	%f988, %f978, %f1867, %f985;
-	fma.rn.f32 	%f989, %f978, %f1868, %f986;
-	fma.rn.f32 	%f990, %f978, %f1869, %f987;
-	mov.b32 	 %f991, %r268;
-	mul.f32 	%f992, %f209, %f991;
-	fma.rn.f32 	%f993, %f978, %f1870, %f992;
-	mul.f32 	%f994, %f989, %f989;
-	fma.rn.f32 	%f995, %f988, %f988, %f994;
-	fma.rn.f32 	%f996, %f990, %f990, %f995;
-	fma.rn.f32 	%f997, %f993, %f993, %f996;
-	sqrt.rn.f32 	%f998, %f997;
-	rcp.rn.f32 	%f999, %f998;
-	mul.f32 	%f1867, %f988, %f999;
-	mul.f32 	%f1868, %f989, %f999;
-	mul.f32 	%f1869, %f990, %f999;
-	mul.f32 	%f1870, %f993, %f999;
-
-BB1_34:
-	mul.f32 	%f1000, %f1868, %f1868;
-	fma.rn.f32 	%f1001, %f1867, %f1867, %f1000;
-	fma.rn.f32 	%f1002, %f1869, %f1869, %f1001;
-	fma.rn.f32 	%f1003, %f1870, %f1870, %f1002;
-	rcp.rn.f32 	%f1004, %f1003;
-	mul.f32 	%f1005, %f1867, %f1004;
-	mul.f32 	%f1006, %f1868, %f1004;
-	mul.f32 	%f1007, %f1869, %f1004;
-	mul.f32 	%f1008, %f1870, %f1004;
-	mul.f32 	%f1009, %f1867, %f1005;
-	mul.f32 	%f1010, %f1868, %f1006;
-	mul.f32 	%f1011, %f1869, %f1007;
-	mul.f32 	%f1012, %f1867, %f1006;
-	mul.f32 	%f1013, %f1869, %f1008;
-	mul.f32 	%f1014, %f1867, %f1007;
-	mul.f32 	%f1015, %f1868, %f1008;
-	mul.f32 	%f1016, %f1868, %f1007;
-	mul.f32 	%f1017, %f1867, %f1008;
-	sub.f32 	%f1018, %f1009, %f1010;
-	sub.f32 	%f1019, %f1018, %f1011;
-	fma.rn.f32 	%f1020, %f1870, %f1008, %f1019;
-	sub.f32 	%f1021, %f1012, %f1013;
-	add.f32 	%f1022, %f1021, %f1021;
-	add.f32 	%f1023, %f1014, %f1015;
-	add.f32 	%f1024, %f1023, %f1023;
-	add.f32 	%f1025, %f1012, %f1013;
-	add.f32 	%f1026, %f1025, %f1025;
-	sub.f32 	%f1027, %f1010, %f1009;
-	sub.f32 	%f1028, %f1027, %f1011;
-	fma.rn.f32 	%f1029, %f1870, %f1008, %f1028;
-	sub.f32 	%f1030, %f1016, %f1017;
-	add.f32 	%f1031, %f1030, %f1030;
-	sub.f32 	%f1032, %f1014, %f1015;
-	add.f32 	%f1033, %f1032, %f1032;
-	add.f32 	%f1034, %f1016, %f1017;
-	add.f32 	%f1035, %f1034, %f1034;
-	neg.f32 	%f1036, %f1009;
-	sub.f32 	%f1037, %f1036, %f1010;
-	add.f32 	%f1038, %f1011, %f1037;
-	fma.rn.f32 	%f1039, %f1870, %f1008, %f1038;
-	mul.f32 	%f1040, %f1863, %f1020;
-	fma.rn.f32 	%f1041, %f1865, %f1022, %f1040;
-	fma.rn.f32 	%f1879, %f1866, %f1024, %f1041;
-	mul.f32 	%f1042, %f1865, %f1029;
-	fma.rn.f32 	%f1043, %f1863, %f1026, %f1042;
-	fma.rn.f32 	%f1876, %f1866, %f1031, %f1043;
-	mul.f32 	%f1044, %f1865, %f1035;
-	fma.rn.f32 	%f1045, %f1863, %f1033, %f1044;
-	fma.rn.f32 	%f1873, %f1866, %f1039, %f1045;
-	mul.f32 	%f1046, %f1862, %f1020;
-	fma.rn.f32 	%f1878, %f1864, %f1022, %f1046;
-	mul.f32 	%f1047, %f1864, %f1029;
-	fma.rn.f32 	%f1875, %f1862, %f1026, %f1047;
-	mul.f32 	%f1048, %f1864, %f1035;
-	fma.rn.f32 	%f1872, %f1862, %f1033, %f1048;
-	mul.f32 	%f1877, %f1861, %f1020;
-	mul.f32 	%f1874, %f1861, %f1026;
-	mul.f32 	%f1871, %f1861, %f1033;
-
-BB1_37:
-	mul.f32 	%f1080, %f1872, %f1876;
-	mul.f32 	%f1081, %f1873, %f1875;
-	sub.f32 	%f1082, %f1081, %f1080;
-	mul.f32 	%f1083, %f1877, %f1082;
-	mul.f32 	%f1084, %f1871, %f1876;
-	mul.f32 	%f1085, %f1873, %f1874;
-	sub.f32 	%f1086, %f1085, %f1084;
-	mul.f32 	%f1087, %f1086, %f1878;
-	sub.f32 	%f1088, %f1083, %f1087;
-	mul.f32 	%f1089, %f1871, %f1875;
-	mul.f32 	%f1090, %f1872, %f1874;
-	sub.f32 	%f1091, %f1090, %f1089;
-	fma.rn.f32 	%f1092, %f1091, %f1879, %f1088;
-	rcp.rn.f32 	%f1093, %f1092;
-	mul.f32 	%f1886, %f1082, %f1093;
-	mul.f32 	%f1094, %f1873, %f1878;
-	mul.f32 	%f1095, %f1872, %f1879;
-	sub.f32 	%f1096, %f1095, %f1094;
-	mul.f32 	%f1887, %f1093, %f1096;
-	mul.f32 	%f1097, %f1875, %f1879;
-	mul.f32 	%f1098, %f1876, %f1878;
-	sub.f32 	%f1099, %f1098, %f1097;
-	mul.f32 	%f1888, %f1093, %f1099;
-	sub.f32 	%f1100, %f1084, %f1085;
-	mul.f32 	%f1883, %f1100, %f1093;
-	mul.f32 	%f1101, %f1871, %f1879;
-	mul.f32 	%f1102, %f1873, %f1877;
-	sub.f32 	%f1103, %f1102, %f1101;
-	mul.f32 	%f1884, %f1093, %f1103;
-	mul.f32 	%f1104, %f1876, %f1877;
-	mul.f32 	%f1105, %f1874, %f1879;
-	sub.f32 	%f1106, %f1105, %f1104;
-	mul.f32 	%f1885, %f1093, %f1106;
-	mul.f32 	%f1880, %f1091, %f1093;
-	mul.f32 	%f1107, %f1872, %f1877;
-	mul.f32 	%f1108, %f1871, %f1878;
-	sub.f32 	%f1109, %f1108, %f1107;
-	mul.f32 	%f1881, %f1109, %f1093;
-	mul.f32 	%f1110, %f1874, %f1878;
-	mul.f32 	%f1111, %f1875, %f1877;
-	sub.f32 	%f1112, %f1111, %f1110;
-	mul.f32 	%f1882, %f1112, %f1093;
-	bra.uni 	BB1_38;
-
-BB1_27:
-	setp.ne.s32	%p14, %r183, 1;
-	mov.f32 	%f1881, %f1880;
-	mov.f32 	%f1883, %f1880;
-	mov.f32 	%f1884, %f1882;
-	mov.f32 	%f1885, %f1880;
-	mov.f32 	%f1886, %f1882;
-	mov.f32 	%f1887, %f1880;
-	mov.f32 	%f1888, %f1880;
-	@%p14 bra 	BB1_38;
-
-	// inline asm
-	call (%rd175), _optix_get_static_transform_from_handle, (%rd173);
-	// inline asm
-	add.s64 	%rd662, %rd175, 64;
-
-BB1_30:
-	// inline asm
-	cvta.to.global.u64 %rd179, %rd662;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r185,%r186,%r187,%r188}, [%rd179];
-	// inline asm
-	mov.b32 	 %f1886, %r185;
-	mov.b32 	 %f1887, %r186;
-	mov.b32 	 %f1888, %r187;
-	add.s64 	%rd183, %rd662, 16;
-	// inline asm
-	cvta.to.global.u64 %rd182, %rd183;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r189,%r190,%r191,%r192}, [%rd182];
-	// inline asm
-	mov.b32 	 %f1883, %r189;
-	mov.b32 	 %f1884, %r190;
-	mov.b32 	 %f1885, %r191;
-	add.s64 	%rd186, %rd662, 32;
-	// inline asm
-	cvta.to.global.u64 %rd185, %rd186;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r193,%r194,%r195,%r196}, [%rd185];
-	// inline asm
-	mov.b32 	 %f1880, %r193;
-	mov.b32 	 %f1881, %r194;
-	mov.b32 	 %f1882, %r195;
-
-BB1_38:
-	setp.eq.s32	%p18, %r647, 0;
-	@%p18 bra 	BB1_39;
-	bra.uni 	BB1_40;
-
-BB1_39:
-	mov.f32 	%f1860, %f1880;
-	mov.f32 	%f1859, %f1881;
-	mov.f32 	%f1858, %f1882;
-	mov.f32 	%f1857, %f1883;
-	mov.f32 	%f1856, %f1884;
-	mov.f32 	%f1855, %f1885;
-	mov.f32 	%f1854, %f1886;
-	mov.f32 	%f1853, %f1887;
-	mov.f32 	%f1852, %f1888;
-	bra.uni 	BB1_41;
-
-BB1_40:
-	mul.f32 	%f1113, %f1857, %f1887;
-	fma.rn.f32 	%f1114, %f1854, %f1886, %f1113;
-	fma.rn.f32 	%f289, %f1860, %f1888, %f1114;
-	mul.f32 	%f1115, %f1856, %f1887;
-	fma.rn.f32 	%f1116, %f1853, %f1886, %f1115;
-	fma.rn.f32 	%f290, %f1859, %f1888, %f1116;
-	mul.f32 	%f1117, %f1855, %f1887;
-	fma.rn.f32 	%f1118, %f1852, %f1886, %f1117;
-	fma.rn.f32 	%f291, %f1858, %f1888, %f1118;
-	mul.f32 	%f1119, %f1857, %f1884;
-	fma.rn.f32 	%f1120, %f1854, %f1883, %f1119;
-	fma.rn.f32 	%f292, %f1860, %f1885, %f1120;
-	mul.f32 	%f1121, %f1856, %f1884;
-	fma.rn.f32 	%f1122, %f1853, %f1883, %f1121;
-	fma.rn.f32 	%f293, %f1859, %f1885, %f1122;
-	mul.f32 	%f1123, %f1855, %f1884;
-	fma.rn.f32 	%f1124, %f1852, %f1883, %f1123;
-	fma.rn.f32 	%f294, %f1858, %f1885, %f1124;
-	mul.f32 	%f1125, %f1857, %f1881;
-	fma.rn.f32 	%f1126, %f1854, %f1880, %f1125;
-	fma.rn.f32 	%f1860, %f1860, %f1882, %f1126;
-	mul.f32 	%f1127, %f1856, %f1881;
-	fma.rn.f32 	%f1128, %f1853, %f1880, %f1127;
-	fma.rn.f32 	%f1859, %f1859, %f1882, %f1128;
-	mul.f32 	%f1129, %f1855, %f1881;
-	fma.rn.f32 	%f1130, %f1852, %f1880, %f1129;
-	fma.rn.f32 	%f1858, %f1858, %f1882, %f1130;
-	mov.f32 	%f1857, %f292;
-	mov.f32 	%f1856, %f293;
-	mov.f32 	%f1855, %f294;
-	mov.f32 	%f1854, %f289;
-	mov.f32 	%f1853, %f290;
-	mov.f32 	%f1852, %f291;
-
-BB1_41:
-	add.s32 	%r647, %r647, 1;
-	setp.lt.u32	%p19, %r647, %r30;
-	@%p19 bra 	BB1_25;
-
-	mul.f32 	%f1131, %f944, %f1853;
-	fma.rn.f32 	%f1132, %f943, %f1854, %f1131;
-	fma.rn.f32 	%f1898, %f1900, %f1852, %f1132;
-	mul.f32 	%f1133, %f944, %f1856;
-	fma.rn.f32 	%f1134, %f943, %f1857, %f1133;
-	fma.rn.f32 	%f1899, %f1900, %f1855, %f1134;
-	mul.f32 	%f1135, %f944, %f1859;
-	fma.rn.f32 	%f1136, %f943, %f1860, %f1135;
-	fma.rn.f32 	%f1900, %f1900, %f1858, %f1136;
-	bra.uni 	BB1_43;
-
-BB1_24:
-	mov.f32 	%f1898, %f943;
-	mov.f32 	%f1899, %f944;
-
-BB1_43:
-	// inline asm
-	call (%f1138), _optix_get_ray_tmax, ();
-	// inline asm
-	ld.const.u64 	%rd294, [params+80];
-	setp.eq.s64	%p20, %rd294, 0;
-	@%p20 bra 	BB1_48;
-
-	ld.u64 	%rd295, [%rd51];
-	ld.const.u64 	%rd296, [params+328];
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r275,%r276,%r277,%r278}, [%rd236];
+	// end inline asm
+	mov.b32 	%f1022, %r275;
+	mul.f32 	%f1023, %f218, %f1022;
+	fma.rn.f32 	%f1024, %f1000, %f1904, %f1023;
+	mul.f32 	%f1025, %f1020, %f1020;
+	fma.rn.f32 	%f1026, %f1019, %f1019, %f1025;
+	fma.rn.f32 	%f1027, %f1021, %f1021, %f1026;
+	fma.rn.f32 	%f1028, %f1024, %f1024, %f1027;
+	sqrt.rn.f32 	%f1029, %f1028;
+	rcp.rn.f32 	%f1030, %f1029;
+	mul.f32 	%f1901, %f1019, %f1030;
+	mul.f32 	%f1902, %f1020, %f1030;
+	mul.f32 	%f1903, %f1021, %f1030;
+	mul.f32 	%f1904, %f1030, %f1024;
+
+$L__BB1_34:
+	mul.f32 	%f1031, %f1902, %f1902;
+	fma.rn.f32 	%f1032, %f1901, %f1901, %f1031;
+	fma.rn.f32 	%f1033, %f1903, %f1903, %f1032;
+	fma.rn.f32 	%f1034, %f1904, %f1904, %f1033;
+	rcp.rn.f32 	%f1035, %f1034;
+	mul.f32 	%f1036, %f1901, %f1035;
+	mul.f32 	%f1037, %f1902, %f1035;
+	mul.f32 	%f1038, %f1903, %f1035;
+	mul.f32 	%f1039, %f1904, %f1035;
+	mul.f32 	%f1040, %f1901, %f1036;
+	mul.f32 	%f1041, %f1902, %f1037;
+	mul.f32 	%f1042, %f1903, %f1038;
+	mul.f32 	%f1043, %f1901, %f1037;
+	mul.f32 	%f1044, %f1903, %f1039;
+	mul.f32 	%f1045, %f1901, %f1038;
+	mul.f32 	%f1046, %f1902, %f1039;
+	mul.f32 	%f1047, %f1902, %f1038;
+	mul.f32 	%f1048, %f1901, %f1039;
+	sub.f32 	%f1049, %f1040, %f1041;
+	sub.f32 	%f1050, %f1049, %f1042;
+	fma.rn.f32 	%f1051, %f1904, %f1039, %f1050;
+	sub.f32 	%f1052, %f1043, %f1044;
+	add.f32 	%f1053, %f1052, %f1052;
+	add.f32 	%f1054, %f1045, %f1046;
+	add.f32 	%f1055, %f1054, %f1054;
+	add.f32 	%f1056, %f1043, %f1044;
+	add.f32 	%f1057, %f1056, %f1056;
+	sub.f32 	%f1058, %f1041, %f1040;
+	sub.f32 	%f1059, %f1058, %f1042;
+	fma.rn.f32 	%f1060, %f1904, %f1039, %f1059;
+	sub.f32 	%f1061, %f1047, %f1048;
+	add.f32 	%f1062, %f1061, %f1061;
+	sub.f32 	%f1063, %f1045, %f1046;
+	add.f32 	%f1064, %f1063, %f1063;
+	add.f32 	%f1065, %f1047, %f1048;
+	add.f32 	%f1066, %f1065, %f1065;
+	neg.f32 	%f1067, %f1040;
+	sub.f32 	%f1068, %f1067, %f1041;
+	add.f32 	%f1069, %f1042, %f1068;
+	fma.rn.f32 	%f1070, %f1904, %f1039, %f1069;
+	mul.f32 	%f1071, %f1897, %f1051;
+	fma.rn.f32 	%f1072, %f1899, %f1053, %f1071;
+	fma.rn.f32 	%f1913, %f1900, %f1055, %f1072;
+	mul.f32 	%f1073, %f1899, %f1060;
+	fma.rn.f32 	%f1074, %f1897, %f1057, %f1073;
+	fma.rn.f32 	%f1910, %f1900, %f1062, %f1074;
+	mul.f32 	%f1075, %f1899, %f1066;
+	fma.rn.f32 	%f1076, %f1897, %f1064, %f1075;
+	fma.rn.f32 	%f1907, %f1900, %f1070, %f1076;
+	mul.f32 	%f1077, %f1896, %f1051;
+	fma.rn.f32 	%f1912, %f1898, %f1053, %f1077;
+	mul.f32 	%f1078, %f1898, %f1060;
+	fma.rn.f32 	%f1909, %f1896, %f1057, %f1078;
+	mul.f32 	%f1079, %f1898, %f1066;
+	fma.rn.f32 	%f1906, %f1896, %f1064, %f1079;
+	mul.f32 	%f1911, %f1895, %f1051;
+	mul.f32 	%f1908, %f1895, %f1057;
+	mul.f32 	%f1905, %f1895, %f1064;
+
+$L__BB1_37:
+	mul.f32 	%f1111, %f1906, %f1910;
+	mul.f32 	%f1112, %f1907, %f1909;
+	sub.f32 	%f1113, %f1112, %f1111;
+	mul.f32 	%f1114, %f1911, %f1113;
+	mul.f32 	%f1115, %f1905, %f1910;
+	mul.f32 	%f1116, %f1907, %f1908;
+	sub.f32 	%f1117, %f1116, %f1115;
+	mul.f32 	%f1118, %f1117, %f1912;
+	sub.f32 	%f1119, %f1114, %f1118;
+	mul.f32 	%f1120, %f1905, %f1909;
+	mul.f32 	%f1121, %f1906, %f1908;
+	sub.f32 	%f1122, %f1121, %f1120;
+	fma.rn.f32 	%f1123, %f1122, %f1913, %f1119;
+	rcp.rn.f32 	%f1124, %f1123;
+	mul.f32 	%f1920, %f1113, %f1124;
+	mul.f32 	%f1125, %f1907, %f1912;
+	mul.f32 	%f1126, %f1906, %f1913;
+	sub.f32 	%f1127, %f1126, %f1125;
+	mul.f32 	%f1921, %f1127, %f1124;
+	mul.f32 	%f1128, %f1909, %f1913;
+	mul.f32 	%f1129, %f1910, %f1912;
+	sub.f32 	%f1130, %f1129, %f1128;
+	mul.f32 	%f1922, %f1130, %f1124;
+	sub.f32 	%f1131, %f1115, %f1116;
+	mul.f32 	%f1917, %f1131, %f1124;
+	mul.f32 	%f1132, %f1905, %f1913;
+	mul.f32 	%f1133, %f1907, %f1911;
+	sub.f32 	%f1134, %f1133, %f1132;
+	mul.f32 	%f1918, %f1134, %f1124;
+	mul.f32 	%f1135, %f1910, %f1911;
+	mul.f32 	%f1136, %f1908, %f1913;
+	sub.f32 	%f1137, %f1136, %f1135;
+	mul.f32 	%f1919, %f1137, %f1124;
+	mul.f32 	%f1914, %f1122, %f1124;
+	mul.f32 	%f1138, %f1906, %f1911;
+	mul.f32 	%f1139, %f1905, %f1912;
+	sub.f32 	%f1140, %f1139, %f1138;
+	mul.f32 	%f1915, %f1140, %f1124;
+	mul.f32 	%f1141, %f1908, %f1912;
+	mul.f32 	%f1142, %f1909, %f1911;
+	sub.f32 	%f1143, %f1142, %f1141;
+	mul.f32 	%f1916, %f1143, %f1124;
+	bra.uni 	$L__BB1_38;
+
+$L__BB1_29:
+	// begin inline asm
+	call (%rd649), _optix_get_instance_inverse_transform_from_handle, (%rd167);
+	// end inline asm
+
+$L__BB1_30:
+	// begin inline asm
+	cvta.to.global.u64 %rd173, %rd649;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r192,%r193,%r194,%r195}, [%rd173];
+	// end inline asm
+	mov.b32 	%f1920, %r192;
+	mov.b32 	%f1921, %r193;
+	mov.b32 	%f1922, %r194;
+	add.s64 	%rd177, %rd649, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd176, %rd177;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r196,%r197,%r198,%r199}, [%rd176];
+	// end inline asm
+	mov.b32 	%f1917, %r196;
+	mov.b32 	%f1918, %r197;
+	mov.b32 	%f1919, %r198;
+	add.s64 	%rd180, %rd649, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd179, %rd180;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r200,%r201,%r202,%r203}, [%rd179];
+	// end inline asm
+	mov.b32 	%f1914, %r200;
+	mov.b32 	%f1915, %r201;
+	mov.b32 	%f1916, %r202;
+
+$L__BB1_38:
+	setp.eq.s32 	%p20, %r656, 0;
+	@%p20 bra 	$L__BB1_40;
+
+	mul.f32 	%f1144, %f1891, %f1921;
+	fma.rn.f32 	%f1145, %f1888, %f1920, %f1144;
+	fma.rn.f32 	%f304, %f1894, %f1922, %f1145;
+	mul.f32 	%f1146, %f1890, %f1921;
+	fma.rn.f32 	%f1147, %f1887, %f1920, %f1146;
+	fma.rn.f32 	%f305, %f1893, %f1922, %f1147;
+	mul.f32 	%f1148, %f1889, %f1921;
+	fma.rn.f32 	%f1149, %f1886, %f1920, %f1148;
+	fma.rn.f32 	%f1922, %f1892, %f1922, %f1149;
+	mul.f32 	%f1150, %f1891, %f1918;
+	fma.rn.f32 	%f1151, %f1888, %f1917, %f1150;
+	fma.rn.f32 	%f307, %f1894, %f1919, %f1151;
+	mul.f32 	%f1152, %f1890, %f1918;
+	fma.rn.f32 	%f1153, %f1887, %f1917, %f1152;
+	fma.rn.f32 	%f308, %f1893, %f1919, %f1153;
+	mul.f32 	%f1154, %f1889, %f1918;
+	fma.rn.f32 	%f1155, %f1886, %f1917, %f1154;
+	fma.rn.f32 	%f1919, %f1892, %f1919, %f1155;
+	mul.f32 	%f1156, %f1891, %f1915;
+	fma.rn.f32 	%f1157, %f1888, %f1914, %f1156;
+	fma.rn.f32 	%f310, %f1894, %f1916, %f1157;
+	mul.f32 	%f1158, %f1890, %f1915;
+	fma.rn.f32 	%f1159, %f1887, %f1914, %f1158;
+	fma.rn.f32 	%f311, %f1893, %f1916, %f1159;
+	mul.f32 	%f1160, %f1889, %f1915;
+	fma.rn.f32 	%f1161, %f1886, %f1914, %f1160;
+	fma.rn.f32 	%f1916, %f1892, %f1916, %f1161;
+	mov.f32 	%f1914, %f310;
+	mov.f32 	%f1915, %f311;
+	mov.f32 	%f1917, %f307;
+	mov.f32 	%f1918, %f308;
+	mov.f32 	%f1920, %f304;
+	mov.f32 	%f1921, %f305;
+
+$L__BB1_40:
+	add.s32 	%r656, %r656, 1;
+	setp.lt.u32 	%p21, %r656, %r187;
+	mov.f32 	%f1886, %f1922;
+	mov.f32 	%f1887, %f1921;
+	mov.f32 	%f1888, %f1920;
+	mov.f32 	%f1889, %f1919;
+	mov.f32 	%f1890, %f1918;
+	mov.f32 	%f1891, %f1917;
+	mov.f32 	%f1892, %f1916;
+	mov.f32 	%f1893, %f1915;
+	mov.f32 	%f1894, %f1914;
+	@%p21 bra 	$L__BB1_25;
+
+$L__BB1_41:
+	mul.f32 	%f1162, %f1942, %f1921;
+	fma.rn.f32 	%f1163, %f1941, %f1920, %f1162;
+	mul.f32 	%f1164, %f1942, %f1918;
+	fma.rn.f32 	%f1165, %f1941, %f1917, %f1164;
+	mul.f32 	%f1166, %f1942, %f1915;
+	fma.rn.f32 	%f1167, %f1941, %f1914, %f1166;
+	fma.rn.f32 	%f1943, %f975, %f1916, %f1167;
+	fma.rn.f32 	%f1942, %f975, %f1919, %f1165;
+	fma.rn.f32 	%f1941, %f975, %f1922, %f1163;
+	bra.uni 	$L__BB1_43;
+
+$L__BB1_42:
+	mov.f32 	%f1943, %f975;
+
+$L__BB1_43:
+	// begin inline asm
+	call (%f1169), _optix_get_ray_tmax, ();
+	// end inline asm
+	ld.const.u64 	%rd286, [params+80];
+	setp.eq.s64 	%p22, %rd286, 0;
+	@%p22 bra 	$L__BB1_48;
+
+	ld.u64 	%rd287, [%rd47];
+	ld.const.u64 	%rd288, [params+328];
+	cvta.to.global.u64 	%rd289, %rd288;
+	cvt.u64.u32 	%rd18, %r1;
+	mul.wide.u32 	%rd290, %r1, 8;
+	add.s64 	%rd291, %rd289, %rd290;
+	st.global.u64 	[%rd291], %rd287;
+	ld.const.u64 	%rd292, [params+336];
+	cvta.to.global.u64 	%rd293, %rd292;
+	mul.wide.u32 	%rd294, %r1, 4;
+	add.s64 	%rd295, %rd293, %rd294;
+	mov.u32 	%r338, 0;
+	st.global.u32 	[%rd295], %r338;
+	ld.const.u64 	%rd296, [params+344];
 	cvta.to.global.u64 	%rd297, %rd296;
-	cvt.u64.u32	%rd20, %r1;
-	mul.wide.u32 	%rd298, %r1, 8;
-	add.s64 	%rd299, %rd297, %rd298;
-	st.global.u64 	[%rd299], %rd295;
-	ld.const.u64 	%rd300, [params+336];
-	cvta.to.global.u64 	%rd301, %rd300;
-	mul.wide.u32 	%rd302, %r1, 4;
-	add.s64 	%rd303, %rd301, %rd302;
-	mov.u32 	%r331, 0;
-	st.global.u32 	[%rd303], %r331;
-	ld.const.u64 	%rd304, [params+344];
-	cvta.to.global.u64 	%rd305, %rd304;
-	add.s64 	%rd21, %rd305, %rd302;
-	ld.global.u32 	%r9, [%rd21];
-	setp.eq.s32	%p21, %r9, 0;
-	@%p21 bra 	BB1_47;
-
-	// inline asm
-	call (%r332), _optix_read_instance_id, ();
-	// inline asm
-	setp.ge.u32	%p22, %r332, %r9;
-	@%p22 bra 	BB1_47;
-
-	st.global.u32 	[%rd21], %r332;
-
-BB1_47:
-	ld.const.u64 	%rd306, [params+72];
-	cvta.to.global.u64 	%rd307, %rd306;
-	shl.b64 	%rd308, %rd20, 2;
-	add.s64 	%rd309, %rd307, %rd308;
-	st.global.f32 	[%rd309], %f1138;
-	bra.uni 	BB1_114;
-
-BB1_48:
-	fma.rn.f32 	%f314, %f1138, %f1898, %f1851;
-	ld.v4.f32 	{%f1139, %f1140, %f1141, %f1142}, [%rd3+208];
-	ld.v4.f32 	{%f1146, %f1147, %f1148, %f1149}, [%rd3+160];
-	fma.rn.f32 	%f1151, %f314, %f1146, %f1139;
-	fma.rn.f32 	%f1153, %f314, %f1147, %f1140;
-	fma.rn.f32 	%f1155, %f314, %f1148, %f1141;
-	fma.rn.f32 	%f315, %f1138, %f1899, %f1850;
-	ld.v4.f32 	{%f1156, %f1157, %f1158, %f1159}, [%rd3+176];
-	fma.rn.f32 	%f1161, %f315, %f1156, %f1151;
-	fma.rn.f32 	%f1163, %f315, %f1157, %f1153;
-	fma.rn.f32 	%f1165, %f315, %f1158, %f1155;
-	fma.rn.f32 	%f316, %f1138, %f1900, %f1849;
-	ld.v4.f32 	{%f1166, %f1167, %f1168, %f1169}, [%rd3+192];
-	fma.rn.f32 	%f317, %f316, %f1166, %f1161;
-	fma.rn.f32 	%f318, %f316, %f1167, %f1163;
-	fma.rn.f32 	%f319, %f316, %f1168, %f1165;
-	abs.f32 	%f320, %f317;
-	abs.f32 	%f321, %f318;
-	setp.eq.f32	%p23, %f320, 0f00000000;
-	setp.eq.f32	%p24, %f321, 0f00000000;
-	and.pred  	%p25, %p23, %p24;
-	mov.b32 	 %r11, %f317;
-	mov.b32 	 %r333, %f318;
-	and.b32  	%r12, %r333, -2147483648;
-	@%p25 bra 	BB1_52;
-	bra.uni 	BB1_49;
-
-BB1_52:
-	shr.s32 	%r340, %r11, 31;
-	and.b32  	%r341, %r340, 1078530011;
-	or.b32  	%r342, %r341, %r12;
-	mov.b32 	 %f1901, %r342;
-	bra.uni 	BB1_53;
-
-BB1_49:
-	setp.eq.f32	%p26, %f320, 0f7F800000;
-	setp.eq.f32	%p27, %f321, 0f7F800000;
-	and.pred  	%p28, %p26, %p27;
-	@%p28 bra 	BB1_51;
-	bra.uni 	BB1_50;
-
-BB1_51:
-	shr.s32 	%r336, %r11, 31;
-	and.b32  	%r337, %r336, 13483017;
-	add.s32 	%r338, %r337, 1061752795;
-	or.b32  	%r339, %r338, %r12;
-	mov.b32 	 %f1901, %r339;
-	bra.uni 	BB1_53;
-
-BB1_50:
-	max.f32 	%f1173, %f321, %f320;
-	min.f32 	%f1174, %f321, %f320;
-	div.rn.f32 	%f1175, %f1174, %f1173;
-	mul.rn.f32 	%f1176, %f1175, %f1175;
-	mov.f32 	%f1177, 0fC0B59883;
-	mov.f32 	%f1178, 0fBF52C7EA;
-	fma.rn.f32 	%f1179, %f1176, %f1178, %f1177;
-	mov.f32 	%f1180, 0fC0D21907;
-	fma.rn.f32 	%f1181, %f1179, %f1176, %f1180;
-	mul.f32 	%f1182, %f1176, %f1181;
-	mul.f32 	%f1183, %f1175, %f1182;
-	add.f32 	%f1184, %f1176, 0f41355DC0;
-	mov.f32 	%f1185, 0f41E6BD60;
-	fma.rn.f32 	%f1186, %f1184, %f1176, %f1185;
-	mov.f32 	%f1187, 0f419D92C8;
-	fma.rn.f32 	%f1188, %f1186, %f1176, %f1187;
-	rcp.rn.f32 	%f1189, %f1188;
-	fma.rn.f32 	%f1190, %f1183, %f1189, %f1175;
-	mov.f32 	%f1191, 0f3FC90FDB;
-	sub.f32 	%f1192, %f1191, %f1190;
-	setp.gt.f32	%p29, %f321, %f320;
-	selp.f32	%f1193, %f1192, %f1190, %p29;
-	mov.f32 	%f1194, 0f40490FDB;
-	sub.f32 	%f1195, %f1194, %f1193;
-	setp.lt.s32	%p30, %r11, 0;
-	selp.f32	%f1196, %f1195, %f1193, %p30;
-	mov.b32 	 %r334, %f1196;
-	or.b32  	%r335, %r334, %r12;
-	mov.b32 	 %f1197, %r335;
-	add.f32 	%f1198, %f320, %f321;
-	setp.gtu.f32	%p31, %f1198, 0f7F800000;
-	selp.f32	%f1901, %f1198, %f1197, %p31;
-
-BB1_53:
-	add.f32 	%f1202, %f1901, 0f40C90FDB;
-	setp.lt.f32	%p32, %f1901, 0f00000000;
-	selp.f32	%f326, %f1202, %f1901, %p32;
-	ld.v2.f32 	{%f1203, %f1204}, [%rd3+288];
-	div.rn.f32 	%f327, %f319, %f1203;
-	ld.v4.f32 	{%f1207, %f1208, %f1209, %f1210}, [%rd3+32];
-	mul.f32 	%f1214, %f318, 0fC0C90FDB;
-	mul.f32 	%f1215, %f1214, %f1207;
-	mul.f32 	%f1216, %f1214, %f1208;
-	mul.f32 	%f1217, %f1214, %f1209;
-	ld.v4.f32 	{%f1218, %f1219, %f1220, %f1221}, [%rd3+48];
-	mul.f32 	%f1225, %f317, 0f40C90FDB;
-	fma.rn.f32 	%f1226, %f1225, %f1218, %f1215;
-	fma.rn.f32 	%f1227, %f1225, %f1219, %f1216;
-	fma.rn.f32 	%f1228, %f1225, %f1220, %f1217;
-	ld.v4.f32 	{%f1229, %f1230, %f1231, %f1232}, [%rd3+64];
-	mov.f32 	%f2039, 0f00000000;
-	fma.rn.f32 	%f2048, %f2039, %f1229, %f1226;
-	fma.rn.f32 	%f2049, %f2039, %f1230, %f1227;
-	fma.rn.f32 	%f2050, %f2039, %f1231, %f1228;
-	mul.f32 	%f1236, %f1207, 0f00000000;
-	mul.f32 	%f1237, %f1208, 0f00000000;
-	mul.f32 	%f1238, %f1209, 0f00000000;
-	fma.rn.f32 	%f1239, %f2039, %f1218, %f1236;
-	fma.rn.f32 	%f1240, %f2039, %f1219, %f1237;
-	fma.rn.f32 	%f1241, %f2039, %f1220, %f1238;
-	fma.rn.f32 	%f2045, %f1203, %f1229, %f1239;
-	fma.rn.f32 	%f2046, %f1203, %f1230, %f1240;
-	fma.rn.f32 	%f2047, %f1203, %f1231, %f1241;
-	mul.f32 	%f1242, %f2049, %f2047;
-	mul.f32 	%f1243, %f2050, %f2046;
-	sub.f32 	%f1244, %f1242, %f1243;
-	mul.f32 	%f1245, %f2050, %f2045;
-	mul.f32 	%f1246, %f2048, %f2047;
-	sub.f32 	%f1247, %f1245, %f1246;
-	mul.f32 	%f1248, %f2048, %f2046;
-	mul.f32 	%f1249, %f2049, %f2045;
-	sub.f32 	%f1250, %f1248, %f1249;
-	mul.f32 	%f1251, %f1244, %f1244;
-	fma.rn.f32 	%f1252, %f1247, %f1247, %f1251;
-	fma.rn.f32 	%f1253, %f1250, %f1250, %f1252;
-	sqrt.rn.f32 	%f1254, %f1253;
-	div.rn.f32 	%f1255, %f1244, %f1254;
-	div.rn.f32 	%f1256, %f1247, %f1254;
-	div.rn.f32 	%f1257, %f1250, %f1254;
-	mul.f32 	%f1258, %f317, %f317;
-	fma.rn.f32 	%f1259, %f318, %f318, %f1258;
-	sqrt.rn.f32 	%f1260, %f1259;
-	sub.f32 	%f1261, %f1204, %f1260;
-	fma.rn.f32 	%f2057, %f1255, %f1261, %f314;
-	fma.rn.f32 	%f2058, %f1256, %f1261, %f315;
-	fma.rn.f32 	%f2059, %f1257, %f1261, %f316;
-	ld.u8 	%rs10, [%rd3+296];
-	setp.eq.s16	%p33, %rs10, 0;
-	neg.f32 	%f1262, %f1255;
-	neg.f32 	%f1263, %f1256;
-	neg.f32 	%f1264, %f1257;
-	selp.f32	%f2056, %f1257, %f1264, %p33;
-	selp.f32	%f2055, %f1256, %f1263, %p33;
-	selp.f32	%f2054, %f1255, %f1262, %p33;
-	selp.f32	%f1265, 0f3F800000, 0fBF800000, %p33;
-	mul.f32 	%f1266, %f1204, %f1265;
-	div.rn.f32 	%f2042, %f2048, %f1266;
-	div.rn.f32 	%f2043, %f2049, %f1266;
-	div.rn.f32 	%f2044, %f2050, %f1266;
-	ld.u64 	%rd23, [%rd51];
-	ld.const.u64 	%rd310, [params+344];
-	cvta.to.global.u64 	%rd311, %rd310;
-	cvt.u64.u32	%rd24, %r1;
-	mul.wide.u32 	%rd312, %r1, 4;
-	add.s64 	%rd25, %rd311, %rd312;
-	ld.global.u32 	%r13, [%rd25];
-	setp.eq.s32	%p34, %r13, 0;
-	@%p34 bra 	BB1_54;
-
-	// inline asm
-	call (%r343), _optix_read_instance_id, ();
-	// inline asm
-	setp.ge.u32	%p35, %r343, %r13;
-	@%p35 bra 	BB1_54;
-
-	mov.f32 	%f1967, 0f00000000;
-	mov.f32 	%f1968, 0f3F800000;
-	mov.f32 	%f1905, %f1968;
-	mov.f32 	%f1904, %f1967;
-	mov.f32 	%f1903, %f1967;
-	mov.f32 	%f1902, %f1967;
-	mov.f32 	%f1909, %f1967;
-	mov.f32 	%f1908, %f1968;
-	mov.f32 	%f1907, %f1967;
-	mov.f32 	%f1906, %f1967;
-	mov.f32 	%f1913, %f1967;
-	mov.f32 	%f1912, %f1967;
-	mov.f32 	%f1911, %f1968;
-	mov.f32 	%f1910, %f1967;
-	@%p2 bra 	BB1_74;
-
-	add.s32 	%r648, %r30, -1;
-	setp.lt.s32	%p37, %r648, 0;
-	@%p37 bra 	BB1_74;
-
-BB1_58:
+	add.s64 	%rd19, %rd297, %rd294;
+	ld.global.u32 	%r10, [%rd19];
+	setp.eq.s32 	%p23, %r10, 0;
+	@%p23 bra 	$L__BB1_47;
+
+	// begin inline asm
+	call (%r339), _optix_read_instance_id, ();
+	// end inline asm
+	setp.ge.u32 	%p24, %r339, %r10;
+	@%p24 bra 	$L__BB1_47;
+
+	st.global.u32 	[%rd19], %r339;
+
+$L__BB1_47:
+	ld.const.u64 	%rd298, [params+72];
+	cvta.to.global.u64 	%rd299, %rd298;
+	shl.b64 	%rd300, %rd18, 2;
+	add.s64 	%rd301, %rd299, %rd300;
+	st.global.f32 	[%rd301], %f1169;
+	bra.uni 	$L__BB1_113;
+
+$L__BB1_48:
+	fma.rn.f32 	%f341, %f1169, %f1941, %f1883;
+	fma.rn.f32 	%f342, %f1169, %f1942, %f1884;
+	fma.rn.f32 	%f343, %f1169, %f1943, %f1885;
+	add.s64 	%rd20, %rd3, 208;
+	ld.v4.f32 	{%f1171, %f1172, %f1173, %f1174}, [%rd3+208];
+	ld.f32 	%f1178, [%rd3+160];
+	fma.rn.f32 	%f1179, %f341, %f1178, %f1171;
+	ld.f32 	%f1180, [%rd3+164];
+	fma.rn.f32 	%f1181, %f341, %f1180, %f1172;
+	ld.f32 	%f1182, [%rd3+168];
+	fma.rn.f32 	%f1183, %f341, %f1182, %f1173;
+	ld.f32 	%f1184, [%rd3+176];
+	fma.rn.f32 	%f1185, %f342, %f1184, %f1179;
+	ld.f32 	%f1186, [%rd3+180];
+	fma.rn.f32 	%f1187, %f342, %f1186, %f1181;
+	ld.f32 	%f1188, [%rd3+184];
+	fma.rn.f32 	%f1189, %f342, %f1188, %f1183;
+	ld.f32 	%f1190, [%rd3+192];
+	fma.rn.f32 	%f344, %f343, %f1190, %f1185;
+	ld.f32 	%f1191, [%rd3+196];
+	fma.rn.f32 	%f345, %f343, %f1191, %f1187;
+	ld.f32 	%f1192, [%rd3+200];
+	fma.rn.f32 	%f346, %f343, %f1192, %f1189;
+	abs.f32 	%f347, %f344;
+	abs.f32 	%f348, %f345;
+	setp.eq.f32 	%p25, %f347, 0f00000000;
+	setp.eq.f32 	%p26, %f348, 0f00000000;
+	and.pred  	%p27, %p25, %p26;
+	mov.b32 	%r12, %f344;
+	mov.b32 	%r340, %f345;
+	and.b32  	%r13, %r340, -2147483648;
+	@%p27 bra 	$L__BB1_52;
+	bra.uni 	$L__BB1_49;
+
+$L__BB1_52:
+	shr.s32 	%r345, %r12, 31;
+	and.b32  	%r346, %r345, 1078530011;
+	or.b32  	%r347, %r346, %r13;
+	mov.b32 	%f1944, %r347;
+	bra.uni 	$L__BB1_53;
+
+$L__BB1_49:
+	setp.eq.f32 	%p28, %f347, 0f7F800000;
+	setp.eq.f32 	%p29, %f348, 0f7F800000;
+	and.pred  	%p30, %p28, %p29;
+	@%p30 bra 	$L__BB1_51;
+	bra.uni 	$L__BB1_50;
+
+$L__BB1_51:
+	setp.lt.s32 	%p34, %r12, 0;
+	selp.b32 	%r343, 1075235812, 1061752795, %p34;
+	or.b32  	%r344, %r343, %r13;
+	mov.b32 	%f1944, %r344;
+	bra.uni 	$L__BB1_53;
+
+$L__BB1_50:
+	setp.lt.s32 	%p31, %r12, 0;
+	min.f32 	%f1193, %f348, %f347;
+	max.f32 	%f1194, %f348, %f347;
+	div.rn.f32 	%f1195, %f1193, %f1194;
+	mul.rn.f32 	%f1196, %f1195, %f1195;
+	mov.f32 	%f1197, 0fC0B59883;
+	mov.f32 	%f1198, 0fBF52C7EA;
+	fma.rn.f32 	%f1199, %f1196, %f1198, %f1197;
+	mov.f32 	%f1200, 0fC0D21907;
+	fma.rn.f32 	%f1201, %f1199, %f1196, %f1200;
+	mul.f32 	%f1202, %f1196, %f1201;
+	mul.f32 	%f1203, %f1195, %f1202;
+	add.f32 	%f1204, %f1196, 0f41355DC0;
+	mov.f32 	%f1205, 0f41E6BD60;
+	fma.rn.f32 	%f1206, %f1204, %f1196, %f1205;
+	mov.f32 	%f1207, 0f419D92C8;
+	fma.rn.f32 	%f1208, %f1206, %f1196, %f1207;
+	rcp.rn.f32 	%f1209, %f1208;
+	fma.rn.f32 	%f1210, %f1203, %f1209, %f1195;
+	mov.f32 	%f1211, 0f3FC90FDB;
+	sub.f32 	%f1212, %f1211, %f1210;
+	setp.gt.f32 	%p32, %f348, %f347;
+	selp.f32 	%f1213, %f1212, %f1210, %p32;
+	mov.f32 	%f1214, 0f40490FDB;
+	sub.f32 	%f1215, %f1214, %f1213;
+	selp.f32 	%f1216, %f1215, %f1213, %p31;
+	mov.b32 	%r341, %f1216;
+	or.b32  	%r342, %r13, %r341;
+	mov.b32 	%f1217, %r342;
+	add.f32 	%f1218, %f347, %f348;
+	setp.le.f32 	%p33, %f1218, 0f7F800000;
+	selp.f32 	%f1944, %f1217, %f1218, %p33;
+
+$L__BB1_53:
+	add.f32 	%f1222, %f1944, 0f40C90FDB;
+	setp.lt.f32 	%p35, %f1944, 0f00000000;
+	mov.f32 	%f2082, 0f00000000;
+	selp.f32 	%f353, %f1222, %f1944, %p35;
+	ld.v2.f32 	{%f1223, %f1224}, [%rd20+80];
+	div.rn.f32 	%f354, %f346, %f1223;
+	ld.v4.f32 	{%f1227, %f1228, %f1229, %f1230}, [%rd20+-176];
+	mul.f32 	%f1234, %f345, 0fC0C90FDB;
+	mul.f32 	%f1235, %f1234, %f1227;
+	mul.f32 	%f1236, %f1234, %f1228;
+	mul.f32 	%f1237, %f1234, %f1229;
+	ld.v4.f32 	{%f1238, %f1239, %f1240, %f1241}, [%rd20+-160];
+	mul.f32 	%f1245, %f344, 0f40C90FDB;
+	fma.rn.f32 	%f1246, %f1245, %f1238, %f1235;
+	fma.rn.f32 	%f1247, %f1245, %f1239, %f1236;
+	fma.rn.f32 	%f1248, %f1245, %f1240, %f1237;
+	ld.f32 	%f1249, [%rd20+-144];
+	fma.rn.f32 	%f2073, %f2082, %f1249, %f1246;
+	ld.f32 	%f1250, [%rd20+-140];
+	fma.rn.f32 	%f2074, %f2082, %f1250, %f1247;
+	ld.f32 	%f1251, [%rd20+-136];
+	fma.rn.f32 	%f2075, %f2082, %f1251, %f1248;
+	mul.f32 	%f1252, %f1227, 0f00000000;
+	mul.f32 	%f1253, %f1228, 0f00000000;
+	mul.f32 	%f1254, %f1229, 0f00000000;
+	fma.rn.f32 	%f1255, %f2082, %f1238, %f1252;
+	fma.rn.f32 	%f1256, %f2082, %f1239, %f1253;
+	fma.rn.f32 	%f1257, %f2082, %f1240, %f1254;
+	fma.rn.f32 	%f2070, %f1223, %f1249, %f1255;
+	fma.rn.f32 	%f2071, %f1223, %f1250, %f1256;
+	fma.rn.f32 	%f2072, %f1223, %f1251, %f1257;
+	mul.f32 	%f1258, %f2074, %f2072;
+	mul.f32 	%f1259, %f2075, %f2071;
+	sub.f32 	%f1260, %f1258, %f1259;
+	mul.f32 	%f1261, %f2075, %f2070;
+	mul.f32 	%f1262, %f2073, %f2072;
+	sub.f32 	%f1263, %f1261, %f1262;
+	mul.f32 	%f1264, %f2073, %f2071;
+	mul.f32 	%f1265, %f2074, %f2070;
+	sub.f32 	%f1266, %f1264, %f1265;
+	mul.f32 	%f1267, %f1260, %f1260;
+	fma.rn.f32 	%f1268, %f1263, %f1263, %f1267;
+	fma.rn.f32 	%f1269, %f1266, %f1266, %f1268;
+	sqrt.rn.f32 	%f1270, %f1269;
+	div.rn.f32 	%f1271, %f1260, %f1270;
+	div.rn.f32 	%f1272, %f1263, %f1270;
+	div.rn.f32 	%f1273, %f1266, %f1270;
+	mul.f32 	%f1274, %f344, %f344;
+	fma.rn.f32 	%f1275, %f345, %f345, %f1274;
+	sqrt.rn.f32 	%f1276, %f1275;
+	sub.f32 	%f1277, %f1224, %f1276;
+	fma.rn.f32 	%f2100, %f1271, %f1277, %f341;
+	fma.rn.f32 	%f2101, %f1272, %f1277, %f342;
+	fma.rn.f32 	%f2102, %f1273, %f1277, %f343;
+	ld.u8 	%rs2, [%rd20+88];
+	setp.eq.s16 	%p36, %rs2, 0;
+	neg.f32 	%f1278, %f1271;
+	neg.f32 	%f1279, %f1272;
+	neg.f32 	%f1280, %f1273;
+	selp.f32 	%f2069, %f1273, %f1280, %p36;
+	selp.f32 	%f2068, %f1272, %f1279, %p36;
+	selp.f32 	%f2067, %f1271, %f1278, %p36;
+	selp.f32 	%f1281, 0f3F800000, 0fBF800000, %p36;
+	mul.f32 	%f1282, %f1224, %f1281;
+	div.rn.f32 	%f2079, %f2073, %f1282;
+	div.rn.f32 	%f2080, %f2074, %f1282;
+	div.rn.f32 	%f2081, %f2075, %f1282;
+	ld.u64 	%rd21, [%rd47];
+	ld.const.u64 	%rd302, [params+344];
+	cvta.to.global.u64 	%rd303, %rd302;
+	cvt.u64.u32 	%rd22, %r1;
+	mul.wide.u32 	%rd304, %r1, 4;
+	add.s64 	%rd23, %rd303, %rd304;
+	ld.global.u32 	%r14, [%rd23];
+	setp.eq.s32 	%p37, %r14, 0;
+	mov.f32 	%f2083, %f2082;
+	mov.f32 	%f2084, %f2082;
+	mov.f32 	%f2097, %f2067;
+	mov.f32 	%f2098, %f2068;
+	mov.f32 	%f2099, %f2069;
+	@%p37 bra 	$L__BB1_101;
+
+	// begin inline asm
+	call (%r348), _optix_read_instance_id, ();
+	// end inline asm
+	setp.ge.u32 	%p38, %r348, %r14;
+	mov.f32 	%f2097, %f2067;
+	mov.f32 	%f2098, %f2068;
+	mov.f32 	%f2099, %f2069;
+	@%p38 bra 	$L__BB1_101;
+
+	// begin inline asm
+	call (%r349), _optix_get_transform_list_size, ();
+	// end inline asm
+	setp.eq.s32 	%p39, %r349, 0;
+	mov.f32 	%f2044, 0f00000000;
+	mov.f32 	%f2043, 0f3F800000;
+	mov.f32 	%f1981, %f2043;
+	mov.f32 	%f1982, %f2044;
+	mov.f32 	%f1983, %f2044;
+	mov.f32 	%f1984, %f2044;
+	mov.f32 	%f1977, %f2044;
+	mov.f32 	%f1978, %f2043;
+	mov.f32 	%f1979, %f2044;
+	mov.f32 	%f1980, %f2044;
+	mov.f32 	%f1973, %f2044;
+	mov.f32 	%f1974, %f2044;
+	mov.f32 	%f1975, %f2043;
+	mov.f32 	%f1976, %f2044;
+	@%p39 bra 	$L__BB1_73;
+
+	// begin inline asm
+	call (%r350), _optix_get_transform_list_size, ();
+	// end inline asm
+	// begin inline asm
+	call (%f1298), _optix_get_ray_time, ();
+	// end inline asm
+	setp.lt.s32 	%p40, %r350, 1;
+	@%p40 bra 	$L__BB1_73;
+
+	add.s32 	%r657, %r350, 1;
+	mov.u32 	%r658, 1;
+
+$L__BB1_58:
 	.pragma "nounroll";
-	// inline asm
-	call (%rd313), _optix_get_transform_list_handle, (%r648);
-	// inline asm
-	// inline asm
-	call (%r345), _optix_get_transform_type_from_handle, (%rd313);
-	// inline asm
-	and.b32  	%r346, %r345, -2;
-	setp.eq.s32	%p38, %r346, 2;
-	@%p38 bra 	BB1_64;
-	bra.uni 	BB1_59;
-
-BB1_64:
-	setp.eq.s32	%p41, %r345, 2;
-	@%p41 bra 	BB1_68;
-	bra.uni 	BB1_65;
-
-BB1_68:
-	// inline asm
-	call (%rd387), _optix_get_matrix_motion_transform_from_handle, (%rd313);
-	// inline asm
-	// inline asm
-	cvta.to.global.u64 %rd389, %rd387;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r434,%r435,%r436,%r437}, [%rd389];
-	// inline asm
-	mov.b32	{%rs13, %rs14}, %r436;
-	add.s64 	%rd393, %rd387, 16;
-	// inline asm
-	cvta.to.global.u64 %rd392, %rd393;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r438,%r439,%r440,%r441}, [%rd392];
-	// inline asm
-	add.s64 	%rd396, %rd387, 32;
-	// inline asm
-	cvta.to.global.u64 %rd395, %rd396;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r442,%r443,%r444,%r445}, [%rd395];
-	// inline asm
-	add.s64 	%rd399, %rd387, 48;
-	// inline asm
-	cvta.to.global.u64 %rd398, %rd399;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r446,%r447,%r448,%r449}, [%rd398];
-	// inline asm
-	add.s64 	%rd402, %rd387, 64;
-	// inline asm
-	cvta.to.global.u64 %rd401, %rd402;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r450,%r451,%r452,%r453}, [%rd401];
-	// inline asm
-	add.s64 	%rd405, %rd387, 80;
-	// inline asm
-	cvta.to.global.u64 %rd404, %rd405;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r454,%r455,%r456,%r457}, [%rd404];
-	// inline asm
-	add.s64 	%rd408, %rd387, 96;
-	// inline asm
-	cvta.to.global.u64 %rd407, %rd408;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r458,%r459,%r460,%r461}, [%rd407];
-	// inline asm
-	add.s64 	%rd411, %rd387, 112;
-	// inline asm
-	cvta.to.global.u64 %rd410, %rd411;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r462,%r463,%r464,%r465}, [%rd410];
-	// inline asm
-	mov.b32 	 %f1408, %r437;
-	mov.b32 	 %f1409, %r438;
-	cvt.u32.u16	%r478, %rs13;
-	add.s32 	%r479, %r478, -1;
-	cvt.rn.f32.s32	%f1410, %r479;
-	sub.f32 	%f1411, %f946, %f1408;
-	mul.f32 	%f1412, %f1411, %f1410;
-	sub.f32 	%f1413, %f1409, %f1408;
-	div.rn.f32 	%f1414, %f1412, %f1413;
-	min.f32 	%f1415, %f1410, %f1414;
-	mov.f32 	%f1416, 0f00000000;
-	max.f32 	%f1417, %f1416, %f1415;
-	cvt.rmi.f32.f32	%f1418, %f1417;
-	cvt.rzi.s32.f32	%r480, %f1418;
-	cvt.s64.s32	%rd33, %r480;
-	mul.wide.s32 	%rd422, %r480, 48;
-	add.s64 	%rd414, %rd396, %rd422;
-	// inline asm
+	add.s32 	%r352, %r657, -2;
+	// begin inline asm
+	call (%rd305), _optix_get_transform_list_handle, (%r352);
+	// end inline asm
+	// begin inline asm
+	call (%r353), _optix_get_transform_type_from_handle, (%rd305);
+	// end inline asm
+	or.b32  	%r354, %r353, 1;
+	setp.eq.s32 	%p41, %r354, 3;
+	@%p41 bra 	$L__BB1_64;
+	bra.uni 	$L__BB1_59;
+
+$L__BB1_64:
+	setp.eq.s32 	%p44, %r353, 2;
+	@%p44 bra 	$L__BB1_68;
+	bra.uni 	$L__BB1_65;
+
+$L__BB1_68:
+	// begin inline asm
+	call (%rd377), _optix_get_matrix_motion_transform_from_handle, (%rd305);
+	// end inline asm
+	// begin inline asm
+	cvta.to.global.u64 %rd379, %rd377;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r442,%r443,%r444,%r445}, [%rd379];
+	// end inline asm
+	add.s64 	%rd383, %rd377, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd382, %rd383;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r446,%r447,%r448,%r449}, [%rd382];
+	// end inline asm
+	add.s64 	%rd386, %rd377, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd385, %rd386;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r450,%r451,%r452,%r453}, [%rd385];
+	// end inline asm
+	add.s64 	%rd389, %rd377, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd388, %rd389;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r454,%r455,%r456,%r457}, [%rd388];
+	// end inline asm
+	add.s64 	%rd392, %rd377, 64;
+	// begin inline asm
+	cvta.to.global.u64 %rd391, %rd392;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r458,%r459,%r460,%r461}, [%rd391];
+	// end inline asm
+	add.s64 	%rd395, %rd377, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd394, %rd395;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r462,%r463,%r464,%r465}, [%rd394];
+	// end inline asm
+	add.s64 	%rd398, %rd377, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd397, %rd398;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r466,%r467,%r468,%r469}, [%rd397];
+	// end inline asm
+	add.s64 	%rd401, %rd377, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd400, %rd401;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r470,%r471,%r472,%r473}, [%rd400];
+	// end inline asm
+	mov.b32 	%f1426, %r445;
+	mov.b32 	%f1427, %r446;
+	and.b32  	%r486, %r444, 65535;
+	add.s32 	%r487, %r486, -1;
+	cvt.rn.f32.s32 	%f1428, %r487;
+	sub.f32 	%f1429, %f1298, %f1426;
+	mul.f32 	%f1430, %f1429, %f1428;
+	sub.f32 	%f1431, %f1427, %f1426;
+	div.rn.f32 	%f1432, %f1430, %f1431;
+	min.f32 	%f1433, %f1428, %f1432;
+	mov.f32 	%f1434, 0f00000000;
+	max.f32 	%f1435, %f1434, %f1433;
+	cvt.rmi.f32.f32 	%f1436, %f1435;
+	sub.f32 	%f456, %f1435, %f1436;
+	cvt.rzi.s32.f32 	%r488, %f1436;
+	cvt.s64.s32 	%rd30, %r488;
+	mul.wide.s32 	%rd412, %r488, 48;
+	add.s64 	%rd404, %rd386, %rd412;
+	// begin inline asm
+	cvta.to.global.u64 %rd403, %rd404;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r474,%r475,%r476,%r477}, [%rd403];
+	// end inline asm
+	mov.b32 	%f1981, %r474;
+	mov.b32 	%f1982, %r475;
+	mov.b32 	%f1983, %r476;
+	mov.b32 	%f1984, %r477;
+	add.s64 	%rd407, %rd404, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd406, %rd407;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r478,%r479,%r480,%r481}, [%rd406];
+	// end inline asm
+	mov.b32 	%f1977, %r478;
+	mov.b32 	%f1978, %r479;
+	mov.b32 	%f1979, %r480;
+	mov.b32 	%f1980, %r481;
+	add.s64 	%rd410, %rd404, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd409, %rd410;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r482,%r483,%r484,%r485}, [%rd409];
+	// end inline asm
+	mov.b32 	%f1973, %r482;
+	mov.b32 	%f1974, %r483;
+	mov.b32 	%f1975, %r484;
+	mov.b32 	%f1976, %r485;
+	setp.leu.f32 	%p46, %f456, 0f00000000;
+	@%p46 bra 	$L__BB1_70;
+
+	mov.f32 	%f1437, 0f3F800000;
+	sub.f32 	%f1438, %f1437, %f456;
+	mul.lo.s64 	%rd422, %rd30, 48;
+	add.s64 	%rd423, %rd377, %rd422;
+	add.s64 	%rd414, %rd423, 80;
+	// begin inline asm
 	cvta.to.global.u64 %rd413, %rd414;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r466,%r467,%r468,%r469}, [%rd413];
-	// inline asm
-	mov.b32 	 %f1938, %r466;
-	mov.b32 	 %f1939, %r467;
-	mov.b32 	 %f1940, %r468;
-	mov.b32 	 %f1941, %r469;
-	add.s64 	%rd417, %rd414, 16;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r489,%r490,%r491,%r492}, [%rd413];
+	// end inline asm
+	mov.b32 	%f1439, %r489;
+	mov.b32 	%f1440, %r490;
+	mov.b32 	%f1441, %r491;
+	mov.b32 	%f1442, %r492;
+	mul.f32 	%f1443, %f456, %f1439;
+	mul.f32 	%f1444, %f456, %f1440;
+	mul.f32 	%f1445, %f456, %f1441;
+	mul.f32 	%f1446, %f456, %f1442;
+	fma.rn.f32 	%f1981, %f1438, %f1981, %f1443;
+	fma.rn.f32 	%f1982, %f1438, %f1982, %f1444;
+	fma.rn.f32 	%f1983, %f1438, %f1983, %f1445;
+	fma.rn.f32 	%f1984, %f1438, %f1984, %f1446;
+	add.s64 	%rd417, %rd423, 96;
+	// begin inline asm
 	cvta.to.global.u64 %rd416, %rd417;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r470,%r471,%r472,%r473}, [%rd416];
-	// inline asm
-	mov.b32 	 %f1934, %r470;
-	mov.b32 	 %f1935, %r471;
-	mov.b32 	 %f1936, %r472;
-	mov.b32 	 %f1937, %r473;
-	add.s64 	%rd420, %rd414, 32;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r493,%r494,%r495,%r496}, [%rd416];
+	// end inline asm
+	mov.b32 	%f1447, %r493;
+	mov.b32 	%f1448, %r494;
+	mov.b32 	%f1449, %r495;
+	mov.b32 	%f1450, %r496;
+	mul.f32 	%f1451, %f456, %f1447;
+	mul.f32 	%f1452, %f456, %f1448;
+	mul.f32 	%f1453, %f456, %f1449;
+	mul.f32 	%f1454, %f456, %f1450;
+	fma.rn.f32 	%f1977, %f1438, %f1977, %f1451;
+	fma.rn.f32 	%f1978, %f1438, %f1978, %f1452;
+	fma.rn.f32 	%f1979, %f1438, %f1979, %f1453;
+	fma.rn.f32 	%f1980, %f1438, %f1980, %f1454;
+	add.s64 	%rd420, %rd423, 112;
+	// begin inline asm
 	cvta.to.global.u64 %rd419, %rd420;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r474,%r475,%r476,%r477}, [%rd419];
-	// inline asm
-	sub.f32 	%f436, %f1417, %f1418;
-	mov.b32 	 %f1930, %r474;
-	mov.b32 	 %f1931, %r475;
-	mov.b32 	 %f1932, %r476;
-	mov.b32 	 %f1933, %r477;
-	setp.leu.f32	%p43, %f436, 0f00000000;
-	@%p43 bra 	BB1_70;
-
-	mul.lo.s64 	%rd432, %rd33, 48;
-	add.s64 	%rd433, %rd387, %rd432;
-	add.s64 	%rd424, %rd433, 80;
-	// inline asm
-	cvta.to.global.u64 %rd423, %rd424;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r481,%r482,%r483,%r484}, [%rd423];
-	// inline asm
-	mov.b32 	 %f1419, %r481;
-	mov.b32 	 %f1420, %r482;
-	mov.b32 	 %f1421, %r483;
-	mov.b32 	 %f1422, %r484;
-	add.s64 	%rd427, %rd433, 96;
-	// inline asm
-	cvta.to.global.u64 %rd426, %rd427;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r485,%r486,%r487,%r488}, [%rd426];
-	// inline asm
-	mov.b32 	 %f1423, %r485;
-	mov.b32 	 %f1424, %r486;
-	mov.b32 	 %f1425, %r487;
-	mov.b32 	 %f1426, %r488;
-	add.s64 	%rd430, %rd433, 112;
-	// inline asm
-	cvta.to.global.u64 %rd429, %rd430;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r489,%r490,%r491,%r492}, [%rd429];
-	// inline asm
-	mov.f32 	%f1427, 0f3F800000;
-	sub.f32 	%f1428, %f1427, %f436;
-	mul.f32 	%f1429, %f436, %f1419;
-	mul.f32 	%f1430, %f436, %f1420;
-	mul.f32 	%f1431, %f436, %f1421;
-	mul.f32 	%f1432, %f436, %f1422;
-	fma.rn.f32 	%f1938, %f1428, %f1938, %f1429;
-	fma.rn.f32 	%f1939, %f1428, %f1939, %f1430;
-	fma.rn.f32 	%f1940, %f1428, %f1940, %f1431;
-	fma.rn.f32 	%f1941, %f1428, %f1941, %f1432;
-	mul.f32 	%f1433, %f436, %f1423;
-	mul.f32 	%f1434, %f436, %f1424;
-	mul.f32 	%f1435, %f436, %f1425;
-	mul.f32 	%f1436, %f436, %f1426;
-	fma.rn.f32 	%f1934, %f1428, %f1934, %f1433;
-	fma.rn.f32 	%f1935, %f1428, %f1935, %f1434;
-	fma.rn.f32 	%f1936, %f1428, %f1936, %f1435;
-	fma.rn.f32 	%f1937, %f1428, %f1937, %f1436;
-	mov.b32 	 %f1437, %r489;
-	mov.b32 	 %f1438, %r490;
-	mov.b32 	 %f1439, %r491;
-	mov.b32 	 %f1440, %r492;
-	mul.f32 	%f1441, %f436, %f1437;
-	mul.f32 	%f1442, %f436, %f1438;
-	mul.f32 	%f1443, %f436, %f1439;
-	mul.f32 	%f1444, %f436, %f1440;
-	fma.rn.f32 	%f1930, %f1428, %f1930, %f1441;
-	fma.rn.f32 	%f1931, %f1428, %f1931, %f1442;
-	fma.rn.f32 	%f1932, %f1428, %f1932, %f1443;
-	fma.rn.f32 	%f1933, %f1428, %f1933, %f1444;
-	bra.uni 	BB1_70;
-
-BB1_59:
-	mov.f32 	%f1930, 0f00000000;
-	mov.f32 	%f1932, 0f3F800000;
-	setp.eq.s32	%p39, %r345, 4;
-	@%p39 bra 	BB1_62;
-	bra.uni 	BB1_60;
-
-BB1_62:
-	// inline asm
-	call (%rd663), _optix_get_instance_transform_from_handle, (%rd313);
-	// inline asm
-	bra.uni 	BB1_63;
-
-BB1_65:
-	// inline asm
-	call (%rd328), _optix_get_srt_motion_transform_from_handle, (%rd313);
-	// inline asm
-	// inline asm
-	cvta.to.global.u64 %rd330, %rd328;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r359,%r360,%r361,%r362}, [%rd330];
-	// inline asm
-	mov.b32	{%rs11, %rs12}, %r361;
-	add.s64 	%rd334, %rd328, 16;
-	// inline asm
-	cvta.to.global.u64 %rd333, %rd334;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r363,%r364,%r365,%r366}, [%rd333];
-	// inline asm
-	add.s64 	%rd337, %rd328, 32;
-	// inline asm
-	cvta.to.global.u64 %rd336, %rd337;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r367,%r368,%r369,%r370}, [%rd336];
-	// inline asm
-	add.s64 	%rd340, %rd328, 48;
-	// inline asm
-	cvta.to.global.u64 %rd339, %rd340;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r371,%r372,%r373,%r374}, [%rd339];
-	// inline asm
-	add.s64 	%rd343, %rd328, 64;
-	// inline asm
-	cvta.to.global.u64 %rd342, %rd343;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r375,%r376,%r377,%r378}, [%rd342];
-	// inline asm
-	add.s64 	%rd346, %rd328, 80;
-	// inline asm
-	cvta.to.global.u64 %rd345, %rd346;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r379,%r380,%r381,%r382}, [%rd345];
-	// inline asm
-	add.s64 	%rd349, %rd328, 96;
-	// inline asm
-	cvta.to.global.u64 %rd348, %rd349;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r383,%r384,%r385,%r386}, [%rd348];
-	// inline asm
-	add.s64 	%rd352, %rd328, 112;
-	// inline asm
-	cvta.to.global.u64 %rd351, %rd352;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r387,%r388,%r389,%r390}, [%rd351];
-	// inline asm
-	add.s64 	%rd355, %rd328, 128;
-	// inline asm
-	cvta.to.global.u64 %rd354, %rd355;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r391,%r392,%r393,%r394}, [%rd354];
-	// inline asm
-	add.s64 	%rd358, %rd328, 144;
-	// inline asm
-	cvta.to.global.u64 %rd357, %rd358;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r395,%r396,%r397,%r398}, [%rd357];
-	// inline asm
-	mov.b32 	 %f1295, %r362;
-	mov.b32 	 %f1296, %r363;
-	cvt.u32.u16	%r415, %rs11;
-	add.s32 	%r416, %r415, -1;
-	cvt.rn.f32.s32	%f1297, %r416;
-	sub.f32 	%f1298, %f946, %f1295;
-	mul.f32 	%f1299, %f1298, %f1297;
-	sub.f32 	%f1300, %f1296, %f1295;
-	div.rn.f32 	%f1301, %f1299, %f1300;
-	min.f32 	%f1302, %f1297, %f1301;
-	mov.f32 	%f1303, 0f00000000;
-	max.f32 	%f1304, %f1303, %f1302;
-	cvt.rmi.f32.f32	%f1305, %f1304;
-	cvt.rzi.s32.f32	%r417, %f1305;
-	cvt.s64.s32	%rd31, %r417;
-	mul.wide.s32 	%rd372, %r417, 64;
-	add.s64 	%rd361, %rd337, %rd372;
-	// inline asm
-	cvta.to.global.u64 %rd360, %rd361;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r399,%r400,%r401,%r402}, [%rd360];
-	// inline asm
-	mov.b32 	 %f1914, %r399;
-	mov.b32 	 %f1915, %r400;
-	mov.b32 	 %f1916, %r401;
-	mov.b32 	 %f1917, %r402;
-	add.s64 	%rd364, %rd361, 16;
-	// inline asm
-	cvta.to.global.u64 %rd363, %rd364;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r403,%r404,%r405,%r406}, [%rd363];
-	// inline asm
-	mov.b32 	 %f1918, %r403;
-	mov.b32 	 %f1919, %r404;
-	mov.b32 	 %f1920, %r405;
-	mov.b32 	 %f1921, %r406;
-	add.s64 	%rd367, %rd361, 32;
-	// inline asm
-	cvta.to.global.u64 %rd366, %rd367;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r407,%r408,%r409,%r410}, [%rd366];
-	// inline asm
-	sub.f32 	%f375, %f1304, %f1305;
-	mov.b32 	 %f1922, %r407;
-	mov.b32 	 %f1923, %r408;
-	mov.b32 	 %f1924, %r409;
-	mov.b32 	 %f1925, %r410;
-	add.s64 	%rd370, %rd361, 48;
-	// inline asm
-	cvta.to.global.u64 %rd369, %rd370;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r411,%r412,%r413,%r414}, [%rd369];
-	// inline asm
-	mov.b32 	 %f1926, %r411;
-	mov.b32 	 %f1927, %r412;
-	mov.b32 	 %f1928, %r413;
-	mov.b32 	 %f1929, %r414;
-	setp.leu.f32	%p42, %f375, 0f00000000;
-	@%p42 bra 	BB1_67;
-
-	shl.b64 	%rd385, %rd31, 6;
-	add.s64 	%rd386, %rd385, %rd328;
-	add.s64 	%rd374, %rd386, 96;
-	// inline asm
-	cvta.to.global.u64 %rd373, %rd374;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r418,%r419,%r420,%r421}, [%rd373];
-	// inline asm
-	mov.b32 	 %f1306, %r418;
-	mov.b32 	 %f1307, %r419;
-	mov.b32 	 %f1308, %r420;
-	mov.b32 	 %f1309, %r421;
-	add.s64 	%rd377, %rd386, 112;
-	// inline asm
-	cvta.to.global.u64 %rd376, %rd377;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r422,%r423,%r424,%r425}, [%rd376];
-	// inline asm
-	mov.b32 	 %f1310, %r422;
-	mov.b32 	 %f1311, %r423;
-	mov.b32 	 %f1312, %r424;
-	mov.b32 	 %f1313, %r425;
-	add.s64 	%rd380, %rd386, 128;
-	// inline asm
-	cvta.to.global.u64 %rd379, %rd380;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r426,%r427,%r428,%r429}, [%rd379];
-	// inline asm
-	mov.b32 	 %f1314, %r426;
-	mov.b32 	 %f1315, %r427;
-	mov.b32 	 %f1316, %r428;
-	mov.b32 	 %f1317, %r429;
-	add.s64 	%rd383, %rd386, 144;
-	// inline asm
-	cvta.to.global.u64 %rd382, %rd383;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r430,%r431,%r432,%r433}, [%rd382];
-	// inline asm
-	mov.f32 	%f1318, 0f3F800000;
-	sub.f32 	%f1319, %f1318, %f375;
-	mul.f32 	%f1320, %f375, %f1306;
-	mul.f32 	%f1321, %f375, %f1307;
-	mul.f32 	%f1322, %f375, %f1308;
-	mul.f32 	%f1323, %f375, %f1309;
-	fma.rn.f32 	%f1914, %f1319, %f1914, %f1320;
-	fma.rn.f32 	%f1915, %f1319, %f1915, %f1321;
-	fma.rn.f32 	%f1916, %f1319, %f1916, %f1322;
-	fma.rn.f32 	%f1917, %f1319, %f1917, %f1323;
-	mul.f32 	%f1324, %f375, %f1310;
-	mul.f32 	%f1325, %f375, %f1311;
-	mul.f32 	%f1326, %f375, %f1312;
-	mul.f32 	%f1327, %f375, %f1313;
-	fma.rn.f32 	%f1918, %f1319, %f1918, %f1324;
-	fma.rn.f32 	%f1919, %f1319, %f1919, %f1325;
-	fma.rn.f32 	%f1920, %f1319, %f1920, %f1326;
-	fma.rn.f32 	%f1921, %f1319, %f1921, %f1327;
-	mul.f32 	%f1328, %f375, %f1314;
-	mul.f32 	%f1329, %f375, %f1315;
-	mul.f32 	%f1330, %f375, %f1316;
-	mul.f32 	%f1331, %f375, %f1317;
-	fma.rn.f32 	%f1922, %f1319, %f1922, %f1328;
-	fma.rn.f32 	%f1332, %f1319, %f1923, %f1329;
-	fma.rn.f32 	%f1333, %f1319, %f1924, %f1330;
-	fma.rn.f32 	%f1334, %f1319, %f1925, %f1331;
-	mov.b32 	 %f1335, %r430;
-	mov.b32 	 %f1336, %r431;
-	mov.b32 	 %f1337, %r432;
-	mov.b32 	 %f1338, %r433;
-	mul.f32 	%f1339, %f375, %f1335;
-	mul.f32 	%f1340, %f375, %f1336;
-	mul.f32 	%f1341, %f375, %f1337;
-	mul.f32 	%f1342, %f375, %f1338;
-	fma.rn.f32 	%f1343, %f1319, %f1926, %f1339;
-	fma.rn.f32 	%f1927, %f1319, %f1927, %f1340;
-	fma.rn.f32 	%f1928, %f1319, %f1928, %f1341;
-	fma.rn.f32 	%f1929, %f1319, %f1929, %f1342;
-	mul.f32 	%f1344, %f1333, %f1333;
-	fma.rn.f32 	%f1345, %f1332, %f1332, %f1344;
-	fma.rn.f32 	%f1346, %f1334, %f1334, %f1345;
-	fma.rn.f32 	%f1347, %f1343, %f1343, %f1346;
-	sqrt.rn.f32 	%f1348, %f1347;
-	rcp.rn.f32 	%f1349, %f1348;
-	mul.f32 	%f1923, %f1332, %f1349;
-	mul.f32 	%f1924, %f1333, %f1349;
-	mul.f32 	%f1925, %f1334, %f1349;
-	mul.f32 	%f1926, %f1343, %f1349;
-
-BB1_67:
-	mul.f32 	%f1350, %f1924, %f1924;
-	fma.rn.f32 	%f1351, %f1923, %f1923, %f1350;
-	fma.rn.f32 	%f1352, %f1925, %f1925, %f1351;
-	fma.rn.f32 	%f1353, %f1926, %f1926, %f1352;
-	rcp.rn.f32 	%f1354, %f1353;
-	mul.f32 	%f1355, %f1923, %f1354;
-	mul.f32 	%f1356, %f1924, %f1354;
-	mul.f32 	%f1357, %f1925, %f1354;
-	mul.f32 	%f1358, %f1926, %f1354;
-	mul.f32 	%f1359, %f1923, %f1355;
-	mul.f32 	%f1360, %f1924, %f1356;
-	mul.f32 	%f1361, %f1925, %f1357;
-	mul.f32 	%f1362, %f1923, %f1356;
-	mul.f32 	%f1363, %f1925, %f1358;
-	mul.f32 	%f1364, %f1923, %f1357;
-	mul.f32 	%f1365, %f1924, %f1358;
-	mul.f32 	%f1366, %f1924, %f1357;
-	mul.f32 	%f1367, %f1923, %f1358;
-	sub.f32 	%f1368, %f1359, %f1360;
-	sub.f32 	%f1369, %f1368, %f1361;
-	fma.rn.f32 	%f1370, %f1926, %f1358, %f1369;
-	sub.f32 	%f1371, %f1362, %f1363;
-	add.f32 	%f1372, %f1371, %f1371;
-	add.f32 	%f1373, %f1364, %f1365;
-	add.f32 	%f1374, %f1373, %f1373;
-	add.f32 	%f1375, %f1362, %f1363;
-	add.f32 	%f1376, %f1375, %f1375;
-	sub.f32 	%f1377, %f1360, %f1359;
-	sub.f32 	%f1378, %f1377, %f1361;
-	fma.rn.f32 	%f1379, %f1926, %f1358, %f1378;
-	sub.f32 	%f1380, %f1366, %f1367;
-	add.f32 	%f1381, %f1380, %f1380;
-	sub.f32 	%f1382, %f1364, %f1365;
-	add.f32 	%f1383, %f1382, %f1382;
-	add.f32 	%f1384, %f1366, %f1367;
-	add.f32 	%f1385, %f1384, %f1384;
-	neg.f32 	%f1386, %f1359;
-	sub.f32 	%f1387, %f1386, %f1360;
-	add.f32 	%f1388, %f1361, %f1387;
-	fma.rn.f32 	%f1389, %f1926, %f1358, %f1388;
-	mul.f32 	%f1390, %f1917, %f1370;
-	fma.rn.f32 	%f1391, %f1920, %f1372, %f1390;
-	fma.rn.f32 	%f1392, %f1922, %f1374, %f1391;
-	sub.f32 	%f1941, %f1927, %f1392;
-	mul.f32 	%f1393, %f1920, %f1379;
-	fma.rn.f32 	%f1394, %f1917, %f1376, %f1393;
-	fma.rn.f32 	%f1395, %f1922, %f1381, %f1394;
-	sub.f32 	%f1937, %f1928, %f1395;
-	mul.f32 	%f1396, %f1920, %f1385;
-	fma.rn.f32 	%f1397, %f1917, %f1383, %f1396;
-	fma.rn.f32 	%f1398, %f1922, %f1389, %f1397;
-	sub.f32 	%f1933, %f1929, %f1398;
-	mul.f32 	%f1399, %f1916, %f1370;
-	fma.rn.f32 	%f1400, %f1919, %f1372, %f1399;
-	fma.rn.f32 	%f1940, %f1921, %f1374, %f1400;
-	mul.f32 	%f1401, %f1919, %f1379;
-	fma.rn.f32 	%f1402, %f1916, %f1376, %f1401;
-	fma.rn.f32 	%f1936, %f1921, %f1381, %f1402;
-	mul.f32 	%f1403, %f1919, %f1385;
-	fma.rn.f32 	%f1404, %f1916, %f1383, %f1403;
-	fma.rn.f32 	%f1932, %f1921, %f1389, %f1404;
-	mul.f32 	%f1405, %f1915, %f1370;
-	fma.rn.f32 	%f1939, %f1918, %f1372, %f1405;
-	mul.f32 	%f1406, %f1918, %f1379;
-	fma.rn.f32 	%f1935, %f1915, %f1376, %f1406;
-	mul.f32 	%f1407, %f1918, %f1385;
-	fma.rn.f32 	%f1931, %f1915, %f1383, %f1407;
-	mul.f32 	%f1938, %f1914, %f1370;
-	mul.f32 	%f1934, %f1914, %f1376;
-	mul.f32 	%f1930, %f1914, %f1383;
-	bra.uni 	BB1_70;
-
-BB1_60:
-	setp.ne.s32	%p40, %r345, 1;
-	mov.f32 	%f1931, %f1930;
-	mov.f32 	%f1933, %f1930;
-	mov.f32 	%f1934, %f1930;
-	mov.f32 	%f1935, %f1932;
-	mov.f32 	%f1936, %f1930;
-	mov.f32 	%f1937, %f1930;
-	mov.f32 	%f1938, %f1932;
-	mov.f32 	%f1939, %f1930;
-	mov.f32 	%f1940, %f1930;
-	mov.f32 	%f1941, %f1930;
-	@%p40 bra 	BB1_70;
-
-	// inline asm
-	call (%rd315), _optix_get_static_transform_from_handle, (%rd313);
-	// inline asm
-	add.s64 	%rd663, %rd315, 16;
-
-BB1_63:
-	// inline asm
-	cvta.to.global.u64 %rd319, %rd663;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r347,%r348,%r349,%r350}, [%rd319];
-	// inline asm
-	mov.b32 	 %f1938, %r347;
-	mov.b32 	 %f1939, %r348;
-	mov.b32 	 %f1940, %r349;
-	mov.b32 	 %f1941, %r350;
-	add.s64 	%rd323, %rd663, 16;
-	// inline asm
-	cvta.to.global.u64 %rd322, %rd323;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r351,%r352,%r353,%r354}, [%rd322];
-	// inline asm
-	mov.b32 	 %f1934, %r351;
-	mov.b32 	 %f1935, %r352;
-	mov.b32 	 %f1936, %r353;
-	mov.b32 	 %f1937, %r354;
-	add.s64 	%rd326, %rd663, 32;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r497,%r498,%r499,%r500}, [%rd419];
+	// end inline asm
+	mov.b32 	%f1455, %r497;
+	mov.b32 	%f1456, %r498;
+	mov.b32 	%f1457, %r499;
+	mov.b32 	%f1458, %r500;
+	mul.f32 	%f1459, %f456, %f1455;
+	mul.f32 	%f1460, %f456, %f1456;
+	mul.f32 	%f1461, %f456, %f1457;
+	mul.f32 	%f1462, %f456, %f1458;
+	fma.rn.f32 	%f1973, %f1438, %f1973, %f1459;
+	fma.rn.f32 	%f1974, %f1438, %f1974, %f1460;
+	fma.rn.f32 	%f1975, %f1438, %f1975, %f1461;
+	fma.rn.f32 	%f1976, %f1438, %f1976, %f1462;
+	bra.uni 	$L__BB1_70;
+
+$L__BB1_59:
+	mov.f32 	%f1973, 0f00000000;
+	mov.f32 	%f1975, 0f3F800000;
+	setp.eq.s32 	%p42, %r353, 4;
+	@%p42 bra 	$L__BB1_62;
+
+	setp.ne.s32 	%p43, %r353, 1;
+	mov.f32 	%f1974, %f1973;
+	mov.f32 	%f1976, %f1973;
+	mov.f32 	%f1977, %f1973;
+	mov.f32 	%f1978, %f1975;
+	mov.f32 	%f1979, %f1973;
+	mov.f32 	%f1980, %f1973;
+	mov.f32 	%f1981, %f1975;
+	mov.f32 	%f1982, %f1973;
+	mov.f32 	%f1983, %f1973;
+	mov.f32 	%f1984, %f1973;
+	@%p43 bra 	$L__BB1_70;
+
+	// begin inline asm
+	call (%rd307), _optix_get_static_transform_from_handle, (%rd305);
+	// end inline asm
+	add.s64 	%rd650, %rd307, 16;
+	bra.uni 	$L__BB1_63;
+
+$L__BB1_65:
+	// begin inline asm
+	call (%rd320), _optix_get_srt_motion_transform_from_handle, (%rd305);
+	// end inline asm
+	// begin inline asm
+	cvta.to.global.u64 %rd322, %rd320;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r367,%r368,%r369,%r370}, [%rd322];
+	// end inline asm
+	add.s64 	%rd326, %rd320, 16;
+	// begin inline asm
 	cvta.to.global.u64 %rd325, %rd326;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r355,%r356,%r357,%r358}, [%rd325];
-	// inline asm
-	mov.b32 	 %f1930, %r355;
-	mov.b32 	 %f1931, %r356;
-	mov.b32 	 %f1932, %r357;
-	mov.b32 	 %f1933, %r358;
-
-BB1_70:
-	add.s32 	%r18, %r648, 1;
-	setp.eq.s32	%p44, %r18, %r30;
-	@%p44 bra 	BB1_71;
-	bra.uni 	BB1_72;
-
-BB1_71:
-	mov.f32 	%f1913, %f1930;
-	mov.f32 	%f1912, %f1931;
-	mov.f32 	%f1911, %f1932;
-	mov.f32 	%f1910, %f1933;
-	mov.f32 	%f1909, %f1934;
-	mov.f32 	%f1908, %f1935;
-	mov.f32 	%f1907, %f1936;
-	mov.f32 	%f1906, %f1937;
-	mov.f32 	%f1905, %f1938;
-	mov.f32 	%f1904, %f1939;
-	mov.f32 	%f1903, %f1940;
-	mov.f32 	%f1902, %f1941;
-	bra.uni 	BB1_73;
-
-BB1_72:
-	mul.f32 	%f1445, %f1909, %f1939;
-	fma.rn.f32 	%f1446, %f1905, %f1938, %f1445;
-	fma.rn.f32 	%f465, %f1913, %f1940, %f1446;
-	mul.f32 	%f1447, %f1908, %f1939;
-	fma.rn.f32 	%f1448, %f1904, %f1938, %f1447;
-	fma.rn.f32 	%f466, %f1912, %f1940, %f1448;
-	mul.f32 	%f1449, %f1907, %f1939;
-	fma.rn.f32 	%f1450, %f1903, %f1938, %f1449;
-	fma.rn.f32 	%f467, %f1911, %f1940, %f1450;
-	mul.f32 	%f1451, %f1906, %f1939;
-	fma.rn.f32 	%f1452, %f1902, %f1938, %f1451;
-	fma.rn.f32 	%f1453, %f1910, %f1940, %f1452;
-	add.f32 	%f468, %f1941, %f1453;
-	mul.f32 	%f1454, %f1909, %f1935;
-	fma.rn.f32 	%f1455, %f1905, %f1934, %f1454;
-	fma.rn.f32 	%f469, %f1913, %f1936, %f1455;
-	mul.f32 	%f1456, %f1908, %f1935;
-	fma.rn.f32 	%f1457, %f1904, %f1934, %f1456;
-	fma.rn.f32 	%f470, %f1912, %f1936, %f1457;
-	mul.f32 	%f1458, %f1907, %f1935;
-	fma.rn.f32 	%f1459, %f1903, %f1934, %f1458;
-	fma.rn.f32 	%f471, %f1911, %f1936, %f1459;
-	mul.f32 	%f1460, %f1906, %f1935;
-	fma.rn.f32 	%f1461, %f1902, %f1934, %f1460;
-	fma.rn.f32 	%f1462, %f1910, %f1936, %f1461;
-	add.f32 	%f472, %f1937, %f1462;
-	mul.f32 	%f1463, %f1909, %f1931;
-	fma.rn.f32 	%f1464, %f1905, %f1930, %f1463;
-	fma.rn.f32 	%f1913, %f1913, %f1932, %f1464;
-	mul.f32 	%f1465, %f1908, %f1931;
-	fma.rn.f32 	%f1466, %f1904, %f1930, %f1465;
-	fma.rn.f32 	%f1912, %f1912, %f1932, %f1466;
-	mul.f32 	%f1467, %f1907, %f1931;
-	fma.rn.f32 	%f1468, %f1903, %f1930, %f1467;
-	fma.rn.f32 	%f1911, %f1911, %f1932, %f1468;
-	mul.f32 	%f1469, %f1906, %f1931;
-	fma.rn.f32 	%f1470, %f1902, %f1930, %f1469;
-	fma.rn.f32 	%f1471, %f1910, %f1932, %f1470;
-	add.f32 	%f1910, %f1933, %f1471;
-	mov.f32 	%f1909, %f469;
-	mov.f32 	%f1908, %f470;
-	mov.f32 	%f1907, %f471;
-	mov.f32 	%f1906, %f472;
-	mov.f32 	%f1905, %f465;
-	mov.f32 	%f1904, %f466;
-	mov.f32 	%f1903, %f467;
-	mov.f32 	%f1902, %f468;
-
-BB1_73:
-	add.s32 	%r648, %r18, -2;
-	setp.gt.s32	%p45, %r648, -1;
-	@%p45 bra 	BB1_58;
-
-BB1_74:
-	mov.u32 	%r649, 0;
-	mov.f32 	%f1966, %f1967;
-	mov.f32 	%f1971, %f1967;
-	mov.f32 	%f1970, %f1968;
-	mov.f32 	%f1969, %f1967;
-	mov.f32 	%f1974, %f1967;
-	mov.f32 	%f1973, %f1967;
-	mov.f32 	%f1972, %f1968;
-	@%p2 bra 	BB1_92;
-
-BB1_75:
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r371,%r372,%r373,%r374}, [%rd325];
+	// end inline asm
+	add.s64 	%rd329, %rd320, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd328, %rd329;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r375,%r376,%r377,%r378}, [%rd328];
+	// end inline asm
+	add.s64 	%rd332, %rd320, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd331, %rd332;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r379,%r380,%r381,%r382}, [%rd331];
+	// end inline asm
+	add.s64 	%rd335, %rd320, 64;
+	// begin inline asm
+	cvta.to.global.u64 %rd334, %rd335;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r383,%r384,%r385,%r386}, [%rd334];
+	// end inline asm
+	add.s64 	%rd338, %rd320, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd337, %rd338;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r387,%r388,%r389,%r390}, [%rd337];
+	// end inline asm
+	add.s64 	%rd341, %rd320, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd340, %rd341;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r391,%r392,%r393,%r394}, [%rd340];
+	// end inline asm
+	add.s64 	%rd344, %rd320, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd343, %rd344;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r395,%r396,%r397,%r398}, [%rd343];
+	// end inline asm
+	add.s64 	%rd347, %rd320, 128;
+	// begin inline asm
+	cvta.to.global.u64 %rd346, %rd347;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r399,%r400,%r401,%r402}, [%rd346];
+	// end inline asm
+	add.s64 	%rd350, %rd320, 144;
+	// begin inline asm
+	cvta.to.global.u64 %rd349, %rd350;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r403,%r404,%r405,%r406}, [%rd349];
+	// end inline asm
+	mov.b32 	%f1313, %r370;
+	mov.b32 	%f1314, %r371;
+	and.b32  	%r423, %r369, 65535;
+	add.s32 	%r424, %r423, -1;
+	cvt.rn.f32.s32 	%f1315, %r424;
+	sub.f32 	%f1316, %f1298, %f1313;
+	mul.f32 	%f1317, %f1316, %f1315;
+	sub.f32 	%f1318, %f1314, %f1313;
+	div.rn.f32 	%f1319, %f1317, %f1318;
+	min.f32 	%f1320, %f1315, %f1319;
+	mov.f32 	%f1321, 0f00000000;
+	max.f32 	%f1322, %f1321, %f1320;
+	cvt.rmi.f32.f32 	%f1323, %f1322;
+	sub.f32 	%f395, %f1322, %f1323;
+	cvt.rzi.s32.f32 	%r425, %f1323;
+	mul.wide.s32 	%rd364, %r425, 64;
+	add.s64 	%rd353, %rd329, %rd364;
+	// begin inline asm
+	cvta.to.global.u64 %rd352, %rd353;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r407,%r408,%r409,%r410}, [%rd352];
+	// end inline asm
+	mov.b32 	%f1957, %r407;
+	mov.b32 	%f1958, %r408;
+	mov.b32 	%f1959, %r409;
+	mov.b32 	%f1960, %r410;
+	add.s64 	%rd356, %rd353, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd355, %rd356;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r411,%r412,%r413,%r414}, [%rd355];
+	// end inline asm
+	mov.b32 	%f1961, %r411;
+	mov.b32 	%f1962, %r412;
+	mov.b32 	%f1963, %r413;
+	mov.b32 	%f1964, %r414;
+	add.s64 	%rd359, %rd353, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd358, %rd359;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r415,%r416,%r417,%r418}, [%rd358];
+	// end inline asm
+	mov.b32 	%f1965, %r415;
+	mov.b32 	%f1966, %r416;
+	mov.b32 	%f1967, %r417;
+	mov.b32 	%f1968, %r418;
+	add.s64 	%rd362, %rd353, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd361, %rd362;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r419,%r420,%r421,%r422}, [%rd361];
+	// end inline asm
+	mov.b32 	%f1969, %r419;
+	mov.b32 	%f1970, %r420;
+	mov.b32 	%f1971, %r421;
+	mov.b32 	%f1972, %r422;
+	setp.leu.f32 	%p45, %f395, 0f00000000;
+	@%p45 bra 	$L__BB1_67;
+
+	mov.f32 	%f1324, 0f3F800000;
+	sub.f32 	%f1325, %f1324, %f395;
+	add.s64 	%rd366, %rd353, 64;
+	// begin inline asm
+	cvta.to.global.u64 %rd365, %rd366;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r426,%r427,%r428,%r429}, [%rd365];
+	// end inline asm
+	mov.b32 	%f1326, %r426;
+	mov.b32 	%f1327, %r427;
+	mov.b32 	%f1328, %r428;
+	mov.b32 	%f1329, %r429;
+	mul.f32 	%f1330, %f395, %f1326;
+	mul.f32 	%f1331, %f395, %f1327;
+	mul.f32 	%f1332, %f395, %f1328;
+	mul.f32 	%f1333, %f395, %f1329;
+	fma.rn.f32 	%f1957, %f1325, %f1957, %f1330;
+	fma.rn.f32 	%f1958, %f1325, %f1958, %f1331;
+	fma.rn.f32 	%f1959, %f1325, %f1959, %f1332;
+	fma.rn.f32 	%f1960, %f1325, %f1960, %f1333;
+	add.s64 	%rd369, %rd353, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd368, %rd369;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r430,%r431,%r432,%r433}, [%rd368];
+	// end inline asm
+	mov.b32 	%f1334, %r430;
+	mov.b32 	%f1335, %r431;
+	mov.b32 	%f1336, %r432;
+	mov.b32 	%f1337, %r433;
+	mul.f32 	%f1338, %f395, %f1334;
+	mul.f32 	%f1339, %f395, %f1335;
+	mul.f32 	%f1340, %f395, %f1336;
+	mul.f32 	%f1341, %f395, %f1337;
+	fma.rn.f32 	%f1961, %f1325, %f1961, %f1338;
+	fma.rn.f32 	%f1962, %f1325, %f1962, %f1339;
+	fma.rn.f32 	%f1963, %f1325, %f1963, %f1340;
+	fma.rn.f32 	%f1964, %f1325, %f1964, %f1341;
+	add.s64 	%rd372, %rd353, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd371, %rd372;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r434,%r435,%r436,%r437}, [%rd371];
+	// end inline asm
+	mov.b32 	%f1342, %r434;
+	mov.b32 	%f1343, %r435;
+	mov.b32 	%f1344, %r436;
+	mov.b32 	%f1345, %r437;
+	mul.f32 	%f1346, %f395, %f1342;
+	mul.f32 	%f1347, %f395, %f1343;
+	mul.f32 	%f1348, %f395, %f1344;
+	mul.f32 	%f1349, %f395, %f1345;
+	fma.rn.f32 	%f1965, %f1325, %f1965, %f1346;
+	fma.rn.f32 	%f1350, %f1325, %f1966, %f1347;
+	fma.rn.f32 	%f1351, %f1325, %f1967, %f1348;
+	fma.rn.f32 	%f1352, %f1325, %f1968, %f1349;
+	add.s64 	%rd375, %rd353, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd374, %rd375;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r438,%r439,%r440,%r441}, [%rd374];
+	// end inline asm
+	mov.b32 	%f1353, %r438;
+	mov.b32 	%f1354, %r439;
+	mov.b32 	%f1355, %r440;
+	mov.b32 	%f1356, %r441;
+	mul.f32 	%f1357, %f395, %f1353;
+	mul.f32 	%f1358, %f395, %f1354;
+	mul.f32 	%f1359, %f395, %f1355;
+	mul.f32 	%f1360, %f395, %f1356;
+	fma.rn.f32 	%f1361, %f1325, %f1969, %f1357;
+	fma.rn.f32 	%f1970, %f1325, %f1970, %f1358;
+	fma.rn.f32 	%f1971, %f1325, %f1971, %f1359;
+	fma.rn.f32 	%f1972, %f1325, %f1972, %f1360;
+	mul.f32 	%f1362, %f1351, %f1351;
+	fma.rn.f32 	%f1363, %f1350, %f1350, %f1362;
+	fma.rn.f32 	%f1364, %f1352, %f1352, %f1363;
+	fma.rn.f32 	%f1365, %f1361, %f1361, %f1364;
+	sqrt.rn.f32 	%f1366, %f1365;
+	rcp.rn.f32 	%f1367, %f1366;
+	mul.f32 	%f1966, %f1350, %f1367;
+	mul.f32 	%f1967, %f1351, %f1367;
+	mul.f32 	%f1968, %f1352, %f1367;
+	mul.f32 	%f1969, %f1367, %f1361;
+
+$L__BB1_67:
+	mul.f32 	%f1368, %f1967, %f1967;
+	fma.rn.f32 	%f1369, %f1966, %f1966, %f1368;
+	fma.rn.f32 	%f1370, %f1968, %f1968, %f1369;
+	fma.rn.f32 	%f1371, %f1969, %f1969, %f1370;
+	rcp.rn.f32 	%f1372, %f1371;
+	mul.f32 	%f1373, %f1966, %f1372;
+	mul.f32 	%f1374, %f1967, %f1372;
+	mul.f32 	%f1375, %f1968, %f1372;
+	mul.f32 	%f1376, %f1969, %f1372;
+	mul.f32 	%f1377, %f1966, %f1373;
+	mul.f32 	%f1378, %f1967, %f1374;
+	mul.f32 	%f1379, %f1968, %f1375;
+	mul.f32 	%f1380, %f1966, %f1374;
+	mul.f32 	%f1381, %f1968, %f1376;
+	mul.f32 	%f1382, %f1966, %f1375;
+	mul.f32 	%f1383, %f1967, %f1376;
+	mul.f32 	%f1384, %f1967, %f1375;
+	mul.f32 	%f1385, %f1966, %f1376;
+	sub.f32 	%f1386, %f1377, %f1378;
+	sub.f32 	%f1387, %f1386, %f1379;
+	fma.rn.f32 	%f1388, %f1969, %f1376, %f1387;
+	sub.f32 	%f1389, %f1380, %f1381;
+	add.f32 	%f1390, %f1389, %f1389;
+	add.f32 	%f1391, %f1382, %f1383;
+	add.f32 	%f1392, %f1391, %f1391;
+	add.f32 	%f1393, %f1380, %f1381;
+	add.f32 	%f1394, %f1393, %f1393;
+	sub.f32 	%f1395, %f1378, %f1377;
+	sub.f32 	%f1396, %f1395, %f1379;
+	fma.rn.f32 	%f1397, %f1969, %f1376, %f1396;
+	sub.f32 	%f1398, %f1384, %f1385;
+	add.f32 	%f1399, %f1398, %f1398;
+	sub.f32 	%f1400, %f1382, %f1383;
+	add.f32 	%f1401, %f1400, %f1400;
+	add.f32 	%f1402, %f1384, %f1385;
+	add.f32 	%f1403, %f1402, %f1402;
+	neg.f32 	%f1404, %f1377;
+	sub.f32 	%f1405, %f1404, %f1378;
+	add.f32 	%f1406, %f1379, %f1405;
+	fma.rn.f32 	%f1407, %f1969, %f1376, %f1406;
+	mul.f32 	%f1408, %f1960, %f1388;
+	fma.rn.f32 	%f1409, %f1963, %f1390, %f1408;
+	fma.rn.f32 	%f1410, %f1965, %f1392, %f1409;
+	sub.f32 	%f1984, %f1970, %f1410;
+	mul.f32 	%f1411, %f1963, %f1397;
+	fma.rn.f32 	%f1412, %f1960, %f1394, %f1411;
+	fma.rn.f32 	%f1413, %f1965, %f1399, %f1412;
+	sub.f32 	%f1980, %f1971, %f1413;
+	mul.f32 	%f1414, %f1963, %f1403;
+	fma.rn.f32 	%f1415, %f1960, %f1401, %f1414;
+	fma.rn.f32 	%f1416, %f1965, %f1407, %f1415;
+	sub.f32 	%f1976, %f1972, %f1416;
+	mul.f32 	%f1417, %f1959, %f1388;
+	fma.rn.f32 	%f1418, %f1962, %f1390, %f1417;
+	fma.rn.f32 	%f1983, %f1964, %f1392, %f1418;
+	mul.f32 	%f1419, %f1962, %f1397;
+	fma.rn.f32 	%f1420, %f1959, %f1394, %f1419;
+	fma.rn.f32 	%f1979, %f1964, %f1399, %f1420;
+	mul.f32 	%f1421, %f1962, %f1403;
+	fma.rn.f32 	%f1422, %f1959, %f1401, %f1421;
+	fma.rn.f32 	%f1975, %f1964, %f1407, %f1422;
+	mul.f32 	%f1423, %f1958, %f1388;
+	fma.rn.f32 	%f1982, %f1961, %f1390, %f1423;
+	mul.f32 	%f1424, %f1961, %f1397;
+	fma.rn.f32 	%f1978, %f1958, %f1394, %f1424;
+	mul.f32 	%f1425, %f1961, %f1403;
+	fma.rn.f32 	%f1974, %f1958, %f1401, %f1425;
+	mul.f32 	%f1981, %f1957, %f1388;
+	mul.f32 	%f1977, %f1957, %f1394;
+	mul.f32 	%f1973, %f1957, %f1401;
+	bra.uni 	$L__BB1_70;
+
+$L__BB1_62:
+	// begin inline asm
+	call (%rd650), _optix_get_instance_transform_from_handle, (%rd305);
+	// end inline asm
+
+$L__BB1_63:
+	// begin inline asm
+	cvta.to.global.u64 %rd311, %rd650;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r355,%r356,%r357,%r358}, [%rd311];
+	// end inline asm
+	mov.b32 	%f1981, %r355;
+	mov.b32 	%f1982, %r356;
+	mov.b32 	%f1983, %r357;
+	mov.b32 	%f1984, %r358;
+	add.s64 	%rd315, %rd650, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd314, %rd315;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r359,%r360,%r361,%r362}, [%rd314];
+	// end inline asm
+	mov.b32 	%f1977, %r359;
+	mov.b32 	%f1978, %r360;
+	mov.b32 	%f1979, %r361;
+	mov.b32 	%f1980, %r362;
+	add.s64 	%rd318, %rd650, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd317, %rd318;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r363,%r364,%r365,%r366}, [%rd317];
+	// end inline asm
+	mov.b32 	%f1973, %r363;
+	mov.b32 	%f1974, %r364;
+	mov.b32 	%f1975, %r365;
+	mov.b32 	%f1976, %r366;
+
+$L__BB1_70:
+	setp.eq.s32 	%p47, %r658, 1;
+	@%p47 bra 	$L__BB1_72;
+
+	mul.f32 	%f1463, %f1952, %f1982;
+	fma.rn.f32 	%f1464, %f1948, %f1981, %f1463;
+	fma.rn.f32 	%f493, %f1956, %f1983, %f1464;
+	mul.f32 	%f1465, %f1951, %f1982;
+	fma.rn.f32 	%f1466, %f1947, %f1981, %f1465;
+	fma.rn.f32 	%f494, %f1955, %f1983, %f1466;
+	mul.f32 	%f1467, %f1950, %f1982;
+	fma.rn.f32 	%f1468, %f1946, %f1981, %f1467;
+	fma.rn.f32 	%f495, %f1954, %f1983, %f1468;
+	mul.f32 	%f1469, %f1949, %f1982;
+	fma.rn.f32 	%f1470, %f1945, %f1981, %f1469;
+	fma.rn.f32 	%f1471, %f1953, %f1983, %f1470;
+	add.f32 	%f1984, %f1984, %f1471;
+	mul.f32 	%f1472, %f1952, %f1978;
+	fma.rn.f32 	%f1473, %f1948, %f1977, %f1472;
+	fma.rn.f32 	%f497, %f1956, %f1979, %f1473;
+	mul.f32 	%f1474, %f1951, %f1978;
+	fma.rn.f32 	%f1475, %f1947, %f1977, %f1474;
+	fma.rn.f32 	%f498, %f1955, %f1979, %f1475;
+	mul.f32 	%f1476, %f1950, %f1978;
+	fma.rn.f32 	%f1477, %f1946, %f1977, %f1476;
+	fma.rn.f32 	%f499, %f1954, %f1979, %f1477;
+	mul.f32 	%f1478, %f1949, %f1978;
+	fma.rn.f32 	%f1479, %f1945, %f1977, %f1478;
+	fma.rn.f32 	%f1480, %f1953, %f1979, %f1479;
+	add.f32 	%f1980, %f1980, %f1480;
+	mul.f32 	%f1481, %f1952, %f1974;
+	fma.rn.f32 	%f1482, %f1948, %f1973, %f1481;
+	fma.rn.f32 	%f501, %f1956, %f1975, %f1482;
+	mul.f32 	%f1483, %f1951, %f1974;
+	fma.rn.f32 	%f1484, %f1947, %f1973, %f1483;
+	fma.rn.f32 	%f502, %f1955, %f1975, %f1484;
+	mul.f32 	%f1485, %f1950, %f1974;
+	fma.rn.f32 	%f1486, %f1946, %f1973, %f1485;
+	fma.rn.f32 	%f503, %f1954, %f1975, %f1486;
+	mul.f32 	%f1487, %f1949, %f1974;
+	fma.rn.f32 	%f1488, %f1945, %f1973, %f1487;
+	fma.rn.f32 	%f1489, %f1953, %f1975, %f1488;
+	add.f32 	%f1976, %f1976, %f1489;
+	mov.f32 	%f1973, %f501;
+	mov.f32 	%f1974, %f502;
+	mov.f32 	%f1975, %f503;
+	mov.f32 	%f1977, %f497;
+	mov.f32 	%f1978, %f498;
+	mov.f32 	%f1979, %f499;
+	mov.f32 	%f1981, %f493;
+	mov.f32 	%f1982, %f494;
+	mov.f32 	%f1983, %f495;
+
+$L__BB1_72:
+	add.s32 	%r658, %r658, -1;
+	add.s32 	%r657, %r657, -1;
+	setp.gt.s32 	%p48, %r657, 1;
+	mov.f32 	%f1945, %f1984;
+	mov.f32 	%f1946, %f1983;
+	mov.f32 	%f1947, %f1982;
+	mov.f32 	%f1948, %f1981;
+	mov.f32 	%f1949, %f1980;
+	mov.f32 	%f1950, %f1979;
+	mov.f32 	%f1951, %f1978;
+	mov.f32 	%f1952, %f1977;
+	mov.f32 	%f1953, %f1976;
+	mov.f32 	%f1954, %f1975;
+	mov.f32 	%f1955, %f1974;
+	mov.f32 	%f1956, %f1973;
+	@%p48 bra 	$L__BB1_58;
+
+$L__BB1_73:
+	// begin inline asm
+	call (%r501), _optix_get_transform_list_size, ();
+	// end inline asm
+	setp.eq.s32 	%p49, %r501, 0;
+	mov.f32 	%f2045, %f2044;
+	mov.f32 	%f2040, %f2044;
+	mov.f32 	%f2041, %f2043;
+	mov.f32 	%f2042, %f2044;
+	mov.f32 	%f2037, %f2044;
+	mov.f32 	%f2038, %f2044;
+	mov.f32 	%f2039, %f2043;
+	@%p49 bra 	$L__BB1_92;
+
+	// begin inline asm
+	call (%r502), _optix_get_transform_list_size, ();
+	// end inline asm
+	// begin inline asm
+	call (%f1499), _optix_get_ray_time, ();
+	// end inline asm
+	setp.eq.s32 	%p50, %r502, 0;
+	@%p50 bra 	$L__BB1_92;
+
+	mov.u32 	%r659, 0;
+
+$L__BB1_76:
 	.pragma "nounroll";
-	// inline asm
-	call (%rd434), _optix_get_transform_list_handle, (%r649);
-	// inline asm
-	// inline asm
-	call (%r495), _optix_get_transform_type_from_handle, (%rd434);
-	// inline asm
-	and.b32  	%r496, %r495, -2;
-	setp.eq.s32	%p47, %r496, 2;
-	@%p47 bra 	BB1_81;
-	bra.uni 	BB1_76;
-
-BB1_81:
-	setp.eq.s32	%p50, %r495, 2;
-	@%p50 bra 	BB1_85;
-	bra.uni 	BB1_82;
-
-BB1_85:
-	// inline asm
-	call (%rd508), _optix_get_matrix_motion_transform_from_handle, (%rd434);
-	// inline asm
-	// inline asm
-	cvta.to.global.u64 %rd510, %rd508;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r584,%r585,%r586,%r587}, [%rd510];
-	// inline asm
-	mov.b32	{%rs17, %rs18}, %r586;
-	add.s64 	%rd514, %rd508, 16;
-	// inline asm
+	// begin inline asm
+	call (%rd424), _optix_get_transform_list_handle, (%r659);
+	// end inline asm
+	// begin inline asm
+	call (%r505), _optix_get_transform_type_from_handle, (%rd424);
+	// end inline asm
+	or.b32  	%r506, %r505, 1;
+	setp.eq.s32 	%p51, %r506, 3;
+	@%p51 bra 	$L__BB1_82;
+	bra.uni 	$L__BB1_77;
+
+$L__BB1_82:
+	setp.eq.s32 	%p54, %r505, 2;
+	@%p54 bra 	$L__BB1_86;
+	bra.uni 	$L__BB1_83;
+
+$L__BB1_86:
+	// begin inline asm
+	call (%rd496), _optix_get_matrix_motion_transform_from_handle, (%rd424);
+	// end inline asm
+	// begin inline asm
+	cvta.to.global.u64 %rd498, %rd496;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r594,%r595,%r596,%r597}, [%rd498];
+	// end inline asm
+	add.s64 	%rd502, %rd496, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd501, %rd502;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r598,%r599,%r600,%r601}, [%rd501];
+	// end inline asm
+	add.s64 	%rd505, %rd496, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd504, %rd505;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r602,%r603,%r604,%r605}, [%rd504];
+	// end inline asm
+	add.s64 	%rd508, %rd496, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd507, %rd508;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r606,%r607,%r608,%r609}, [%rd507];
+	// end inline asm
+	add.s64 	%rd511, %rd496, 64;
+	// begin inline asm
+	cvta.to.global.u64 %rd510, %rd511;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r610,%r611,%r612,%r613}, [%rd510];
+	// end inline asm
+	add.s64 	%rd514, %rd496, 80;
+	// begin inline asm
 	cvta.to.global.u64 %rd513, %rd514;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r588,%r589,%r590,%r591}, [%rd513];
-	// inline asm
-	add.s64 	%rd517, %rd508, 32;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r614,%r615,%r616,%r617}, [%rd513];
+	// end inline asm
+	add.s64 	%rd517, %rd496, 96;
+	// begin inline asm
 	cvta.to.global.u64 %rd516, %rd517;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r592,%r593,%r594,%r595}, [%rd516];
-	// inline asm
-	add.s64 	%rd520, %rd508, 48;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r618,%r619,%r620,%r621}, [%rd516];
+	// end inline asm
+	add.s64 	%rd520, %rd496, 112;
+	// begin inline asm
 	cvta.to.global.u64 %rd519, %rd520;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r596,%r597,%r598,%r599}, [%rd519];
-	// inline asm
-	add.s64 	%rd523, %rd508, 64;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r622,%r623,%r624,%r625}, [%rd519];
+	// end inline asm
+	mov.b32 	%f1603, %r597;
+	mov.b32 	%f1604, %r598;
+	and.b32  	%r638, %r596, 65535;
+	add.s32 	%r639, %r638, -1;
+	cvt.rn.f32.s32 	%f1605, %r639;
+	sub.f32 	%f1606, %f1499, %f1603;
+	mul.f32 	%f1607, %f1606, %f1605;
+	sub.f32 	%f1608, %f1604, %f1603;
+	div.rn.f32 	%f1609, %f1607, %f1608;
+	min.f32 	%f1610, %f1605, %f1609;
+	mov.f32 	%f1611, 0f00000000;
+	max.f32 	%f1612, %f1611, %f1610;
+	cvt.rmi.f32.f32 	%f1613, %f1612;
+	sub.f32 	%f588, %f1612, %f1613;
+	cvt.rzi.s32.f32 	%r640, %f1613;
+	cvt.s64.s32 	%rd37, %r640;
+	mul.wide.s32 	%rd531, %r640, 48;
+	add.s64 	%rd523, %rd505, %rd531;
+	// begin inline asm
 	cvta.to.global.u64 %rd522, %rd523;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r600,%r601,%r602,%r603}, [%rd522];
-	// inline asm
-	add.s64 	%rd526, %rd508, 80;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r626,%r627,%r628,%r629}, [%rd522];
+	// end inline asm
+	mov.b32 	%f2034, %r626;
+	mov.b32 	%f2035, %r627;
+	mov.b32 	%f2036, %r628;
+	add.s64 	%rd526, %rd523, 16;
+	// begin inline asm
 	cvta.to.global.u64 %rd525, %rd526;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r604,%r605,%r606,%r607}, [%rd525];
-	// inline asm
-	add.s64 	%rd529, %rd508, 96;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r630,%r631,%r632,%r633}, [%rd525];
+	// end inline asm
+	mov.b32 	%f2031, %r630;
+	mov.b32 	%f2032, %r631;
+	mov.b32 	%f2033, %r632;
+	add.s64 	%rd529, %rd523, 32;
+	// begin inline asm
 	cvta.to.global.u64 %rd528, %rd529;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r608,%r609,%r610,%r611}, [%rd528];
-	// inline asm
-	add.s64 	%rd532, %rd508, 112;
-	// inline asm
-	cvta.to.global.u64 %rd531, %rd532;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r612,%r613,%r614,%r615}, [%rd531];
-	// inline asm
-	mov.b32 	 %f1583, %r587;
-	mov.b32 	 %f1584, %r588;
-	cvt.u32.u16	%r628, %rs17;
-	add.s32 	%r629, %r628, -1;
-	cvt.rn.f32.s32	%f1585, %r629;
-	sub.f32 	%f1586, %f946, %f1583;
-	mul.f32 	%f1587, %f1586, %f1585;
-	sub.f32 	%f1588, %f1584, %f1583;
-	div.rn.f32 	%f1589, %f1587, %f1588;
-	min.f32 	%f1590, %f1585, %f1589;
-	mov.f32 	%f1591, 0f00000000;
-	max.f32 	%f1592, %f1591, %f1590;
-	cvt.rmi.f32.f32	%f1593, %f1592;
-	cvt.rzi.s32.f32	%r630, %f1593;
-	cvt.s64.s32	%rd41, %r630;
-	mul.wide.s32 	%rd543, %r630, 48;
-	add.s64 	%rd535, %rd517, %rd543;
-	// inline asm
-	cvta.to.global.u64 %rd534, %rd535;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r616,%r617,%r618,%r619}, [%rd534];
-	// inline asm
-	mov.b32 	 %f1991, %r616;
-	mov.b32 	 %f1992, %r617;
-	mov.b32 	 %f1993, %r618;
-	add.s64 	%rd538, %rd535, 16;
-	// inline asm
-	cvta.to.global.u64 %rd537, %rd538;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r620,%r621,%r622,%r623}, [%rd537];
-	// inline asm
-	mov.b32 	 %f1988, %r620;
-	mov.b32 	 %f1989, %r621;
-	mov.b32 	 %f1990, %r622;
-	add.s64 	%rd541, %rd535, 32;
-	// inline asm
-	cvta.to.global.u64 %rd540, %rd541;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r624,%r625,%r626,%r627}, [%rd540];
-	// inline asm
-	sub.f32 	%f565, %f1592, %f1593;
-	mov.b32 	 %f1985, %r624;
-	mov.b32 	 %f1986, %r625;
-	mov.b32 	 %f1987, %r626;
-	setp.leu.f32	%p52, %f565, 0f00000000;
-	@%p52 bra 	BB1_87;
-
-	mul.lo.s64 	%rd553, %rd41, 48;
-	add.s64 	%rd554, %rd508, %rd553;
-	add.s64 	%rd545, %rd554, 80;
-	// inline asm
-	cvta.to.global.u64 %rd544, %rd545;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r631,%r632,%r633,%r634}, [%rd544];
-	// inline asm
-	mov.b32 	 %f1594, %r631;
-	mov.b32 	 %f1595, %r632;
-	mov.b32 	 %f1596, %r633;
-	add.s64 	%rd548, %rd554, 96;
-	// inline asm
-	cvta.to.global.u64 %rd547, %rd548;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r635,%r636,%r637,%r638}, [%rd547];
-	// inline asm
-	mov.b32 	 %f1597, %r635;
-	mov.b32 	 %f1598, %r636;
-	mov.b32 	 %f1599, %r637;
-	add.s64 	%rd551, %rd554, 112;
-	// inline asm
-	cvta.to.global.u64 %rd550, %rd551;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r639,%r640,%r641,%r642}, [%rd550];
-	// inline asm
-	mov.f32 	%f1600, 0f3F800000;
-	sub.f32 	%f1601, %f1600, %f565;
-	mul.f32 	%f1602, %f565, %f1594;
-	mul.f32 	%f1603, %f565, %f1595;
-	mul.f32 	%f1604, %f565, %f1596;
-	fma.rn.f32 	%f1991, %f1601, %f1991, %f1602;
-	fma.rn.f32 	%f1992, %f1601, %f1992, %f1603;
-	fma.rn.f32 	%f1993, %f1601, %f1993, %f1604;
-	mul.f32 	%f1605, %f565, %f1597;
-	mul.f32 	%f1606, %f565, %f1598;
-	mul.f32 	%f1607, %f565, %f1599;
-	fma.rn.f32 	%f1988, %f1601, %f1988, %f1605;
-	fma.rn.f32 	%f1989, %f1601, %f1989, %f1606;
-	fma.rn.f32 	%f1990, %f1601, %f1990, %f1607;
-	mov.b32 	 %f1608, %r639;
-	mov.b32 	 %f1609, %r640;
-	mov.b32 	 %f1610, %r641;
-	mul.f32 	%f1611, %f565, %f1608;
-	mul.f32 	%f1612, %f565, %f1609;
-	mul.f32 	%f1613, %f565, %f1610;
-	fma.rn.f32 	%f1985, %f1601, %f1985, %f1611;
-	fma.rn.f32 	%f1986, %f1601, %f1986, %f1612;
-	fma.rn.f32 	%f1987, %f1601, %f1987, %f1613;
-	bra.uni 	BB1_87;
-
-BB1_76:
-	mov.f32 	%f1994, 0f00000000;
-	mov.f32 	%f1996, 0f3F800000;
-	setp.eq.s32	%p48, %r495, 4;
-	@%p48 bra 	BB1_79;
-	bra.uni 	BB1_77;
-
-BB1_79:
-	// inline asm
-	call (%rd664), _optix_get_instance_inverse_transform_from_handle, (%rd434);
-	// inline asm
-	bra.uni 	BB1_80;
-
-BB1_82:
-	// inline asm
-	call (%rd449), _optix_get_srt_motion_transform_from_handle, (%rd434);
-	// inline asm
-	// inline asm
-	cvta.to.global.u64 %rd451, %rd449;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r509,%r510,%r511,%r512}, [%rd451];
-	// inline asm
-	mov.b32	{%rs15, %rs16}, %r511;
-	add.s64 	%rd455, %rd449, 16;
-	// inline asm
-	cvta.to.global.u64 %rd454, %rd455;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r513,%r514,%r515,%r516}, [%rd454];
-	// inline asm
-	add.s64 	%rd458, %rd449, 32;
-	// inline asm
-	cvta.to.global.u64 %rd457, %rd458;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r517,%r518,%r519,%r520}, [%rd457];
-	// inline asm
-	add.s64 	%rd461, %rd449, 48;
-	// inline asm
-	cvta.to.global.u64 %rd460, %rd461;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r521,%r522,%r523,%r524}, [%rd460];
-	// inline asm
-	add.s64 	%rd464, %rd449, 64;
-	// inline asm
-	cvta.to.global.u64 %rd463, %rd464;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r525,%r526,%r527,%r528}, [%rd463];
-	// inline asm
-	add.s64 	%rd467, %rd449, 80;
-	// inline asm
-	cvta.to.global.u64 %rd466, %rd467;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r529,%r530,%r531,%r532}, [%rd466];
-	// inline asm
-	add.s64 	%rd470, %rd449, 96;
-	// inline asm
-	cvta.to.global.u64 %rd469, %rd470;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r533,%r534,%r535,%r536}, [%rd469];
-	// inline asm
-	add.s64 	%rd473, %rd449, 112;
-	// inline asm
-	cvta.to.global.u64 %rd472, %rd473;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r537,%r538,%r539,%r540}, [%rd472];
-	// inline asm
-	add.s64 	%rd476, %rd449, 128;
-	// inline asm
-	cvta.to.global.u64 %rd475, %rd476;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r541,%r542,%r543,%r544}, [%rd475];
-	// inline asm
-	add.s64 	%rd479, %rd449, 144;
-	// inline asm
-	cvta.to.global.u64 %rd478, %rd479;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r545,%r546,%r547,%r548}, [%rd478];
-	// inline asm
-	mov.b32 	 %f1491, %r512;
-	mov.b32 	 %f1492, %r513;
-	cvt.u32.u16	%r565, %rs15;
-	add.s32 	%r566, %r565, -1;
-	cvt.rn.f32.s32	%f1493, %r566;
-	sub.f32 	%f1494, %f946, %f1491;
-	mul.f32 	%f1495, %f1494, %f1493;
-	sub.f32 	%f1496, %f1492, %f1491;
-	div.rn.f32 	%f1497, %f1495, %f1496;
-	min.f32 	%f1498, %f1493, %f1497;
-	mov.f32 	%f1499, 0f00000000;
-	max.f32 	%f1500, %f1499, %f1498;
-	cvt.rmi.f32.f32	%f1501, %f1500;
-	cvt.rzi.s32.f32	%r567, %f1501;
-	cvt.s64.s32	%rd39, %r567;
-	mul.wide.s32 	%rd493, %r567, 64;
-	add.s64 	%rd482, %rd458, %rd493;
-	// inline asm
-	cvta.to.global.u64 %rd481, %rd482;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r549,%r550,%r551,%r552}, [%rd481];
-	// inline asm
-	mov.b32 	 %f1975, %r549;
-	mov.b32 	 %f1976, %r550;
-	mov.b32 	 %f1977, %r551;
-	add.s64 	%rd485, %rd482, 16;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r634,%r635,%r636,%r637}, [%rd528];
+	// end inline asm
+	mov.b32 	%f2028, %r634;
+	mov.b32 	%f2029, %r635;
+	mov.b32 	%f2030, %r636;
+	setp.leu.f32 	%p56, %f588, 0f00000000;
+	@%p56 bra 	$L__BB1_88;
+
+	mov.f32 	%f1614, 0f3F800000;
+	sub.f32 	%f1615, %f1614, %f588;
+	mul.lo.s64 	%rd541, %rd37, 48;
+	add.s64 	%rd542, %rd496, %rd541;
+	add.s64 	%rd533, %rd542, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd532, %rd533;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r641,%r642,%r643,%r644}, [%rd532];
+	// end inline asm
+	mov.b32 	%f1616, %r641;
+	mov.b32 	%f1617, %r642;
+	mov.b32 	%f1618, %r643;
+	mul.f32 	%f1619, %f588, %f1616;
+	mul.f32 	%f1620, %f588, %f1617;
+	mul.f32 	%f1621, %f588, %f1618;
+	fma.rn.f32 	%f2034, %f1615, %f2034, %f1619;
+	fma.rn.f32 	%f2035, %f1615, %f2035, %f1620;
+	fma.rn.f32 	%f2036, %f1615, %f2036, %f1621;
+	add.s64 	%rd536, %rd542, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd535, %rd536;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r645,%r646,%r647,%r648}, [%rd535];
+	// end inline asm
+	mov.b32 	%f1622, %r645;
+	mov.b32 	%f1623, %r646;
+	mov.b32 	%f1624, %r647;
+	mul.f32 	%f1625, %f588, %f1622;
+	mul.f32 	%f1626, %f588, %f1623;
+	mul.f32 	%f1627, %f588, %f1624;
+	fma.rn.f32 	%f2031, %f1615, %f2031, %f1625;
+	fma.rn.f32 	%f2032, %f1615, %f2032, %f1626;
+	fma.rn.f32 	%f2033, %f1615, %f2033, %f1627;
+	add.s64 	%rd539, %rd542, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd538, %rd539;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r649,%r650,%r651,%r652}, [%rd538];
+	// end inline asm
+	mov.b32 	%f1628, %r649;
+	mov.b32 	%f1629, %r650;
+	mov.b32 	%f1630, %r651;
+	mul.f32 	%f1631, %f588, %f1628;
+	mul.f32 	%f1632, %f588, %f1629;
+	mul.f32 	%f1633, %f588, %f1630;
+	fma.rn.f32 	%f2028, %f1615, %f2028, %f1631;
+	fma.rn.f32 	%f2029, %f1615, %f2029, %f1632;
+	fma.rn.f32 	%f2030, %f1615, %f2030, %f1633;
+	bra.uni 	$L__BB1_88;
+
+$L__BB1_77:
+	mov.f32 	%f2037, 0f00000000;
+	mov.f32 	%f2039, 0f3F800000;
+	setp.eq.s32 	%p52, %r505, 4;
+	@%p52 bra 	$L__BB1_80;
+
+	setp.ne.s32 	%p53, %r505, 1;
+	mov.f32 	%f2038, %f2037;
+	mov.f32 	%f2040, %f2037;
+	mov.f32 	%f2041, %f2039;
+	mov.f32 	%f2042, %f2037;
+	mov.f32 	%f2043, %f2039;
+	mov.f32 	%f2044, %f2037;
+	mov.f32 	%f2045, %f2037;
+	@%p53 bra 	$L__BB1_89;
+
+	// begin inline asm
+	call (%rd426), _optix_get_static_transform_from_handle, (%rd424);
+	// end inline asm
+	add.s64 	%rd651, %rd426, 64;
+	bra.uni 	$L__BB1_81;
+
+$L__BB1_83:
+	// begin inline asm
+	call (%rd439), _optix_get_srt_motion_transform_from_handle, (%rd424);
+	// end inline asm
+	// begin inline asm
+	cvta.to.global.u64 %rd441, %rd439;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r519,%r520,%r521,%r522}, [%rd441];
+	// end inline asm
+	add.s64 	%rd445, %rd439, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd444, %rd445;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r523,%r524,%r525,%r526}, [%rd444];
+	// end inline asm
+	add.s64 	%rd448, %rd439, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd447, %rd448;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r527,%r528,%r529,%r530}, [%rd447];
+	// end inline asm
+	add.s64 	%rd451, %rd439, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd450, %rd451;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r531,%r532,%r533,%r534}, [%rd450];
+	// end inline asm
+	add.s64 	%rd454, %rd439, 64;
+	// begin inline asm
+	cvta.to.global.u64 %rd453, %rd454;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r535,%r536,%r537,%r538}, [%rd453];
+	// end inline asm
+	add.s64 	%rd457, %rd439, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd456, %rd457;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r539,%r540,%r541,%r542}, [%rd456];
+	// end inline asm
+	add.s64 	%rd460, %rd439, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd459, %rd460;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r543,%r544,%r545,%r546}, [%rd459];
+	// end inline asm
+	add.s64 	%rd463, %rd439, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd462, %rd463;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r547,%r548,%r549,%r550}, [%rd462];
+	// end inline asm
+	add.s64 	%rd466, %rd439, 128;
+	// begin inline asm
+	cvta.to.global.u64 %rd465, %rd466;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r551,%r552,%r553,%r554}, [%rd465];
+	// end inline asm
+	add.s64 	%rd469, %rd439, 144;
+	// begin inline asm
+	cvta.to.global.u64 %rd468, %rd469;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r555,%r556,%r557,%r558}, [%rd468];
+	// end inline asm
+	mov.b32 	%f1511, %r522;
+	mov.b32 	%f1512, %r523;
+	and.b32  	%r575, %r521, 65535;
+	add.s32 	%r576, %r575, -1;
+	cvt.rn.f32.s32 	%f1513, %r576;
+	sub.f32 	%f1514, %f1499, %f1511;
+	mul.f32 	%f1515, %f1514, %f1513;
+	sub.f32 	%f1516, %f1512, %f1511;
+	div.rn.f32 	%f1517, %f1515, %f1516;
+	min.f32 	%f1518, %f1513, %f1517;
+	mov.f32 	%f1519, 0f00000000;
+	max.f32 	%f1520, %f1519, %f1518;
+	cvt.rmi.f32.f32 	%f1521, %f1520;
+	sub.f32 	%f548, %f1520, %f1521;
+	cvt.rzi.s32.f32 	%r577, %f1521;
+	mul.wide.s32 	%rd483, %r577, 64;
+	add.s64 	%rd472, %rd448, %rd483;
+	// begin inline asm
+	cvta.to.global.u64 %rd471, %rd472;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r559,%r560,%r561,%r562}, [%rd471];
+	// end inline asm
+	mov.b32 	%f2018, %r559;
+	mov.b32 	%f2019, %r560;
+	mov.b32 	%f2020, %r561;
+	add.s64 	%rd475, %rd472, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd474, %rd475;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r563,%r564,%r565,%r566}, [%rd474];
+	// end inline asm
+	mov.b32 	%f2021, %r563;
+	mov.b32 	%f2022, %r564;
+	mov.b32 	%f2023, %r566;
+	add.s64 	%rd478, %rd472, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd477, %rd478;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r567,%r568,%r569,%r570}, [%rd477];
+	// end inline asm
+	mov.b32 	%f2024, %r568;
+	mov.b32 	%f2025, %r569;
+	mov.b32 	%f2026, %r570;
+	add.s64 	%rd481, %rd472, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd480, %rd481;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r571,%r572,%r573,%r574}, [%rd480];
+	// end inline asm
+	mov.b32 	%f2027, %r571;
+	setp.leu.f32 	%p55, %f548, 0f00000000;
+	@%p55 bra 	$L__BB1_85;
+
+	mov.f32 	%f1522, 0f3F800000;
+	sub.f32 	%f1523, %f1522, %f548;
+	add.s64 	%rd485, %rd472, 64;
+	// begin inline asm
 	cvta.to.global.u64 %rd484, %rd485;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r553,%r554,%r555,%r556}, [%rd484];
-	// inline asm
-	mov.b32 	 %f1978, %r553;
-	mov.b32 	 %f1979, %r554;
-	mov.b32 	 %f1980, %r556;
-	add.s64 	%rd488, %rd482, 32;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r578,%r579,%r580,%r581}, [%rd484];
+	// end inline asm
+	mov.b32 	%f1524, %r578;
+	mov.b32 	%f1525, %r579;
+	mov.b32 	%f1526, %r580;
+	mul.f32 	%f1527, %f548, %f1524;
+	mul.f32 	%f1528, %f548, %f1525;
+	mul.f32 	%f1529, %f548, %f1526;
+	fma.rn.f32 	%f2018, %f1523, %f2018, %f1527;
+	fma.rn.f32 	%f2019, %f1523, %f2019, %f1528;
+	fma.rn.f32 	%f2020, %f1523, %f2020, %f1529;
+	add.s64 	%rd488, %rd472, 80;
+	// begin inline asm
 	cvta.to.global.u64 %rd487, %rd488;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r557,%r558,%r559,%r560}, [%rd487];
-	// inline asm
-	sub.f32 	%f525, %f1500, %f1501;
-	mov.b32 	 %f1981, %r558;
-	mov.b32 	 %f1982, %r559;
-	mov.b32 	 %f1983, %r560;
-	add.s64 	%rd491, %rd482, 48;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r582,%r583,%r584,%r585}, [%rd487];
+	// end inline asm
+	mov.b32 	%f1530, %r582;
+	mov.b32 	%f1531, %r583;
+	mov.b32 	%f1532, %r585;
+	mul.f32 	%f1533, %f548, %f1530;
+	mul.f32 	%f1534, %f548, %f1531;
+	mul.f32 	%f1535, %f548, %f1532;
+	fma.rn.f32 	%f2021, %f1523, %f2021, %f1533;
+	fma.rn.f32 	%f2022, %f1523, %f2022, %f1534;
+	fma.rn.f32 	%f2023, %f1523, %f2023, %f1535;
+	add.s64 	%rd491, %rd472, 96;
+	// begin inline asm
 	cvta.to.global.u64 %rd490, %rd491;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r561,%r562,%r563,%r564}, [%rd490];
-	// inline asm
-	mov.b32 	 %f1984, %r561;
-	setp.leu.f32	%p51, %f525, 0f00000000;
-	@%p51 bra 	BB1_84;
-
-	shl.b64 	%rd506, %rd39, 6;
-	add.s64 	%rd507, %rd506, %rd449;
-	add.s64 	%rd495, %rd507, 96;
-	// inline asm
-	cvta.to.global.u64 %rd494, %rd495;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r568,%r569,%r570,%r571}, [%rd494];
-	// inline asm
-	mov.b32 	 %f1502, %r568;
-	mov.b32 	 %f1503, %r569;
-	mov.b32 	 %f1504, %r570;
-	add.s64 	%rd498, %rd507, 112;
-	// inline asm
-	cvta.to.global.u64 %rd497, %rd498;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r572,%r573,%r574,%r575}, [%rd497];
-	// inline asm
-	mov.b32 	 %f1505, %r572;
-	mov.b32 	 %f1506, %r573;
-	mov.b32 	 %f1507, %r575;
-	add.s64 	%rd501, %rd507, 128;
-	// inline asm
-	cvta.to.global.u64 %rd500, %rd501;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r576,%r577,%r578,%r579}, [%rd500];
-	// inline asm
-	mov.b32 	 %f1508, %r577;
-	mov.b32 	 %f1509, %r578;
-	mov.b32 	 %f1510, %r579;
-	add.s64 	%rd504, %rd507, 144;
-	// inline asm
-	cvta.to.global.u64 %rd503, %rd504;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r580,%r581,%r582,%r583}, [%rd503];
-	// inline asm
-	mov.f32 	%f1511, 0f3F800000;
-	sub.f32 	%f1512, %f1511, %f525;
-	mul.f32 	%f1513, %f525, %f1502;
-	mul.f32 	%f1514, %f525, %f1503;
-	mul.f32 	%f1515, %f525, %f1504;
-	fma.rn.f32 	%f1975, %f1512, %f1975, %f1513;
-	fma.rn.f32 	%f1976, %f1512, %f1976, %f1514;
-	fma.rn.f32 	%f1977, %f1512, %f1977, %f1515;
-	mul.f32 	%f1516, %f525, %f1505;
-	mul.f32 	%f1517, %f525, %f1506;
-	mul.f32 	%f1518, %f525, %f1507;
-	fma.rn.f32 	%f1978, %f1512, %f1978, %f1516;
-	fma.rn.f32 	%f1979, %f1512, %f1979, %f1517;
-	fma.rn.f32 	%f1980, %f1512, %f1980, %f1518;
-	mul.f32 	%f1519, %f525, %f1508;
-	mul.f32 	%f1520, %f525, %f1509;
-	mul.f32 	%f1521, %f525, %f1510;
-	fma.rn.f32 	%f1522, %f1512, %f1981, %f1519;
-	fma.rn.f32 	%f1523, %f1512, %f1982, %f1520;
-	fma.rn.f32 	%f1524, %f1512, %f1983, %f1521;
-	mov.b32 	 %f1525, %r580;
-	mul.f32 	%f1526, %f525, %f1525;
-	fma.rn.f32 	%f1527, %f1512, %f1984, %f1526;
-	mul.f32 	%f1528, %f1523, %f1523;
-	fma.rn.f32 	%f1529, %f1522, %f1522, %f1528;
-	fma.rn.f32 	%f1530, %f1524, %f1524, %f1529;
-	fma.rn.f32 	%f1531, %f1527, %f1527, %f1530;
-	sqrt.rn.f32 	%f1532, %f1531;
-	rcp.rn.f32 	%f1533, %f1532;
-	mul.f32 	%f1981, %f1522, %f1533;
-	mul.f32 	%f1982, %f1523, %f1533;
-	mul.f32 	%f1983, %f1524, %f1533;
-	mul.f32 	%f1984, %f1527, %f1533;
-
-BB1_84:
-	mul.f32 	%f1534, %f1982, %f1982;
-	fma.rn.f32 	%f1535, %f1981, %f1981, %f1534;
-	fma.rn.f32 	%f1536, %f1983, %f1983, %f1535;
-	fma.rn.f32 	%f1537, %f1984, %f1984, %f1536;
-	rcp.rn.f32 	%f1538, %f1537;
-	mul.f32 	%f1539, %f1981, %f1538;
-	mul.f32 	%f1540, %f1982, %f1538;
-	mul.f32 	%f1541, %f1983, %f1538;
-	mul.f32 	%f1542, %f1984, %f1538;
-	mul.f32 	%f1543, %f1981, %f1539;
-	mul.f32 	%f1544, %f1982, %f1540;
-	mul.f32 	%f1545, %f1983, %f1541;
-	mul.f32 	%f1546, %f1981, %f1540;
-	mul.f32 	%f1547, %f1983, %f1542;
-	mul.f32 	%f1548, %f1981, %f1541;
-	mul.f32 	%f1549, %f1982, %f1542;
-	mul.f32 	%f1550, %f1982, %f1541;
-	mul.f32 	%f1551, %f1981, %f1542;
-	sub.f32 	%f1552, %f1543, %f1544;
-	sub.f32 	%f1553, %f1552, %f1545;
-	fma.rn.f32 	%f1554, %f1984, %f1542, %f1553;
-	sub.f32 	%f1555, %f1546, %f1547;
-	add.f32 	%f1556, %f1555, %f1555;
-	add.f32 	%f1557, %f1548, %f1549;
-	add.f32 	%f1558, %f1557, %f1557;
-	add.f32 	%f1559, %f1546, %f1547;
-	add.f32 	%f1560, %f1559, %f1559;
-	sub.f32 	%f1561, %f1544, %f1543;
-	sub.f32 	%f1562, %f1561, %f1545;
-	fma.rn.f32 	%f1563, %f1984, %f1542, %f1562;
-	sub.f32 	%f1564, %f1550, %f1551;
-	add.f32 	%f1565, %f1564, %f1564;
-	sub.f32 	%f1566, %f1548, %f1549;
-	add.f32 	%f1567, %f1566, %f1566;
-	add.f32 	%f1568, %f1550, %f1551;
-	add.f32 	%f1569, %f1568, %f1568;
-	neg.f32 	%f1570, %f1543;
-	sub.f32 	%f1571, %f1570, %f1544;
-	add.f32 	%f1572, %f1545, %f1571;
-	fma.rn.f32 	%f1573, %f1984, %f1542, %f1572;
-	mul.f32 	%f1574, %f1977, %f1554;
-	fma.rn.f32 	%f1575, %f1979, %f1556, %f1574;
-	fma.rn.f32 	%f1993, %f1980, %f1558, %f1575;
-	mul.f32 	%f1576, %f1979, %f1563;
-	fma.rn.f32 	%f1577, %f1977, %f1560, %f1576;
-	fma.rn.f32 	%f1990, %f1980, %f1565, %f1577;
-	mul.f32 	%f1578, %f1979, %f1569;
-	fma.rn.f32 	%f1579, %f1977, %f1567, %f1578;
-	fma.rn.f32 	%f1987, %f1980, %f1573, %f1579;
-	mul.f32 	%f1580, %f1976, %f1554;
-	fma.rn.f32 	%f1992, %f1978, %f1556, %f1580;
-	mul.f32 	%f1581, %f1978, %f1563;
-	fma.rn.f32 	%f1989, %f1976, %f1560, %f1581;
-	mul.f32 	%f1582, %f1978, %f1569;
-	fma.rn.f32 	%f1986, %f1976, %f1567, %f1582;
-	mul.f32 	%f1991, %f1975, %f1554;
-	mul.f32 	%f1988, %f1975, %f1560;
-	mul.f32 	%f1985, %f1975, %f1567;
-
-BB1_87:
-	mul.f32 	%f1614, %f1986, %f1990;
-	mul.f32 	%f1615, %f1987, %f1989;
-	sub.f32 	%f1616, %f1615, %f1614;
-	mul.f32 	%f1617, %f1991, %f1616;
-	mul.f32 	%f1618, %f1985, %f1990;
-	mul.f32 	%f1619, %f1987, %f1988;
-	sub.f32 	%f1620, %f1619, %f1618;
-	mul.f32 	%f1621, %f1620, %f1992;
-	sub.f32 	%f1622, %f1617, %f1621;
-	mul.f32 	%f1623, %f1985, %f1989;
-	mul.f32 	%f1624, %f1986, %f1988;
-	sub.f32 	%f1625, %f1624, %f1623;
-	fma.rn.f32 	%f1626, %f1625, %f1993, %f1622;
-	rcp.rn.f32 	%f1627, %f1626;
-	mul.f32 	%f2000, %f1616, %f1627;
-	mul.f32 	%f1628, %f1987, %f1992;
-	mul.f32 	%f1629, %f1986, %f1993;
-	sub.f32 	%f1630, %f1629, %f1628;
-	mul.f32 	%f2001, %f1627, %f1630;
-	mul.f32 	%f1631, %f1989, %f1993;
-	mul.f32 	%f1632, %f1990, %f1992;
-	sub.f32 	%f1633, %f1632, %f1631;
-	mul.f32 	%f2002, %f1627, %f1633;
-	sub.f32 	%f1634, %f1618, %f1619;
-	mul.f32 	%f1997, %f1634, %f1627;
-	mul.f32 	%f1635, %f1985, %f1993;
-	mul.f32 	%f1636, %f1987, %f1991;
-	sub.f32 	%f1637, %f1636, %f1635;
-	mul.f32 	%f1998, %f1627, %f1637;
-	mul.f32 	%f1638, %f1990, %f1991;
-	mul.f32 	%f1639, %f1988, %f1993;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r586,%r587,%r588,%r589}, [%rd490];
+	// end inline asm
+	mov.b32 	%f1536, %r587;
+	mov.b32 	%f1537, %r588;
+	mov.b32 	%f1538, %r589;
+	mul.f32 	%f1539, %f548, %f1536;
+	mul.f32 	%f1540, %f548, %f1537;
+	mul.f32 	%f1541, %f548, %f1538;
+	fma.rn.f32 	%f1542, %f1523, %f2024, %f1539;
+	fma.rn.f32 	%f1543, %f1523, %f2025, %f1540;
+	fma.rn.f32 	%f1544, %f1523, %f2026, %f1541;
+	add.s64 	%rd494, %rd472, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd493, %rd494;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r590,%r591,%r592,%r593}, [%rd493];
+	// end inline asm
+	mov.b32 	%f1545, %r590;
+	mul.f32 	%f1546, %f548, %f1545;
+	fma.rn.f32 	%f1547, %f1523, %f2027, %f1546;
+	mul.f32 	%f1548, %f1543, %f1543;
+	fma.rn.f32 	%f1549, %f1542, %f1542, %f1548;
+	fma.rn.f32 	%f1550, %f1544, %f1544, %f1549;
+	fma.rn.f32 	%f1551, %f1547, %f1547, %f1550;
+	sqrt.rn.f32 	%f1552, %f1551;
+	rcp.rn.f32 	%f1553, %f1552;
+	mul.f32 	%f2024, %f1542, %f1553;
+	mul.f32 	%f2025, %f1543, %f1553;
+	mul.f32 	%f2026, %f1544, %f1553;
+	mul.f32 	%f2027, %f1553, %f1547;
+
+$L__BB1_85:
+	mul.f32 	%f1554, %f2025, %f2025;
+	fma.rn.f32 	%f1555, %f2024, %f2024, %f1554;
+	fma.rn.f32 	%f1556, %f2026, %f2026, %f1555;
+	fma.rn.f32 	%f1557, %f2027, %f2027, %f1556;
+	rcp.rn.f32 	%f1558, %f1557;
+	mul.f32 	%f1559, %f2024, %f1558;
+	mul.f32 	%f1560, %f2025, %f1558;
+	mul.f32 	%f1561, %f2026, %f1558;
+	mul.f32 	%f1562, %f2027, %f1558;
+	mul.f32 	%f1563, %f2024, %f1559;
+	mul.f32 	%f1564, %f2025, %f1560;
+	mul.f32 	%f1565, %f2026, %f1561;
+	mul.f32 	%f1566, %f2024, %f1560;
+	mul.f32 	%f1567, %f2026, %f1562;
+	mul.f32 	%f1568, %f2024, %f1561;
+	mul.f32 	%f1569, %f2025, %f1562;
+	mul.f32 	%f1570, %f2025, %f1561;
+	mul.f32 	%f1571, %f2024, %f1562;
+	sub.f32 	%f1572, %f1563, %f1564;
+	sub.f32 	%f1573, %f1572, %f1565;
+	fma.rn.f32 	%f1574, %f2027, %f1562, %f1573;
+	sub.f32 	%f1575, %f1566, %f1567;
+	add.f32 	%f1576, %f1575, %f1575;
+	add.f32 	%f1577, %f1568, %f1569;
+	add.f32 	%f1578, %f1577, %f1577;
+	add.f32 	%f1579, %f1566, %f1567;
+	add.f32 	%f1580, %f1579, %f1579;
+	sub.f32 	%f1581, %f1564, %f1563;
+	sub.f32 	%f1582, %f1581, %f1565;
+	fma.rn.f32 	%f1583, %f2027, %f1562, %f1582;
+	sub.f32 	%f1584, %f1570, %f1571;
+	add.f32 	%f1585, %f1584, %f1584;
+	sub.f32 	%f1586, %f1568, %f1569;
+	add.f32 	%f1587, %f1586, %f1586;
+	add.f32 	%f1588, %f1570, %f1571;
+	add.f32 	%f1589, %f1588, %f1588;
+	neg.f32 	%f1590, %f1563;
+	sub.f32 	%f1591, %f1590, %f1564;
+	add.f32 	%f1592, %f1565, %f1591;
+	fma.rn.f32 	%f1593, %f2027, %f1562, %f1592;
+	mul.f32 	%f1594, %f2020, %f1574;
+	fma.rn.f32 	%f1595, %f2022, %f1576, %f1594;
+	fma.rn.f32 	%f2036, %f2023, %f1578, %f1595;
+	mul.f32 	%f1596, %f2022, %f1583;
+	fma.rn.f32 	%f1597, %f2020, %f1580, %f1596;
+	fma.rn.f32 	%f2033, %f2023, %f1585, %f1597;
+	mul.f32 	%f1598, %f2022, %f1589;
+	fma.rn.f32 	%f1599, %f2020, %f1587, %f1598;
+	fma.rn.f32 	%f2030, %f2023, %f1593, %f1599;
+	mul.f32 	%f1600, %f2019, %f1574;
+	fma.rn.f32 	%f2035, %f2021, %f1576, %f1600;
+	mul.f32 	%f1601, %f2021, %f1583;
+	fma.rn.f32 	%f2032, %f2019, %f1580, %f1601;
+	mul.f32 	%f1602, %f2021, %f1589;
+	fma.rn.f32 	%f2029, %f2019, %f1587, %f1602;
+	mul.f32 	%f2034, %f2018, %f1574;
+	mul.f32 	%f2031, %f2018, %f1580;
+	mul.f32 	%f2028, %f2018, %f1587;
+
+$L__BB1_88:
+	mul.f32 	%f1634, %f2029, %f2033;
+	mul.f32 	%f1635, %f2030, %f2032;
+	sub.f32 	%f1636, %f1635, %f1634;
+	mul.f32 	%f1637, %f2034, %f1636;
+	mul.f32 	%f1638, %f2028, %f2033;
+	mul.f32 	%f1639, %f2030, %f2031;
 	sub.f32 	%f1640, %f1639, %f1638;
-	mul.f32 	%f1999, %f1627, %f1640;
-	mul.f32 	%f1994, %f1625, %f1627;
-	mul.f32 	%f1641, %f1986, %f1991;
-	mul.f32 	%f1642, %f1985, %f1992;
-	sub.f32 	%f1643, %f1642, %f1641;
-	mul.f32 	%f1995, %f1643, %f1627;
-	mul.f32 	%f1644, %f1988, %f1992;
-	mul.f32 	%f1645, %f1989, %f1991;
-	sub.f32 	%f1646, %f1645, %f1644;
-	mul.f32 	%f1996, %f1646, %f1627;
-	bra.uni 	BB1_88;
-
-BB1_77:
-	setp.ne.s32	%p49, %r495, 1;
-	mov.f32 	%f1995, %f1994;
-	mov.f32 	%f1997, %f1994;
-	mov.f32 	%f1998, %f1996;
-	mov.f32 	%f1999, %f1994;
-	mov.f32 	%f2000, %f1996;
-	mov.f32 	%f2001, %f1994;
-	mov.f32 	%f2002, %f1994;
-	@%p49 bra 	BB1_88;
-
-	// inline asm
-	call (%rd436), _optix_get_static_transform_from_handle, (%rd434);
-	// inline asm
-	add.s64 	%rd664, %rd436, 64;
-
-BB1_80:
-	// inline asm
-	cvta.to.global.u64 %rd440, %rd664;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r497,%r498,%r499,%r500}, [%rd440];
-	// inline asm
-	mov.b32 	 %f2000, %r497;
-	mov.b32 	 %f2001, %r498;
-	mov.b32 	 %f2002, %r499;
-	add.s64 	%rd444, %rd664, 16;
-	// inline asm
-	cvta.to.global.u64 %rd443, %rd444;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r501,%r502,%r503,%r504}, [%rd443];
-	// inline asm
-	mov.b32 	 %f1997, %r501;
-	mov.b32 	 %f1998, %r502;
-	mov.b32 	 %f1999, %r503;
-	add.s64 	%rd447, %rd664, 32;
-	// inline asm
-	cvta.to.global.u64 %rd446, %rd447;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r505,%r506,%r507,%r508}, [%rd446];
-	// inline asm
-	mov.b32 	 %f1994, %r505;
-	mov.b32 	 %f1995, %r506;
-	mov.b32 	 %f1996, %r507;
-
-BB1_88:
-	setp.eq.s32	%p53, %r649, 0;
-	@%p53 bra 	BB1_89;
-	bra.uni 	BB1_90;
-
-BB1_89:
-	mov.f32 	%f1974, %f1994;
-	mov.f32 	%f1973, %f1995;
-	mov.f32 	%f1972, %f1996;
-	mov.f32 	%f1971, %f1997;
-	mov.f32 	%f1970, %f1998;
-	mov.f32 	%f1969, %f1999;
-	mov.f32 	%f1968, %f2000;
-	mov.f32 	%f1967, %f2001;
-	mov.f32 	%f1966, %f2002;
-	bra.uni 	BB1_91;
-
-BB1_90:
-	mul.f32 	%f1647, %f1971, %f2001;
-	fma.rn.f32 	%f1648, %f1968, %f2000, %f1647;
-	fma.rn.f32 	%f605, %f1974, %f2002, %f1648;
-	mul.f32 	%f1649, %f1970, %f2001;
-	fma.rn.f32 	%f1650, %f1967, %f2000, %f1649;
-	fma.rn.f32 	%f606, %f1973, %f2002, %f1650;
-	mul.f32 	%f1651, %f1969, %f2001;
-	fma.rn.f32 	%f1652, %f1966, %f2000, %f1651;
-	fma.rn.f32 	%f607, %f1972, %f2002, %f1652;
-	mul.f32 	%f1653, %f1971, %f1998;
-	fma.rn.f32 	%f1654, %f1968, %f1997, %f1653;
-	fma.rn.f32 	%f608, %f1974, %f1999, %f1654;
-	mul.f32 	%f1655, %f1970, %f1998;
-	fma.rn.f32 	%f1656, %f1967, %f1997, %f1655;
-	fma.rn.f32 	%f609, %f1973, %f1999, %f1656;
-	mul.f32 	%f1657, %f1969, %f1998;
-	fma.rn.f32 	%f1658, %f1966, %f1997, %f1657;
-	fma.rn.f32 	%f610, %f1972, %f1999, %f1658;
-	mul.f32 	%f1659, %f1971, %f1995;
-	fma.rn.f32 	%f1660, %f1968, %f1994, %f1659;
-	fma.rn.f32 	%f1974, %f1974, %f1996, %f1660;
-	mul.f32 	%f1661, %f1970, %f1995;
-	fma.rn.f32 	%f1662, %f1967, %f1994, %f1661;
-	fma.rn.f32 	%f1973, %f1973, %f1996, %f1662;
-	mul.f32 	%f1663, %f1969, %f1995;
-	fma.rn.f32 	%f1664, %f1966, %f1994, %f1663;
-	fma.rn.f32 	%f1972, %f1972, %f1996, %f1664;
-	mov.f32 	%f1971, %f608;
-	mov.f32 	%f1970, %f609;
-	mov.f32 	%f1969, %f610;
-	mov.f32 	%f1968, %f605;
-	mov.f32 	%f1967, %f606;
-	mov.f32 	%f1966, %f607;
-
-BB1_91:
-	add.s32 	%r649, %r649, 1;
-	setp.lt.u32	%p54, %r649, %r30;
-	@%p54 bra 	BB1_75;
-
-BB1_92:
-	fma.rn.f32 	%f1665, %f2057, %f1905, %f1902;
-	fma.rn.f32 	%f1666, %f2058, %f1904, %f1665;
-	fma.rn.f32 	%f1667, %f2057, %f1909, %f1906;
-	fma.rn.f32 	%f1668, %f2058, %f1908, %f1667;
-	fma.rn.f32 	%f1669, %f2057, %f1913, %f1910;
-	fma.rn.f32 	%f1670, %f2058, %f1912, %f1669;
-	fma.rn.f32 	%f2057, %f2059, %f1903, %f1666;
-	fma.rn.f32 	%f2058, %f2059, %f1907, %f1668;
-	fma.rn.f32 	%f2059, %f2059, %f1911, %f1670;
-	ld.const.u64 	%rd555, [params+112];
-	setp.eq.s64	%p55, %rd555, 0;
-	mov.f32 	%f2051, %f2054;
-	mov.f32 	%f2052, %f2055;
-	mov.f32 	%f2053, %f2056;
-	@%p55 bra 	BB1_94;
-
-	mul.f32 	%f1671, %f2054, %f1968;
-	fma.rn.f32 	%f1672, %f2055, %f1971, %f1671;
-	mul.f32 	%f1673, %f2054, %f1967;
-	fma.rn.f32 	%f1674, %f2055, %f1970, %f1673;
-	mul.f32 	%f1675, %f2054, %f1966;
-	fma.rn.f32 	%f1676, %f2055, %f1969, %f1675;
-	fma.rn.f32 	%f1677, %f2056, %f1974, %f1672;
-	fma.rn.f32 	%f1678, %f2056, %f1973, %f1674;
-	fma.rn.f32 	%f1679, %f2056, %f1972, %f1676;
-	mul.f32 	%f1680, %f1677, %f1677;
-	fma.rn.f32 	%f1681, %f1678, %f1678, %f1680;
-	fma.rn.f32 	%f1682, %f1679, %f1679, %f1681;
-	sqrt.rn.f32 	%f1683, %f1682;
-	div.rn.f32 	%f2051, %f1677, %f1683;
-	div.rn.f32 	%f2052, %f1678, %f1683;
-	div.rn.f32 	%f2053, %f1679, %f1683;
-
-BB1_94:
-	ld.const.u64 	%rd556, [params+136];
-	setp.eq.s64	%p56, %rd556, 0;
-	@%p56 bra 	BB1_96;
-
-	mul.f32 	%f1684, %f2054, %f1968;
-	fma.rn.f32 	%f1685, %f2055, %f1971, %f1684;
-	mul.f32 	%f1686, %f2054, %f1967;
-	fma.rn.f32 	%f1687, %f2055, %f1970, %f1686;
-	mul.f32 	%f1688, %f2054, %f1966;
-	fma.rn.f32 	%f1689, %f2055, %f1969, %f1688;
-	fma.rn.f32 	%f1690, %f2056, %f1974, %f1685;
-	fma.rn.f32 	%f1691, %f2056, %f1973, %f1687;
-	fma.rn.f32 	%f1692, %f2056, %f1972, %f1689;
-	mul.f32 	%f1693, %f1690, %f1690;
-	fma.rn.f32 	%f1694, %f1691, %f1691, %f1693;
-	fma.rn.f32 	%f1695, %f1692, %f1692, %f1694;
-	sqrt.rn.f32 	%f1696, %f1695;
-	div.rn.f32 	%f2054, %f1690, %f1696;
-	div.rn.f32 	%f2055, %f1691, %f1696;
-	div.rn.f32 	%f2056, %f1692, %f1696;
-
-BB1_96:
-	ld.const.u64 	%rd557, [params+184];
-	setp.eq.s64	%p57, %rd557, 0;
-	@%p57 bra 	BB1_98;
-
-	mul.f32 	%f1697, %f2048, %f1905;
-	fma.rn.f32 	%f1698, %f2049, %f1904, %f1697;
-	mul.f32 	%f1699, %f2048, %f1909;
-	fma.rn.f32 	%f1700, %f2049, %f1908, %f1699;
-	mul.f32 	%f1701, %f2048, %f1913;
-	fma.rn.f32 	%f1702, %f2049, %f1912, %f1701;
-	fma.rn.f32 	%f2048, %f2050, %f1903, %f1698;
-	fma.rn.f32 	%f2049, %f2050, %f1907, %f1700;
-	fma.rn.f32 	%f2050, %f2050, %f1911, %f1702;
-	mul.f32 	%f1703, %f2045, %f1905;
-	fma.rn.f32 	%f1704, %f2046, %f1904, %f1703;
-	mul.f32 	%f1705, %f2045, %f1909;
-	fma.rn.f32 	%f1706, %f2046, %f1908, %f1705;
-	mul.f32 	%f1707, %f2045, %f1913;
-	fma.rn.f32 	%f1708, %f2046, %f1912, %f1707;
-	fma.rn.f32 	%f2045, %f2047, %f1903, %f1704;
-	fma.rn.f32 	%f2046, %f2047, %f1907, %f1706;
-	fma.rn.f32 	%f2047, %f2047, %f1911, %f1708;
-
-BB1_98:
-	ld.const.u64 	%rd558, [params+280];
-	ld.const.u64 	%rd559, [params+232];
-	mov.f32 	%f2039, 0f00000000;
-	or.b64  	%rd560, %rd558, %rd559;
-	setp.eq.s64	%p58, %rd560, 0;
-	@%p58 bra 	BB1_99;
-
-	mul.f32 	%f1712, %f2054, %f1905;
-	fma.rn.f32 	%f1713, %f2055, %f1909, %f1712;
-	mul.f32 	%f1714, %f2054, %f1904;
-	fma.rn.f32 	%f1715, %f2055, %f1908, %f1714;
-	mul.f32 	%f1716, %f2054, %f1903;
-	fma.rn.f32 	%f1717, %f2055, %f1907, %f1716;
-	fma.rn.f32 	%f1718, %f2056, %f1913, %f1713;
-	fma.rn.f32 	%f1719, %f2056, %f1912, %f1715;
-	fma.rn.f32 	%f1720, %f2056, %f1911, %f1717;
-	mul.f32 	%f1721, %f1718, %f1718;
-	fma.rn.f32 	%f1722, %f1719, %f1719, %f1721;
-	fma.rn.f32 	%f1723, %f1720, %f1720, %f1722;
-	sqrt.rn.f32 	%f1724, %f1723;
-	div.rn.f32 	%f1725, %f1718, %f1724;
-	div.rn.f32 	%f1726, %f1719, %f1724;
-	div.rn.f32 	%f1727, %f1720, %f1724;
-	mul.f32 	%f1728, %f1725, %f1968;
-	mul.f32 	%f1729, %f1725, %f1967;
-	mul.f32 	%f1730, %f1725, %f1966;
-	fma.rn.f32 	%f1731, %f1726, %f1971, %f1728;
-	fma.rn.f32 	%f1732, %f1726, %f1970, %f1729;
-	fma.rn.f32 	%f1733, %f1726, %f1969, %f1730;
-	fma.rn.f32 	%f1734, %f1727, %f1974, %f1731;
-	fma.rn.f32 	%f1735, %f1727, %f1973, %f1732;
-	fma.rn.f32 	%f1736, %f1727, %f1972, %f1733;
-	mul.f32 	%f1737, %f1734, %f1734;
-	fma.rn.f32 	%f1738, %f1735, %f1735, %f1737;
-	fma.rn.f32 	%f1739, %f1736, %f1736, %f1738;
-	sqrt.rn.f32 	%f1740, %f1739;
-	rcp.rn.f32 	%f1741, %f1740;
-	mul.f32 	%f1742, %f1741, %f1734;
-	mul.f32 	%f1743, %f1741, %f1735;
-	mul.f32 	%f1744, %f1741, %f1736;
-	mul.f32 	%f1745, %f2042, %f1968;
-	fma.rn.f32 	%f1746, %f2043, %f1971, %f1745;
-	mul.f32 	%f1747, %f2042, %f1967;
-	fma.rn.f32 	%f1748, %f2043, %f1970, %f1747;
-	mul.f32 	%f1749, %f2042, %f1966;
-	fma.rn.f32 	%f1750, %f2043, %f1969, %f1749;
-	fma.rn.f32 	%f1751, %f2044, %f1974, %f1746;
-	fma.rn.f32 	%f1752, %f2044, %f1973, %f1748;
-	fma.rn.f32 	%f1753, %f2044, %f1972, %f1750;
-	mul.f32 	%f1754, %f1751, %f1741;
-	mul.f32 	%f1755, %f1752, %f1741;
-	mul.f32 	%f1756, %f1753, %f1741;
-	mul.f32 	%f1757, %f1968, 0f00000000;
-	mov.f32 	%f1758, 0f00000000;
-	fma.rn.f32 	%f1759, %f1758, %f1971, %f1757;
-	mul.f32 	%f1760, %f1967, 0f00000000;
-	fma.rn.f32 	%f1761, %f1758, %f1970, %f1760;
-	mul.f32 	%f1762, %f1966, 0f00000000;
-	fma.rn.f32 	%f1763, %f1758, %f1969, %f1762;
-	fma.rn.f32 	%f1764, %f1758, %f1974, %f1759;
-	fma.rn.f32 	%f1765, %f1758, %f1973, %f1761;
-	fma.rn.f32 	%f1766, %f1758, %f1972, %f1763;
-	mul.f32 	%f1767, %f1764, %f1741;
-	mul.f32 	%f1768, %f1765, %f1741;
-	mul.f32 	%f1769, %f1766, %f1741;
-	mul.f32 	%f1770, %f1742, %f1754;
-	fma.rn.f32 	%f1771, %f1743, %f1755, %f1770;
-	fma.rn.f32 	%f1772, %f1744, %f1756, %f1771;
-	mul.f32 	%f1773, %f1742, %f1772;
-	mul.f32 	%f1774, %f1743, %f1772;
-	mul.f32 	%f1775, %f1744, %f1772;
-	sub.f32 	%f2042, %f1754, %f1773;
-	sub.f32 	%f2043, %f1755, %f1774;
-	sub.f32 	%f2044, %f1756, %f1775;
-	mul.f32 	%f1776, %f1742, %f1767;
-	fma.rn.f32 	%f1777, %f1743, %f1768, %f1776;
-	fma.rn.f32 	%f1778, %f1744, %f1769, %f1777;
-	mul.f32 	%f1779, %f1742, %f1778;
-	mul.f32 	%f1780, %f1743, %f1778;
-	mul.f32 	%f1781, %f1744, %f1778;
-	sub.f32 	%f2039, %f1767, %f1779;
-	sub.f32 	%f2040, %f1768, %f1780;
-	sub.f32 	%f2041, %f1769, %f1781;
-	bra.uni 	BB1_101;
-
-BB1_54:
-	mov.f32 	%f2040, %f2039;
-	mov.f32 	%f2041, %f2039;
-	mov.f32 	%f2051, %f2054;
-	mov.f32 	%f2052, %f2055;
-	mov.f32 	%f2053, %f2056;
-	bra.uni 	BB1_102;
-
-BB1_99:
-	mov.f32 	%f2040, %f2039;
-	mov.f32 	%f2041, %f2039;
-
-BB1_101:
-	st.global.u32 	[%rd25], %r343;
-
-BB1_102:
-	ld.const.u64 	%rd561, [params+328];
-	cvta.to.global.u64 	%rd562, %rd561;
-	shl.b64 	%rd563, %rd24, 3;
-	add.s64 	%rd564, %rd562, %rd563;
-	st.global.u64 	[%rd564], %rd23;
-	ld.const.u64 	%rd565, [params+336];
-	cvta.to.global.u64 	%rd566, %rd565;
-	shl.b64 	%rd567, %rd24, 2;
-	add.s64 	%rd568, %rd566, %rd567;
-	mov.u32 	%r643, 0;
-	st.global.u32 	[%rd568], %r643;
-	ld.const.u64 	%rd569, [params+160];
-	cvta.to.global.u64 	%rd570, %rd569;
-	add.s64 	%rd571, %rd570, %rd567;
-	st.global.f32 	[%rd571], %f2057;
-	ld.const.u64 	%rd572, [params+168];
+	mul.f32 	%f1641, %f1640, %f2035;
+	sub.f32 	%f1642, %f1637, %f1641;
+	mul.f32 	%f1643, %f2028, %f2032;
+	mul.f32 	%f1644, %f2029, %f2031;
+	sub.f32 	%f1645, %f1644, %f1643;
+	fma.rn.f32 	%f1646, %f1645, %f2036, %f1642;
+	rcp.rn.f32 	%f1647, %f1646;
+	mul.f32 	%f2043, %f1636, %f1647;
+	mul.f32 	%f1648, %f2030, %f2035;
+	mul.f32 	%f1649, %f2029, %f2036;
+	sub.f32 	%f1650, %f1649, %f1648;
+	mul.f32 	%f2044, %f1650, %f1647;
+	mul.f32 	%f1651, %f2032, %f2036;
+	mul.f32 	%f1652, %f2033, %f2035;
+	sub.f32 	%f1653, %f1652, %f1651;
+	mul.f32 	%f2045, %f1653, %f1647;
+	sub.f32 	%f1654, %f1638, %f1639;
+	mul.f32 	%f2040, %f1654, %f1647;
+	mul.f32 	%f1655, %f2028, %f2036;
+	mul.f32 	%f1656, %f2030, %f2034;
+	sub.f32 	%f1657, %f1656, %f1655;
+	mul.f32 	%f2041, %f1657, %f1647;
+	mul.f32 	%f1658, %f2033, %f2034;
+	mul.f32 	%f1659, %f2031, %f2036;
+	sub.f32 	%f1660, %f1659, %f1658;
+	mul.f32 	%f2042, %f1660, %f1647;
+	mul.f32 	%f2037, %f1645, %f1647;
+	mul.f32 	%f1661, %f2029, %f2034;
+	mul.f32 	%f1662, %f2028, %f2035;
+	sub.f32 	%f1663, %f1662, %f1661;
+	mul.f32 	%f2038, %f1663, %f1647;
+	mul.f32 	%f1664, %f2031, %f2035;
+	mul.f32 	%f1665, %f2032, %f2034;
+	sub.f32 	%f1666, %f1665, %f1664;
+	mul.f32 	%f2039, %f1666, %f1647;
+	bra.uni 	$L__BB1_89;
+
+$L__BB1_80:
+	// begin inline asm
+	call (%rd651), _optix_get_instance_inverse_transform_from_handle, (%rd424);
+	// end inline asm
+
+$L__BB1_81:
+	// begin inline asm
+	cvta.to.global.u64 %rd430, %rd651;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r507,%r508,%r509,%r510}, [%rd430];
+	// end inline asm
+	mov.b32 	%f2043, %r507;
+	mov.b32 	%f2044, %r508;
+	mov.b32 	%f2045, %r509;
+	add.s64 	%rd434, %rd651, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd433, %rd434;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r511,%r512,%r513,%r514}, [%rd433];
+	// end inline asm
+	mov.b32 	%f2040, %r511;
+	mov.b32 	%f2041, %r512;
+	mov.b32 	%f2042, %r513;
+	add.s64 	%rd437, %rd651, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd436, %rd437;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r515,%r516,%r517,%r518}, [%rd436];
+	// end inline asm
+	mov.b32 	%f2037, %r515;
+	mov.b32 	%f2038, %r516;
+	mov.b32 	%f2039, %r517;
+
+$L__BB1_89:
+	setp.eq.s32 	%p57, %r659, 0;
+	@%p57 bra 	$L__BB1_91;
+
+	mul.f32 	%f1667, %f2014, %f2044;
+	fma.rn.f32 	%f1668, %f2011, %f2043, %f1667;
+	fma.rn.f32 	%f634, %f2017, %f2045, %f1668;
+	mul.f32 	%f1669, %f2013, %f2044;
+	fma.rn.f32 	%f1670, %f2010, %f2043, %f1669;
+	fma.rn.f32 	%f635, %f2016, %f2045, %f1670;
+	mul.f32 	%f1671, %f2012, %f2044;
+	fma.rn.f32 	%f1672, %f2009, %f2043, %f1671;
+	fma.rn.f32 	%f2045, %f2015, %f2045, %f1672;
+	mul.f32 	%f1673, %f2014, %f2041;
+	fma.rn.f32 	%f1674, %f2011, %f2040, %f1673;
+	fma.rn.f32 	%f637, %f2017, %f2042, %f1674;
+	mul.f32 	%f1675, %f2013, %f2041;
+	fma.rn.f32 	%f1676, %f2010, %f2040, %f1675;
+	fma.rn.f32 	%f638, %f2016, %f2042, %f1676;
+	mul.f32 	%f1677, %f2012, %f2041;
+	fma.rn.f32 	%f1678, %f2009, %f2040, %f1677;
+	fma.rn.f32 	%f2042, %f2015, %f2042, %f1678;
+	mul.f32 	%f1679, %f2014, %f2038;
+	fma.rn.f32 	%f1680, %f2011, %f2037, %f1679;
+	fma.rn.f32 	%f640, %f2017, %f2039, %f1680;
+	mul.f32 	%f1681, %f2013, %f2038;
+	fma.rn.f32 	%f1682, %f2010, %f2037, %f1681;
+	fma.rn.f32 	%f641, %f2016, %f2039, %f1682;
+	mul.f32 	%f1683, %f2012, %f2038;
+	fma.rn.f32 	%f1684, %f2009, %f2037, %f1683;
+	fma.rn.f32 	%f2039, %f2015, %f2039, %f1684;
+	mov.f32 	%f2037, %f640;
+	mov.f32 	%f2038, %f641;
+	mov.f32 	%f2040, %f637;
+	mov.f32 	%f2041, %f638;
+	mov.f32 	%f2043, %f634;
+	mov.f32 	%f2044, %f635;
+
+$L__BB1_91:
+	add.s32 	%r659, %r659, 1;
+	setp.lt.u32 	%p58, %r659, %r502;
+	mov.f32 	%f2009, %f2045;
+	mov.f32 	%f2010, %f2044;
+	mov.f32 	%f2011, %f2043;
+	mov.f32 	%f2012, %f2042;
+	mov.f32 	%f2013, %f2041;
+	mov.f32 	%f2014, %f2040;
+	mov.f32 	%f2015, %f2039;
+	mov.f32 	%f2016, %f2038;
+	mov.f32 	%f2017, %f2037;
+	@%p58 bra 	$L__BB1_76;
+
+$L__BB1_92:
+	fma.rn.f32 	%f1685, %f2100, %f1981, %f1984;
+	fma.rn.f32 	%f1686, %f2101, %f1982, %f1685;
+	fma.rn.f32 	%f1687, %f2100, %f1977, %f1980;
+	fma.rn.f32 	%f1688, %f2101, %f1978, %f1687;
+	fma.rn.f32 	%f1689, %f2100, %f1973, %f1976;
+	fma.rn.f32 	%f1690, %f2101, %f1974, %f1689;
+	fma.rn.f32 	%f2100, %f2102, %f1983, %f1686;
+	fma.rn.f32 	%f2101, %f2102, %f1979, %f1688;
+	fma.rn.f32 	%f2102, %f2102, %f1975, %f1690;
+	ld.const.u64 	%rd543, [params+112];
+	setp.eq.s64 	%p59, %rd543, 0;
+	mov.f32 	%f2064, %f2067;
+	mov.f32 	%f2065, %f2068;
+	mov.f32 	%f2066, %f2069;
+	@%p59 bra 	$L__BB1_94;
+
+	mul.f32 	%f1691, %f2067, %f2043;
+	fma.rn.f32 	%f1692, %f2068, %f2040, %f1691;
+	mul.f32 	%f1693, %f2067, %f2044;
+	fma.rn.f32 	%f1694, %f2068, %f2041, %f1693;
+	mul.f32 	%f1695, %f2067, %f2045;
+	fma.rn.f32 	%f1696, %f2068, %f2042, %f1695;
+	fma.rn.f32 	%f1697, %f2069, %f2037, %f1692;
+	fma.rn.f32 	%f1698, %f2069, %f2038, %f1694;
+	fma.rn.f32 	%f1699, %f2069, %f2039, %f1696;
+	mul.f32 	%f1700, %f1697, %f1697;
+	fma.rn.f32 	%f1701, %f1698, %f1698, %f1700;
+	fma.rn.f32 	%f1702, %f1699, %f1699, %f1701;
+	sqrt.rn.f32 	%f1703, %f1702;
+	div.rn.f32 	%f2064, %f1697, %f1703;
+	div.rn.f32 	%f2065, %f1698, %f1703;
+	div.rn.f32 	%f2066, %f1699, %f1703;
+
+$L__BB1_94:
+	ld.const.u64 	%rd544, [params+136];
+	setp.eq.s64 	%p60, %rd544, 0;
+	@%p60 bra 	$L__BB1_96;
+
+	mul.f32 	%f1704, %f2067, %f2043;
+	fma.rn.f32 	%f1705, %f2068, %f2040, %f1704;
+	mul.f32 	%f1706, %f2067, %f2044;
+	fma.rn.f32 	%f1707, %f2068, %f2041, %f1706;
+	mul.f32 	%f1708, %f2067, %f2045;
+	fma.rn.f32 	%f1709, %f2068, %f2042, %f1708;
+	fma.rn.f32 	%f1710, %f2069, %f2037, %f1705;
+	fma.rn.f32 	%f1711, %f2069, %f2038, %f1707;
+	fma.rn.f32 	%f1712, %f2069, %f2039, %f1709;
+	mul.f32 	%f1713, %f1710, %f1710;
+	fma.rn.f32 	%f1714, %f1711, %f1711, %f1713;
+	fma.rn.f32 	%f1715, %f1712, %f1712, %f1714;
+	sqrt.rn.f32 	%f1716, %f1715;
+	div.rn.f32 	%f2067, %f1710, %f1716;
+	div.rn.f32 	%f2068, %f1711, %f1716;
+	div.rn.f32 	%f2069, %f1712, %f1716;
+
+$L__BB1_96:
+	mov.f32 	%f2099, %f2069;
+	mov.f32 	%f2098, %f2068;
+	mov.f32 	%f2097, %f2067;
+	ld.const.u64 	%rd545, [params+184];
+	setp.eq.s64 	%p61, %rd545, 0;
+	@%p61 bra 	$L__BB1_98;
+
+	mul.f32 	%f1717, %f2073, %f1981;
+	fma.rn.f32 	%f1718, %f2074, %f1982, %f1717;
+	mul.f32 	%f1719, %f2073, %f1977;
+	fma.rn.f32 	%f1720, %f2074, %f1978, %f1719;
+	mul.f32 	%f1721, %f2073, %f1973;
+	fma.rn.f32 	%f1722, %f2074, %f1974, %f1721;
+	fma.rn.f32 	%f2073, %f2075, %f1983, %f1718;
+	fma.rn.f32 	%f2074, %f2075, %f1979, %f1720;
+	fma.rn.f32 	%f2075, %f2075, %f1975, %f1722;
+	mul.f32 	%f1723, %f2070, %f1981;
+	fma.rn.f32 	%f1724, %f2071, %f1982, %f1723;
+	mul.f32 	%f1725, %f2070, %f1977;
+	fma.rn.f32 	%f1726, %f2071, %f1978, %f1725;
+	mul.f32 	%f1727, %f2070, %f1973;
+	fma.rn.f32 	%f1728, %f2071, %f1974, %f1727;
+	fma.rn.f32 	%f2070, %f2072, %f1983, %f1724;
+	fma.rn.f32 	%f2071, %f2072, %f1979, %f1726;
+	fma.rn.f32 	%f2072, %f2072, %f1975, %f1728;
+
+$L__BB1_98:
+	ld.const.u64 	%rd546, [params+232];
+	ld.const.u64 	%rd547, [params+280];
+	or.b64  	%rd548, %rd546, %rd547;
+	setp.eq.s64 	%p62, %rd548, 0;
+	mov.f32 	%f2082, 0f00000000;
+	mov.f32 	%f2083, %f2082;
+	mov.f32 	%f2084, %f2082;
+	@%p62 bra 	$L__BB1_100;
+
+	mul.f32 	%f1732, %f2097, %f1981;
+	fma.rn.f32 	%f1733, %f2098, %f1977, %f1732;
+	mul.f32 	%f1734, %f2097, %f1982;
+	fma.rn.f32 	%f1735, %f2098, %f1978, %f1734;
+	mul.f32 	%f1736, %f2097, %f1983;
+	fma.rn.f32 	%f1737, %f2098, %f1979, %f1736;
+	fma.rn.f32 	%f1738, %f2099, %f1973, %f1733;
+	fma.rn.f32 	%f1739, %f2099, %f1974, %f1735;
+	fma.rn.f32 	%f1740, %f2099, %f1975, %f1737;
+	mul.f32 	%f1741, %f1738, %f1738;
+	fma.rn.f32 	%f1742, %f1739, %f1739, %f1741;
+	fma.rn.f32 	%f1743, %f1740, %f1740, %f1742;
+	sqrt.rn.f32 	%f1744, %f1743;
+	div.rn.f32 	%f1745, %f1738, %f1744;
+	div.rn.f32 	%f1746, %f1739, %f1744;
+	div.rn.f32 	%f1747, %f1740, %f1744;
+	mul.f32 	%f1748, %f1745, %f2043;
+	mul.f32 	%f1749, %f1745, %f2044;
+	mul.f32 	%f1750, %f1745, %f2045;
+	fma.rn.f32 	%f1751, %f1746, %f2040, %f1748;
+	fma.rn.f32 	%f1752, %f1746, %f2041, %f1749;
+	fma.rn.f32 	%f1753, %f1746, %f2042, %f1750;
+	fma.rn.f32 	%f1754, %f1747, %f2037, %f1751;
+	fma.rn.f32 	%f1755, %f1747, %f2038, %f1752;
+	fma.rn.f32 	%f1756, %f1747, %f2039, %f1753;
+	mul.f32 	%f1757, %f1754, %f1754;
+	fma.rn.f32 	%f1758, %f1755, %f1755, %f1757;
+	fma.rn.f32 	%f1759, %f1756, %f1756, %f1758;
+	sqrt.rn.f32 	%f1760, %f1759;
+	rcp.rn.f32 	%f1761, %f1760;
+	mul.f32 	%f1762, %f1761, %f1754;
+	mul.f32 	%f1763, %f1761, %f1755;
+	mul.f32 	%f1764, %f1761, %f1756;
+	mul.f32 	%f1765, %f2079, %f2043;
+	fma.rn.f32 	%f1766, %f2080, %f2040, %f1765;
+	mul.f32 	%f1767, %f2079, %f2044;
+	fma.rn.f32 	%f1768, %f2080, %f2041, %f1767;
+	mul.f32 	%f1769, %f2079, %f2045;
+	fma.rn.f32 	%f1770, %f2080, %f2042, %f1769;
+	fma.rn.f32 	%f1771, %f2081, %f2037, %f1766;
+	fma.rn.f32 	%f1772, %f2081, %f2038, %f1768;
+	fma.rn.f32 	%f1773, %f2081, %f2039, %f1770;
+	mul.f32 	%f1774, %f1771, %f1761;
+	mul.f32 	%f1775, %f1772, %f1761;
+	mul.f32 	%f1776, %f1773, %f1761;
+	mul.f32 	%f1777, %f2043, 0f00000000;
+	mov.f32 	%f1778, 0f00000000;
+	fma.rn.f32 	%f1779, %f1778, %f2040, %f1777;
+	mul.f32 	%f1780, %f2044, 0f00000000;
+	fma.rn.f32 	%f1781, %f1778, %f2041, %f1780;
+	mul.f32 	%f1782, %f2045, 0f00000000;
+	fma.rn.f32 	%f1783, %f1778, %f2042, %f1782;
+	fma.rn.f32 	%f1784, %f1778, %f2037, %f1779;
+	fma.rn.f32 	%f1785, %f1778, %f2038, %f1781;
+	fma.rn.f32 	%f1786, %f1778, %f2039, %f1783;
+	mul.f32 	%f1787, %f1784, %f1761;
+	mul.f32 	%f1788, %f1785, %f1761;
+	mul.f32 	%f1789, %f1786, %f1761;
+	mul.f32 	%f1790, %f1762, %f1774;
+	fma.rn.f32 	%f1791, %f1763, %f1775, %f1790;
+	fma.rn.f32 	%f1792, %f1764, %f1776, %f1791;
+	mul.f32 	%f1793, %f1762, %f1792;
+	mul.f32 	%f1794, %f1763, %f1792;
+	mul.f32 	%f1795, %f1764, %f1792;
+	sub.f32 	%f2079, %f1774, %f1793;
+	sub.f32 	%f2080, %f1775, %f1794;
+	sub.f32 	%f2081, %f1776, %f1795;
+	mul.f32 	%f1796, %f1762, %f1787;
+	fma.rn.f32 	%f1797, %f1763, %f1788, %f1796;
+	fma.rn.f32 	%f1798, %f1764, %f1789, %f1797;
+	mul.f32 	%f1799, %f1762, %f1798;
+	mul.f32 	%f1800, %f1763, %f1798;
+	mul.f32 	%f1801, %f1764, %f1798;
+	sub.f32 	%f2082, %f1787, %f1799;
+	sub.f32 	%f2083, %f1788, %f1800;
+	sub.f32 	%f2084, %f1789, %f1801;
+
+$L__BB1_100:
+	st.global.u32 	[%rd23], %r348;
+	mov.f32 	%f2067, %f2064;
+	mov.f32 	%f2068, %f2065;
+	mov.f32 	%f2069, %f2066;
+
+$L__BB1_101:
+	ld.const.u64 	%rd549, [params+328];
+	cvta.to.global.u64 	%rd550, %rd549;
+	shl.b64 	%rd551, %rd22, 3;
+	add.s64 	%rd552, %rd550, %rd551;
+	st.global.u64 	[%rd552], %rd21;
+	ld.const.u64 	%rd553, [params+336];
+	cvta.to.global.u64 	%rd554, %rd553;
+	shl.b64 	%rd555, %rd22, 2;
+	add.s64 	%rd556, %rd554, %rd555;
+	mov.u32 	%r653, 0;
+	st.global.u32 	[%rd556], %r653;
+	ld.const.u64 	%rd557, [params+160];
+	cvta.to.global.u64 	%rd558, %rd557;
+	add.s64 	%rd559, %rd558, %rd555;
+	st.global.f32 	[%rd559], %f2100;
+	ld.const.u64 	%rd560, [params+168];
+	cvta.to.global.u64 	%rd561, %rd560;
+	add.s64 	%rd562, %rd561, %rd555;
+	st.global.f32 	[%rd562], %f2101;
+	ld.const.u64 	%rd563, [params+176];
+	cvta.to.global.u64 	%rd564, %rd563;
+	add.s64 	%rd565, %rd564, %rd555;
+	st.global.f32 	[%rd565], %f2102;
+	ld.const.u64 	%rd566, [params+72];
+	cvta.to.global.u64 	%rd567, %rd566;
+	add.s64 	%rd568, %rd567, %rd555;
+	st.global.f32 	[%rd568], %f1169;
+	ld.const.u64 	%rd38, [params+96];
+	setp.eq.s64 	%p63, %rd38, 0;
+	@%p63 bra 	$L__BB1_103;
+
+	cvta.to.global.u64 	%rd569, %rd38;
+	add.s64 	%rd571, %rd569, %rd555;
+	mul.f32 	%f1802, %f353, 0f3E22F983;
+	st.global.f32 	[%rd571], %f1802;
+	ld.const.u64 	%rd572, [params+104];
 	cvta.to.global.u64 	%rd573, %rd572;
-	add.s64 	%rd574, %rd573, %rd567;
-	st.global.f32 	[%rd574], %f2058;
-	ld.const.u64 	%rd575, [params+176];
-	cvta.to.global.u64 	%rd576, %rd575;
-	add.s64 	%rd577, %rd576, %rd567;
-	st.global.f32 	[%rd577], %f2059;
-	ld.const.u64 	%rd578, [params+72];
+	add.s64 	%rd574, %rd573, %rd555;
+	st.global.f32 	[%rd574], %f354;
+
+$L__BB1_103:
+	ld.const.u64 	%rd39, [params+112];
+	setp.eq.s64 	%p64, %rd39, 0;
+	@%p64 bra 	$L__BB1_105;
+
+	cvta.to.global.u64 	%rd575, %rd39;
+	add.s64 	%rd577, %rd575, %rd555;
+	st.global.f32 	[%rd577], %f2067;
+	ld.const.u64 	%rd578, [params+120];
 	cvta.to.global.u64 	%rd579, %rd578;
-	add.s64 	%rd580, %rd579, %rd567;
-	st.global.f32 	[%rd580], %f1138;
-	ld.const.u64 	%rd42, [params+96];
-	setp.eq.s64	%p59, %rd42, 0;
-	@%p59 bra 	BB1_104;
-
-	cvta.to.global.u64 	%rd581, %rd42;
-	add.s64 	%rd583, %rd581, %rd567;
-	mul.f32 	%f1782, %f326, 0f3E22F983;
-	st.global.f32 	[%rd583], %f1782;
-	ld.const.u64 	%rd584, [params+104];
-	cvta.to.global.u64 	%rd585, %rd584;
-	add.s64 	%rd586, %rd585, %rd567;
-	st.global.f32 	[%rd586], %f327;
-
-BB1_104:
-	ld.const.u64 	%rd43, [params+112];
-	setp.eq.s64	%p60, %rd43, 0;
-	@%p60 bra 	BB1_106;
-
-	cvta.to.global.u64 	%rd587, %rd43;
-	add.s64 	%rd589, %rd587, %rd567;
-	st.global.f32 	[%rd589], %f2051;
-	ld.const.u64 	%rd590, [params+120];
+	add.s64 	%rd580, %rd579, %rd555;
+	st.global.f32 	[%rd580], %f2068;
+	ld.const.u64 	%rd581, [params+128];
+	cvta.to.global.u64 	%rd582, %rd581;
+	add.s64 	%rd583, %rd582, %rd555;
+	st.global.f32 	[%rd583], %f2069;
+
+$L__BB1_105:
+	ld.const.u64 	%rd40, [params+136];
+	setp.eq.s64 	%p65, %rd40, 0;
+	@%p65 bra 	$L__BB1_107;
+
+	cvta.to.global.u64 	%rd584, %rd40;
+	add.s64 	%rd586, %rd584, %rd555;
+	st.global.f32 	[%rd586], %f2097;
+	ld.const.u64 	%rd587, [params+144];
+	cvta.to.global.u64 	%rd588, %rd587;
+	add.s64 	%rd589, %rd588, %rd555;
+	st.global.f32 	[%rd589], %f2098;
+	ld.const.u64 	%rd590, [params+152];
 	cvta.to.global.u64 	%rd591, %rd590;
-	add.s64 	%rd592, %rd591, %rd567;
-	st.global.f32 	[%rd592], %f2052;
-	ld.const.u64 	%rd593, [params+128];
-	cvta.to.global.u64 	%rd594, %rd593;
-	add.s64 	%rd595, %rd594, %rd567;
-	st.global.f32 	[%rd595], %f2053;
-
-BB1_106:
-	ld.const.u64 	%rd44, [params+136];
-	setp.eq.s64	%p61, %rd44, 0;
-	@%p61 bra 	BB1_108;
-
-	cvta.to.global.u64 	%rd596, %rd44;
-	add.s64 	%rd598, %rd596, %rd567;
-	st.global.f32 	[%rd598], %f2054;
-	ld.const.u64 	%rd599, [params+144];
+	add.s64 	%rd592, %rd591, %rd555;
+	st.global.f32 	[%rd592], %f2099;
+
+$L__BB1_107:
+	ld.const.u64 	%rd41, [params+184];
+	setp.eq.s64 	%p66, %rd41, 0;
+	@%p66 bra 	$L__BB1_109;
+
+	cvta.to.global.u64 	%rd593, %rd41;
+	add.s64 	%rd595, %rd593, %rd555;
+	st.global.f32 	[%rd595], %f2073;
+	ld.const.u64 	%rd596, [params+192];
+	cvta.to.global.u64 	%rd597, %rd596;
+	add.s64 	%rd598, %rd597, %rd555;
+	st.global.f32 	[%rd598], %f2074;
+	ld.const.u64 	%rd599, [params+200];
 	cvta.to.global.u64 	%rd600, %rd599;
-	add.s64 	%rd601, %rd600, %rd567;
-	st.global.f32 	[%rd601], %f2055;
-	ld.const.u64 	%rd602, [params+152];
+	add.s64 	%rd601, %rd600, %rd555;
+	st.global.f32 	[%rd601], %f2075;
+	ld.const.u64 	%rd602, [params+208];
 	cvta.to.global.u64 	%rd603, %rd602;
-	add.s64 	%rd604, %rd603, %rd567;
-	st.global.f32 	[%rd604], %f2056;
-
-BB1_108:
-	ld.const.u64 	%rd45, [params+184];
-	setp.eq.s64	%p62, %rd45, 0;
-	@%p62 bra 	BB1_110;
-
-	cvta.to.global.u64 	%rd605, %rd45;
-	add.s64 	%rd607, %rd605, %rd567;
-	st.global.f32 	[%rd607], %f2048;
-	ld.const.u64 	%rd608, [params+192];
+	add.s64 	%rd604, %rd603, %rd555;
+	st.global.f32 	[%rd604], %f2070;
+	ld.const.u64 	%rd605, [params+216];
+	cvta.to.global.u64 	%rd606, %rd605;
+	add.s64 	%rd607, %rd606, %rd555;
+	st.global.f32 	[%rd607], %f2071;
+	ld.const.u64 	%rd608, [params+224];
 	cvta.to.global.u64 	%rd609, %rd608;
-	add.s64 	%rd610, %rd609, %rd567;
-	st.global.f32 	[%rd610], %f2049;
-	ld.const.u64 	%rd611, [params+200];
-	cvta.to.global.u64 	%rd612, %rd611;
-	add.s64 	%rd613, %rd612, %rd567;
-	st.global.f32 	[%rd613], %f2050;
-	ld.const.u64 	%rd614, [params+208];
+	add.s64 	%rd610, %rd609, %rd555;
+	st.global.f32 	[%rd610], %f2072;
+
+$L__BB1_109:
+	ld.const.u64 	%rd42, [params+232];
+	setp.eq.s64 	%p67, %rd42, 0;
+	@%p67 bra 	$L__BB1_111;
+
+	cvta.to.global.u64 	%rd611, %rd42;
+	add.s64 	%rd613, %rd611, %rd555;
+	st.global.f32 	[%rd613], %f2079;
+	ld.const.u64 	%rd614, [params+240];
 	cvta.to.global.u64 	%rd615, %rd614;
-	add.s64 	%rd616, %rd615, %rd567;
-	st.global.f32 	[%rd616], %f2045;
-	ld.const.u64 	%rd617, [params+216];
+	add.s64 	%rd616, %rd615, %rd555;
+	st.global.f32 	[%rd616], %f2080;
+	ld.const.u64 	%rd617, [params+248];
 	cvta.to.global.u64 	%rd618, %rd617;
-	add.s64 	%rd619, %rd618, %rd567;
-	st.global.f32 	[%rd619], %f2046;
-	ld.const.u64 	%rd620, [params+224];
+	add.s64 	%rd619, %rd618, %rd555;
+	st.global.f32 	[%rd619], %f2081;
+	ld.const.u64 	%rd620, [params+256];
 	cvta.to.global.u64 	%rd621, %rd620;
-	add.s64 	%rd622, %rd621, %rd567;
-	st.global.f32 	[%rd622], %f2047;
-
-BB1_110:
-	ld.const.u64 	%rd46, [params+232];
-	setp.eq.s64	%p63, %rd46, 0;
-	@%p63 bra 	BB1_112;
-
-	cvta.to.global.u64 	%rd623, %rd46;
-	add.s64 	%rd625, %rd623, %rd567;
-	st.global.f32 	[%rd625], %f2042;
-	ld.const.u64 	%rd626, [params+240];
+	add.s64 	%rd622, %rd621, %rd555;
+	st.global.f32 	[%rd622], %f2082;
+	ld.const.u64 	%rd623, [params+264];
+	cvta.to.global.u64 	%rd624, %rd623;
+	add.s64 	%rd625, %rd624, %rd555;
+	st.global.f32 	[%rd625], %f2083;
+	ld.const.u64 	%rd626, [params+272];
 	cvta.to.global.u64 	%rd627, %rd626;
-	add.s64 	%rd628, %rd627, %rd567;
-	st.global.f32 	[%rd628], %f2043;
-	ld.const.u64 	%rd629, [params+248];
-	cvta.to.global.u64 	%rd630, %rd629;
-	add.s64 	%rd631, %rd630, %rd567;
-	st.global.f32 	[%rd631], %f2044;
-	ld.const.u64 	%rd632, [params+256];
+	add.s64 	%rd628, %rd627, %rd555;
+	st.global.f32 	[%rd628], %f2084;
+
+$L__BB1_111:
+	ld.const.u64 	%rd43, [params+280];
+	setp.eq.s64 	%p68, %rd43, 0;
+	@%p68 bra 	$L__BB1_113;
+
+	cvta.to.global.u64 	%rd629, %rd43;
+	add.s64 	%rd631, %rd629, %rd555;
+	st.global.f32 	[%rd631], %f2079;
+	ld.const.u64 	%rd632, [params+288];
 	cvta.to.global.u64 	%rd633, %rd632;
-	add.s64 	%rd634, %rd633, %rd567;
-	st.global.f32 	[%rd634], %f2039;
-	ld.const.u64 	%rd635, [params+264];
+	add.s64 	%rd634, %rd633, %rd555;
+	st.global.f32 	[%rd634], %f2080;
+	ld.const.u64 	%rd635, [params+296];
 	cvta.to.global.u64 	%rd636, %rd635;
-	add.s64 	%rd637, %rd636, %rd567;
-	st.global.f32 	[%rd637], %f2040;
-	ld.const.u64 	%rd638, [params+272];
+	add.s64 	%rd637, %rd636, %rd555;
+	st.global.f32 	[%rd637], %f2081;
+	ld.const.u64 	%rd638, [params+304];
 	cvta.to.global.u64 	%rd639, %rd638;
-	add.s64 	%rd640, %rd639, %rd567;
-	st.global.f32 	[%rd640], %f2041;
-
-BB1_112:
-	ld.const.u64 	%rd47, [params+280];
-	setp.eq.s64	%p64, %rd47, 0;
-	@%p64 bra 	BB1_114;
-
-	cvta.to.global.u64 	%rd641, %rd47;
-	add.s64 	%rd643, %rd641, %rd567;
-	st.global.f32 	[%rd643], %f2042;
-	ld.const.u64 	%rd644, [params+288];
+	add.s64 	%rd640, %rd639, %rd555;
+	st.global.f32 	[%rd640], %f2082;
+	ld.const.u64 	%rd641, [params+312];
+	cvta.to.global.u64 	%rd642, %rd641;
+	add.s64 	%rd643, %rd642, %rd555;
+	st.global.f32 	[%rd643], %f2083;
+	ld.const.u64 	%rd644, [params+320];
 	cvta.to.global.u64 	%rd645, %rd644;
-	add.s64 	%rd646, %rd645, %rd567;
-	st.global.f32 	[%rd646], %f2043;
-	ld.const.u64 	%rd647, [params+296];
-	cvta.to.global.u64 	%rd648, %rd647;
-	add.s64 	%rd649, %rd648, %rd567;
-	st.global.f32 	[%rd649], %f2044;
-	ld.const.u64 	%rd650, [params+304];
-	cvta.to.global.u64 	%rd651, %rd650;
-	add.s64 	%rd652, %rd651, %rd567;
-	st.global.f32 	[%rd652], %f2039;
-	ld.const.u64 	%rd653, [params+312];
-	cvta.to.global.u64 	%rd654, %rd653;
-	add.s64 	%rd655, %rd654, %rd567;
-	st.global.f32 	[%rd655], %f2040;
-	ld.const.u64 	%rd656, [params+320];
-	cvta.to.global.u64 	%rd657, %rd656;
-	add.s64 	%rd658, %rd657, %rd567;
-	st.global.f32 	[%rd658], %f2041;
-
-BB1_114:
+	add.s64 	%rd646, %rd645, %rd555;
+	st.global.f32 	[%rd646], %f2084;
+
+$L__BB1_113:
 	ret;
-}
 
+}
 	// .globl	__intersection__disk
-.visible .entry __intersection__disk(
-
-)
+.visible .entry __intersection__disk()
 {
-	.reg .pred 	%p<21>;
-	.reg .b16 	%rs<9>;
-	.reg .f32 	%f<932>;
-	.reg .b32 	%r<315>;
-	.reg .b64 	%rd<265>;
-
-
-	// inline asm
-	call (%rd18), _optix_get_sbt_data_ptr_64, ();
-	// inline asm
-	ld.u64 	%rd1, [%rd18+8];
-	// inline asm
-	call (%f314), _optix_get_world_ray_origin_x, ();
-	// inline asm
-	// inline asm
-	call (%f315), _optix_get_world_ray_origin_y, ();
-	// inline asm
-	// inline asm
-	call (%f880), _optix_get_world_ray_origin_z, ();
-	// inline asm
-	// inline asm
-	call (%r8), _optix_get_transform_list_size, ();
-	// inline asm
-	setp.eq.s32	%p1, %r8, 0;
-	@%p1 bra 	BB2_1;
-
-	mov.u32 	%r313, 0;
-	// inline asm
-	call (%f317), _optix_get_ray_time, ();
-	// inline asm
-
-BB2_3:
+	.reg .pred 	%p<22>;
+	.reg .f32 	%f<977>;
+	.reg .b32 	%r<318>;
+	.reg .b64 	%rd<258>;
+
+
+	// begin inline asm
+	call (%rd16), _optix_get_sbt_data_ptr_64, ();
+	// end inline asm
+	ld.u64 	%rd1, [%rd16+8];
+	// begin inline asm
+	call (%f916), _optix_get_world_ray_origin_x, ();
+	// end inline asm
+	// begin inline asm
+	call (%f917), _optix_get_world_ray_origin_y, ();
+	// end inline asm
+	// begin inline asm
+	call (%f918), _optix_get_world_ray_origin_z, ();
+	// end inline asm
+	// begin inline asm
+	call (%r9), _optix_get_transform_list_size, ();
+	// end inline asm
+	setp.eq.s32 	%p1, %r9, 0;
+	@%p1 bra 	$L__BB2_21;
+
+	// begin inline asm
+	call (%r10), _optix_get_transform_list_size, ();
+	// end inline asm
+	// begin inline asm
+	call (%f344), _optix_get_ray_time, ();
+	// end inline asm
+	setp.eq.s32 	%p2, %r10, 0;
+	@%p2 bra 	$L__BB2_19;
+
+	mov.u32 	%r316, 0;
+
+$L__BB2_3:
 	.pragma "nounroll";
-	// inline asm
-	call (%rd19), _optix_get_transform_list_handle, (%r313);
-	// inline asm
-	// inline asm
-	call (%r11), _optix_get_transform_type_from_handle, (%rd19);
-	// inline asm
-	and.b32  	%r12, %r11, -2;
-	setp.eq.s32	%p2, %r12, 2;
-	@%p2 bra 	BB2_9;
-	bra.uni 	BB2_4;
-
-BB2_9:
-	setp.eq.s32	%p5, %r11, 2;
-	@%p5 bra 	BB2_13;
-	bra.uni 	BB2_10;
-
-BB2_13:
-	// inline asm
-	call (%rd93), _optix_get_matrix_motion_transform_from_handle, (%rd19);
-	// inline asm
-	// inline asm
-	cvta.to.global.u64 %rd95, %rd93;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r100,%r101,%r102,%r103}, [%rd95];
-	// inline asm
-	mov.b32	{%rs3, %rs4}, %r102;
-	add.s64 	%rd99, %rd93, 16;
-	// inline asm
-	cvta.to.global.u64 %rd98, %rd99;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r104,%r105,%r106,%r107}, [%rd98];
-	// inline asm
-	add.s64 	%rd102, %rd93, 32;
-	// inline asm
-	cvta.to.global.u64 %rd101, %rd102;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r108,%r109,%r110,%r111}, [%rd101];
-	// inline asm
-	add.s64 	%rd105, %rd93, 48;
-	// inline asm
-	cvta.to.global.u64 %rd104, %rd105;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r112,%r113,%r114,%r115}, [%rd104];
-	// inline asm
-	add.s64 	%rd108, %rd93, 64;
-	// inline asm
-	cvta.to.global.u64 %rd107, %rd108;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r116,%r117,%r118,%r119}, [%rd107];
-	// inline asm
-	add.s64 	%rd111, %rd93, 80;
-	// inline asm
-	cvta.to.global.u64 %rd110, %rd111;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r120,%r121,%r122,%r123}, [%rd110];
-	// inline asm
-	add.s64 	%rd114, %rd93, 96;
-	// inline asm
-	cvta.to.global.u64 %rd113, %rd114;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r124,%r125,%r126,%r127}, [%rd113];
-	// inline asm
-	add.s64 	%rd117, %rd93, 112;
-	// inline asm
-	cvta.to.global.u64 %rd116, %rd117;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r128,%r129,%r130,%r131}, [%rd116];
-	// inline asm
-	mov.b32 	 %f444, %r103;
-	mov.b32 	 %f445, %r104;
-	cvt.u32.u16	%r144, %rs3;
-	add.s32 	%r145, %r144, -1;
-	cvt.rn.f32.s32	%f446, %r145;
-	sub.f32 	%f447, %f317, %f444;
-	mul.f32 	%f448, %f447, %f446;
-	sub.f32 	%f449, %f445, %f444;
-	div.rn.f32 	%f450, %f448, %f449;
-	min.f32 	%f451, %f446, %f450;
-	mov.f32 	%f452, 0f00000000;
-	max.f32 	%f453, %f452, %f451;
-	cvt.rmi.f32.f32	%f454, %f453;
-	cvt.rzi.s32.f32	%r146, %f454;
-	mul.wide.s32 	%rd128, %r146, 48;
-	add.s64 	%rd120, %rd102, %rd128;
-	// inline asm
-	cvta.to.global.u64 %rd119, %rd120;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r132,%r133,%r134,%r135}, [%rd119];
-	// inline asm
-	mov.b32 	 %f852, %r132;
-	mov.b32 	 %f853, %r133;
-	mov.b32 	 %f854, %r134;
-	mov.b32 	 %f855, %r135;
-	add.s64 	%rd123, %rd120, 16;
-	// inline asm
-	cvta.to.global.u64 %rd122, %rd123;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r136,%r137,%r138,%r139}, [%rd122];
-	// inline asm
-	mov.b32 	 %f848, %r136;
-	mov.b32 	 %f849, %r137;
-	mov.b32 	 %f850, %r138;
-	mov.b32 	 %f851, %r139;
-	add.s64 	%rd126, %rd120, 32;
-	// inline asm
+	// begin inline asm
+	call (%rd17), _optix_get_transform_list_handle, (%r316);
+	// end inline asm
+	// begin inline asm
+	call (%r13), _optix_get_transform_type_from_handle, (%rd17);
+	// end inline asm
+	or.b32  	%r14, %r13, 1;
+	setp.eq.s32 	%p3, %r14, 3;
+	@%p3 bra 	$L__BB2_9;
+	bra.uni 	$L__BB2_4;
+
+$L__BB2_9:
+	setp.eq.s32 	%p6, %r13, 2;
+	@%p6 bra 	$L__BB2_13;
+	bra.uni 	$L__BB2_10;
+
+$L__BB2_13:
+	// begin inline asm
+	call (%rd89), _optix_get_matrix_motion_transform_from_handle, (%rd17);
+	// end inline asm
+	// begin inline asm
+	cvta.to.global.u64 %rd91, %rd89;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r102,%r103,%r104,%r105}, [%rd91];
+	// end inline asm
+	add.s64 	%rd95, %rd89, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd94, %rd95;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r106,%r107,%r108,%r109}, [%rd94];
+	// end inline asm
+	add.s64 	%rd98, %rd89, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd97, %rd98;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r110,%r111,%r112,%r113}, [%rd97];
+	// end inline asm
+	add.s64 	%rd101, %rd89, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd100, %rd101;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r114,%r115,%r116,%r117}, [%rd100];
+	// end inline asm
+	add.s64 	%rd104, %rd89, 64;
+	// begin inline asm
+	cvta.to.global.u64 %rd103, %rd104;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r118,%r119,%r120,%r121}, [%rd103];
+	// end inline asm
+	add.s64 	%rd107, %rd89, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd106, %rd107;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r122,%r123,%r124,%r125}, [%rd106];
+	// end inline asm
+	add.s64 	%rd110, %rd89, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd109, %rd110;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r126,%r127,%r128,%r129}, [%rd109];
+	// end inline asm
+	add.s64 	%rd113, %rd89, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd112, %rd113;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r130,%r131,%r132,%r133}, [%rd112];
+	// end inline asm
+	mov.b32 	%f472, %r105;
+	mov.b32 	%f473, %r106;
+	and.b32  	%r146, %r104, 65535;
+	add.s32 	%r147, %r146, -1;
+	cvt.rn.f32.s32 	%f474, %r147;
+	sub.f32 	%f475, %f344, %f472;
+	mul.f32 	%f476, %f475, %f474;
+	sub.f32 	%f477, %f473, %f472;
+	div.rn.f32 	%f478, %f476, %f477;
+	min.f32 	%f479, %f474, %f478;
+	mov.f32 	%f480, 0f00000000;
+	max.f32 	%f481, %f480, %f479;
+	cvt.rmi.f32.f32 	%f482, %f481;
+	sub.f32 	%f90, %f481, %f482;
+	cvt.rzi.s32.f32 	%r148, %f482;
+	mul.wide.s32 	%rd124, %r148, 48;
+	add.s64 	%rd116, %rd98, %rd124;
+	// begin inline asm
+	cvta.to.global.u64 %rd115, %rd116;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r134,%r135,%r136,%r137}, [%rd115];
+	// end inline asm
+	mov.b32 	%f871, %r134;
+	mov.b32 	%f870, %r135;
+	mov.b32 	%f869, %r136;
+	mov.b32 	%f868, %r137;
+	add.s64 	%rd119, %rd116, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd118, %rd119;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r138,%r139,%r140,%r141}, [%rd118];
+	// end inline asm
+	mov.b32 	%f875, %r138;
+	mov.b32 	%f874, %r139;
+	mov.b32 	%f873, %r140;
+	mov.b32 	%f872, %r141;
+	add.s64 	%rd122, %rd116, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd121, %rd122;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r142,%r143,%r144,%r145}, [%rd121];
+	// end inline asm
+	mov.b32 	%f879, %r142;
+	mov.b32 	%f878, %r143;
+	mov.b32 	%f877, %r144;
+	mov.b32 	%f876, %r145;
+	setp.leu.f32 	%p8, %f90, 0f00000000;
+	@%p8 bra 	$L__BB2_15;
+
+	cvt.rmi.f32.f32 	%f839, %f481;
+	cvt.rzi.s32.f32 	%r315, %f839;
+	cvt.s64.s32 	%rd255, %r315;
+	mov.f32 	%f483, 0f3F800000;
+	sub.f32 	%f484, %f483, %f90;
+	mul.lo.s64 	%rd134, %rd255, 48;
+	add.s64 	%rd135, %rd89, %rd134;
+	add.s64 	%rd126, %rd135, 80;
+	// begin inline asm
 	cvta.to.global.u64 %rd125, %rd126;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r140,%r141,%r142,%r143}, [%rd125];
-	// inline asm
-	sub.f32 	%f98, %f453, %f454;
-	mov.b32 	 %f844, %r140;
-	mov.b32 	 %f845, %r141;
-	mov.b32 	 %f846, %r142;
-	mov.b32 	 %f847, %r143;
-	setp.leu.f32	%p7, %f98, 0f00000000;
-	@%p7 bra 	BB2_15;
-
-	cvt.rmi.f32.f32	%f815, %f453;
-	cvt.rzi.s32.f32	%r312, %f815;
-	cvt.s64.s32	%rd262, %r312;
-	mul.lo.s64 	%rd138, %rd262, 48;
-	add.s64 	%rd139, %rd93, %rd138;
-	add.s64 	%rd130, %rd139, 80;
-	// inline asm
-	cvta.to.global.u64 %rd129, %rd130;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r147,%r148,%r149,%r150}, [%rd129];
-	// inline asm
-	mov.b32 	 %f455, %r147;
-	mov.b32 	 %f456, %r148;
-	mov.b32 	 %f457, %r149;
-	mov.b32 	 %f458, %r150;
-	add.s64 	%rd133, %rd139, 96;
-	// inline asm
-	cvta.to.global.u64 %rd132, %rd133;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r151,%r152,%r153,%r154}, [%rd132];
-	// inline asm
-	mov.b32 	 %f459, %r151;
-	mov.b32 	 %f460, %r152;
-	mov.b32 	 %f461, %r153;
-	mov.b32 	 %f462, %r154;
-	add.s64 	%rd136, %rd139, 112;
-	// inline asm
-	cvta.to.global.u64 %rd135, %rd136;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r155,%r156,%r157,%r158}, [%rd135];
-	// inline asm
-	mov.f32 	%f463, 0f3F800000;
-	sub.f32 	%f464, %f463, %f98;
-	mul.f32 	%f465, %f98, %f455;
-	mul.f32 	%f466, %f98, %f456;
-	mul.f32 	%f467, %f98, %f457;
-	mul.f32 	%f468, %f98, %f458;
-	fma.rn.f32 	%f852, %f464, %f852, %f465;
-	fma.rn.f32 	%f853, %f464, %f853, %f466;
-	fma.rn.f32 	%f854, %f464, %f854, %f467;
-	fma.rn.f32 	%f855, %f464, %f855, %f468;
-	mul.f32 	%f469, %f98, %f459;
-	mul.f32 	%f470, %f98, %f460;
-	mul.f32 	%f471, %f98, %f461;
-	mul.f32 	%f472, %f98, %f462;
-	fma.rn.f32 	%f848, %f464, %f848, %f469;
-	fma.rn.f32 	%f849, %f464, %f849, %f470;
-	fma.rn.f32 	%f850, %f464, %f850, %f471;
-	fma.rn.f32 	%f851, %f464, %f851, %f472;
-	mov.b32 	 %f473, %r155;
-	mov.b32 	 %f474, %r156;
-	mov.b32 	 %f475, %r157;
-	mov.b32 	 %f476, %r158;
-	mul.f32 	%f477, %f98, %f473;
-	mul.f32 	%f478, %f98, %f474;
-	mul.f32 	%f479, %f98, %f475;
-	mul.f32 	%f480, %f98, %f476;
-	fma.rn.f32 	%f844, %f464, %f844, %f477;
-	fma.rn.f32 	%f845, %f464, %f845, %f478;
-	fma.rn.f32 	%f846, %f464, %f846, %f479;
-	fma.rn.f32 	%f847, %f464, %f847, %f480;
-	bra.uni 	BB2_15;
-
-BB2_4:
-	mov.f32 	%f856, 0f00000000;
-	mov.f32 	%f858, 0f3F800000;
-	setp.eq.s32	%p3, %r11, 4;
-	@%p3 bra 	BB2_7;
-	bra.uni 	BB2_5;
-
-BB2_7:
-	// inline asm
-	call (%rd263), _optix_get_instance_inverse_transform_from_handle, (%rd19);
-	// inline asm
-	bra.uni 	BB2_8;
-
-BB2_10:
-	// inline asm
-	call (%rd34), _optix_get_srt_motion_transform_from_handle, (%rd19);
-	// inline asm
-	// inline asm
-	cvta.to.global.u64 %rd36, %rd34;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r25,%r26,%r27,%r28}, [%rd36];
-	// inline asm
-	mov.b32	{%rs1, %rs2}, %r27;
-	add.s64 	%rd40, %rd34, 16;
-	// inline asm
-	cvta.to.global.u64 %rd39, %rd40;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r29,%r30,%r31,%r32}, [%rd39];
-	// inline asm
-	add.s64 	%rd43, %rd34, 32;
-	// inline asm
-	cvta.to.global.u64 %rd42, %rd43;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r33,%r34,%r35,%r36}, [%rd42];
-	// inline asm
-	add.s64 	%rd46, %rd34, 48;
-	// inline asm
-	cvta.to.global.u64 %rd45, %rd46;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r37,%r38,%r39,%r40}, [%rd45];
-	// inline asm
-	add.s64 	%rd49, %rd34, 64;
-	// inline asm
-	cvta.to.global.u64 %rd48, %rd49;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r41,%r42,%r43,%r44}, [%rd48];
-	// inline asm
-	add.s64 	%rd52, %rd34, 80;
-	// inline asm
-	cvta.to.global.u64 %rd51, %rd52;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r45,%r46,%r47,%r48}, [%rd51];
-	// inline asm
-	add.s64 	%rd55, %rd34, 96;
-	// inline asm
-	cvta.to.global.u64 %rd54, %rd55;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r49,%r50,%r51,%r52}, [%rd54];
-	// inline asm
-	add.s64 	%rd58, %rd34, 112;
-	// inline asm
-	cvta.to.global.u64 %rd57, %rd58;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r53,%r54,%r55,%r56}, [%rd57];
-	// inline asm
-	add.s64 	%rd61, %rd34, 128;
-	// inline asm
-	cvta.to.global.u64 %rd60, %rd61;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r57,%r58,%r59,%r60}, [%rd60];
-	// inline asm
-	add.s64 	%rd64, %rd34, 144;
-	// inline asm
-	cvta.to.global.u64 %rd63, %rd64;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r61,%r62,%r63,%r64}, [%rd63];
-	// inline asm
-	mov.b32 	 %f331, %r28;
-	mov.b32 	 %f332, %r29;
-	cvt.u32.u16	%r81, %rs1;
-	add.s32 	%r82, %r81, -1;
-	cvt.rn.f32.s32	%f333, %r82;
-	sub.f32 	%f334, %f317, %f331;
-	mul.f32 	%f335, %f334, %f333;
-	sub.f32 	%f336, %f332, %f331;
-	div.rn.f32 	%f337, %f335, %f336;
-	min.f32 	%f338, %f333, %f337;
-	mov.f32 	%f339, 0f00000000;
-	max.f32 	%f340, %f339, %f338;
-	cvt.rmi.f32.f32	%f341, %f340;
-	cvt.rzi.s32.f32	%r83, %f341;
-	mul.wide.s32 	%rd78, %r83, 64;
-	add.s64 	%rd67, %rd43, %rd78;
-	// inline asm
-	cvta.to.global.u64 %rd66, %rd67;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r65,%r66,%r67,%r68}, [%rd66];
-	// inline asm
-	mov.b32 	 %f828, %r65;
-	mov.b32 	 %f829, %r66;
-	mov.b32 	 %f830, %r67;
-	mov.b32 	 %f831, %r68;
-	add.s64 	%rd70, %rd67, 16;
-	// inline asm
-	cvta.to.global.u64 %rd69, %rd70;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r69,%r70,%r71,%r72}, [%rd69];
-	// inline asm
-	mov.b32 	 %f832, %r69;
-	mov.b32 	 %f833, %r70;
-	mov.b32 	 %f834, %r71;
-	mov.b32 	 %f835, %r72;
-	add.s64 	%rd73, %rd67, 32;
-	// inline asm
-	cvta.to.global.u64 %rd72, %rd73;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r73,%r74,%r75,%r76}, [%rd72];
-	// inline asm
-	sub.f32 	%f37, %f340, %f341;
-	mov.b32 	 %f836, %r73;
-	mov.b32 	 %f837, %r74;
-	mov.b32 	 %f838, %r75;
-	mov.b32 	 %f839, %r76;
-	add.s64 	%rd76, %rd67, 48;
-	// inline asm
-	cvta.to.global.u64 %rd75, %rd76;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r77,%r78,%r79,%r80}, [%rd75];
-	// inline asm
-	mov.b32 	 %f840, %r77;
-	mov.b32 	 %f841, %r78;
-	mov.b32 	 %f842, %r79;
-	mov.b32 	 %f843, %r80;
-	setp.leu.f32	%p6, %f37, 0f00000000;
-	@%p6 bra 	BB2_12;
-
-	cvt.rmi.f32.f32	%f814, %f340;
-	cvt.rzi.s32.f32	%r311, %f814;
-	cvt.s64.s32	%rd261, %r311;
-	shl.b64 	%rd91, %rd261, 6;
-	add.s64 	%rd92, %rd91, %rd34;
-	add.s64 	%rd80, %rd92, 96;
-	// inline asm
-	cvta.to.global.u64 %rd79, %rd80;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r84,%r85,%r86,%r87}, [%rd79];
-	// inline asm
-	mov.b32 	 %f342, %r84;
-	mov.b32 	 %f343, %r85;
-	mov.b32 	 %f344, %r86;
-	mov.b32 	 %f345, %r87;
-	add.s64 	%rd83, %rd92, 112;
-	// inline asm
-	cvta.to.global.u64 %rd82, %rd83;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r88,%r89,%r90,%r91}, [%rd82];
-	// inline asm
-	mov.b32 	 %f346, %r88;
-	mov.b32 	 %f347, %r89;
-	mov.b32 	 %f348, %r90;
-	mov.b32 	 %f349, %r91;
-	add.s64 	%rd86, %rd92, 128;
-	// inline asm
-	cvta.to.global.u64 %rd85, %rd86;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r92,%r93,%r94,%r95}, [%rd85];
-	// inline asm
-	mov.b32 	 %f350, %r92;
-	mov.b32 	 %f351, %r93;
-	mov.b32 	 %f352, %r94;
-	mov.b32 	 %f353, %r95;
-	add.s64 	%rd89, %rd92, 144;
-	// inline asm
-	cvta.to.global.u64 %rd88, %rd89;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r96,%r97,%r98,%r99}, [%rd88];
-	// inline asm
-	mov.f32 	%f354, 0f3F800000;
-	sub.f32 	%f355, %f354, %f37;
-	mul.f32 	%f356, %f37, %f342;
-	mul.f32 	%f357, %f37, %f343;
-	mul.f32 	%f358, %f37, %f344;
-	mul.f32 	%f359, %f37, %f345;
-	fma.rn.f32 	%f828, %f355, %f828, %f356;
-	fma.rn.f32 	%f829, %f355, %f829, %f357;
-	fma.rn.f32 	%f830, %f355, %f830, %f358;
-	fma.rn.f32 	%f831, %f355, %f831, %f359;
-	mul.f32 	%f360, %f37, %f346;
-	mul.f32 	%f361, %f37, %f347;
-	mul.f32 	%f362, %f37, %f348;
-	mul.f32 	%f363, %f37, %f349;
-	fma.rn.f32 	%f832, %f355, %f832, %f360;
-	fma.rn.f32 	%f833, %f355, %f833, %f361;
-	fma.rn.f32 	%f834, %f355, %f834, %f362;
-	fma.rn.f32 	%f835, %f355, %f835, %f363;
-	mul.f32 	%f364, %f37, %f350;
-	mul.f32 	%f365, %f37, %f351;
-	mul.f32 	%f366, %f37, %f352;
-	mul.f32 	%f367, %f37, %f353;
-	fma.rn.f32 	%f836, %f355, %f836, %f364;
-	fma.rn.f32 	%f368, %f355, %f837, %f365;
-	fma.rn.f32 	%f369, %f355, %f838, %f366;
-	fma.rn.f32 	%f370, %f355, %f839, %f367;
-	mov.b32 	 %f371, %r96;
-	mov.b32 	 %f372, %r97;
-	mov.b32 	 %f373, %r98;
-	mov.b32 	 %f374, %r99;
-	mul.f32 	%f375, %f37, %f371;
-	mul.f32 	%f376, %f37, %f372;
-	mul.f32 	%f377, %f37, %f373;
-	mul.f32 	%f378, %f37, %f374;
-	fma.rn.f32 	%f379, %f355, %f840, %f375;
-	fma.rn.f32 	%f841, %f355, %f841, %f376;
-	fma.rn.f32 	%f842, %f355, %f842, %f377;
-	fma.rn.f32 	%f843, %f355, %f843, %f378;
-	mul.f32 	%f380, %f369, %f369;
-	fma.rn.f32 	%f381, %f368, %f368, %f380;
-	fma.rn.f32 	%f382, %f370, %f370, %f381;
-	fma.rn.f32 	%f383, %f379, %f379, %f382;
-	sqrt.rn.f32 	%f384, %f383;
-	rcp.rn.f32 	%f385, %f384;
-	mul.f32 	%f837, %f368, %f385;
-	mul.f32 	%f838, %f369, %f385;
-	mul.f32 	%f839, %f370, %f385;
-	mul.f32 	%f840, %f379, %f385;
-
-BB2_12:
-	mul.f32 	%f386, %f838, %f838;
-	fma.rn.f32 	%f387, %f837, %f837, %f386;
-	fma.rn.f32 	%f388, %f839, %f839, %f387;
-	fma.rn.f32 	%f389, %f840, %f840, %f388;
-	rcp.rn.f32 	%f390, %f389;
-	mul.f32 	%f391, %f837, %f390;
-	mul.f32 	%f392, %f838, %f390;
-	mul.f32 	%f393, %f839, %f390;
-	mul.f32 	%f394, %f840, %f390;
-	mul.f32 	%f395, %f837, %f391;
-	mul.f32 	%f396, %f838, %f392;
-	mul.f32 	%f397, %f839, %f393;
-	mul.f32 	%f398, %f837, %f392;
-	mul.f32 	%f399, %f839, %f394;
-	mul.f32 	%f400, %f837, %f393;
-	mul.f32 	%f401, %f838, %f394;
-	mul.f32 	%f402, %f838, %f393;
-	mul.f32 	%f403, %f837, %f394;
-	sub.f32 	%f404, %f395, %f396;
-	sub.f32 	%f405, %f404, %f397;
-	fma.rn.f32 	%f406, %f840, %f394, %f405;
-	sub.f32 	%f407, %f398, %f399;
-	add.f32 	%f408, %f407, %f407;
-	add.f32 	%f409, %f400, %f401;
-	add.f32 	%f410, %f409, %f409;
-	add.f32 	%f411, %f398, %f399;
-	add.f32 	%f412, %f411, %f411;
-	sub.f32 	%f413, %f396, %f395;
-	sub.f32 	%f414, %f413, %f397;
-	fma.rn.f32 	%f415, %f840, %f394, %f414;
-	sub.f32 	%f416, %f402, %f403;
-	add.f32 	%f417, %f416, %f416;
-	sub.f32 	%f418, %f400, %f401;
-	add.f32 	%f419, %f418, %f418;
-	add.f32 	%f420, %f402, %f403;
-	add.f32 	%f421, %f420, %f420;
-	neg.f32 	%f422, %f395;
-	sub.f32 	%f423, %f422, %f396;
-	add.f32 	%f424, %f397, %f423;
-	fma.rn.f32 	%f425, %f840, %f394, %f424;
-	mul.f32 	%f426, %f831, %f406;
-	fma.rn.f32 	%f427, %f834, %f408, %f426;
-	fma.rn.f32 	%f428, %f836, %f410, %f427;
-	sub.f32 	%f855, %f841, %f428;
-	mul.f32 	%f429, %f834, %f415;
-	fma.rn.f32 	%f430, %f831, %f412, %f429;
-	fma.rn.f32 	%f431, %f836, %f417, %f430;
-	sub.f32 	%f851, %f842, %f431;
-	mul.f32 	%f432, %f834, %f421;
-	fma.rn.f32 	%f433, %f831, %f419, %f432;
-	fma.rn.f32 	%f434, %f836, %f425, %f433;
-	sub.f32 	%f847, %f843, %f434;
-	mul.f32 	%f435, %f830, %f406;
-	fma.rn.f32 	%f436, %f833, %f408, %f435;
-	fma.rn.f32 	%f854, %f835, %f410, %f436;
-	mul.f32 	%f437, %f833, %f415;
-	fma.rn.f32 	%f438, %f830, %f412, %f437;
-	fma.rn.f32 	%f850, %f835, %f417, %f438;
-	mul.f32 	%f439, %f833, %f421;
-	fma.rn.f32 	%f440, %f830, %f419, %f439;
-	fma.rn.f32 	%f846, %f835, %f425, %f440;
-	mul.f32 	%f441, %f829, %f406;
-	fma.rn.f32 	%f853, %f832, %f408, %f441;
-	mul.f32 	%f442, %f832, %f415;
-	fma.rn.f32 	%f849, %f829, %f412, %f442;
-	mul.f32 	%f443, %f832, %f421;
-	fma.rn.f32 	%f845, %f829, %f419, %f443;
-	mul.f32 	%f852, %f828, %f406;
-	mul.f32 	%f848, %f828, %f412;
-	mul.f32 	%f844, %f828, %f419;
-
-BB2_15:
-	mul.f32 	%f481, %f845, %f850;
-	mul.f32 	%f482, %f846, %f849;
-	sub.f32 	%f483, %f482, %f481;
-	mul.f32 	%f484, %f852, %f483;
-	mul.f32 	%f485, %f844, %f850;
-	mul.f32 	%f486, %f846, %f848;
-	sub.f32 	%f487, %f486, %f485;
-	mul.f32 	%f488, %f487, %f853;
-	sub.f32 	%f489, %f484, %f488;
-	mul.f32 	%f490, %f844, %f849;
-	mul.f32 	%f491, %f845, %f848;
-	sub.f32 	%f492, %f491, %f490;
-	fma.rn.f32 	%f493, %f492, %f854, %f489;
-	rcp.rn.f32 	%f494, %f493;
-	mul.f32 	%f864, %f483, %f494;
-	mul.f32 	%f495, %f846, %f853;
-	mul.f32 	%f496, %f845, %f854;
-	sub.f32 	%f497, %f496, %f495;
-	mul.f32 	%f865, %f494, %f497;
-	mul.f32 	%f498, %f849, %f854;
-	mul.f32 	%f499, %f850, %f853;
-	sub.f32 	%f500, %f499, %f498;
-	mul.f32 	%f866, %f494, %f500;
-	sub.f32 	%f501, %f485, %f486;
-	mul.f32 	%f860, %f501, %f494;
-	mul.f32 	%f502, %f844, %f854;
-	mul.f32 	%f503, %f846, %f852;
-	sub.f32 	%f504, %f503, %f502;
-	mul.f32 	%f861, %f494, %f504;
-	mul.f32 	%f505, %f850, %f852;
-	mul.f32 	%f506, %f848, %f854;
-	sub.f32 	%f507, %f506, %f505;
-	mul.f32 	%f862, %f494, %f507;
-	mul.f32 	%f856, %f492, %f494;
-	mul.f32 	%f508, %f845, %f852;
-	mul.f32 	%f509, %f844, %f853;
-	sub.f32 	%f510, %f509, %f508;
-	mul.f32 	%f857, %f510, %f494;
-	mul.f32 	%f511, %f848, %f853;
-	mul.f32 	%f512, %f849, %f852;
-	sub.f32 	%f513, %f512, %f511;
-	mul.f32 	%f858, %f513, %f494;
-	mul.f32 	%f514, %f855, %f864;
-	neg.f32 	%f515, %f514;
-	mul.f32 	%f516, %f851, %f865;
-	sub.f32 	%f517, %f515, %f516;
-	mul.f32 	%f518, %f847, %f866;
-	sub.f32 	%f867, %f517, %f518;
-	mul.f32 	%f519, %f855, %f860;
-	neg.f32 	%f520, %f519;
-	mul.f32 	%f521, %f851, %f861;
-	sub.f32 	%f522, %f520, %f521;
-	mul.f32 	%f523, %f847, %f862;
-	sub.f32 	%f863, %f522, %f523;
-	mul.f32 	%f524, %f855, %f856;
-	neg.f32 	%f525, %f524;
-	mul.f32 	%f526, %f851, %f857;
-	sub.f32 	%f527, %f525, %f526;
-	mul.f32 	%f528, %f847, %f858;
-	sub.f32 	%f859, %f527, %f528;
-	bra.uni 	BB2_16;
-
-BB2_5:
-	setp.ne.s32	%p4, %r11, 1;
-	mov.f32 	%f857, %f856;
-	mov.f32 	%f859, %f856;
-	mov.f32 	%f860, %f856;
-	mov.f32 	%f861, %f858;
-	mov.f32 	%f862, %f856;
-	mov.f32 	%f863, %f856;
-	mov.f32 	%f864, %f858;
-	mov.f32 	%f865, %f856;
-	mov.f32 	%f866, %f856;
-	mov.f32 	%f867, %f856;
-	@%p4 bra 	BB2_16;
-
-	// inline asm
-	call (%rd21), _optix_get_static_transform_from_handle, (%rd19);
-	// inline asm
-	add.s64 	%rd263, %rd21, 64;
-
-BB2_8:
-	// inline asm
-	cvta.to.global.u64 %rd25, %rd263;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r13,%r14,%r15,%r16}, [%rd25];
-	// inline asm
-	mov.b32 	 %f864, %r13;
-	mov.b32 	 %f865, %r14;
-	mov.b32 	 %f866, %r15;
-	mov.b32 	 %f867, %r16;
-	add.s64 	%rd29, %rd263, 16;
-	// inline asm
-	cvta.to.global.u64 %rd28, %rd29;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r17,%r18,%r19,%r20}, [%rd28];
-	// inline asm
-	mov.b32 	 %f860, %r17;
-	mov.b32 	 %f861, %r18;
-	mov.b32 	 %f862, %r19;
-	mov.b32 	 %f863, %r20;
-	add.s64 	%rd32, %rd263, 32;
-	// inline asm
-	cvta.to.global.u64 %rd31, %rd32;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r21,%r22,%r23,%r24}, [%rd31];
-	// inline asm
-	mov.b32 	 %f856, %r21;
-	mov.b32 	 %f857, %r22;
-	mov.b32 	 %f858, %r23;
-	mov.b32 	 %f859, %r24;
-
-BB2_16:
-	setp.eq.s32	%p8, %r313, 0;
-	@%p8 bra 	BB2_17;
-	bra.uni 	BB2_18;
-
-BB2_17:
-	mov.f32 	%f827, %f867;
-	mov.f32 	%f826, %f866;
-	mov.f32 	%f825, %f865;
-	mov.f32 	%f824, %f864;
-	mov.f32 	%f823, %f863;
-	mov.f32 	%f822, %f862;
-	mov.f32 	%f821, %f861;
-	mov.f32 	%f820, %f860;
-	mov.f32 	%f819, %f859;
-	mov.f32 	%f818, %f858;
-	mov.f32 	%f817, %f857;
-	mov.f32 	%f816, %f856;
-	bra.uni 	BB2_19;
-
-BB2_18:
-	mul.f32 	%f529, %f820, %f865;
-	fma.rn.f32 	%f530, %f824, %f864, %f529;
-	fma.rn.f32 	%f151, %f816, %f866, %f530;
-	mul.f32 	%f531, %f821, %f865;
-	fma.rn.f32 	%f532, %f825, %f864, %f531;
-	fma.rn.f32 	%f152, %f817, %f866, %f532;
-	mul.f32 	%f533, %f822, %f865;
-	fma.rn.f32 	%f534, %f826, %f864, %f533;
-	fma.rn.f32 	%f153, %f818, %f866, %f534;
-	mul.f32 	%f535, %f823, %f865;
-	fma.rn.f32 	%f536, %f827, %f864, %f535;
-	fma.rn.f32 	%f537, %f819, %f866, %f536;
-	add.f32 	%f154, %f867, %f537;
-	mul.f32 	%f538, %f820, %f861;
-	fma.rn.f32 	%f539, %f824, %f860, %f538;
-	fma.rn.f32 	%f155, %f816, %f862, %f539;
-	mul.f32 	%f540, %f821, %f861;
-	fma.rn.f32 	%f541, %f825, %f860, %f540;
-	fma.rn.f32 	%f156, %f817, %f862, %f541;
-	mul.f32 	%f542, %f822, %f861;
-	fma.rn.f32 	%f543, %f826, %f860, %f542;
-	fma.rn.f32 	%f157, %f818, %f862, %f543;
-	mul.f32 	%f544, %f823, %f861;
-	fma.rn.f32 	%f545, %f827, %f860, %f544;
-	fma.rn.f32 	%f546, %f819, %f862, %f545;
-	add.f32 	%f158, %f863, %f546;
-	mul.f32 	%f547, %f820, %f857;
-	fma.rn.f32 	%f548, %f824, %f856, %f547;
-	fma.rn.f32 	%f816, %f816, %f858, %f548;
-	mul.f32 	%f549, %f821, %f857;
-	fma.rn.f32 	%f550, %f825, %f856, %f549;
-	fma.rn.f32 	%f817, %f817, %f858, %f550;
-	mul.f32 	%f551, %f822, %f857;
-	fma.rn.f32 	%f552, %f826, %f856, %f551;
-	fma.rn.f32 	%f818, %f818, %f858, %f552;
-	mul.f32 	%f553, %f823, %f857;
-	fma.rn.f32 	%f554, %f827, %f856, %f553;
-	fma.rn.f32 	%f555, %f819, %f858, %f554;
-	add.f32 	%f819, %f859, %f555;
-	mov.f32 	%f827, %f154;
-	mov.f32 	%f826, %f153;
-	mov.f32 	%f825, %f152;
-	mov.f32 	%f824, %f151;
-	mov.f32 	%f823, %f158;
-	mov.f32 	%f822, %f157;
-	mov.f32 	%f821, %f156;
-	mov.f32 	%f820, %f155;
-
-BB2_19:
-	add.s32 	%r313, %r313, 1;
-	setp.lt.u32	%p9, %r313, %r8;
-	@%p9 bra 	BB2_3;
-
-	mul.f32 	%f556, %f314, %f824;
-	fma.rn.f32 	%f557, %f315, %f825, %f556;
-	fma.rn.f32 	%f558, %f880, %f826, %f557;
-	add.f32 	%f882, %f827, %f558;
-	mul.f32 	%f559, %f314, %f820;
-	fma.rn.f32 	%f560, %f315, %f821, %f559;
-	fma.rn.f32 	%f561, %f880, %f822, %f560;
-	add.f32 	%f881, %f823, %f561;
-	mul.f32 	%f562, %f314, %f816;
-	fma.rn.f32 	%f563, %f315, %f817, %f562;
-	fma.rn.f32 	%f564, %f880, %f818, %f563;
-	add.f32 	%f880, %f819, %f564;
-	bra.uni 	BB2_21;
-
-BB2_1:
-	mov.f32 	%f881, %f315;
-	mov.f32 	%f882, %f314;
-
-BB2_21:
-	setp.eq.s32	%p20, %r8, 0;
-	// inline asm
-	call (%f565), _optix_get_world_ray_direction_x, ();
-	// inline asm
-	// inline asm
-	call (%f566), _optix_get_world_ray_direction_y, ();
-	// inline asm
-	// inline asm
-	call (%f931), _optix_get_world_ray_direction_z, ();
-	// inline asm
-	// inline asm
-	call (%f568), _optix_get_ray_time, ();
-	// inline asm
-	mov.u32 	%r314, 0;
-	@%p20 bra 	BB2_22;
-
-BB2_23:
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r149,%r150,%r151,%r152}, [%rd125];
+	// end inline asm
+	mov.b32 	%f485, %r149;
+	mov.b32 	%f486, %r150;
+	mov.b32 	%f487, %r151;
+	mov.b32 	%f488, %r152;
+	mul.f32 	%f489, %f90, %f485;
+	mul.f32 	%f490, %f90, %f486;
+	mul.f32 	%f491, %f90, %f487;
+	mul.f32 	%f492, %f90, %f488;
+	fma.rn.f32 	%f871, %f484, %f871, %f489;
+	fma.rn.f32 	%f870, %f484, %f870, %f490;
+	fma.rn.f32 	%f869, %f484, %f869, %f491;
+	fma.rn.f32 	%f868, %f484, %f868, %f492;
+	add.s64 	%rd129, %rd135, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd128, %rd129;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r153,%r154,%r155,%r156}, [%rd128];
+	// end inline asm
+	mov.b32 	%f493, %r153;
+	mov.b32 	%f494, %r154;
+	mov.b32 	%f495, %r155;
+	mov.b32 	%f496, %r156;
+	mul.f32 	%f497, %f90, %f493;
+	mul.f32 	%f498, %f90, %f494;
+	mul.f32 	%f499, %f90, %f495;
+	mul.f32 	%f500, %f90, %f496;
+	fma.rn.f32 	%f875, %f484, %f875, %f497;
+	fma.rn.f32 	%f874, %f484, %f874, %f498;
+	fma.rn.f32 	%f873, %f484, %f873, %f499;
+	fma.rn.f32 	%f872, %f484, %f872, %f500;
+	add.s64 	%rd132, %rd135, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd131, %rd132;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r157,%r158,%r159,%r160}, [%rd131];
+	// end inline asm
+	mov.b32 	%f501, %r157;
+	mov.b32 	%f502, %r158;
+	mov.b32 	%f503, %r159;
+	mov.b32 	%f504, %r160;
+	mul.f32 	%f505, %f90, %f501;
+	mul.f32 	%f506, %f90, %f502;
+	mul.f32 	%f507, %f90, %f503;
+	mul.f32 	%f508, %f90, %f504;
+	fma.rn.f32 	%f879, %f484, %f879, %f505;
+	fma.rn.f32 	%f878, %f484, %f878, %f506;
+	fma.rn.f32 	%f877, %f484, %f877, %f507;
+	fma.rn.f32 	%f876, %f484, %f876, %f508;
+	bra.uni 	$L__BB2_15;
+
+$L__BB2_4:
+	mov.f32 	%f880, 0f00000000;
+	mov.f32 	%f883, 0f3F800000;
+	setp.eq.s32 	%p4, %r13, 4;
+	@%p4 bra 	$L__BB2_7;
+
+	setp.ne.s32 	%p5, %r13, 1;
+	mov.f32 	%f881, %f880;
+	mov.f32 	%f882, %f880;
+	mov.f32 	%f884, %f880;
+	mov.f32 	%f885, %f880;
+	mov.f32 	%f886, %f883;
+	mov.f32 	%f887, %f880;
+	mov.f32 	%f888, %f880;
+	mov.f32 	%f889, %f883;
+	mov.f32 	%f890, %f880;
+	mov.f32 	%f891, %f880;
+	@%p5 bra 	$L__BB2_16;
+
+	// begin inline asm
+	call (%rd19), _optix_get_static_transform_from_handle, (%rd17);
+	// end inline asm
+	add.s64 	%rd256, %rd19, 64;
+	bra.uni 	$L__BB2_8;
+
+$L__BB2_10:
+	// begin inline asm
+	call (%rd32), _optix_get_srt_motion_transform_from_handle, (%rd17);
+	// end inline asm
+	// begin inline asm
+	cvta.to.global.u64 %rd34, %rd32;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r27,%r28,%r29,%r30}, [%rd34];
+	// end inline asm
+	add.s64 	%rd38, %rd32, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd37, %rd38;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r31,%r32,%r33,%r34}, [%rd37];
+	// end inline asm
+	add.s64 	%rd41, %rd32, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd40, %rd41;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r35,%r36,%r37,%r38}, [%rd40];
+	// end inline asm
+	add.s64 	%rd44, %rd32, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd43, %rd44;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r39,%r40,%r41,%r42}, [%rd43];
+	// end inline asm
+	add.s64 	%rd47, %rd32, 64;
+	// begin inline asm
+	cvta.to.global.u64 %rd46, %rd47;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r43,%r44,%r45,%r46}, [%rd46];
+	// end inline asm
+	add.s64 	%rd50, %rd32, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd49, %rd50;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r47,%r48,%r49,%r50}, [%rd49];
+	// end inline asm
+	add.s64 	%rd53, %rd32, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd52, %rd53;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r51,%r52,%r53,%r54}, [%rd52];
+	// end inline asm
+	add.s64 	%rd56, %rd32, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd55, %rd56;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r55,%r56,%r57,%r58}, [%rd55];
+	// end inline asm
+	add.s64 	%rd59, %rd32, 128;
+	// begin inline asm
+	cvta.to.global.u64 %rd58, %rd59;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r59,%r60,%r61,%r62}, [%rd58];
+	// end inline asm
+	add.s64 	%rd62, %rd32, 144;
+	// begin inline asm
+	cvta.to.global.u64 %rd61, %rd62;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r63,%r64,%r65,%r66}, [%rd61];
+	// end inline asm
+	mov.b32 	%f359, %r30;
+	mov.b32 	%f360, %r31;
+	and.b32  	%r83, %r29, 65535;
+	add.s32 	%r84, %r83, -1;
+	cvt.rn.f32.s32 	%f361, %r84;
+	sub.f32 	%f362, %f344, %f359;
+	mul.f32 	%f363, %f362, %f361;
+	sub.f32 	%f364, %f360, %f359;
+	div.rn.f32 	%f365, %f363, %f364;
+	min.f32 	%f366, %f361, %f365;
+	mov.f32 	%f367, 0f00000000;
+	max.f32 	%f368, %f367, %f366;
+	cvt.rmi.f32.f32 	%f369, %f368;
+	sub.f32 	%f29, %f368, %f369;
+	cvt.rzi.s32.f32 	%r85, %f369;
+	mul.wide.s32 	%rd76, %r85, 64;
+	add.s64 	%rd65, %rd41, %rd76;
+	// begin inline asm
+	cvta.to.global.u64 %rd64, %rd65;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r67,%r68,%r69,%r70}, [%rd64];
+	// end inline asm
+	mov.b32 	%f852, %r67;
+	mov.b32 	%f853, %r68;
+	mov.b32 	%f854, %r69;
+	mov.b32 	%f855, %r70;
+	add.s64 	%rd68, %rd65, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd67, %rd68;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r71,%r72,%r73,%r74}, [%rd67];
+	// end inline asm
+	mov.b32 	%f856, %r71;
+	mov.b32 	%f857, %r72;
+	mov.b32 	%f858, %r73;
+	mov.b32 	%f859, %r74;
+	add.s64 	%rd71, %rd65, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd70, %rd71;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r75,%r76,%r77,%r78}, [%rd70];
+	// end inline asm
+	mov.b32 	%f860, %r75;
+	mov.b32 	%f861, %r76;
+	mov.b32 	%f862, %r77;
+	mov.b32 	%f863, %r78;
+	add.s64 	%rd74, %rd65, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd73, %rd74;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r79,%r80,%r81,%r82}, [%rd73];
+	// end inline asm
+	mov.b32 	%f864, %r79;
+	mov.b32 	%f865, %r80;
+	mov.b32 	%f866, %r81;
+	mov.b32 	%f867, %r82;
+	setp.leu.f32 	%p7, %f29, 0f00000000;
+	@%p7 bra 	$L__BB2_12;
+
+	mov.f32 	%f370, 0f3F800000;
+	sub.f32 	%f371, %f370, %f29;
+	add.s64 	%rd78, %rd65, 64;
+	// begin inline asm
+	cvta.to.global.u64 %rd77, %rd78;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r86,%r87,%r88,%r89}, [%rd77];
+	// end inline asm
+	mov.b32 	%f372, %r86;
+	mov.b32 	%f373, %r87;
+	mov.b32 	%f374, %r88;
+	mov.b32 	%f375, %r89;
+	mul.f32 	%f376, %f29, %f372;
+	mul.f32 	%f377, %f29, %f373;
+	mul.f32 	%f378, %f29, %f374;
+	mul.f32 	%f379, %f29, %f375;
+	fma.rn.f32 	%f852, %f371, %f852, %f376;
+	fma.rn.f32 	%f853, %f371, %f853, %f377;
+	fma.rn.f32 	%f854, %f371, %f854, %f378;
+	fma.rn.f32 	%f855, %f371, %f855, %f379;
+	add.s64 	%rd81, %rd65, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd80, %rd81;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r90,%r91,%r92,%r93}, [%rd80];
+	// end inline asm
+	mov.b32 	%f380, %r90;
+	mov.b32 	%f381, %r91;
+	mov.b32 	%f382, %r92;
+	mov.b32 	%f383, %r93;
+	mul.f32 	%f384, %f29, %f380;
+	mul.f32 	%f385, %f29, %f381;
+	mul.f32 	%f386, %f29, %f382;
+	mul.f32 	%f387, %f29, %f383;
+	fma.rn.f32 	%f856, %f371, %f856, %f384;
+	fma.rn.f32 	%f857, %f371, %f857, %f385;
+	fma.rn.f32 	%f858, %f371, %f858, %f386;
+	fma.rn.f32 	%f859, %f371, %f859, %f387;
+	add.s64 	%rd84, %rd65, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd83, %rd84;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r94,%r95,%r96,%r97}, [%rd83];
+	// end inline asm
+	mov.b32 	%f388, %r94;
+	mov.b32 	%f389, %r95;
+	mov.b32 	%f390, %r96;
+	mov.b32 	%f391, %r97;
+	mul.f32 	%f392, %f29, %f388;
+	mul.f32 	%f393, %f29, %f389;
+	mul.f32 	%f394, %f29, %f390;
+	mul.f32 	%f395, %f29, %f391;
+	fma.rn.f32 	%f860, %f371, %f860, %f392;
+	fma.rn.f32 	%f396, %f371, %f861, %f393;
+	fma.rn.f32 	%f397, %f371, %f862, %f394;
+	fma.rn.f32 	%f398, %f371, %f863, %f395;
+	add.s64 	%rd87, %rd65, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd86, %rd87;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r98,%r99,%r100,%r101}, [%rd86];
+	// end inline asm
+	mov.b32 	%f399, %r98;
+	mov.b32 	%f400, %r99;
+	mov.b32 	%f401, %r100;
+	mov.b32 	%f402, %r101;
+	mul.f32 	%f403, %f29, %f399;
+	mul.f32 	%f404, %f29, %f400;
+	mul.f32 	%f405, %f29, %f401;
+	mul.f32 	%f406, %f29, %f402;
+	fma.rn.f32 	%f407, %f371, %f864, %f403;
+	fma.rn.f32 	%f865, %f371, %f865, %f404;
+	fma.rn.f32 	%f866, %f371, %f866, %f405;
+	fma.rn.f32 	%f867, %f371, %f867, %f406;
+	mul.f32 	%f408, %f397, %f397;
+	fma.rn.f32 	%f409, %f396, %f396, %f408;
+	fma.rn.f32 	%f410, %f398, %f398, %f409;
+	fma.rn.f32 	%f411, %f407, %f407, %f410;
+	sqrt.rn.f32 	%f412, %f411;
+	rcp.rn.f32 	%f413, %f412;
+	mul.f32 	%f861, %f396, %f413;
+	mul.f32 	%f862, %f397, %f413;
+	mul.f32 	%f863, %f398, %f413;
+	mul.f32 	%f864, %f413, %f407;
+
+$L__BB2_12:
+	mul.f32 	%f414, %f862, %f862;
+	fma.rn.f32 	%f415, %f861, %f861, %f414;
+	fma.rn.f32 	%f416, %f863, %f863, %f415;
+	fma.rn.f32 	%f417, %f864, %f864, %f416;
+	rcp.rn.f32 	%f418, %f417;
+	mul.f32 	%f419, %f861, %f418;
+	mul.f32 	%f420, %f862, %f418;
+	mul.f32 	%f421, %f863, %f418;
+	mul.f32 	%f422, %f864, %f418;
+	mul.f32 	%f423, %f861, %f419;
+	mul.f32 	%f424, %f862, %f420;
+	mul.f32 	%f425, %f863, %f421;
+	mul.f32 	%f426, %f861, %f420;
+	mul.f32 	%f427, %f863, %f422;
+	mul.f32 	%f428, %f861, %f421;
+	mul.f32 	%f429, %f862, %f422;
+	mul.f32 	%f430, %f862, %f421;
+	mul.f32 	%f431, %f861, %f422;
+	sub.f32 	%f432, %f423, %f424;
+	sub.f32 	%f433, %f432, %f425;
+	fma.rn.f32 	%f434, %f864, %f422, %f433;
+	sub.f32 	%f435, %f426, %f427;
+	add.f32 	%f436, %f435, %f435;
+	add.f32 	%f437, %f428, %f429;
+	add.f32 	%f438, %f437, %f437;
+	add.f32 	%f439, %f426, %f427;
+	add.f32 	%f440, %f439, %f439;
+	sub.f32 	%f441, %f424, %f423;
+	sub.f32 	%f442, %f441, %f425;
+	fma.rn.f32 	%f443, %f864, %f422, %f442;
+	sub.f32 	%f444, %f430, %f431;
+	add.f32 	%f445, %f444, %f444;
+	sub.f32 	%f446, %f428, %f429;
+	add.f32 	%f447, %f446, %f446;
+	add.f32 	%f448, %f430, %f431;
+	add.f32 	%f449, %f448, %f448;
+	neg.f32 	%f450, %f423;
+	sub.f32 	%f451, %f450, %f424;
+	add.f32 	%f452, %f425, %f451;
+	fma.rn.f32 	%f453, %f864, %f422, %f452;
+	mul.f32 	%f454, %f855, %f434;
+	fma.rn.f32 	%f455, %f858, %f436, %f454;
+	fma.rn.f32 	%f456, %f860, %f438, %f455;
+	sub.f32 	%f868, %f865, %f456;
+	mul.f32 	%f457, %f858, %f443;
+	fma.rn.f32 	%f458, %f855, %f440, %f457;
+	fma.rn.f32 	%f459, %f860, %f445, %f458;
+	sub.f32 	%f872, %f866, %f459;
+	mul.f32 	%f460, %f858, %f449;
+	fma.rn.f32 	%f461, %f855, %f447, %f460;
+	fma.rn.f32 	%f462, %f860, %f453, %f461;
+	sub.f32 	%f876, %f867, %f462;
+	mul.f32 	%f463, %f854, %f434;
+	fma.rn.f32 	%f464, %f857, %f436, %f463;
+	fma.rn.f32 	%f869, %f859, %f438, %f464;
+	mul.f32 	%f465, %f857, %f443;
+	fma.rn.f32 	%f466, %f854, %f440, %f465;
+	fma.rn.f32 	%f873, %f859, %f445, %f466;
+	mul.f32 	%f467, %f857, %f449;
+	fma.rn.f32 	%f468, %f854, %f447, %f467;
+	fma.rn.f32 	%f877, %f859, %f453, %f468;
+	mul.f32 	%f469, %f853, %f434;
+	fma.rn.f32 	%f870, %f856, %f436, %f469;
+	mul.f32 	%f470, %f856, %f443;
+	fma.rn.f32 	%f874, %f853, %f440, %f470;
+	mul.f32 	%f471, %f856, %f449;
+	fma.rn.f32 	%f878, %f853, %f447, %f471;
+	mul.f32 	%f871, %f852, %f434;
+	mul.f32 	%f875, %f852, %f440;
+	mul.f32 	%f879, %f852, %f447;
+
+$L__BB2_15:
+	mul.f32 	%f509, %f873, %f878;
+	mul.f32 	%f510, %f874, %f877;
+	sub.f32 	%f511, %f510, %f509;
+	mul.f32 	%f512, %f871, %f511;
+	mul.f32 	%f513, %f873, %f879;
+	mul.f32 	%f514, %f875, %f877;
+	sub.f32 	%f515, %f514, %f513;
+	mul.f32 	%f516, %f870, %f515;
+	sub.f32 	%f517, %f512, %f516;
+	mul.f32 	%f518, %f874, %f879;
+	mul.f32 	%f519, %f875, %f878;
+	sub.f32 	%f520, %f519, %f518;
+	fma.rn.f32 	%f521, %f869, %f520, %f517;
+	rcp.rn.f32 	%f522, %f521;
+	mul.f32 	%f883, %f511, %f522;
+	mul.f32 	%f523, %f870, %f877;
+	mul.f32 	%f524, %f869, %f878;
+	sub.f32 	%f525, %f524, %f523;
+	mul.f32 	%f882, %f525, %f522;
+	mul.f32 	%f526, %f869, %f874;
+	mul.f32 	%f527, %f870, %f873;
+	sub.f32 	%f528, %f527, %f526;
+	mul.f32 	%f881, %f528, %f522;
+	sub.f32 	%f529, %f513, %f514;
+	mul.f32 	%f887, %f529, %f522;
+	mul.f32 	%f530, %f869, %f879;
+	mul.f32 	%f531, %f871, %f877;
+	sub.f32 	%f532, %f531, %f530;
+	mul.f32 	%f886, %f532, %f522;
+	mul.f32 	%f533, %f871, %f873;
+	mul.f32 	%f534, %f869, %f875;
+	sub.f32 	%f535, %f534, %f533;
+	mul.f32 	%f885, %f535, %f522;
+	mul.f32 	%f891, %f520, %f522;
+	mul.f32 	%f536, %f871, %f878;
+	mul.f32 	%f537, %f870, %f879;
+	sub.f32 	%f538, %f537, %f536;
+	mul.f32 	%f890, %f538, %f522;
+	mul.f32 	%f539, %f870, %f875;
+	mul.f32 	%f540, %f871, %f874;
+	sub.f32 	%f541, %f540, %f539;
+	mul.f32 	%f889, %f541, %f522;
+	mul.f32 	%f542, %f868, %f883;
+	neg.f32 	%f543, %f542;
+	mul.f32 	%f544, %f872, %f882;
+	sub.f32 	%f545, %f543, %f544;
+	mul.f32 	%f546, %f876, %f881;
+	sub.f32 	%f880, %f545, %f546;
+	mul.f32 	%f547, %f868, %f887;
+	neg.f32 	%f548, %f547;
+	mul.f32 	%f549, %f872, %f886;
+	sub.f32 	%f550, %f548, %f549;
+	mul.f32 	%f551, %f876, %f885;
+	sub.f32 	%f884, %f550, %f551;
+	mul.f32 	%f552, %f868, %f891;
+	neg.f32 	%f553, %f552;
+	mul.f32 	%f554, %f872, %f890;
+	sub.f32 	%f555, %f553, %f554;
+	mul.f32 	%f556, %f876, %f889;
+	sub.f32 	%f888, %f555, %f556;
+	bra.uni 	$L__BB2_16;
+
+$L__BB2_7:
+	// begin inline asm
+	call (%rd256), _optix_get_instance_inverse_transform_from_handle, (%rd17);
+	// end inline asm
+
+$L__BB2_8:
+	// begin inline asm
+	cvta.to.global.u64 %rd23, %rd256;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r15,%r16,%r17,%r18}, [%rd23];
+	// end inline asm
+	mov.b32 	%f883, %r15;
+	mov.b32 	%f882, %r16;
+	mov.b32 	%f881, %r17;
+	mov.b32 	%f880, %r18;
+	add.s64 	%rd27, %rd256, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd26, %rd27;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r19,%r20,%r21,%r22}, [%rd26];
+	// end inline asm
+	mov.b32 	%f887, %r19;
+	mov.b32 	%f886, %r20;
+	mov.b32 	%f885, %r21;
+	mov.b32 	%f884, %r22;
+	add.s64 	%rd30, %rd256, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd29, %rd30;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r23,%r24,%r25,%r26}, [%rd29];
+	// end inline asm
+	mov.b32 	%f891, %r23;
+	mov.b32 	%f890, %r24;
+	mov.b32 	%f889, %r25;
+	mov.b32 	%f888, %r26;
+
+$L__BB2_16:
+	setp.eq.s32 	%p9, %r316, 0;
+	@%p9 bra 	$L__BB2_18;
+
+	mul.f32 	%f557, %f848, %f883;
+	fma.rn.f32 	%f558, %f844, %f882, %f557;
+	fma.rn.f32 	%f151, %f840, %f881, %f558;
+	mul.f32 	%f559, %f849, %f883;
+	fma.rn.f32 	%f560, %f845, %f882, %f559;
+	fma.rn.f32 	%f152, %f841, %f881, %f560;
+	mul.f32 	%f561, %f850, %f883;
+	fma.rn.f32 	%f562, %f846, %f882, %f561;
+	fma.rn.f32 	%f153, %f842, %f881, %f562;
+	mul.f32 	%f563, %f851, %f883;
+	fma.rn.f32 	%f564, %f847, %f882, %f563;
+	fma.rn.f32 	%f565, %f843, %f881, %f564;
+	add.f32 	%f880, %f880, %f565;
+	mul.f32 	%f566, %f848, %f887;
+	fma.rn.f32 	%f567, %f844, %f886, %f566;
+	fma.rn.f32 	%f155, %f840, %f885, %f567;
+	mul.f32 	%f568, %f849, %f887;
+	fma.rn.f32 	%f569, %f845, %f886, %f568;
+	fma.rn.f32 	%f156, %f841, %f885, %f569;
+	mul.f32 	%f570, %f850, %f887;
+	fma.rn.f32 	%f571, %f846, %f886, %f570;
+	fma.rn.f32 	%f157, %f842, %f885, %f571;
+	mul.f32 	%f572, %f851, %f887;
+	fma.rn.f32 	%f573, %f847, %f886, %f572;
+	fma.rn.f32 	%f574, %f843, %f885, %f573;
+	add.f32 	%f884, %f884, %f574;
+	mul.f32 	%f575, %f848, %f891;
+	fma.rn.f32 	%f576, %f844, %f890, %f575;
+	fma.rn.f32 	%f159, %f840, %f889, %f576;
+	mul.f32 	%f577, %f849, %f891;
+	fma.rn.f32 	%f578, %f845, %f890, %f577;
+	fma.rn.f32 	%f160, %f841, %f889, %f578;
+	mul.f32 	%f579, %f850, %f891;
+	fma.rn.f32 	%f580, %f846, %f890, %f579;
+	fma.rn.f32 	%f161, %f842, %f889, %f580;
+	mul.f32 	%f581, %f851, %f891;
+	fma.rn.f32 	%f582, %f847, %f890, %f581;
+	fma.rn.f32 	%f583, %f843, %f889, %f582;
+	add.f32 	%f888, %f888, %f583;
+	mov.f32 	%f881, %f153;
+	mov.f32 	%f882, %f152;
+	mov.f32 	%f883, %f151;
+	mov.f32 	%f885, %f157;
+	mov.f32 	%f886, %f156;
+	mov.f32 	%f887, %f155;
+	mov.f32 	%f889, %f161;
+	mov.f32 	%f890, %f160;
+	mov.f32 	%f891, %f159;
+
+$L__BB2_18:
+	add.s32 	%r316, %r316, 1;
+	setp.lt.u32 	%p10, %r316, %r10;
+	mov.f32 	%f840, %f891;
+	mov.f32 	%f841, %f890;
+	mov.f32 	%f842, %f889;
+	mov.f32 	%f843, %f888;
+	mov.f32 	%f844, %f887;
+	mov.f32 	%f845, %f886;
+	mov.f32 	%f846, %f885;
+	mov.f32 	%f847, %f884;
+	mov.f32 	%f848, %f883;
+	mov.f32 	%f849, %f882;
+	mov.f32 	%f850, %f881;
+	mov.f32 	%f851, %f880;
+	@%p10 bra 	$L__BB2_3;
+
+$L__BB2_19:
+	mul.f32 	%f584, %f916, %f883;
+	fma.rn.f32 	%f585, %f917, %f882, %f584;
+	fma.rn.f32 	%f586, %f918, %f881, %f585;
+	mul.f32 	%f587, %f916, %f887;
+	fma.rn.f32 	%f588, %f917, %f886, %f587;
+	fma.rn.f32 	%f589, %f918, %f885, %f588;
+	mul.f32 	%f590, %f916, %f891;
+	fma.rn.f32 	%f591, %f917, %f890, %f590;
+	fma.rn.f32 	%f592, %f918, %f889, %f591;
+	add.f32 	%f918, %f888, %f592;
+	add.f32 	%f917, %f884, %f589;
+	add.f32 	%f916, %f880, %f586;
+
+$L__BB2_21:
+	// begin inline asm
+	call (%f974), _optix_get_world_ray_direction_x, ();
+	// end inline asm
+	// begin inline asm
+	call (%f975), _optix_get_world_ray_direction_y, ();
+	// end inline asm
+	// begin inline asm
+	call (%f595), _optix_get_world_ray_direction_z, ();
+	// end inline asm
+	// begin inline asm
+	call (%r161), _optix_get_transform_list_size, ();
+	// end inline asm
+	setp.eq.s32 	%p11, %r161, 0;
+	@%p11 bra 	$L__BB2_41;
+
+	// begin inline asm
+	call (%r162), _optix_get_transform_list_size, ();
+	// end inline asm
+	// begin inline asm
+	call (%f596), _optix_get_ray_time, ();
+	// end inline asm
+	setp.eq.s32 	%p12, %r162, 0;
+	@%p12 bra 	$L__BB2_40;
+
+	mov.u32 	%r317, 0;
+
+$L__BB2_24:
 	.pragma "nounroll";
-	// inline asm
-	call (%rd140), _optix_get_transform_list_handle, (%r314);
-	// inline asm
-	// inline asm
-	call (%r161), _optix_get_transform_type_from_handle, (%rd140);
-	// inline asm
-	and.b32  	%r162, %r161, -2;
-	setp.eq.s32	%p11, %r162, 2;
-	@%p11 bra 	BB2_29;
-	bra.uni 	BB2_24;
-
-BB2_29:
-	setp.eq.s32	%p14, %r161, 2;
-	@%p14 bra 	BB2_33;
-	bra.uni 	BB2_30;
-
-BB2_33:
-	// inline asm
-	call (%rd214), _optix_get_matrix_motion_transform_from_handle, (%rd140);
-	// inline asm
-	// inline asm
-	cvta.to.global.u64 %rd216, %rd214;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r250,%r251,%r252,%r253}, [%rd216];
-	// inline asm
-	mov.b32	{%rs7, %rs8}, %r252;
-	add.s64 	%rd220, %rd214, 16;
-	// inline asm
+	// begin inline asm
+	call (%rd136), _optix_get_transform_list_handle, (%r317);
+	// end inline asm
+	// begin inline asm
+	call (%r165), _optix_get_transform_type_from_handle, (%rd136);
+	// end inline asm
+	or.b32  	%r166, %r165, 1;
+	setp.eq.s32 	%p13, %r166, 3;
+	@%p13 bra 	$L__BB2_30;
+	bra.uni 	$L__BB2_25;
+
+$L__BB2_30:
+	setp.eq.s32 	%p16, %r165, 2;
+	@%p16 bra 	$L__BB2_34;
+	bra.uni 	$L__BB2_31;
+
+$L__BB2_34:
+	// begin inline asm
+	call (%rd208), _optix_get_matrix_motion_transform_from_handle, (%rd136);
+	// end inline asm
+	// begin inline asm
+	cvta.to.global.u64 %rd210, %rd208;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r254,%r255,%r256,%r257}, [%rd210];
+	// end inline asm
+	add.s64 	%rd214, %rd208, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd213, %rd214;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r258,%r259,%r260,%r261}, [%rd213];
+	// end inline asm
+	add.s64 	%rd217, %rd208, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd216, %rd217;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r262,%r263,%r264,%r265}, [%rd216];
+	// end inline asm
+	add.s64 	%rd220, %rd208, 48;
+	// begin inline asm
 	cvta.to.global.u64 %rd219, %rd220;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r254,%r255,%r256,%r257}, [%rd219];
-	// inline asm
-	add.s64 	%rd223, %rd214, 32;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r266,%r267,%r268,%r269}, [%rd219];
+	// end inline asm
+	add.s64 	%rd223, %rd208, 64;
+	// begin inline asm
 	cvta.to.global.u64 %rd222, %rd223;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r258,%r259,%r260,%r261}, [%rd222];
-	// inline asm
-	add.s64 	%rd226, %rd214, 48;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r270,%r271,%r272,%r273}, [%rd222];
+	// end inline asm
+	add.s64 	%rd226, %rd208, 80;
+	// begin inline asm
 	cvta.to.global.u64 %rd225, %rd226;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r262,%r263,%r264,%r265}, [%rd225];
-	// inline asm
-	add.s64 	%rd229, %rd214, 64;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r274,%r275,%r276,%r277}, [%rd225];
+	// end inline asm
+	add.s64 	%rd229, %rd208, 96;
+	// begin inline asm
 	cvta.to.global.u64 %rd228, %rd229;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r266,%r267,%r268,%r269}, [%rd228];
-	// inline asm
-	add.s64 	%rd232, %rd214, 80;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r278,%r279,%r280,%r281}, [%rd228];
+	// end inline asm
+	add.s64 	%rd232, %rd208, 112;
+	// begin inline asm
 	cvta.to.global.u64 %rd231, %rd232;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r270,%r271,%r272,%r273}, [%rd231];
-	// inline asm
-	add.s64 	%rd235, %rd214, 96;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r282,%r283,%r284,%r285}, [%rd231];
+	// end inline asm
+	mov.b32 	%f700, %r257;
+	mov.b32 	%f701, %r258;
+	and.b32  	%r298, %r256, 65535;
+	add.s32 	%r299, %r298, -1;
+	cvt.rn.f32.s32 	%f702, %r299;
+	sub.f32 	%f703, %f596, %f700;
+	mul.f32 	%f704, %f703, %f702;
+	sub.f32 	%f705, %f701, %f700;
+	div.rn.f32 	%f706, %f704, %f705;
+	min.f32 	%f707, %f702, %f706;
+	mov.f32 	%f708, 0f00000000;
+	max.f32 	%f709, %f708, %f707;
+	cvt.rmi.f32.f32 	%f710, %f709;
+	sub.f32 	%f258, %f709, %f710;
+	cvt.rzi.s32.f32 	%r300, %f710;
+	cvt.s64.s32 	%rd15, %r300;
+	mul.wide.s32 	%rd243, %r300, 48;
+	add.s64 	%rd235, %rd217, %rd243;
+	// begin inline asm
 	cvta.to.global.u64 %rd234, %rd235;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r274,%r275,%r276,%r277}, [%rd234];
-	// inline asm
-	add.s64 	%rd238, %rd214, 112;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r286,%r287,%r288,%r289}, [%rd234];
+	// end inline asm
+	mov.b32 	%f944, %r286;
+	mov.b32 	%f945, %r287;
+	mov.b32 	%f946, %r288;
+	add.s64 	%rd238, %rd235, 16;
+	// begin inline asm
 	cvta.to.global.u64 %rd237, %rd238;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r278,%r279,%r280,%r281}, [%rd237];
-	// inline asm
-	mov.b32 	 %f671, %r253;
-	mov.b32 	 %f672, %r254;
-	cvt.u32.u16	%r294, %rs7;
-	add.s32 	%r295, %r294, -1;
-	cvt.rn.f32.s32	%f673, %r295;
-	sub.f32 	%f674, %f568, %f671;
-	mul.f32 	%f675, %f674, %f673;
-	sub.f32 	%f676, %f672, %f671;
-	div.rn.f32 	%f677, %f675, %f676;
-	min.f32 	%f678, %f673, %f677;
-	mov.f32 	%f679, 0f00000000;
-	max.f32 	%f680, %f679, %f678;
-	cvt.rmi.f32.f32	%f681, %f680;
-	cvt.rzi.s32.f32	%r296, %f681;
-	cvt.s64.s32	%rd17, %r296;
-	mul.wide.s32 	%rd249, %r296, 48;
-	add.s64 	%rd241, %rd223, %rd249;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r290,%r291,%r292,%r293}, [%rd237];
+	// end inline asm
+	mov.b32 	%f941, %r290;
+	mov.b32 	%f942, %r291;
+	mov.b32 	%f943, %r292;
+	add.s64 	%rd241, %rd235, 32;
+	// begin inline asm
 	cvta.to.global.u64 %rd240, %rd241;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r282,%r283,%r284,%r285}, [%rd240];
-	// inline asm
-	mov.b32 	 %f908, %r282;
-	mov.b32 	 %f909, %r283;
-	mov.b32 	 %f910, %r284;
-	add.s64 	%rd244, %rd241, 16;
-	// inline asm
-	cvta.to.global.u64 %rd243, %rd244;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r286,%r287,%r288,%r289}, [%rd243];
-	// inline asm
-	mov.b32 	 %f905, %r286;
-	mov.b32 	 %f906, %r287;
-	mov.b32 	 %f907, %r288;
-	add.s64 	%rd247, %rd241, 32;
-	// inline asm
-	cvta.to.global.u64 %rd246, %rd247;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r290,%r291,%r292,%r293}, [%rd246];
-	// inline asm
-	sub.f32 	%f249, %f680, %f681;
-	mov.b32 	 %f902, %r290;
-	mov.b32 	 %f903, %r291;
-	mov.b32 	 %f904, %r292;
-	setp.leu.f32	%p16, %f249, 0f00000000;
-	@%p16 bra 	BB2_35;
-
-	mul.lo.s64 	%rd259, %rd17, 48;
-	add.s64 	%rd260, %rd214, %rd259;
-	add.s64 	%rd251, %rd260, 80;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r294,%r295,%r296,%r297}, [%rd240];
+	// end inline asm
+	mov.b32 	%f938, %r294;
+	mov.b32 	%f939, %r295;
+	mov.b32 	%f940, %r296;
+	setp.leu.f32 	%p18, %f258, 0f00000000;
+	@%p18 bra 	$L__BB2_36;
+
+	mov.f32 	%f711, 0f3F800000;
+	sub.f32 	%f712, %f711, %f258;
+	mul.lo.s64 	%rd253, %rd15, 48;
+	add.s64 	%rd254, %rd208, %rd253;
+	add.s64 	%rd245, %rd254, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd244, %rd245;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r301,%r302,%r303,%r304}, [%rd244];
+	// end inline asm
+	mov.b32 	%f713, %r301;
+	mov.b32 	%f714, %r302;
+	mov.b32 	%f715, %r303;
+	mul.f32 	%f716, %f258, %f713;
+	mul.f32 	%f717, %f258, %f714;
+	mul.f32 	%f718, %f258, %f715;
+	fma.rn.f32 	%f944, %f712, %f944, %f716;
+	fma.rn.f32 	%f945, %f712, %f945, %f717;
+	fma.rn.f32 	%f946, %f712, %f946, %f718;
+	add.s64 	%rd248, %rd254, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd247, %rd248;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r305,%r306,%r307,%r308}, [%rd247];
+	// end inline asm
+	mov.b32 	%f719, %r305;
+	mov.b32 	%f720, %r306;
+	mov.b32 	%f721, %r307;
+	mul.f32 	%f722, %f258, %f719;
+	mul.f32 	%f723, %f258, %f720;
+	mul.f32 	%f724, %f258, %f721;
+	fma.rn.f32 	%f941, %f712, %f941, %f722;
+	fma.rn.f32 	%f942, %f712, %f942, %f723;
+	fma.rn.f32 	%f943, %f712, %f943, %f724;
+	add.s64 	%rd251, %rd254, 112;
+	// begin inline asm
 	cvta.to.global.u64 %rd250, %rd251;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r297,%r298,%r299,%r300}, [%rd250];
-	// inline asm
-	mov.b32 	 %f682, %r297;
-	mov.b32 	 %f683, %r298;
-	mov.b32 	 %f684, %r299;
-	add.s64 	%rd254, %rd260, 96;
-	// inline asm
-	cvta.to.global.u64 %rd253, %rd254;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r301,%r302,%r303,%r304}, [%rd253];
-	// inline asm
-	mov.b32 	 %f685, %r301;
-	mov.b32 	 %f686, %r302;
-	mov.b32 	 %f687, %r303;
-	add.s64 	%rd257, %rd260, 112;
-	// inline asm
-	cvta.to.global.u64 %rd256, %rd257;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r305,%r306,%r307,%r308}, [%rd256];
-	// inline asm
-	mov.f32 	%f688, 0f3F800000;
-	sub.f32 	%f689, %f688, %f249;
-	mul.f32 	%f690, %f249, %f682;
-	mul.f32 	%f691, %f249, %f683;
-	mul.f32 	%f692, %f249, %f684;
-	fma.rn.f32 	%f908, %f689, %f908, %f690;
-	fma.rn.f32 	%f909, %f689, %f909, %f691;
-	fma.rn.f32 	%f910, %f689, %f910, %f692;
-	mul.f32 	%f693, %f249, %f685;
-	mul.f32 	%f694, %f249, %f686;
-	mul.f32 	%f695, %f249, %f687;
-	fma.rn.f32 	%f905, %f689, %f905, %f693;
-	fma.rn.f32 	%f906, %f689, %f906, %f694;
-	fma.rn.f32 	%f907, %f689, %f907, %f695;
-	mov.b32 	 %f696, %r305;
-	mov.b32 	 %f697, %r306;
-	mov.b32 	 %f698, %r307;
-	mul.f32 	%f699, %f249, %f696;
-	mul.f32 	%f700, %f249, %f697;
-	mul.f32 	%f701, %f249, %f698;
-	fma.rn.f32 	%f902, %f689, %f902, %f699;
-	fma.rn.f32 	%f903, %f689, %f903, %f700;
-	fma.rn.f32 	%f904, %f689, %f904, %f701;
-	bra.uni 	BB2_35;
-
-BB2_24:
-	mov.f32 	%f911, 0f00000000;
-	mov.f32 	%f913, 0f3F800000;
-	setp.eq.s32	%p12, %r161, 4;
-	@%p12 bra 	BB2_27;
-	bra.uni 	BB2_25;
-
-BB2_27:
-	// inline asm
-	call (%rd264), _optix_get_instance_inverse_transform_from_handle, (%rd140);
-	// inline asm
-	bra.uni 	BB2_28;
-
-BB2_30:
-	// inline asm
-	call (%rd155), _optix_get_srt_motion_transform_from_handle, (%rd140);
-	// inline asm
-	// inline asm
-	cvta.to.global.u64 %rd157, %rd155;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r175,%r176,%r177,%r178}, [%rd157];
-	// inline asm
-	mov.b32	{%rs5, %rs6}, %r177;
-	add.s64 	%rd161, %rd155, 16;
-	// inline asm
-	cvta.to.global.u64 %rd160, %rd161;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r179,%r180,%r181,%r182}, [%rd160];
-	// inline asm
-	add.s64 	%rd164, %rd155, 32;
-	// inline asm
-	cvta.to.global.u64 %rd163, %rd164;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r183,%r184,%r185,%r186}, [%rd163];
-	// inline asm
-	add.s64 	%rd167, %rd155, 48;
-	// inline asm
-	cvta.to.global.u64 %rd166, %rd167;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r187,%r188,%r189,%r190}, [%rd166];
-	// inline asm
-	add.s64 	%rd170, %rd155, 64;
-	// inline asm
-	cvta.to.global.u64 %rd169, %rd170;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r191,%r192,%r193,%r194}, [%rd169];
-	// inline asm
-	add.s64 	%rd173, %rd155, 80;
-	// inline asm
-	cvta.to.global.u64 %rd172, %rd173;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r195,%r196,%r197,%r198}, [%rd172];
-	// inline asm
-	add.s64 	%rd176, %rd155, 96;
-	// inline asm
-	cvta.to.global.u64 %rd175, %rd176;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r199,%r200,%r201,%r202}, [%rd175];
-	// inline asm
-	add.s64 	%rd179, %rd155, 112;
-	// inline asm
-	cvta.to.global.u64 %rd178, %rd179;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r203,%r204,%r205,%r206}, [%rd178];
-	// inline asm
-	add.s64 	%rd182, %rd155, 128;
-	// inline asm
-	cvta.to.global.u64 %rd181, %rd182;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r207,%r208,%r209,%r210}, [%rd181];
-	// inline asm
-	add.s64 	%rd185, %rd155, 144;
-	// inline asm
-	cvta.to.global.u64 %rd184, %rd185;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r211,%r212,%r213,%r214}, [%rd184];
-	// inline asm
-	mov.b32 	 %f579, %r178;
-	mov.b32 	 %f580, %r179;
-	cvt.u32.u16	%r231, %rs5;
-	add.s32 	%r232, %r231, -1;
-	cvt.rn.f32.s32	%f581, %r232;
-	sub.f32 	%f582, %f568, %f579;
-	mul.f32 	%f583, %f582, %f581;
-	sub.f32 	%f584, %f580, %f579;
-	div.rn.f32 	%f585, %f583, %f584;
-	min.f32 	%f586, %f581, %f585;
-	mov.f32 	%f587, 0f00000000;
-	max.f32 	%f588, %f587, %f586;
-	cvt.rmi.f32.f32	%f589, %f588;
-	cvt.rzi.s32.f32	%r233, %f589;
-	cvt.s64.s32	%rd15, %r233;
-	mul.wide.s32 	%rd199, %r233, 64;
-	add.s64 	%rd188, %rd164, %rd199;
-	// inline asm
-	cvta.to.global.u64 %rd187, %rd188;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r215,%r216,%r217,%r218}, [%rd187];
-	// inline asm
-	mov.b32 	 %f892, %r215;
-	mov.b32 	 %f893, %r216;
-	mov.b32 	 %f894, %r217;
-	add.s64 	%rd191, %rd188, 16;
-	// inline asm
-	cvta.to.global.u64 %rd190, %rd191;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r219,%r220,%r221,%r222}, [%rd190];
-	// inline asm
-	mov.b32 	 %f895, %r219;
-	mov.b32 	 %f896, %r220;
-	mov.b32 	 %f897, %r222;
-	add.s64 	%rd194, %rd188, 32;
-	// inline asm
-	cvta.to.global.u64 %rd193, %rd194;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r223,%r224,%r225,%r226}, [%rd193];
-	// inline asm
-	sub.f32 	%f209, %f588, %f589;
-	mov.b32 	 %f898, %r224;
-	mov.b32 	 %f899, %r225;
-	mov.b32 	 %f900, %r226;
-	add.s64 	%rd197, %rd188, 48;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r309,%r310,%r311,%r312}, [%rd250];
+	// end inline asm
+	mov.b32 	%f725, %r309;
+	mov.b32 	%f726, %r310;
+	mov.b32 	%f727, %r311;
+	mul.f32 	%f728, %f258, %f725;
+	mul.f32 	%f729, %f258, %f726;
+	mul.f32 	%f730, %f258, %f727;
+	fma.rn.f32 	%f938, %f712, %f938, %f728;
+	fma.rn.f32 	%f939, %f712, %f939, %f729;
+	fma.rn.f32 	%f940, %f712, %f940, %f730;
+	bra.uni 	$L__BB2_36;
+
+$L__BB2_25:
+	mov.f32 	%f947, 0f00000000;
+	mov.f32 	%f949, 0f3F800000;
+	setp.eq.s32 	%p14, %r165, 4;
+	@%p14 bra 	$L__BB2_28;
+
+	setp.ne.s32 	%p15, %r165, 1;
+	mov.f32 	%f948, %f947;
+	mov.f32 	%f950, %f947;
+	mov.f32 	%f951, %f949;
+	mov.f32 	%f952, %f947;
+	mov.f32 	%f953, %f949;
+	mov.f32 	%f954, %f947;
+	mov.f32 	%f955, %f947;
+	@%p15 bra 	$L__BB2_37;
+
+	// begin inline asm
+	call (%rd138), _optix_get_static_transform_from_handle, (%rd136);
+	// end inline asm
+	add.s64 	%rd257, %rd138, 64;
+	bra.uni 	$L__BB2_29;
+
+$L__BB2_31:
+	// begin inline asm
+	call (%rd151), _optix_get_srt_motion_transform_from_handle, (%rd136);
+	// end inline asm
+	// begin inline asm
+	cvta.to.global.u64 %rd153, %rd151;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r179,%r180,%r181,%r182}, [%rd153];
+	// end inline asm
+	add.s64 	%rd157, %rd151, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd156, %rd157;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r183,%r184,%r185,%r186}, [%rd156];
+	// end inline asm
+	add.s64 	%rd160, %rd151, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd159, %rd160;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r187,%r188,%r189,%r190}, [%rd159];
+	// end inline asm
+	add.s64 	%rd163, %rd151, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd162, %rd163;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r191,%r192,%r193,%r194}, [%rd162];
+	// end inline asm
+	add.s64 	%rd166, %rd151, 64;
+	// begin inline asm
+	cvta.to.global.u64 %rd165, %rd166;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r195,%r196,%r197,%r198}, [%rd165];
+	// end inline asm
+	add.s64 	%rd169, %rd151, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd168, %rd169;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r199,%r200,%r201,%r202}, [%rd168];
+	// end inline asm
+	add.s64 	%rd172, %rd151, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd171, %rd172;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r203,%r204,%r205,%r206}, [%rd171];
+	// end inline asm
+	add.s64 	%rd175, %rd151, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd174, %rd175;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r207,%r208,%r209,%r210}, [%rd174];
+	// end inline asm
+	add.s64 	%rd178, %rd151, 128;
+	// begin inline asm
+	cvta.to.global.u64 %rd177, %rd178;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r211,%r212,%r213,%r214}, [%rd177];
+	// end inline asm
+	add.s64 	%rd181, %rd151, 144;
+	// begin inline asm
+	cvta.to.global.u64 %rd180, %rd181;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r215,%r216,%r217,%r218}, [%rd180];
+	// end inline asm
+	mov.b32 	%f608, %r182;
+	mov.b32 	%f609, %r183;
+	and.b32  	%r235, %r181, 65535;
+	add.s32 	%r236, %r235, -1;
+	cvt.rn.f32.s32 	%f610, %r236;
+	sub.f32 	%f611, %f596, %f608;
+	mul.f32 	%f612, %f611, %f610;
+	sub.f32 	%f613, %f609, %f608;
+	div.rn.f32 	%f614, %f612, %f613;
+	min.f32 	%f615, %f610, %f614;
+	mov.f32 	%f616, 0f00000000;
+	max.f32 	%f617, %f616, %f615;
+	cvt.rmi.f32.f32 	%f618, %f617;
+	sub.f32 	%f218, %f617, %f618;
+	cvt.rzi.s32.f32 	%r237, %f618;
+	mul.wide.s32 	%rd195, %r237, 64;
+	add.s64 	%rd184, %rd160, %rd195;
+	// begin inline asm
+	cvta.to.global.u64 %rd183, %rd184;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r219,%r220,%r221,%r222}, [%rd183];
+	// end inline asm
+	mov.b32 	%f928, %r219;
+	mov.b32 	%f929, %r220;
+	mov.b32 	%f930, %r221;
+	add.s64 	%rd187, %rd184, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd186, %rd187;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r223,%r224,%r225,%r226}, [%rd186];
+	// end inline asm
+	mov.b32 	%f931, %r223;
+	mov.b32 	%f932, %r224;
+	mov.b32 	%f933, %r226;
+	add.s64 	%rd190, %rd184, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd189, %rd190;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r227,%r228,%r229,%r230}, [%rd189];
+	// end inline asm
+	mov.b32 	%f934, %r228;
+	mov.b32 	%f935, %r229;
+	mov.b32 	%f936, %r230;
+	add.s64 	%rd193, %rd184, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd192, %rd193;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r231,%r232,%r233,%r234}, [%rd192];
+	// end inline asm
+	mov.b32 	%f937, %r231;
+	setp.leu.f32 	%p17, %f218, 0f00000000;
+	@%p17 bra 	$L__BB2_33;
+
+	mov.f32 	%f619, 0f3F800000;
+	sub.f32 	%f620, %f619, %f218;
+	add.s64 	%rd197, %rd184, 64;
+	// begin inline asm
 	cvta.to.global.u64 %rd196, %rd197;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r227,%r228,%r229,%r230}, [%rd196];
-	// inline asm
-	mov.b32 	 %f901, %r227;
-	setp.leu.f32	%p15, %f209, 0f00000000;
-	@%p15 bra 	BB2_32;
-
-	shl.b64 	%rd212, %rd15, 6;
-	add.s64 	%rd213, %rd212, %rd155;
-	add.s64 	%rd201, %rd213, 96;
-	// inline asm
-	cvta.to.global.u64 %rd200, %rd201;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r234,%r235,%r236,%r237}, [%rd200];
-	// inline asm
-	mov.b32 	 %f590, %r234;
-	mov.b32 	 %f591, %r235;
-	mov.b32 	 %f592, %r236;
-	add.s64 	%rd204, %rd213, 112;
-	// inline asm
-	cvta.to.global.u64 %rd203, %rd204;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r238,%r239,%r240,%r241}, [%rd203];
-	// inline asm
-	mov.b32 	 %f593, %r238;
-	mov.b32 	 %f594, %r239;
-	mov.b32 	 %f595, %r241;
-	add.s64 	%rd207, %rd213, 128;
-	// inline asm
-	cvta.to.global.u64 %rd206, %rd207;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r242,%r243,%r244,%r245}, [%rd206];
-	// inline asm
-	mov.b32 	 %f596, %r243;
-	mov.b32 	 %f597, %r244;
-	mov.b32 	 %f598, %r245;
-	add.s64 	%rd210, %rd213, 144;
-	// inline asm
-	cvta.to.global.u64 %rd209, %rd210;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r246,%r247,%r248,%r249}, [%rd209];
-	// inline asm
-	mov.f32 	%f599, 0f3F800000;
-	sub.f32 	%f600, %f599, %f209;
-	mul.f32 	%f601, %f209, %f590;
-	mul.f32 	%f602, %f209, %f591;
-	mul.f32 	%f603, %f209, %f592;
-	fma.rn.f32 	%f892, %f600, %f892, %f601;
-	fma.rn.f32 	%f893, %f600, %f893, %f602;
-	fma.rn.f32 	%f894, %f600, %f894, %f603;
-	mul.f32 	%f604, %f209, %f593;
-	mul.f32 	%f605, %f209, %f594;
-	mul.f32 	%f606, %f209, %f595;
-	fma.rn.f32 	%f895, %f600, %f895, %f604;
-	fma.rn.f32 	%f896, %f600, %f896, %f605;
-	fma.rn.f32 	%f897, %f600, %f897, %f606;
-	mul.f32 	%f607, %f209, %f596;
-	mul.f32 	%f608, %f209, %f597;
-	mul.f32 	%f609, %f209, %f598;
-	fma.rn.f32 	%f610, %f600, %f898, %f607;
-	fma.rn.f32 	%f611, %f600, %f899, %f608;
-	fma.rn.f32 	%f612, %f600, %f900, %f609;
-	mov.b32 	 %f613, %r246;
-	mul.f32 	%f614, %f209, %f613;
-	fma.rn.f32 	%f615, %f600, %f901, %f614;
-	mul.f32 	%f616, %f611, %f611;
-	fma.rn.f32 	%f617, %f610, %f610, %f616;
-	fma.rn.f32 	%f618, %f612, %f612, %f617;
-	fma.rn.f32 	%f619, %f615, %f615, %f618;
-	sqrt.rn.f32 	%f620, %f619;
-	rcp.rn.f32 	%f621, %f620;
-	mul.f32 	%f898, %f610, %f621;
-	mul.f32 	%f899, %f611, %f621;
-	mul.f32 	%f900, %f612, %f621;
-	mul.f32 	%f901, %f615, %f621;
-
-BB2_32:
-	mul.f32 	%f622, %f899, %f899;
-	fma.rn.f32 	%f623, %f898, %f898, %f622;
-	fma.rn.f32 	%f624, %f900, %f900, %f623;
-	fma.rn.f32 	%f625, %f901, %f901, %f624;
-	rcp.rn.f32 	%f626, %f625;
-	mul.f32 	%f627, %f898, %f626;
-	mul.f32 	%f628, %f899, %f626;
-	mul.f32 	%f629, %f900, %f626;
-	mul.f32 	%f630, %f901, %f626;
-	mul.f32 	%f631, %f898, %f627;
-	mul.f32 	%f632, %f899, %f628;
-	mul.f32 	%f633, %f900, %f629;
-	mul.f32 	%f634, %f898, %f628;
-	mul.f32 	%f635, %f900, %f630;
-	mul.f32 	%f636, %f898, %f629;
-	mul.f32 	%f637, %f899, %f630;
-	mul.f32 	%f638, %f899, %f629;
-	mul.f32 	%f639, %f898, %f630;
-	sub.f32 	%f640, %f631, %f632;
-	sub.f32 	%f641, %f640, %f633;
-	fma.rn.f32 	%f642, %f901, %f630, %f641;
-	sub.f32 	%f643, %f634, %f635;
-	add.f32 	%f644, %f643, %f643;
-	add.f32 	%f645, %f636, %f637;
-	add.f32 	%f646, %f645, %f645;
-	add.f32 	%f647, %f634, %f635;
-	add.f32 	%f648, %f647, %f647;
-	sub.f32 	%f649, %f632, %f631;
-	sub.f32 	%f650, %f649, %f633;
-	fma.rn.f32 	%f651, %f901, %f630, %f650;
-	sub.f32 	%f652, %f638, %f639;
-	add.f32 	%f653, %f652, %f652;
-	sub.f32 	%f654, %f636, %f637;
-	add.f32 	%f655, %f654, %f654;
-	add.f32 	%f656, %f638, %f639;
-	add.f32 	%f657, %f656, %f656;
-	neg.f32 	%f658, %f631;
-	sub.f32 	%f659, %f658, %f632;
-	add.f32 	%f660, %f633, %f659;
-	fma.rn.f32 	%f661, %f901, %f630, %f660;
-	mul.f32 	%f662, %f894, %f642;
-	fma.rn.f32 	%f663, %f896, %f644, %f662;
-	fma.rn.f32 	%f910, %f897, %f646, %f663;
-	mul.f32 	%f664, %f896, %f651;
-	fma.rn.f32 	%f665, %f894, %f648, %f664;
-	fma.rn.f32 	%f907, %f897, %f653, %f665;
-	mul.f32 	%f666, %f896, %f657;
-	fma.rn.f32 	%f667, %f894, %f655, %f666;
-	fma.rn.f32 	%f904, %f897, %f661, %f667;
-	mul.f32 	%f668, %f893, %f642;
-	fma.rn.f32 	%f909, %f895, %f644, %f668;
-	mul.f32 	%f669, %f895, %f651;
-	fma.rn.f32 	%f906, %f893, %f648, %f669;
-	mul.f32 	%f670, %f895, %f657;
-	fma.rn.f32 	%f903, %f893, %f655, %f670;
-	mul.f32 	%f908, %f892, %f642;
-	mul.f32 	%f905, %f892, %f648;
-	mul.f32 	%f902, %f892, %f655;
-
-BB2_35:
-	mul.f32 	%f702, %f903, %f907;
-	mul.f32 	%f703, %f904, %f906;
-	sub.f32 	%f704, %f703, %f702;
-	mul.f32 	%f705, %f908, %f704;
-	mul.f32 	%f706, %f902, %f907;
-	mul.f32 	%f707, %f904, %f905;
-	sub.f32 	%f708, %f707, %f706;
-	mul.f32 	%f709, %f708, %f909;
-	sub.f32 	%f710, %f705, %f709;
-	mul.f32 	%f711, %f902, %f906;
-	mul.f32 	%f712, %f903, %f905;
-	sub.f32 	%f713, %f712, %f711;
-	fma.rn.f32 	%f714, %f713, %f910, %f710;
-	rcp.rn.f32 	%f715, %f714;
-	mul.f32 	%f917, %f704, %f715;
-	mul.f32 	%f716, %f904, %f909;
-	mul.f32 	%f717, %f903, %f910;
-	sub.f32 	%f718, %f717, %f716;
-	mul.f32 	%f918, %f715, %f718;
-	mul.f32 	%f719, %f906, %f910;
-	mul.f32 	%f720, %f907, %f909;
-	sub.f32 	%f721, %f720, %f719;
-	mul.f32 	%f919, %f715, %f721;
-	sub.f32 	%f722, %f706, %f707;
-	mul.f32 	%f914, %f722, %f715;
-	mul.f32 	%f723, %f902, %f910;
-	mul.f32 	%f724, %f904, %f908;
-	sub.f32 	%f725, %f724, %f723;
-	mul.f32 	%f915, %f715, %f725;
-	mul.f32 	%f726, %f907, %f908;
-	mul.f32 	%f727, %f905, %f910;
-	sub.f32 	%f728, %f727, %f726;
-	mul.f32 	%f916, %f715, %f728;
-	mul.f32 	%f911, %f713, %f715;
-	mul.f32 	%f729, %f903, %f908;
-	mul.f32 	%f730, %f902, %f909;
-	sub.f32 	%f731, %f730, %f729;
-	mul.f32 	%f912, %f731, %f715;
-	mul.f32 	%f732, %f905, %f909;
-	mul.f32 	%f733, %f906, %f908;
-	sub.f32 	%f734, %f733, %f732;
-	mul.f32 	%f913, %f734, %f715;
-	bra.uni 	BB2_36;
-
-BB2_25:
-	setp.ne.s32	%p13, %r161, 1;
-	mov.f32 	%f912, %f911;
-	mov.f32 	%f914, %f911;
-	mov.f32 	%f915, %f913;
-	mov.f32 	%f916, %f911;
-	mov.f32 	%f917, %f913;
-	mov.f32 	%f918, %f911;
-	mov.f32 	%f919, %f911;
-	@%p13 bra 	BB2_36;
-
-	// inline asm
-	call (%rd142), _optix_get_static_transform_from_handle, (%rd140);
-	// inline asm
-	add.s64 	%rd264, %rd142, 64;
-
-BB2_28:
-	// inline asm
-	cvta.to.global.u64 %rd146, %rd264;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r163,%r164,%r165,%r166}, [%rd146];
-	// inline asm
-	mov.b32 	 %f917, %r163;
-	mov.b32 	 %f918, %r164;
-	mov.b32 	 %f919, %r165;
-	add.s64 	%rd150, %rd264, 16;
-	// inline asm
-	cvta.to.global.u64 %rd149, %rd150;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r167,%r168,%r169,%r170}, [%rd149];
-	// inline asm
-	mov.b32 	 %f914, %r167;
-	mov.b32 	 %f915, %r168;
-	mov.b32 	 %f916, %r169;
-	add.s64 	%rd153, %rd264, 32;
-	// inline asm
-	cvta.to.global.u64 %rd152, %rd153;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r171,%r172,%r173,%r174}, [%rd152];
-	// inline asm
-	mov.b32 	 %f911, %r171;
-	mov.b32 	 %f912, %r172;
-	mov.b32 	 %f913, %r173;
-
-BB2_36:
-	setp.eq.s32	%p17, %r314, 0;
-	@%p17 bra 	BB2_37;
-	bra.uni 	BB2_38;
-
-BB2_37:
-	mov.f32 	%f891, %f911;
-	mov.f32 	%f890, %f912;
-	mov.f32 	%f889, %f913;
-	mov.f32 	%f888, %f914;
-	mov.f32 	%f887, %f915;
-	mov.f32 	%f886, %f916;
-	mov.f32 	%f885, %f917;
-	mov.f32 	%f884, %f918;
-	mov.f32 	%f883, %f919;
-	bra.uni 	BB2_39;
-
-BB2_38:
-	mul.f32 	%f735, %f888, %f918;
-	fma.rn.f32 	%f736, %f885, %f917, %f735;
-	fma.rn.f32 	%f289, %f891, %f919, %f736;
-	mul.f32 	%f737, %f887, %f918;
-	fma.rn.f32 	%f738, %f884, %f917, %f737;
-	fma.rn.f32 	%f290, %f890, %f919, %f738;
-	mul.f32 	%f739, %f886, %f918;
-	fma.rn.f32 	%f740, %f883, %f917, %f739;
-	fma.rn.f32 	%f291, %f889, %f919, %f740;
-	mul.f32 	%f741, %f888, %f915;
-	fma.rn.f32 	%f742, %f885, %f914, %f741;
-	fma.rn.f32 	%f292, %f891, %f916, %f742;
-	mul.f32 	%f743, %f887, %f915;
-	fma.rn.f32 	%f744, %f884, %f914, %f743;
-	fma.rn.f32 	%f293, %f890, %f916, %f744;
-	mul.f32 	%f745, %f886, %f915;
-	fma.rn.f32 	%f746, %f883, %f914, %f745;
-	fma.rn.f32 	%f294, %f889, %f916, %f746;
-	mul.f32 	%f747, %f888, %f912;
-	fma.rn.f32 	%f748, %f885, %f911, %f747;
-	fma.rn.f32 	%f891, %f891, %f913, %f748;
-	mul.f32 	%f749, %f887, %f912;
-	fma.rn.f32 	%f750, %f884, %f911, %f749;
-	fma.rn.f32 	%f890, %f890, %f913, %f750;
-	mul.f32 	%f751, %f886, %f912;
-	fma.rn.f32 	%f752, %f883, %f911, %f751;
-	fma.rn.f32 	%f889, %f889, %f913, %f752;
-	mov.f32 	%f888, %f292;
-	mov.f32 	%f887, %f293;
-	mov.f32 	%f886, %f294;
-	mov.f32 	%f885, %f289;
-	mov.f32 	%f884, %f290;
-	mov.f32 	%f883, %f291;
-
-BB2_39:
-	add.s32 	%r314, %r314, 1;
-	setp.lt.u32	%p18, %r314, %r8;
-	@%p18 bra 	BB2_23;
-
-	mul.f32 	%f753, %f566, %f884;
-	fma.rn.f32 	%f754, %f565, %f885, %f753;
-	fma.rn.f32 	%f929, %f931, %f883, %f754;
-	mul.f32 	%f755, %f566, %f887;
-	fma.rn.f32 	%f756, %f565, %f888, %f755;
-	fma.rn.f32 	%f930, %f931, %f886, %f756;
-	mul.f32 	%f757, %f566, %f890;
-	fma.rn.f32 	%f758, %f565, %f891, %f757;
-	fma.rn.f32 	%f931, %f931, %f889, %f758;
-	bra.uni 	BB2_41;
-
-BB2_22:
-	mov.f32 	%f929, %f565;
-	mov.f32 	%f930, %f566;
-
-BB2_41:
-	ld.v4.f32 	{%f761, %f762, %f763, %f764}, [%rd1+208];
-	ld.v4.f32 	{%f768, %f769, %f770, %f771}, [%rd1+160];
-	fma.rn.f32 	%f773, %f882, %f768, %f761;
-	fma.rn.f32 	%f775, %f882, %f769, %f762;
-	fma.rn.f32 	%f777, %f882, %f770, %f763;
-	ld.v4.f32 	{%f778, %f779, %f780, %f781}, [%rd1+176];
-	fma.rn.f32 	%f783, %f881, %f778, %f773;
-	fma.rn.f32 	%f785, %f881, %f779, %f775;
-	fma.rn.f32 	%f787, %f881, %f780, %f777;
-	ld.v4.f32 	{%f788, %f789, %f790, %f791}, [%rd1+192];
-	fma.rn.f32 	%f793, %f880, %f788, %f783;
-	fma.rn.f32 	%f795, %f880, %f789, %f785;
-	fma.rn.f32 	%f797, %f880, %f790, %f787;
-	mul.f32 	%f798, %f929, %f768;
-	mul.f32 	%f799, %f929, %f769;
-	mul.f32 	%f800, %f929, %f770;
-	fma.rn.f32 	%f801, %f930, %f778, %f798;
-	fma.rn.f32 	%f802, %f930, %f779, %f799;
-	fma.rn.f32 	%f803, %f930, %f780, %f800;
-	fma.rn.f32 	%f804, %f931, %f788, %f801;
-	fma.rn.f32 	%f805, %f931, %f789, %f802;
-	fma.rn.f32 	%f806, %f931, %f790, %f803;
-	rcp.rn.f32 	%f807, %f806;
-	mul.f32 	%f808, %f797, %f807;
-	neg.f32 	%f313, %f808;
-	fma.rn.f32 	%f809, %f313, %f804, %f793;
-	fma.rn.f32 	%f810, %f313, %f805, %f795;
-	mul.f32 	%f811, %f810, %f810;
-	fma.rn.f32 	%f812, %f809, %f809, %f811;
-	setp.gtu.f32	%p19, %f812, 0f3F800000;
-	@%p19 bra 	BB2_43;
-
-	mov.u32 	%r310, 254;
-	// inline asm
-	call (%r309), _optix_report_intersection_0, (%f313, %r310);
-	// inline asm
-
-BB2_43:
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r238,%r239,%r240,%r241}, [%rd196];
+	// end inline asm
+	mov.b32 	%f621, %r238;
+	mov.b32 	%f622, %r239;
+	mov.b32 	%f623, %r240;
+	mul.f32 	%f624, %f218, %f621;
+	mul.f32 	%f625, %f218, %f622;
+	mul.f32 	%f626, %f218, %f623;
+	fma.rn.f32 	%f928, %f620, %f928, %f624;
+	fma.rn.f32 	%f929, %f620, %f929, %f625;
+	fma.rn.f32 	%f930, %f620, %f930, %f626;
+	add.s64 	%rd200, %rd184, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd199, %rd200;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r242,%r243,%r244,%r245}, [%rd199];
+	// end inline asm
+	mov.b32 	%f627, %r242;
+	mov.b32 	%f628, %r243;
+	mov.b32 	%f629, %r245;
+	mul.f32 	%f630, %f218, %f627;
+	mul.f32 	%f631, %f218, %f628;
+	mul.f32 	%f632, %f218, %f629;
+	fma.rn.f32 	%f931, %f620, %f931, %f630;
+	fma.rn.f32 	%f932, %f620, %f932, %f631;
+	fma.rn.f32 	%f933, %f620, %f933, %f632;
+	add.s64 	%rd203, %rd184, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd202, %rd203;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r246,%r247,%r248,%r249}, [%rd202];
+	// end inline asm
+	mov.b32 	%f633, %r247;
+	mov.b32 	%f634, %r248;
+	mov.b32 	%f635, %r249;
+	mul.f32 	%f636, %f218, %f633;
+	mul.f32 	%f637, %f218, %f634;
+	mul.f32 	%f638, %f218, %f635;
+	fma.rn.f32 	%f639, %f620, %f934, %f636;
+	fma.rn.f32 	%f640, %f620, %f935, %f637;
+	fma.rn.f32 	%f641, %f620, %f936, %f638;
+	add.s64 	%rd206, %rd184, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd205, %rd206;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r250,%r251,%r252,%r253}, [%rd205];
+	// end inline asm
+	mov.b32 	%f642, %r250;
+	mul.f32 	%f643, %f218, %f642;
+	fma.rn.f32 	%f644, %f620, %f937, %f643;
+	mul.f32 	%f645, %f640, %f640;
+	fma.rn.f32 	%f646, %f639, %f639, %f645;
+	fma.rn.f32 	%f647, %f641, %f641, %f646;
+	fma.rn.f32 	%f648, %f644, %f644, %f647;
+	sqrt.rn.f32 	%f649, %f648;
+	rcp.rn.f32 	%f650, %f649;
+	mul.f32 	%f934, %f639, %f650;
+	mul.f32 	%f935, %f640, %f650;
+	mul.f32 	%f936, %f641, %f650;
+	mul.f32 	%f937, %f650, %f644;
+
+$L__BB2_33:
+	mul.f32 	%f651, %f935, %f935;
+	fma.rn.f32 	%f652, %f934, %f934, %f651;
+	fma.rn.f32 	%f653, %f936, %f936, %f652;
+	fma.rn.f32 	%f654, %f937, %f937, %f653;
+	rcp.rn.f32 	%f655, %f654;
+	mul.f32 	%f656, %f934, %f655;
+	mul.f32 	%f657, %f935, %f655;
+	mul.f32 	%f658, %f936, %f655;
+	mul.f32 	%f659, %f937, %f655;
+	mul.f32 	%f660, %f934, %f656;
+	mul.f32 	%f661, %f935, %f657;
+	mul.f32 	%f662, %f936, %f658;
+	mul.f32 	%f663, %f934, %f657;
+	mul.f32 	%f664, %f936, %f659;
+	mul.f32 	%f665, %f934, %f658;
+	mul.f32 	%f666, %f935, %f659;
+	mul.f32 	%f667, %f935, %f658;
+	mul.f32 	%f668, %f934, %f659;
+	sub.f32 	%f669, %f660, %f661;
+	sub.f32 	%f670, %f669, %f662;
+	fma.rn.f32 	%f671, %f937, %f659, %f670;
+	sub.f32 	%f672, %f663, %f664;
+	add.f32 	%f673, %f672, %f672;
+	add.f32 	%f674, %f665, %f666;
+	add.f32 	%f675, %f674, %f674;
+	add.f32 	%f676, %f663, %f664;
+	add.f32 	%f677, %f676, %f676;
+	sub.f32 	%f678, %f661, %f660;
+	sub.f32 	%f679, %f678, %f662;
+	fma.rn.f32 	%f680, %f937, %f659, %f679;
+	sub.f32 	%f681, %f667, %f668;
+	add.f32 	%f682, %f681, %f681;
+	sub.f32 	%f683, %f665, %f666;
+	add.f32 	%f684, %f683, %f683;
+	add.f32 	%f685, %f667, %f668;
+	add.f32 	%f686, %f685, %f685;
+	neg.f32 	%f687, %f660;
+	sub.f32 	%f688, %f687, %f661;
+	add.f32 	%f689, %f662, %f688;
+	fma.rn.f32 	%f690, %f937, %f659, %f689;
+	mul.f32 	%f691, %f930, %f671;
+	fma.rn.f32 	%f692, %f932, %f673, %f691;
+	fma.rn.f32 	%f946, %f933, %f675, %f692;
+	mul.f32 	%f693, %f932, %f680;
+	fma.rn.f32 	%f694, %f930, %f677, %f693;
+	fma.rn.f32 	%f943, %f933, %f682, %f694;
+	mul.f32 	%f695, %f932, %f686;
+	fma.rn.f32 	%f696, %f930, %f684, %f695;
+	fma.rn.f32 	%f940, %f933, %f690, %f696;
+	mul.f32 	%f697, %f929, %f671;
+	fma.rn.f32 	%f945, %f931, %f673, %f697;
+	mul.f32 	%f698, %f931, %f680;
+	fma.rn.f32 	%f942, %f929, %f677, %f698;
+	mul.f32 	%f699, %f931, %f686;
+	fma.rn.f32 	%f939, %f929, %f684, %f699;
+	mul.f32 	%f944, %f928, %f671;
+	mul.f32 	%f941, %f928, %f677;
+	mul.f32 	%f938, %f928, %f684;
+
+$L__BB2_36:
+	mul.f32 	%f731, %f939, %f943;
+	mul.f32 	%f732, %f940, %f942;
+	sub.f32 	%f733, %f732, %f731;
+	mul.f32 	%f734, %f944, %f733;
+	mul.f32 	%f735, %f938, %f943;
+	mul.f32 	%f736, %f940, %f941;
+	sub.f32 	%f737, %f736, %f735;
+	mul.f32 	%f738, %f737, %f945;
+	sub.f32 	%f739, %f734, %f738;
+	mul.f32 	%f740, %f938, %f942;
+	mul.f32 	%f741, %f939, %f941;
+	sub.f32 	%f742, %f741, %f740;
+	fma.rn.f32 	%f743, %f742, %f946, %f739;
+	rcp.rn.f32 	%f744, %f743;
+	mul.f32 	%f953, %f733, %f744;
+	mul.f32 	%f745, %f940, %f945;
+	mul.f32 	%f746, %f939, %f946;
+	sub.f32 	%f747, %f746, %f745;
+	mul.f32 	%f954, %f747, %f744;
+	mul.f32 	%f748, %f942, %f946;
+	mul.f32 	%f749, %f943, %f945;
+	sub.f32 	%f750, %f749, %f748;
+	mul.f32 	%f955, %f750, %f744;
+	sub.f32 	%f751, %f735, %f736;
+	mul.f32 	%f950, %f751, %f744;
+	mul.f32 	%f752, %f938, %f946;
+	mul.f32 	%f753, %f940, %f944;
+	sub.f32 	%f754, %f753, %f752;
+	mul.f32 	%f951, %f754, %f744;
+	mul.f32 	%f755, %f943, %f944;
+	mul.f32 	%f756, %f941, %f946;
+	sub.f32 	%f757, %f756, %f755;
+	mul.f32 	%f952, %f757, %f744;
+	mul.f32 	%f947, %f742, %f744;
+	mul.f32 	%f758, %f939, %f944;
+	mul.f32 	%f759, %f938, %f945;
+	sub.f32 	%f760, %f759, %f758;
+	mul.f32 	%f948, %f760, %f744;
+	mul.f32 	%f761, %f941, %f945;
+	mul.f32 	%f762, %f942, %f944;
+	sub.f32 	%f763, %f762, %f761;
+	mul.f32 	%f949, %f763, %f744;
+	bra.uni 	$L__BB2_37;
+
+$L__BB2_28:
+	// begin inline asm
+	call (%rd257), _optix_get_instance_inverse_transform_from_handle, (%rd136);
+	// end inline asm
+
+$L__BB2_29:
+	// begin inline asm
+	cvta.to.global.u64 %rd142, %rd257;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r167,%r168,%r169,%r170}, [%rd142];
+	// end inline asm
+	mov.b32 	%f953, %r167;
+	mov.b32 	%f954, %r168;
+	mov.b32 	%f955, %r169;
+	add.s64 	%rd146, %rd257, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd145, %rd146;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r171,%r172,%r173,%r174}, [%rd145];
+	// end inline asm
+	mov.b32 	%f950, %r171;
+	mov.b32 	%f951, %r172;
+	mov.b32 	%f952, %r173;
+	add.s64 	%rd149, %rd257, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd148, %rd149;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r175,%r176,%r177,%r178}, [%rd148];
+	// end inline asm
+	mov.b32 	%f947, %r175;
+	mov.b32 	%f948, %r176;
+	mov.b32 	%f949, %r177;
+
+$L__BB2_37:
+	setp.eq.s32 	%p19, %r317, 0;
+	@%p19 bra 	$L__BB2_39;
+
+	mul.f32 	%f764, %f924, %f954;
+	fma.rn.f32 	%f765, %f921, %f953, %f764;
+	fma.rn.f32 	%f304, %f927, %f955, %f765;
+	mul.f32 	%f766, %f923, %f954;
+	fma.rn.f32 	%f767, %f920, %f953, %f766;
+	fma.rn.f32 	%f305, %f926, %f955, %f767;
+	mul.f32 	%f768, %f922, %f954;
+	fma.rn.f32 	%f769, %f919, %f953, %f768;
+	fma.rn.f32 	%f955, %f925, %f955, %f769;
+	mul.f32 	%f770, %f924, %f951;
+	fma.rn.f32 	%f771, %f921, %f950, %f770;
+	fma.rn.f32 	%f307, %f927, %f952, %f771;
+	mul.f32 	%f772, %f923, %f951;
+	fma.rn.f32 	%f773, %f920, %f950, %f772;
+	fma.rn.f32 	%f308, %f926, %f952, %f773;
+	mul.f32 	%f774, %f922, %f951;
+	fma.rn.f32 	%f775, %f919, %f950, %f774;
+	fma.rn.f32 	%f952, %f925, %f952, %f775;
+	mul.f32 	%f776, %f924, %f948;
+	fma.rn.f32 	%f777, %f921, %f947, %f776;
+	fma.rn.f32 	%f310, %f927, %f949, %f777;
+	mul.f32 	%f778, %f923, %f948;
+	fma.rn.f32 	%f779, %f920, %f947, %f778;
+	fma.rn.f32 	%f311, %f926, %f949, %f779;
+	mul.f32 	%f780, %f922, %f948;
+	fma.rn.f32 	%f781, %f919, %f947, %f780;
+	fma.rn.f32 	%f949, %f925, %f949, %f781;
+	mov.f32 	%f947, %f310;
+	mov.f32 	%f948, %f311;
+	mov.f32 	%f950, %f307;
+	mov.f32 	%f951, %f308;
+	mov.f32 	%f953, %f304;
+	mov.f32 	%f954, %f305;
+
+$L__BB2_39:
+	add.s32 	%r317, %r317, 1;
+	setp.lt.u32 	%p20, %r317, %r162;
+	mov.f32 	%f919, %f955;
+	mov.f32 	%f920, %f954;
+	mov.f32 	%f921, %f953;
+	mov.f32 	%f922, %f952;
+	mov.f32 	%f923, %f951;
+	mov.f32 	%f924, %f950;
+	mov.f32 	%f925, %f949;
+	mov.f32 	%f926, %f948;
+	mov.f32 	%f927, %f947;
+	@%p20 bra 	$L__BB2_24;
+
+$L__BB2_40:
+	mul.f32 	%f782, %f975, %f954;
+	fma.rn.f32 	%f783, %f974, %f953, %f782;
+	mul.f32 	%f784, %f975, %f951;
+	fma.rn.f32 	%f785, %f974, %f950, %f784;
+	mul.f32 	%f786, %f975, %f948;
+	fma.rn.f32 	%f787, %f974, %f947, %f786;
+	fma.rn.f32 	%f976, %f595, %f949, %f787;
+	fma.rn.f32 	%f975, %f595, %f952, %f785;
+	fma.rn.f32 	%f974, %f595, %f955, %f783;
+	bra.uni 	$L__BB2_42;
+
+$L__BB2_41:
+	mov.f32 	%f976, %f595;
+
+$L__BB2_42:
+	ld.v4.f32 	{%f791, %f792, %f793, %f794}, [%rd1+208];
+	ld.f32 	%f798, [%rd1+160];
+	fma.rn.f32 	%f799, %f916, %f798, %f791;
+	ld.f32 	%f800, [%rd1+164];
+	fma.rn.f32 	%f801, %f916, %f800, %f792;
+	ld.f32 	%f802, [%rd1+168];
+	fma.rn.f32 	%f803, %f916, %f802, %f793;
+	ld.f32 	%f804, [%rd1+176];
+	fma.rn.f32 	%f805, %f917, %f804, %f799;
+	ld.f32 	%f806, [%rd1+180];
+	fma.rn.f32 	%f807, %f917, %f806, %f801;
+	ld.f32 	%f808, [%rd1+184];
+	fma.rn.f32 	%f809, %f917, %f808, %f803;
+	ld.f32 	%f810, [%rd1+192];
+	fma.rn.f32 	%f811, %f918, %f810, %f805;
+	ld.f32 	%f812, [%rd1+196];
+	fma.rn.f32 	%f813, %f918, %f812, %f807;
+	ld.f32 	%f814, [%rd1+200];
+	fma.rn.f32 	%f815, %f918, %f814, %f809;
+	ld.v4.f32 	{%f816, %f817, %f818, %f819}, [%rd1+160];
+	mul.f32 	%f823, %f974, %f816;
+	mul.f32 	%f824, %f974, %f817;
+	mul.f32 	%f825, %f974, %f818;
+	fma.rn.f32 	%f826, %f975, %f804, %f823;
+	fma.rn.f32 	%f827, %f975, %f806, %f824;
+	fma.rn.f32 	%f828, %f975, %f808, %f825;
+	fma.rn.f32 	%f829, %f976, %f810, %f826;
+	fma.rn.f32 	%f830, %f976, %f812, %f827;
+	fma.rn.f32 	%f831, %f976, %f814, %f828;
+	rcp.rn.f32 	%f832, %f831;
+	mul.f32 	%f833, %f815, %f832;
+	neg.f32 	%f340, %f833;
+	fma.rn.f32 	%f834, %f340, %f829, %f811;
+	fma.rn.f32 	%f835, %f340, %f830, %f813;
+	mul.f32 	%f836, %f835, %f835;
+	fma.rn.f32 	%f837, %f834, %f834, %f836;
+	setp.gtu.f32 	%p21, %f837, 0f3F800000;
+	@%p21 bra 	$L__BB2_44;
+
+	mov.u32 	%r314, 254;
+	// begin inline asm
+	call (%r313), _optix_report_intersection_0, (%f340, %r314);
+	// end inline asm
+
+$L__BB2_44:
 	ret;
-}
 
+}
 	// .globl	__closesthit__disk
-.visible .entry __closesthit__disk(
-
-)
+.visible .entry __closesthit__disk()
 {
-	.reg .pred 	%p<67>;
-	.reg .b16 	%rs<18>;
-	.reg .f32 	%f<2066>;
-	.reg .b32 	%r<650>;
-	.reg .b64 	%rd<671>;
-
-
-	// inline asm
-	call (%r23), _optix_get_launch_dimension_x, ();
-	// inline asm
-	// inline asm
-	call (%r24), _optix_get_launch_dimension_y, ();
-	// inline asm
-	// inline asm
-	call (%r26), _optix_get_launch_index_x, ();
-	// inline asm
-	// inline asm
-	call (%r27), _optix_get_launch_index_y, ();
-	// inline asm
-	// inline asm
-	call (%r28), _optix_get_launch_index_z, ();
-	// inline asm
-	mad.lo.s32 	%r29, %r28, %r24, %r27;
-	mad.lo.s32 	%r1, %r29, %r23, %r26;
+	.reg .pred 	%p<71>;
+	.reg .b16 	%rs<2>;
+	.reg .f32 	%f<2118>;
+	.reg .b32 	%r<660>;
+	.reg .b64 	%rd<658>;
+
+
+	// begin inline asm
+	call (%r27), _optix_get_launch_dimension_x, ();
+	// end inline asm
+	// begin inline asm
+	call (%r28), _optix_get_launch_dimension_y, ();
+	// end inline asm
+	// begin inline asm
+	call (%r30), _optix_get_launch_index_x, ();
+	// end inline asm
+	// begin inline asm
+	call (%r31), _optix_get_launch_index_y, ();
+	// end inline asm
+	// begin inline asm
+	call (%r32), _optix_get_launch_index_z, ();
+	// end inline asm
+	mad.lo.s32 	%r33, %r32, %r28, %r31;
+	mad.lo.s32 	%r1, %r33, %r27, %r30;
 	ld.const.u64 	%rd1, [params+352];
-	setp.eq.s64	%p1, %rd1, 0;
-	@%p1 bra 	BB3_2;
+	setp.eq.s64 	%p1, %rd1, 0;
+	@%p1 bra 	$L__BB3_2;
 
-	cvta.to.global.u64 	%rd49, %rd1;
-	cvt.u64.u32	%rd50, %r1;
-	add.s64 	%rd51, %rd49, %rd50;
+	cvta.to.global.u64 	%rd45, %rd1;
+	cvt.u64.u32 	%rd46, %r1;
+	add.s64 	%rd47, %rd45, %rd46;
 	mov.u16 	%rs1, 1;
-	st.global.u8 	[%rd51], %rs1;
-	bra.uni 	BB3_116;
-
-BB3_2:
-	// inline asm
-	call (%rd52), _optix_get_sbt_data_ptr_64, ();
-	// inline asm
-	ld.u64 	%rd3, [%rd52+8];
-	// inline asm
-	call (%f684), _optix_get_world_ray_origin_x, ();
-	// inline asm
-	// inline asm
-	call (%f685), _optix_get_world_ray_origin_y, ();
-	// inline asm
-	// inline asm
-	call (%f1853), _optix_get_world_ray_origin_z, ();
-	// inline asm
-	// inline asm
-	call (%r30), _optix_get_transform_list_size, ();
-	// inline asm
-	setp.eq.s32	%p2, %r30, 0;
-	@%p2 bra 	BB3_3;
-
-	mov.u32 	%r646, 0;
-	// inline asm
-	call (%f687), _optix_get_ray_time, ();
-	// inline asm
-
-BB3_5:
+	st.global.u8 	[%rd47], %rs1;
+	bra.uni 	$L__BB3_117;
+
+$L__BB3_2:
+	// begin inline asm
+	call (%rd48), _optix_get_sbt_data_ptr_64, ();
+	// end inline asm
+	ld.u64 	%rd3, [%rd48+8];
+	// begin inline asm
+	call (%f1896), _optix_get_world_ray_origin_x, ();
+	// end inline asm
+	// begin inline asm
+	call (%f1897), _optix_get_world_ray_origin_y, ();
+	// end inline asm
+	// begin inline asm
+	call (%f1898), _optix_get_world_ray_origin_z, ();
+	// end inline asm
+	// begin inline asm
+	call (%r34), _optix_get_transform_list_size, ();
+	// end inline asm
+	setp.eq.s32 	%p2, %r34, 0;
+	@%p2 bra 	$L__BB3_23;
+
+	// begin inline asm
+	call (%r35), _optix_get_transform_list_size, ();
+	// end inline asm
+	// begin inline asm
+	call (%f722), _optix_get_ray_time, ();
+	// end inline asm
+	setp.eq.s32 	%p3, %r35, 0;
+	@%p3 bra 	$L__BB3_21;
+
+	mov.u32 	%r655, 0;
+
+$L__BB3_5:
 	.pragma "nounroll";
-	// inline asm
-	call (%rd53), _optix_get_transform_list_handle, (%r646);
-	// inline asm
-	// inline asm
-	call (%r33), _optix_get_transform_type_from_handle, (%rd53);
-	// inline asm
-	and.b32  	%r34, %r33, -2;
-	setp.eq.s32	%p3, %r34, 2;
-	@%p3 bra 	BB3_11;
-	bra.uni 	BB3_6;
-
-BB3_11:
-	setp.eq.s32	%p6, %r33, 2;
-	@%p6 bra 	BB3_15;
-	bra.uni 	BB3_12;
-
-BB3_15:
-	// inline asm
-	call (%rd127), _optix_get_matrix_motion_transform_from_handle, (%rd53);
-	// inline asm
-	// inline asm
-	cvta.to.global.u64 %rd129, %rd127;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r122,%r123,%r124,%r125}, [%rd129];
-	// inline asm
-	mov.b32	{%rs4, %rs5}, %r124;
-	add.s64 	%rd133, %rd127, 16;
-	// inline asm
+	// begin inline asm
+	call (%rd49), _optix_get_transform_list_handle, (%r655);
+	// end inline asm
+	// begin inline asm
+	call (%r38), _optix_get_transform_type_from_handle, (%rd49);
+	// end inline asm
+	or.b32  	%r39, %r38, 1;
+	setp.eq.s32 	%p4, %r39, 3;
+	@%p4 bra 	$L__BB3_11;
+	bra.uni 	$L__BB3_6;
+
+$L__BB3_11:
+	setp.eq.s32 	%p7, %r38, 2;
+	@%p7 bra 	$L__BB3_15;
+	bra.uni 	$L__BB3_12;
+
+$L__BB3_15:
+	// begin inline asm
+	call (%rd121), _optix_get_matrix_motion_transform_from_handle, (%rd49);
+	// end inline asm
+	// begin inline asm
+	cvta.to.global.u64 %rd123, %rd121;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r127,%r128,%r129,%r130}, [%rd123];
+	// end inline asm
+	add.s64 	%rd127, %rd121, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd126, %rd127;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r131,%r132,%r133,%r134}, [%rd126];
+	// end inline asm
+	add.s64 	%rd130, %rd121, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd129, %rd130;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r135,%r136,%r137,%r138}, [%rd129];
+	// end inline asm
+	add.s64 	%rd133, %rd121, 48;
+	// begin inline asm
 	cvta.to.global.u64 %rd132, %rd133;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r126,%r127,%r128,%r129}, [%rd132];
-	// inline asm
-	add.s64 	%rd136, %rd127, 32;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r139,%r140,%r141,%r142}, [%rd132];
+	// end inline asm
+	add.s64 	%rd136, %rd121, 64;
+	// begin inline asm
 	cvta.to.global.u64 %rd135, %rd136;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r130,%r131,%r132,%r133}, [%rd135];
-	// inline asm
-	add.s64 	%rd139, %rd127, 48;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r143,%r144,%r145,%r146}, [%rd135];
+	// end inline asm
+	add.s64 	%rd139, %rd121, 80;
+	// begin inline asm
 	cvta.to.global.u64 %rd138, %rd139;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r134,%r135,%r136,%r137}, [%rd138];
-	// inline asm
-	add.s64 	%rd142, %rd127, 64;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r147,%r148,%r149,%r150}, [%rd138];
+	// end inline asm
+	add.s64 	%rd142, %rd121, 96;
+	// begin inline asm
 	cvta.to.global.u64 %rd141, %rd142;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r138,%r139,%r140,%r141}, [%rd141];
-	// inline asm
-	add.s64 	%rd145, %rd127, 80;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r151,%r152,%r153,%r154}, [%rd141];
+	// end inline asm
+	add.s64 	%rd145, %rd121, 112;
+	// begin inline asm
 	cvta.to.global.u64 %rd144, %rd145;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r142,%r143,%r144,%r145}, [%rd144];
-	// inline asm
-	add.s64 	%rd148, %rd127, 96;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r155,%r156,%r157,%r158}, [%rd144];
+	// end inline asm
+	mov.b32 	%f850, %r130;
+	mov.b32 	%f851, %r131;
+	and.b32  	%r171, %r129, 65535;
+	add.s32 	%r172, %r171, -1;
+	cvt.rn.f32.s32 	%f852, %r172;
+	sub.f32 	%f853, %f722, %f850;
+	mul.f32 	%f854, %f853, %f852;
+	sub.f32 	%f855, %f851, %f850;
+	div.rn.f32 	%f856, %f854, %f855;
+	min.f32 	%f857, %f852, %f856;
+	mov.f32 	%f858, 0f00000000;
+	max.f32 	%f859, %f858, %f857;
+	cvt.rmi.f32.f32 	%f860, %f859;
+	sub.f32 	%f90, %f859, %f860;
+	cvt.rzi.s32.f32 	%r173, %f860;
+	mul.wide.s32 	%rd156, %r173, 48;
+	add.s64 	%rd148, %rd130, %rd156;
+	// begin inline asm
 	cvta.to.global.u64 %rd147, %rd148;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r146,%r147,%r148,%r149}, [%rd147];
-	// inline asm
-	add.s64 	%rd151, %rd127, 112;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r159,%r160,%r161,%r162}, [%rd147];
+	// end inline asm
+	mov.b32 	%f1851, %r159;
+	mov.b32 	%f1850, %r160;
+	mov.b32 	%f1849, %r161;
+	mov.b32 	%f1848, %r162;
+	add.s64 	%rd151, %rd148, 16;
+	// begin inline asm
 	cvta.to.global.u64 %rd150, %rd151;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r150,%r151,%r152,%r153}, [%rd150];
-	// inline asm
-	mov.b32 	 %f814, %r125;
-	mov.b32 	 %f815, %r126;
-	cvt.u32.u16	%r166, %rs4;
-	add.s32 	%r167, %r166, -1;
-	cvt.rn.f32.s32	%f816, %r167;
-	sub.f32 	%f817, %f687, %f814;
-	mul.f32 	%f818, %f817, %f816;
-	sub.f32 	%f819, %f815, %f814;
-	div.rn.f32 	%f820, %f818, %f819;
-	min.f32 	%f821, %f816, %f820;
-	mov.f32 	%f822, 0f00000000;
-	max.f32 	%f823, %f822, %f821;
-	cvt.rmi.f32.f32	%f824, %f823;
-	cvt.rzi.s32.f32	%r168, %f824;
-	mul.wide.s32 	%rd162, %r168, 48;
-	add.s64 	%rd154, %rd136, %rd162;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r163,%r164,%r165,%r166}, [%rd150];
+	// end inline asm
+	mov.b32 	%f1855, %r163;
+	mov.b32 	%f1854, %r164;
+	mov.b32 	%f1853, %r165;
+	mov.b32 	%f1852, %r166;
+	add.s64 	%rd154, %rd148, 32;
+	// begin inline asm
 	cvta.to.global.u64 %rd153, %rd154;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r154,%r155,%r156,%r157}, [%rd153];
-	// inline asm
-	mov.b32 	 %f1820, %r154;
-	mov.b32 	 %f1819, %r155;
-	mov.b32 	 %f1818, %r156;
-	mov.b32 	 %f1817, %r157;
-	add.s64 	%rd157, %rd154, 16;
-	// inline asm
-	cvta.to.global.u64 %rd156, %rd157;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r158,%r159,%r160,%r161}, [%rd156];
-	// inline asm
-	mov.b32 	 %f1824, %r158;
-	mov.b32 	 %f1823, %r159;
-	mov.b32 	 %f1822, %r160;
-	mov.b32 	 %f1821, %r161;
-	add.s64 	%rd160, %rd154, 32;
-	// inline asm
-	cvta.to.global.u64 %rd159, %rd160;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r162,%r163,%r164,%r165}, [%rd159];
-	// inline asm
-	sub.f32 	%f98, %f823, %f824;
-	mov.b32 	 %f1828, %r162;
-	mov.b32 	 %f1827, %r163;
-	mov.b32 	 %f1826, %r164;
-	mov.b32 	 %f1825, %r165;
-	setp.leu.f32	%p8, %f98, 0f00000000;
-	@%p8 bra 	BB3_17;
-
-	cvt.rmi.f32.f32	%f1788, %f823;
-	cvt.rzi.s32.f32	%r645, %f1788;
-	cvt.s64.s32	%rd666, %r645;
-	mul.lo.s64 	%rd172, %rd666, 48;
-	add.s64 	%rd173, %rd127, %rd172;
-	add.s64 	%rd164, %rd173, 80;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r167,%r168,%r169,%r170}, [%rd153];
+	// end inline asm
+	mov.b32 	%f1859, %r167;
+	mov.b32 	%f1858, %r168;
+	mov.b32 	%f1857, %r169;
+	mov.b32 	%f1856, %r170;
+	setp.leu.f32 	%p9, %f90, 0f00000000;
+	@%p9 bra 	$L__BB3_17;
+
+	cvt.rmi.f32.f32 	%f1819, %f859;
+	cvt.rzi.s32.f32 	%r654, %f1819;
+	cvt.s64.s32 	%rd653, %r654;
+	mov.f32 	%f861, 0f3F800000;
+	sub.f32 	%f862, %f861, %f90;
+	mul.lo.s64 	%rd166, %rd653, 48;
+	add.s64 	%rd167, %rd121, %rd166;
+	add.s64 	%rd158, %rd167, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd157, %rd158;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r174,%r175,%r176,%r177}, [%rd157];
+	// end inline asm
+	mov.b32 	%f863, %r174;
+	mov.b32 	%f864, %r175;
+	mov.b32 	%f865, %r176;
+	mov.b32 	%f866, %r177;
+	mul.f32 	%f867, %f90, %f863;
+	mul.f32 	%f868, %f90, %f864;
+	mul.f32 	%f869, %f90, %f865;
+	mul.f32 	%f870, %f90, %f866;
+	fma.rn.f32 	%f1851, %f862, %f1851, %f867;
+	fma.rn.f32 	%f1850, %f862, %f1850, %f868;
+	fma.rn.f32 	%f1849, %f862, %f1849, %f869;
+	fma.rn.f32 	%f1848, %f862, %f1848, %f870;
+	add.s64 	%rd161, %rd167, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd160, %rd161;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r178,%r179,%r180,%r181}, [%rd160];
+	// end inline asm
+	mov.b32 	%f871, %r178;
+	mov.b32 	%f872, %r179;
+	mov.b32 	%f873, %r180;
+	mov.b32 	%f874, %r181;
+	mul.f32 	%f875, %f90, %f871;
+	mul.f32 	%f876, %f90, %f872;
+	mul.f32 	%f877, %f90, %f873;
+	mul.f32 	%f878, %f90, %f874;
+	fma.rn.f32 	%f1855, %f862, %f1855, %f875;
+	fma.rn.f32 	%f1854, %f862, %f1854, %f876;
+	fma.rn.f32 	%f1853, %f862, %f1853, %f877;
+	fma.rn.f32 	%f1852, %f862, %f1852, %f878;
+	add.s64 	%rd164, %rd167, 112;
+	// begin inline asm
 	cvta.to.global.u64 %rd163, %rd164;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r169,%r170,%r171,%r172}, [%rd163];
-	// inline asm
-	mov.b32 	 %f825, %r169;
-	mov.b32 	 %f826, %r170;
-	mov.b32 	 %f827, %r171;
-	mov.b32 	 %f828, %r172;
-	add.s64 	%rd167, %rd173, 96;
-	// inline asm
-	cvta.to.global.u64 %rd166, %rd167;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r173,%r174,%r175,%r176}, [%rd166];
-	// inline asm
-	mov.b32 	 %f829, %r173;
-	mov.b32 	 %f830, %r174;
-	mov.b32 	 %f831, %r175;
-	mov.b32 	 %f832, %r176;
-	add.s64 	%rd170, %rd173, 112;
-	// inline asm
-	cvta.to.global.u64 %rd169, %rd170;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r177,%r178,%r179,%r180}, [%rd169];
-	// inline asm
-	mov.f32 	%f833, 0f3F800000;
-	sub.f32 	%f834, %f833, %f98;
-	mul.f32 	%f835, %f98, %f825;
-	mul.f32 	%f836, %f98, %f826;
-	mul.f32 	%f837, %f98, %f827;
-	mul.f32 	%f838, %f98, %f828;
-	fma.rn.f32 	%f1820, %f834, %f1820, %f835;
-	fma.rn.f32 	%f1819, %f834, %f1819, %f836;
-	fma.rn.f32 	%f1818, %f834, %f1818, %f837;
-	fma.rn.f32 	%f1817, %f834, %f1817, %f838;
-	mul.f32 	%f839, %f98, %f829;
-	mul.f32 	%f840, %f98, %f830;
-	mul.f32 	%f841, %f98, %f831;
-	mul.f32 	%f842, %f98, %f832;
-	fma.rn.f32 	%f1824, %f834, %f1824, %f839;
-	fma.rn.f32 	%f1823, %f834, %f1823, %f840;
-	fma.rn.f32 	%f1822, %f834, %f1822, %f841;
-	fma.rn.f32 	%f1821, %f834, %f1821, %f842;
-	mov.b32 	 %f843, %r177;
-	mov.b32 	 %f844, %r178;
-	mov.b32 	 %f845, %r179;
-	mov.b32 	 %f846, %r180;
-	mul.f32 	%f847, %f98, %f843;
-	mul.f32 	%f848, %f98, %f844;
-	mul.f32 	%f849, %f98, %f845;
-	mul.f32 	%f850, %f98, %f846;
-	fma.rn.f32 	%f1828, %f834, %f1828, %f847;
-	fma.rn.f32 	%f1827, %f834, %f1827, %f848;
-	fma.rn.f32 	%f1826, %f834, %f1826, %f849;
-	fma.rn.f32 	%f1825, %f834, %f1825, %f850;
-	bra.uni 	BB3_17;
-
-BB3_6:
-	mov.f32 	%f1829, 0f00000000;
-	mov.f32 	%f1832, 0f3F800000;
-	setp.eq.s32	%p4, %r33, 4;
-	@%p4 bra 	BB3_9;
-	bra.uni 	BB3_7;
-
-BB3_9:
-	// inline asm
-	call (%rd667), _optix_get_instance_inverse_transform_from_handle, (%rd53);
-	// inline asm
-	bra.uni 	BB3_10;
-
-BB3_12:
-	// inline asm
-	call (%rd68), _optix_get_srt_motion_transform_from_handle, (%rd53);
-	// inline asm
-	// inline asm
-	cvta.to.global.u64 %rd70, %rd68;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r47,%r48,%r49,%r50}, [%rd70];
-	// inline asm
-	mov.b32	{%rs2, %rs3}, %r49;
-	add.s64 	%rd74, %rd68, 16;
-	// inline asm
-	cvta.to.global.u64 %rd73, %rd74;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r51,%r52,%r53,%r54}, [%rd73];
-	// inline asm
-	add.s64 	%rd77, %rd68, 32;
-	// inline asm
-	cvta.to.global.u64 %rd76, %rd77;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r55,%r56,%r57,%r58}, [%rd76];
-	// inline asm
-	add.s64 	%rd80, %rd68, 48;
-	// inline asm
-	cvta.to.global.u64 %rd79, %rd80;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r59,%r60,%r61,%r62}, [%rd79];
-	// inline asm
-	add.s64 	%rd83, %rd68, 64;
-	// inline asm
-	cvta.to.global.u64 %rd82, %rd83;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r63,%r64,%r65,%r66}, [%rd82];
-	// inline asm
-	add.s64 	%rd86, %rd68, 80;
-	// inline asm
-	cvta.to.global.u64 %rd85, %rd86;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r67,%r68,%r69,%r70}, [%rd85];
-	// inline asm
-	add.s64 	%rd89, %rd68, 96;
-	// inline asm
-	cvta.to.global.u64 %rd88, %rd89;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r71,%r72,%r73,%r74}, [%rd88];
-	// inline asm
-	add.s64 	%rd92, %rd68, 112;
-	// inline asm
-	cvta.to.global.u64 %rd91, %rd92;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r75,%r76,%r77,%r78}, [%rd91];
-	// inline asm
-	add.s64 	%rd95, %rd68, 128;
-	// inline asm
-	cvta.to.global.u64 %rd94, %rd95;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r79,%r80,%r81,%r82}, [%rd94];
-	// inline asm
-	add.s64 	%rd98, %rd68, 144;
-	// inline asm
-	cvta.to.global.u64 %rd97, %rd98;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r83,%r84,%r85,%r86}, [%rd97];
-	// inline asm
-	mov.b32 	 %f701, %r50;
-	mov.b32 	 %f702, %r51;
-	cvt.u32.u16	%r103, %rs2;
-	add.s32 	%r104, %r103, -1;
-	cvt.rn.f32.s32	%f703, %r104;
-	sub.f32 	%f704, %f687, %f701;
-	mul.f32 	%f705, %f704, %f703;
-	sub.f32 	%f706, %f702, %f701;
-	div.rn.f32 	%f707, %f705, %f706;
-	min.f32 	%f708, %f703, %f707;
-	mov.f32 	%f709, 0f00000000;
-	max.f32 	%f710, %f709, %f708;
-	cvt.rmi.f32.f32	%f711, %f710;
-	cvt.rzi.s32.f32	%r105, %f711;
-	mul.wide.s32 	%rd112, %r105, 64;
-	add.s64 	%rd101, %rd77, %rd112;
-	// inline asm
-	cvta.to.global.u64 %rd100, %rd101;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r87,%r88,%r89,%r90}, [%rd100];
-	// inline asm
-	mov.b32 	 %f1801, %r87;
-	mov.b32 	 %f1802, %r88;
-	mov.b32 	 %f1803, %r89;
-	mov.b32 	 %f1804, %r90;
-	add.s64 	%rd104, %rd101, 16;
-	// inline asm
-	cvta.to.global.u64 %rd103, %rd104;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r91,%r92,%r93,%r94}, [%rd103];
-	// inline asm
-	mov.b32 	 %f1805, %r91;
-	mov.b32 	 %f1806, %r92;
-	mov.b32 	 %f1807, %r93;
-	mov.b32 	 %f1808, %r94;
-	add.s64 	%rd107, %rd101, 32;
-	// inline asm
-	cvta.to.global.u64 %rd106, %rd107;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r95,%r96,%r97,%r98}, [%rd106];
-	// inline asm
-	sub.f32 	%f37, %f710, %f711;
-	mov.b32 	 %f1809, %r95;
-	mov.b32 	 %f1810, %r96;
-	mov.b32 	 %f1811, %r97;
-	mov.b32 	 %f1812, %r98;
-	add.s64 	%rd110, %rd101, 48;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r182,%r183,%r184,%r185}, [%rd163];
+	// end inline asm
+	mov.b32 	%f879, %r182;
+	mov.b32 	%f880, %r183;
+	mov.b32 	%f881, %r184;
+	mov.b32 	%f882, %r185;
+	mul.f32 	%f883, %f90, %f879;
+	mul.f32 	%f884, %f90, %f880;
+	mul.f32 	%f885, %f90, %f881;
+	mul.f32 	%f886, %f90, %f882;
+	fma.rn.f32 	%f1859, %f862, %f1859, %f883;
+	fma.rn.f32 	%f1858, %f862, %f1858, %f884;
+	fma.rn.f32 	%f1857, %f862, %f1857, %f885;
+	fma.rn.f32 	%f1856, %f862, %f1856, %f886;
+	bra.uni 	$L__BB3_17;
+
+$L__BB3_6:
+	mov.f32 	%f1860, 0f00000000;
+	mov.f32 	%f1863, 0f3F800000;
+	setp.eq.s32 	%p5, %r38, 4;
+	@%p5 bra 	$L__BB3_9;
+
+	setp.ne.s32 	%p6, %r38, 1;
+	mov.f32 	%f1861, %f1860;
+	mov.f32 	%f1862, %f1860;
+	mov.f32 	%f1864, %f1860;
+	mov.f32 	%f1865, %f1860;
+	mov.f32 	%f1866, %f1863;
+	mov.f32 	%f1867, %f1860;
+	mov.f32 	%f1868, %f1860;
+	mov.f32 	%f1869, %f1863;
+	mov.f32 	%f1870, %f1860;
+	mov.f32 	%f1871, %f1860;
+	@%p6 bra 	$L__BB3_18;
+
+	// begin inline asm
+	call (%rd51), _optix_get_static_transform_from_handle, (%rd49);
+	// end inline asm
+	add.s64 	%rd654, %rd51, 64;
+	bra.uni 	$L__BB3_10;
+
+$L__BB3_12:
+	// begin inline asm
+	call (%rd64), _optix_get_srt_motion_transform_from_handle, (%rd49);
+	// end inline asm
+	// begin inline asm
+	cvta.to.global.u64 %rd66, %rd64;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r52,%r53,%r54,%r55}, [%rd66];
+	// end inline asm
+	add.s64 	%rd70, %rd64, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd69, %rd70;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r56,%r57,%r58,%r59}, [%rd69];
+	// end inline asm
+	add.s64 	%rd73, %rd64, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd72, %rd73;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r60,%r61,%r62,%r63}, [%rd72];
+	// end inline asm
+	add.s64 	%rd76, %rd64, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd75, %rd76;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r64,%r65,%r66,%r67}, [%rd75];
+	// end inline asm
+	add.s64 	%rd79, %rd64, 64;
+	// begin inline asm
+	cvta.to.global.u64 %rd78, %rd79;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r68,%r69,%r70,%r71}, [%rd78];
+	// end inline asm
+	add.s64 	%rd82, %rd64, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd81, %rd82;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r72,%r73,%r74,%r75}, [%rd81];
+	// end inline asm
+	add.s64 	%rd85, %rd64, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd84, %rd85;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r76,%r77,%r78,%r79}, [%rd84];
+	// end inline asm
+	add.s64 	%rd88, %rd64, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd87, %rd88;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r80,%r81,%r82,%r83}, [%rd87];
+	// end inline asm
+	add.s64 	%rd91, %rd64, 128;
+	// begin inline asm
+	cvta.to.global.u64 %rd90, %rd91;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r84,%r85,%r86,%r87}, [%rd90];
+	// end inline asm
+	add.s64 	%rd94, %rd64, 144;
+	// begin inline asm
+	cvta.to.global.u64 %rd93, %rd94;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r88,%r89,%r90,%r91}, [%rd93];
+	// end inline asm
+	mov.b32 	%f737, %r55;
+	mov.b32 	%f738, %r56;
+	and.b32  	%r108, %r54, 65535;
+	add.s32 	%r109, %r108, -1;
+	cvt.rn.f32.s32 	%f739, %r109;
+	sub.f32 	%f740, %f722, %f737;
+	mul.f32 	%f741, %f740, %f739;
+	sub.f32 	%f742, %f738, %f737;
+	div.rn.f32 	%f743, %f741, %f742;
+	min.f32 	%f744, %f739, %f743;
+	mov.f32 	%f745, 0f00000000;
+	max.f32 	%f746, %f745, %f744;
+	cvt.rmi.f32.f32 	%f747, %f746;
+	sub.f32 	%f29, %f746, %f747;
+	cvt.rzi.s32.f32 	%r110, %f747;
+	mul.wide.s32 	%rd108, %r110, 64;
+	add.s64 	%rd97, %rd73, %rd108;
+	// begin inline asm
+	cvta.to.global.u64 %rd96, %rd97;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r92,%r93,%r94,%r95}, [%rd96];
+	// end inline asm
+	mov.b32 	%f1832, %r92;
+	mov.b32 	%f1833, %r93;
+	mov.b32 	%f1834, %r94;
+	mov.b32 	%f1835, %r95;
+	add.s64 	%rd100, %rd97, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd99, %rd100;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r96,%r97,%r98,%r99}, [%rd99];
+	// end inline asm
+	mov.b32 	%f1836, %r96;
+	mov.b32 	%f1837, %r97;
+	mov.b32 	%f1838, %r98;
+	mov.b32 	%f1839, %r99;
+	add.s64 	%rd103, %rd97, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd102, %rd103;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r100,%r101,%r102,%r103}, [%rd102];
+	// end inline asm
+	mov.b32 	%f1840, %r100;
+	mov.b32 	%f1841, %r101;
+	mov.b32 	%f1842, %r102;
+	mov.b32 	%f1843, %r103;
+	add.s64 	%rd106, %rd97, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd105, %rd106;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r104,%r105,%r106,%r107}, [%rd105];
+	// end inline asm
+	mov.b32 	%f1844, %r104;
+	mov.b32 	%f1845, %r105;
+	mov.b32 	%f1846, %r106;
+	mov.b32 	%f1847, %r107;
+	setp.leu.f32 	%p8, %f29, 0f00000000;
+	@%p8 bra 	$L__BB3_14;
+
+	mov.f32 	%f748, 0f3F800000;
+	sub.f32 	%f749, %f748, %f29;
+	add.s64 	%rd110, %rd97, 64;
+	// begin inline asm
 	cvta.to.global.u64 %rd109, %rd110;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r99,%r100,%r101,%r102}, [%rd109];
-	// inline asm
-	mov.b32 	 %f1813, %r99;
-	mov.b32 	 %f1814, %r100;
-	mov.b32 	 %f1815, %r101;
-	mov.b32 	 %f1816, %r102;
-	setp.leu.f32	%p7, %f37, 0f00000000;
-	@%p7 bra 	BB3_14;
-
-	cvt.rmi.f32.f32	%f1787, %f710;
-	cvt.rzi.s32.f32	%r644, %f1787;
-	cvt.s64.s32	%rd665, %r644;
-	shl.b64 	%rd125, %rd665, 6;
-	add.s64 	%rd126, %rd125, %rd68;
-	add.s64 	%rd114, %rd126, 96;
-	// inline asm
-	cvta.to.global.u64 %rd113, %rd114;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r106,%r107,%r108,%r109}, [%rd113];
-	// inline asm
-	mov.b32 	 %f712, %r106;
-	mov.b32 	 %f713, %r107;
-	mov.b32 	 %f714, %r108;
-	mov.b32 	 %f715, %r109;
-	add.s64 	%rd117, %rd126, 112;
-	// inline asm
-	cvta.to.global.u64 %rd116, %rd117;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r110,%r111,%r112,%r113}, [%rd116];
-	// inline asm
-	mov.b32 	 %f716, %r110;
-	mov.b32 	 %f717, %r111;
-	mov.b32 	 %f718, %r112;
-	mov.b32 	 %f719, %r113;
-	add.s64 	%rd120, %rd126, 128;
-	// inline asm
-	cvta.to.global.u64 %rd119, %rd120;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r114,%r115,%r116,%r117}, [%rd119];
-	// inline asm
-	mov.b32 	 %f720, %r114;
-	mov.b32 	 %f721, %r115;
-	mov.b32 	 %f722, %r116;
-	mov.b32 	 %f723, %r117;
-	add.s64 	%rd123, %rd126, 144;
-	// inline asm
-	cvta.to.global.u64 %rd122, %rd123;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r118,%r119,%r120,%r121}, [%rd122];
-	// inline asm
-	mov.f32 	%f724, 0f3F800000;
-	sub.f32 	%f725, %f724, %f37;
-	mul.f32 	%f726, %f37, %f712;
-	mul.f32 	%f727, %f37, %f713;
-	mul.f32 	%f728, %f37, %f714;
-	mul.f32 	%f729, %f37, %f715;
-	fma.rn.f32 	%f1801, %f725, %f1801, %f726;
-	fma.rn.f32 	%f1802, %f725, %f1802, %f727;
-	fma.rn.f32 	%f1803, %f725, %f1803, %f728;
-	fma.rn.f32 	%f1804, %f725, %f1804, %f729;
-	mul.f32 	%f730, %f37, %f716;
-	mul.f32 	%f731, %f37, %f717;
-	mul.f32 	%f732, %f37, %f718;
-	mul.f32 	%f733, %f37, %f719;
-	fma.rn.f32 	%f1805, %f725, %f1805, %f730;
-	fma.rn.f32 	%f1806, %f725, %f1806, %f731;
-	fma.rn.f32 	%f1807, %f725, %f1807, %f732;
-	fma.rn.f32 	%f1808, %f725, %f1808, %f733;
-	mul.f32 	%f734, %f37, %f720;
-	mul.f32 	%f735, %f37, %f721;
-	mul.f32 	%f736, %f37, %f722;
-	mul.f32 	%f737, %f37, %f723;
-	fma.rn.f32 	%f1809, %f725, %f1809, %f734;
-	fma.rn.f32 	%f738, %f725, %f1810, %f735;
-	fma.rn.f32 	%f739, %f725, %f1811, %f736;
-	fma.rn.f32 	%f740, %f725, %f1812, %f737;
-	mov.b32 	 %f741, %r118;
-	mov.b32 	 %f742, %r119;
-	mov.b32 	 %f743, %r120;
-	mov.b32 	 %f744, %r121;
-	mul.f32 	%f745, %f37, %f741;
-	mul.f32 	%f746, %f37, %f742;
-	mul.f32 	%f747, %f37, %f743;
-	mul.f32 	%f748, %f37, %f744;
-	fma.rn.f32 	%f749, %f725, %f1813, %f745;
-	fma.rn.f32 	%f1814, %f725, %f1814, %f746;
-	fma.rn.f32 	%f1815, %f725, %f1815, %f747;
-	fma.rn.f32 	%f1816, %f725, %f1816, %f748;
-	mul.f32 	%f750, %f739, %f739;
-	fma.rn.f32 	%f751, %f738, %f738, %f750;
-	fma.rn.f32 	%f752, %f740, %f740, %f751;
-	fma.rn.f32 	%f753, %f749, %f749, %f752;
-	sqrt.rn.f32 	%f754, %f753;
-	rcp.rn.f32 	%f755, %f754;
-	mul.f32 	%f1810, %f738, %f755;
-	mul.f32 	%f1811, %f739, %f755;
-	mul.f32 	%f1812, %f740, %f755;
-	mul.f32 	%f1813, %f749, %f755;
-
-BB3_14:
-	mul.f32 	%f756, %f1811, %f1811;
-	fma.rn.f32 	%f757, %f1810, %f1810, %f756;
-	fma.rn.f32 	%f758, %f1812, %f1812, %f757;
-	fma.rn.f32 	%f759, %f1813, %f1813, %f758;
-	rcp.rn.f32 	%f760, %f759;
-	mul.f32 	%f761, %f1810, %f760;
-	mul.f32 	%f762, %f1811, %f760;
-	mul.f32 	%f763, %f1812, %f760;
-	mul.f32 	%f764, %f1813, %f760;
-	mul.f32 	%f765, %f1810, %f761;
-	mul.f32 	%f766, %f1811, %f762;
-	mul.f32 	%f767, %f1812, %f763;
-	mul.f32 	%f768, %f1810, %f762;
-	mul.f32 	%f769, %f1812, %f764;
-	mul.f32 	%f770, %f1810, %f763;
-	mul.f32 	%f771, %f1811, %f764;
-	mul.f32 	%f772, %f1811, %f763;
-	mul.f32 	%f773, %f1810, %f764;
-	sub.f32 	%f774, %f765, %f766;
-	sub.f32 	%f775, %f774, %f767;
-	fma.rn.f32 	%f776, %f1813, %f764, %f775;
-	sub.f32 	%f777, %f768, %f769;
-	add.f32 	%f778, %f777, %f777;
-	add.f32 	%f779, %f770, %f771;
-	add.f32 	%f780, %f779, %f779;
-	add.f32 	%f781, %f768, %f769;
-	add.f32 	%f782, %f781, %f781;
-	sub.f32 	%f783, %f766, %f765;
-	sub.f32 	%f784, %f783, %f767;
-	fma.rn.f32 	%f785, %f1813, %f764, %f784;
-	sub.f32 	%f786, %f772, %f773;
-	add.f32 	%f787, %f786, %f786;
-	sub.f32 	%f788, %f770, %f771;
-	add.f32 	%f789, %f788, %f788;
-	add.f32 	%f790, %f772, %f773;
-	add.f32 	%f791, %f790, %f790;
-	neg.f32 	%f792, %f765;
-	sub.f32 	%f793, %f792, %f766;
-	add.f32 	%f794, %f767, %f793;
-	fma.rn.f32 	%f795, %f1813, %f764, %f794;
-	mul.f32 	%f796, %f1804, %f776;
-	fma.rn.f32 	%f797, %f1807, %f778, %f796;
-	fma.rn.f32 	%f798, %f1809, %f780, %f797;
-	sub.f32 	%f1817, %f1814, %f798;
-	mul.f32 	%f799, %f1807, %f785;
-	fma.rn.f32 	%f800, %f1804, %f782, %f799;
-	fma.rn.f32 	%f801, %f1809, %f787, %f800;
-	sub.f32 	%f1821, %f1815, %f801;
-	mul.f32 	%f802, %f1807, %f791;
-	fma.rn.f32 	%f803, %f1804, %f789, %f802;
-	fma.rn.f32 	%f804, %f1809, %f795, %f803;
-	sub.f32 	%f1825, %f1816, %f804;
-	mul.f32 	%f805, %f1803, %f776;
-	fma.rn.f32 	%f806, %f1806, %f778, %f805;
-	fma.rn.f32 	%f1818, %f1808, %f780, %f806;
-	mul.f32 	%f807, %f1806, %f785;
-	fma.rn.f32 	%f808, %f1803, %f782, %f807;
-	fma.rn.f32 	%f1822, %f1808, %f787, %f808;
-	mul.f32 	%f809, %f1806, %f791;
-	fma.rn.f32 	%f810, %f1803, %f789, %f809;
-	fma.rn.f32 	%f1826, %f1808, %f795, %f810;
-	mul.f32 	%f811, %f1802, %f776;
-	fma.rn.f32 	%f1819, %f1805, %f778, %f811;
-	mul.f32 	%f812, %f1805, %f785;
-	fma.rn.f32 	%f1823, %f1802, %f782, %f812;
-	mul.f32 	%f813, %f1805, %f791;
-	fma.rn.f32 	%f1827, %f1802, %f789, %f813;
-	mul.f32 	%f1820, %f1801, %f776;
-	mul.f32 	%f1824, %f1801, %f782;
-	mul.f32 	%f1828, %f1801, %f789;
-
-BB3_17:
-	mul.f32 	%f851, %f1822, %f1827;
-	mul.f32 	%f852, %f1823, %f1826;
-	sub.f32 	%f853, %f852, %f851;
-	mul.f32 	%f854, %f1820, %f853;
-	mul.f32 	%f855, %f1822, %f1828;
-	mul.f32 	%f856, %f1824, %f1826;
-	sub.f32 	%f857, %f856, %f855;
-	mul.f32 	%f858, %f1819, %f857;
-	sub.f32 	%f859, %f854, %f858;
-	mul.f32 	%f860, %f1823, %f1828;
-	mul.f32 	%f861, %f1824, %f1827;
-	sub.f32 	%f862, %f861, %f860;
-	fma.rn.f32 	%f863, %f1818, %f862, %f859;
-	rcp.rn.f32 	%f864, %f863;
-	mul.f32 	%f1832, %f864, %f853;
-	mul.f32 	%f865, %f1819, %f1826;
-	mul.f32 	%f866, %f1818, %f1827;
-	sub.f32 	%f867, %f866, %f865;
-	mul.f32 	%f1831, %f864, %f867;
-	mul.f32 	%f868, %f1818, %f1823;
-	mul.f32 	%f869, %f1819, %f1822;
-	sub.f32 	%f870, %f869, %f868;
-	mul.f32 	%f1830, %f870, %f864;
-	sub.f32 	%f871, %f855, %f856;
-	mul.f32 	%f1836, %f864, %f871;
-	mul.f32 	%f872, %f1818, %f1828;
-	mul.f32 	%f873, %f1820, %f1826;
-	sub.f32 	%f874, %f873, %f872;
-	mul.f32 	%f1835, %f864, %f874;
-	mul.f32 	%f875, %f1820, %f1822;
-	mul.f32 	%f876, %f1818, %f1824;
-	sub.f32 	%f877, %f876, %f875;
-	mul.f32 	%f1834, %f877, %f864;
-	mul.f32 	%f1840, %f864, %f862;
-	mul.f32 	%f878, %f1820, %f1827;
-	mul.f32 	%f879, %f1819, %f1828;
-	sub.f32 	%f880, %f879, %f878;
-	mul.f32 	%f1839, %f864, %f880;
-	mul.f32 	%f881, %f1819, %f1824;
-	mul.f32 	%f882, %f1820, %f1823;
-	sub.f32 	%f883, %f882, %f881;
-	mul.f32 	%f1838, %f883, %f864;
-	mul.f32 	%f884, %f1817, %f1832;
-	neg.f32 	%f885, %f884;
-	mul.f32 	%f886, %f1821, %f1831;
-	sub.f32 	%f887, %f885, %f886;
-	mul.f32 	%f888, %f1825, %f1830;
-	sub.f32 	%f1829, %f887, %f888;
-	mul.f32 	%f889, %f1817, %f1836;
-	neg.f32 	%f890, %f889;
-	mul.f32 	%f891, %f1821, %f1835;
-	sub.f32 	%f892, %f890, %f891;
-	mul.f32 	%f893, %f1825, %f1834;
-	sub.f32 	%f1833, %f892, %f893;
-	mul.f32 	%f894, %f1817, %f1840;
-	neg.f32 	%f895, %f894;
-	mul.f32 	%f896, %f1821, %f1839;
-	sub.f32 	%f897, %f895, %f896;
-	mul.f32 	%f898, %f1825, %f1838;
-	sub.f32 	%f1837, %f897, %f898;
-	bra.uni 	BB3_18;
-
-BB3_7:
-	setp.ne.s32	%p5, %r33, 1;
-	mov.f32 	%f1830, %f1829;
-	mov.f32 	%f1831, %f1829;
-	mov.f32 	%f1833, %f1829;
-	mov.f32 	%f1834, %f1829;
-	mov.f32 	%f1835, %f1832;
-	mov.f32 	%f1836, %f1829;
-	mov.f32 	%f1837, %f1829;
-	mov.f32 	%f1838, %f1832;
-	mov.f32 	%f1839, %f1829;
-	mov.f32 	%f1840, %f1829;
-	@%p5 bra 	BB3_18;
-
-	// inline asm
-	call (%rd55), _optix_get_static_transform_from_handle, (%rd53);
-	// inline asm
-	add.s64 	%rd667, %rd55, 64;
-
-BB3_10:
-	// inline asm
-	cvta.to.global.u64 %rd59, %rd667;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r35,%r36,%r37,%r38}, [%rd59];
-	// inline asm
-	mov.b32 	 %f1832, %r35;
-	mov.b32 	 %f1831, %r36;
-	mov.b32 	 %f1830, %r37;
-	mov.b32 	 %f1829, %r38;
-	add.s64 	%rd63, %rd667, 16;
-	// inline asm
-	cvta.to.global.u64 %rd62, %rd63;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r39,%r40,%r41,%r42}, [%rd62];
-	// inline asm
-	mov.b32 	 %f1836, %r39;
-	mov.b32 	 %f1835, %r40;
-	mov.b32 	 %f1834, %r41;
-	mov.b32 	 %f1833, %r42;
-	add.s64 	%rd66, %rd667, 32;
-	// inline asm
-	cvta.to.global.u64 %rd65, %rd66;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r43,%r44,%r45,%r46}, [%rd65];
-	// inline asm
-	mov.b32 	 %f1840, %r43;
-	mov.b32 	 %f1839, %r44;
-	mov.b32 	 %f1838, %r45;
-	mov.b32 	 %f1837, %r46;
-
-BB3_18:
-	setp.eq.s32	%p9, %r646, 0;
-	@%p9 bra 	BB3_19;
-	bra.uni 	BB3_20;
-
-BB3_19:
-	mov.f32 	%f1800, %f1829;
-	mov.f32 	%f1799, %f1830;
-	mov.f32 	%f1798, %f1831;
-	mov.f32 	%f1797, %f1832;
-	mov.f32 	%f1796, %f1833;
-	mov.f32 	%f1795, %f1834;
-	mov.f32 	%f1794, %f1835;
-	mov.f32 	%f1793, %f1836;
-	mov.f32 	%f1792, %f1837;
-	mov.f32 	%f1791, %f1838;
-	mov.f32 	%f1790, %f1839;
-	mov.f32 	%f1789, %f1840;
-	bra.uni 	BB3_21;
-
-BB3_20:
-	mul.f32 	%f899, %f1797, %f1832;
-	fma.rn.f32 	%f900, %f1793, %f1831, %f899;
-	fma.rn.f32 	%f151, %f1789, %f1830, %f900;
-	mul.f32 	%f901, %f1798, %f1832;
-	fma.rn.f32 	%f902, %f1794, %f1831, %f901;
-	fma.rn.f32 	%f152, %f1790, %f1830, %f902;
-	mul.f32 	%f903, %f1799, %f1832;
-	fma.rn.f32 	%f904, %f1795, %f1831, %f903;
-	fma.rn.f32 	%f153, %f1791, %f1830, %f904;
-	mul.f32 	%f905, %f1800, %f1832;
-	fma.rn.f32 	%f906, %f1796, %f1831, %f905;
-	fma.rn.f32 	%f907, %f1792, %f1830, %f906;
-	add.f32 	%f154, %f1829, %f907;
-	mul.f32 	%f908, %f1797, %f1836;
-	fma.rn.f32 	%f909, %f1793, %f1835, %f908;
-	fma.rn.f32 	%f155, %f1789, %f1834, %f909;
-	mul.f32 	%f910, %f1798, %f1836;
-	fma.rn.f32 	%f911, %f1794, %f1835, %f910;
-	fma.rn.f32 	%f156, %f1790, %f1834, %f911;
-	mul.f32 	%f912, %f1799, %f1836;
-	fma.rn.f32 	%f913, %f1795, %f1835, %f912;
-	fma.rn.f32 	%f157, %f1791, %f1834, %f913;
-	mul.f32 	%f914, %f1800, %f1836;
-	fma.rn.f32 	%f915, %f1796, %f1835, %f914;
-	fma.rn.f32 	%f916, %f1792, %f1834, %f915;
-	add.f32 	%f158, %f1833, %f916;
-	mul.f32 	%f917, %f1797, %f1840;
-	fma.rn.f32 	%f918, %f1793, %f1839, %f917;
-	fma.rn.f32 	%f1789, %f1789, %f1838, %f918;
-	mul.f32 	%f919, %f1798, %f1840;
-	fma.rn.f32 	%f920, %f1794, %f1839, %f919;
-	fma.rn.f32 	%f1790, %f1790, %f1838, %f920;
-	mul.f32 	%f921, %f1799, %f1840;
-	fma.rn.f32 	%f922, %f1795, %f1839, %f921;
-	fma.rn.f32 	%f1791, %f1791, %f1838, %f922;
-	mul.f32 	%f923, %f1800, %f1840;
-	fma.rn.f32 	%f924, %f1796, %f1839, %f923;
-	fma.rn.f32 	%f925, %f1792, %f1838, %f924;
-	add.f32 	%f1792, %f1837, %f925;
-	mov.f32 	%f1800, %f154;
-	mov.f32 	%f1799, %f153;
-	mov.f32 	%f1798, %f152;
-	mov.f32 	%f1797, %f151;
-	mov.f32 	%f1796, %f158;
-	mov.f32 	%f1795, %f157;
-	mov.f32 	%f1794, %f156;
-	mov.f32 	%f1793, %f155;
-
-BB3_21:
-	add.s32 	%r646, %r646, 1;
-	setp.lt.u32	%p10, %r646, %r30;
-	@%p10 bra 	BB3_5;
-
-	mul.f32 	%f926, %f684, %f1797;
-	fma.rn.f32 	%f927, %f685, %f1798, %f926;
-	fma.rn.f32 	%f928, %f1853, %f1799, %f927;
-	add.f32 	%f1855, %f1800, %f928;
-	mul.f32 	%f929, %f684, %f1793;
-	fma.rn.f32 	%f930, %f685, %f1794, %f929;
-	fma.rn.f32 	%f931, %f1853, %f1795, %f930;
-	add.f32 	%f1854, %f1796, %f931;
-	mul.f32 	%f932, %f684, %f1789;
-	fma.rn.f32 	%f933, %f685, %f1790, %f932;
-	fma.rn.f32 	%f934, %f1853, %f1791, %f933;
-	add.f32 	%f1853, %f1792, %f934;
-	bra.uni 	BB3_23;
-
-BB3_3:
-	mov.f32 	%f1854, %f685;
-	mov.f32 	%f1855, %f684;
-
-BB3_23:
-	// inline asm
-	call (%f935), _optix_get_world_ray_direction_x, ();
-	// inline asm
-	// inline asm
-	call (%f936), _optix_get_world_ray_direction_y, ();
-	// inline asm
-	// inline asm
-	call (%f1904), _optix_get_world_ray_direction_z, ();
-	// inline asm
-	// inline asm
-	call (%f938), _optix_get_ray_time, ();
-	// inline asm
-	mov.u32 	%r647, 0;
-	@%p2 bra 	BB3_24;
-
-BB3_25:
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r111,%r112,%r113,%r114}, [%rd109];
+	// end inline asm
+	mov.b32 	%f750, %r111;
+	mov.b32 	%f751, %r112;
+	mov.b32 	%f752, %r113;
+	mov.b32 	%f753, %r114;
+	mul.f32 	%f754, %f29, %f750;
+	mul.f32 	%f755, %f29, %f751;
+	mul.f32 	%f756, %f29, %f752;
+	mul.f32 	%f757, %f29, %f753;
+	fma.rn.f32 	%f1832, %f749, %f1832, %f754;
+	fma.rn.f32 	%f1833, %f749, %f1833, %f755;
+	fma.rn.f32 	%f1834, %f749, %f1834, %f756;
+	fma.rn.f32 	%f1835, %f749, %f1835, %f757;
+	add.s64 	%rd113, %rd97, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd112, %rd113;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r115,%r116,%r117,%r118}, [%rd112];
+	// end inline asm
+	mov.b32 	%f758, %r115;
+	mov.b32 	%f759, %r116;
+	mov.b32 	%f760, %r117;
+	mov.b32 	%f761, %r118;
+	mul.f32 	%f762, %f29, %f758;
+	mul.f32 	%f763, %f29, %f759;
+	mul.f32 	%f764, %f29, %f760;
+	mul.f32 	%f765, %f29, %f761;
+	fma.rn.f32 	%f1836, %f749, %f1836, %f762;
+	fma.rn.f32 	%f1837, %f749, %f1837, %f763;
+	fma.rn.f32 	%f1838, %f749, %f1838, %f764;
+	fma.rn.f32 	%f1839, %f749, %f1839, %f765;
+	add.s64 	%rd116, %rd97, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd115, %rd116;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r119,%r120,%r121,%r122}, [%rd115];
+	// end inline asm
+	mov.b32 	%f766, %r119;
+	mov.b32 	%f767, %r120;
+	mov.b32 	%f768, %r121;
+	mov.b32 	%f769, %r122;
+	mul.f32 	%f770, %f29, %f766;
+	mul.f32 	%f771, %f29, %f767;
+	mul.f32 	%f772, %f29, %f768;
+	mul.f32 	%f773, %f29, %f769;
+	fma.rn.f32 	%f1840, %f749, %f1840, %f770;
+	fma.rn.f32 	%f774, %f749, %f1841, %f771;
+	fma.rn.f32 	%f775, %f749, %f1842, %f772;
+	fma.rn.f32 	%f776, %f749, %f1843, %f773;
+	add.s64 	%rd119, %rd97, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd118, %rd119;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r123,%r124,%r125,%r126}, [%rd118];
+	// end inline asm
+	mov.b32 	%f777, %r123;
+	mov.b32 	%f778, %r124;
+	mov.b32 	%f779, %r125;
+	mov.b32 	%f780, %r126;
+	mul.f32 	%f781, %f29, %f777;
+	mul.f32 	%f782, %f29, %f778;
+	mul.f32 	%f783, %f29, %f779;
+	mul.f32 	%f784, %f29, %f780;
+	fma.rn.f32 	%f785, %f749, %f1844, %f781;
+	fma.rn.f32 	%f1845, %f749, %f1845, %f782;
+	fma.rn.f32 	%f1846, %f749, %f1846, %f783;
+	fma.rn.f32 	%f1847, %f749, %f1847, %f784;
+	mul.f32 	%f786, %f775, %f775;
+	fma.rn.f32 	%f787, %f774, %f774, %f786;
+	fma.rn.f32 	%f788, %f776, %f776, %f787;
+	fma.rn.f32 	%f789, %f785, %f785, %f788;
+	sqrt.rn.f32 	%f790, %f789;
+	rcp.rn.f32 	%f791, %f790;
+	mul.f32 	%f1841, %f774, %f791;
+	mul.f32 	%f1842, %f775, %f791;
+	mul.f32 	%f1843, %f776, %f791;
+	mul.f32 	%f1844, %f791, %f785;
+
+$L__BB3_14:
+	mul.f32 	%f792, %f1842, %f1842;
+	fma.rn.f32 	%f793, %f1841, %f1841, %f792;
+	fma.rn.f32 	%f794, %f1843, %f1843, %f793;
+	fma.rn.f32 	%f795, %f1844, %f1844, %f794;
+	rcp.rn.f32 	%f796, %f795;
+	mul.f32 	%f797, %f1841, %f796;
+	mul.f32 	%f798, %f1842, %f796;
+	mul.f32 	%f799, %f1843, %f796;
+	mul.f32 	%f800, %f1844, %f796;
+	mul.f32 	%f801, %f1841, %f797;
+	mul.f32 	%f802, %f1842, %f798;
+	mul.f32 	%f803, %f1843, %f799;
+	mul.f32 	%f804, %f1841, %f798;
+	mul.f32 	%f805, %f1843, %f800;
+	mul.f32 	%f806, %f1841, %f799;
+	mul.f32 	%f807, %f1842, %f800;
+	mul.f32 	%f808, %f1842, %f799;
+	mul.f32 	%f809, %f1841, %f800;
+	sub.f32 	%f810, %f801, %f802;
+	sub.f32 	%f811, %f810, %f803;
+	fma.rn.f32 	%f812, %f1844, %f800, %f811;
+	sub.f32 	%f813, %f804, %f805;
+	add.f32 	%f814, %f813, %f813;
+	add.f32 	%f815, %f806, %f807;
+	add.f32 	%f816, %f815, %f815;
+	add.f32 	%f817, %f804, %f805;
+	add.f32 	%f818, %f817, %f817;
+	sub.f32 	%f819, %f802, %f801;
+	sub.f32 	%f820, %f819, %f803;
+	fma.rn.f32 	%f821, %f1844, %f800, %f820;
+	sub.f32 	%f822, %f808, %f809;
+	add.f32 	%f823, %f822, %f822;
+	sub.f32 	%f824, %f806, %f807;
+	add.f32 	%f825, %f824, %f824;
+	add.f32 	%f826, %f808, %f809;
+	add.f32 	%f827, %f826, %f826;
+	neg.f32 	%f828, %f801;
+	sub.f32 	%f829, %f828, %f802;
+	add.f32 	%f830, %f803, %f829;
+	fma.rn.f32 	%f831, %f1844, %f800, %f830;
+	mul.f32 	%f832, %f1835, %f812;
+	fma.rn.f32 	%f833, %f1838, %f814, %f832;
+	fma.rn.f32 	%f834, %f1840, %f816, %f833;
+	sub.f32 	%f1848, %f1845, %f834;
+	mul.f32 	%f835, %f1838, %f821;
+	fma.rn.f32 	%f836, %f1835, %f818, %f835;
+	fma.rn.f32 	%f837, %f1840, %f823, %f836;
+	sub.f32 	%f1852, %f1846, %f837;
+	mul.f32 	%f838, %f1838, %f827;
+	fma.rn.f32 	%f839, %f1835, %f825, %f838;
+	fma.rn.f32 	%f840, %f1840, %f831, %f839;
+	sub.f32 	%f1856, %f1847, %f840;
+	mul.f32 	%f841, %f1834, %f812;
+	fma.rn.f32 	%f842, %f1837, %f814, %f841;
+	fma.rn.f32 	%f1849, %f1839, %f816, %f842;
+	mul.f32 	%f843, %f1837, %f821;
+	fma.rn.f32 	%f844, %f1834, %f818, %f843;
+	fma.rn.f32 	%f1853, %f1839, %f823, %f844;
+	mul.f32 	%f845, %f1837, %f827;
+	fma.rn.f32 	%f846, %f1834, %f825, %f845;
+	fma.rn.f32 	%f1857, %f1839, %f831, %f846;
+	mul.f32 	%f847, %f1833, %f812;
+	fma.rn.f32 	%f1850, %f1836, %f814, %f847;
+	mul.f32 	%f848, %f1836, %f821;
+	fma.rn.f32 	%f1854, %f1833, %f818, %f848;
+	mul.f32 	%f849, %f1836, %f827;
+	fma.rn.f32 	%f1858, %f1833, %f825, %f849;
+	mul.f32 	%f1851, %f1832, %f812;
+	mul.f32 	%f1855, %f1832, %f818;
+	mul.f32 	%f1859, %f1832, %f825;
+
+$L__BB3_17:
+	mul.f32 	%f887, %f1853, %f1858;
+	mul.f32 	%f888, %f1854, %f1857;
+	sub.f32 	%f889, %f888, %f887;
+	mul.f32 	%f890, %f1851, %f889;
+	mul.f32 	%f891, %f1853, %f1859;
+	mul.f32 	%f892, %f1855, %f1857;
+	sub.f32 	%f893, %f892, %f891;
+	mul.f32 	%f894, %f1850, %f893;
+	sub.f32 	%f895, %f890, %f894;
+	mul.f32 	%f896, %f1854, %f1859;
+	mul.f32 	%f897, %f1855, %f1858;
+	sub.f32 	%f898, %f897, %f896;
+	fma.rn.f32 	%f899, %f1849, %f898, %f895;
+	rcp.rn.f32 	%f900, %f899;
+	mul.f32 	%f1863, %f889, %f900;
+	mul.f32 	%f901, %f1850, %f1857;
+	mul.f32 	%f902, %f1849, %f1858;
+	sub.f32 	%f903, %f902, %f901;
+	mul.f32 	%f1862, %f903, %f900;
+	mul.f32 	%f904, %f1849, %f1854;
+	mul.f32 	%f905, %f1850, %f1853;
+	sub.f32 	%f906, %f905, %f904;
+	mul.f32 	%f1861, %f906, %f900;
+	sub.f32 	%f907, %f891, %f892;
+	mul.f32 	%f1867, %f907, %f900;
+	mul.f32 	%f908, %f1849, %f1859;
+	mul.f32 	%f909, %f1851, %f1857;
+	sub.f32 	%f910, %f909, %f908;
+	mul.f32 	%f1866, %f910, %f900;
+	mul.f32 	%f911, %f1851, %f1853;
+	mul.f32 	%f912, %f1849, %f1855;
+	sub.f32 	%f913, %f912, %f911;
+	mul.f32 	%f1865, %f913, %f900;
+	mul.f32 	%f1871, %f898, %f900;
+	mul.f32 	%f914, %f1851, %f1858;
+	mul.f32 	%f915, %f1850, %f1859;
+	sub.f32 	%f916, %f915, %f914;
+	mul.f32 	%f1870, %f916, %f900;
+	mul.f32 	%f917, %f1850, %f1855;
+	mul.f32 	%f918, %f1851, %f1854;
+	sub.f32 	%f919, %f918, %f917;
+	mul.f32 	%f1869, %f919, %f900;
+	mul.f32 	%f920, %f1848, %f1863;
+	neg.f32 	%f921, %f920;
+	mul.f32 	%f922, %f1852, %f1862;
+	sub.f32 	%f923, %f921, %f922;
+	mul.f32 	%f924, %f1856, %f1861;
+	sub.f32 	%f1860, %f923, %f924;
+	mul.f32 	%f925, %f1848, %f1867;
+	neg.f32 	%f926, %f925;
+	mul.f32 	%f927, %f1852, %f1866;
+	sub.f32 	%f928, %f926, %f927;
+	mul.f32 	%f929, %f1856, %f1865;
+	sub.f32 	%f1864, %f928, %f929;
+	mul.f32 	%f930, %f1848, %f1871;
+	neg.f32 	%f931, %f930;
+	mul.f32 	%f932, %f1852, %f1870;
+	sub.f32 	%f933, %f931, %f932;
+	mul.f32 	%f934, %f1856, %f1869;
+	sub.f32 	%f1868, %f933, %f934;
+	bra.uni 	$L__BB3_18;
+
+$L__BB3_9:
+	// begin inline asm
+	call (%rd654), _optix_get_instance_inverse_transform_from_handle, (%rd49);
+	// end inline asm
+
+$L__BB3_10:
+	// begin inline asm
+	cvta.to.global.u64 %rd55, %rd654;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r40,%r41,%r42,%r43}, [%rd55];
+	// end inline asm
+	mov.b32 	%f1863, %r40;
+	mov.b32 	%f1862, %r41;
+	mov.b32 	%f1861, %r42;
+	mov.b32 	%f1860, %r43;
+	add.s64 	%rd59, %rd654, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd58, %rd59;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r44,%r45,%r46,%r47}, [%rd58];
+	// end inline asm
+	mov.b32 	%f1867, %r44;
+	mov.b32 	%f1866, %r45;
+	mov.b32 	%f1865, %r46;
+	mov.b32 	%f1864, %r47;
+	add.s64 	%rd62, %rd654, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd61, %rd62;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r48,%r49,%r50,%r51}, [%rd61];
+	// end inline asm
+	mov.b32 	%f1871, %r48;
+	mov.b32 	%f1870, %r49;
+	mov.b32 	%f1869, %r50;
+	mov.b32 	%f1868, %r51;
+
+$L__BB3_18:
+	setp.eq.s32 	%p10, %r655, 0;
+	@%p10 bra 	$L__BB3_20;
+
+	mul.f32 	%f935, %f1828, %f1863;
+	fma.rn.f32 	%f936, %f1824, %f1862, %f935;
+	fma.rn.f32 	%f151, %f1820, %f1861, %f936;
+	mul.f32 	%f937, %f1829, %f1863;
+	fma.rn.f32 	%f938, %f1825, %f1862, %f937;
+	fma.rn.f32 	%f152, %f1821, %f1861, %f938;
+	mul.f32 	%f939, %f1830, %f1863;
+	fma.rn.f32 	%f940, %f1826, %f1862, %f939;
+	fma.rn.f32 	%f153, %f1822, %f1861, %f940;
+	mul.f32 	%f941, %f1831, %f1863;
+	fma.rn.f32 	%f942, %f1827, %f1862, %f941;
+	fma.rn.f32 	%f943, %f1823, %f1861, %f942;
+	add.f32 	%f1860, %f1860, %f943;
+	mul.f32 	%f944, %f1828, %f1867;
+	fma.rn.f32 	%f945, %f1824, %f1866, %f944;
+	fma.rn.f32 	%f155, %f1820, %f1865, %f945;
+	mul.f32 	%f946, %f1829, %f1867;
+	fma.rn.f32 	%f947, %f1825, %f1866, %f946;
+	fma.rn.f32 	%f156, %f1821, %f1865, %f947;
+	mul.f32 	%f948, %f1830, %f1867;
+	fma.rn.f32 	%f949, %f1826, %f1866, %f948;
+	fma.rn.f32 	%f157, %f1822, %f1865, %f949;
+	mul.f32 	%f950, %f1831, %f1867;
+	fma.rn.f32 	%f951, %f1827, %f1866, %f950;
+	fma.rn.f32 	%f952, %f1823, %f1865, %f951;
+	add.f32 	%f1864, %f1864, %f952;
+	mul.f32 	%f953, %f1828, %f1871;
+	fma.rn.f32 	%f954, %f1824, %f1870, %f953;
+	fma.rn.f32 	%f159, %f1820, %f1869, %f954;
+	mul.f32 	%f955, %f1829, %f1871;
+	fma.rn.f32 	%f956, %f1825, %f1870, %f955;
+	fma.rn.f32 	%f160, %f1821, %f1869, %f956;
+	mul.f32 	%f957, %f1830, %f1871;
+	fma.rn.f32 	%f958, %f1826, %f1870, %f957;
+	fma.rn.f32 	%f161, %f1822, %f1869, %f958;
+	mul.f32 	%f959, %f1831, %f1871;
+	fma.rn.f32 	%f960, %f1827, %f1870, %f959;
+	fma.rn.f32 	%f961, %f1823, %f1869, %f960;
+	add.f32 	%f1868, %f1868, %f961;
+	mov.f32 	%f1861, %f153;
+	mov.f32 	%f1862, %f152;
+	mov.f32 	%f1863, %f151;
+	mov.f32 	%f1865, %f157;
+	mov.f32 	%f1866, %f156;
+	mov.f32 	%f1867, %f155;
+	mov.f32 	%f1869, %f161;
+	mov.f32 	%f1870, %f160;
+	mov.f32 	%f1871, %f159;
+
+$L__BB3_20:
+	add.s32 	%r655, %r655, 1;
+	setp.lt.u32 	%p11, %r655, %r35;
+	mov.f32 	%f1820, %f1871;
+	mov.f32 	%f1821, %f1870;
+	mov.f32 	%f1822, %f1869;
+	mov.f32 	%f1823, %f1868;
+	mov.f32 	%f1824, %f1867;
+	mov.f32 	%f1825, %f1866;
+	mov.f32 	%f1826, %f1865;
+	mov.f32 	%f1827, %f1864;
+	mov.f32 	%f1828, %f1863;
+	mov.f32 	%f1829, %f1862;
+	mov.f32 	%f1830, %f1861;
+	mov.f32 	%f1831, %f1860;
+	@%p11 bra 	$L__BB3_5;
+
+$L__BB3_21:
+	mul.f32 	%f962, %f1896, %f1863;
+	fma.rn.f32 	%f963, %f1897, %f1862, %f962;
+	fma.rn.f32 	%f964, %f1898, %f1861, %f963;
+	mul.f32 	%f965, %f1896, %f1867;
+	fma.rn.f32 	%f966, %f1897, %f1866, %f965;
+	fma.rn.f32 	%f967, %f1898, %f1865, %f966;
+	mul.f32 	%f968, %f1896, %f1871;
+	fma.rn.f32 	%f969, %f1897, %f1870, %f968;
+	fma.rn.f32 	%f970, %f1898, %f1869, %f969;
+	add.f32 	%f1898, %f1868, %f970;
+	add.f32 	%f1897, %f1864, %f967;
+	add.f32 	%f1896, %f1860, %f964;
+
+$L__BB3_23:
+	// begin inline asm
+	call (%f1954), _optix_get_world_ray_direction_x, ();
+	// end inline asm
+	// begin inline asm
+	call (%f1955), _optix_get_world_ray_direction_y, ();
+	// end inline asm
+	// begin inline asm
+	call (%f973), _optix_get_world_ray_direction_z, ();
+	// end inline asm
+	// begin inline asm
+	call (%r186), _optix_get_transform_list_size, ();
+	// end inline asm
+	setp.eq.s32 	%p12, %r186, 0;
+	@%p12 bra 	$L__BB3_43;
+
+	// begin inline asm
+	call (%r187), _optix_get_transform_list_size, ();
+	// end inline asm
+	// begin inline asm
+	call (%f974), _optix_get_ray_time, ();
+	// end inline asm
+	setp.eq.s32 	%p13, %r187, 0;
+	@%p13 bra 	$L__BB3_42;
+
+	mov.u32 	%r656, 0;
+
+$L__BB3_26:
 	.pragma "nounroll";
-	// inline asm
-	call (%rd174), _optix_get_transform_list_handle, (%r647);
-	// inline asm
-	// inline asm
-	call (%r183), _optix_get_transform_type_from_handle, (%rd174);
-	// inline asm
-	and.b32  	%r184, %r183, -2;
-	setp.eq.s32	%p12, %r184, 2;
-	@%p12 bra 	BB3_31;
-	bra.uni 	BB3_26;
-
-BB3_31:
-	setp.eq.s32	%p15, %r183, 2;
-	@%p15 bra 	BB3_35;
-	bra.uni 	BB3_32;
-
-BB3_35:
-	// inline asm
-	call (%rd248), _optix_get_matrix_motion_transform_from_handle, (%rd174);
-	// inline asm
-	// inline asm
-	cvta.to.global.u64 %rd250, %rd248;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r272,%r273,%r274,%r275}, [%rd250];
-	// inline asm
-	mov.b32	{%rs8, %rs9}, %r274;
-	add.s64 	%rd254, %rd248, 16;
-	// inline asm
-	cvta.to.global.u64 %rd253, %rd254;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r276,%r277,%r278,%r279}, [%rd253];
-	// inline asm
-	add.s64 	%rd257, %rd248, 32;
-	// inline asm
-	cvta.to.global.u64 %rd256, %rd257;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r280,%r281,%r282,%r283}, [%rd256];
-	// inline asm
-	add.s64 	%rd260, %rd248, 48;
-	// inline asm
-	cvta.to.global.u64 %rd259, %rd260;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r284,%r285,%r286,%r287}, [%rd259];
-	// inline asm
-	add.s64 	%rd263, %rd248, 64;
-	// inline asm
-	cvta.to.global.u64 %rd262, %rd263;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r288,%r289,%r290,%r291}, [%rd262];
-	// inline asm
-	add.s64 	%rd266, %rd248, 80;
-	// inline asm
-	cvta.to.global.u64 %rd265, %rd266;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r292,%r293,%r294,%r295}, [%rd265];
-	// inline asm
-	add.s64 	%rd269, %rd248, 96;
-	// inline asm
-	cvta.to.global.u64 %rd268, %rd269;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r296,%r297,%r298,%r299}, [%rd268];
-	// inline asm
-	add.s64 	%rd272, %rd248, 112;
-	// inline asm
-	cvta.to.global.u64 %rd271, %rd272;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r300,%r301,%r302,%r303}, [%rd271];
-	// inline asm
-	mov.b32 	 %f1041, %r275;
-	mov.b32 	 %f1042, %r276;
-	cvt.u32.u16	%r316, %rs8;
-	add.s32 	%r317, %r316, -1;
-	cvt.rn.f32.s32	%f1043, %r317;
-	sub.f32 	%f1044, %f938, %f1041;
-	mul.f32 	%f1045, %f1044, %f1043;
-	sub.f32 	%f1046, %f1042, %f1041;
-	div.rn.f32 	%f1047, %f1045, %f1046;
-	min.f32 	%f1048, %f1043, %f1047;
-	mov.f32 	%f1049, 0f00000000;
-	max.f32 	%f1050, %f1049, %f1048;
-	cvt.rmi.f32.f32	%f1051, %f1050;
-	cvt.rzi.s32.f32	%r318, %f1051;
-	cvt.s64.s32	%rd19, %r318;
-	mul.wide.s32 	%rd283, %r318, 48;
-	add.s64 	%rd275, %rd257, %rd283;
-	// inline asm
-	cvta.to.global.u64 %rd274, %rd275;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r304,%r305,%r306,%r307}, [%rd274];
-	// inline asm
-	mov.b32 	 %f1881, %r304;
-	mov.b32 	 %f1882, %r305;
-	mov.b32 	 %f1883, %r306;
-	add.s64 	%rd278, %rd275, 16;
-	// inline asm
-	cvta.to.global.u64 %rd277, %rd278;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r308,%r309,%r310,%r311}, [%rd277];
-	// inline asm
-	mov.b32 	 %f1878, %r308;
-	mov.b32 	 %f1879, %r309;
-	mov.b32 	 %f1880, %r310;
-	add.s64 	%rd281, %rd275, 32;
-	// inline asm
-	cvta.to.global.u64 %rd280, %rd281;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r312,%r313,%r314,%r315}, [%rd280];
-	// inline asm
-	sub.f32 	%f249, %f1050, %f1051;
-	mov.b32 	 %f1875, %r312;
-	mov.b32 	 %f1876, %r313;
-	mov.b32 	 %f1877, %r314;
-	setp.leu.f32	%p17, %f249, 0f00000000;
-	@%p17 bra 	BB3_37;
-
-	mul.lo.s64 	%rd293, %rd19, 48;
-	add.s64 	%rd294, %rd248, %rd293;
-	add.s64 	%rd285, %rd294, 80;
-	// inline asm
-	cvta.to.global.u64 %rd284, %rd285;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r319,%r320,%r321,%r322}, [%rd284];
-	// inline asm
-	mov.b32 	 %f1052, %r319;
-	mov.b32 	 %f1053, %r320;
-	mov.b32 	 %f1054, %r321;
-	add.s64 	%rd288, %rd294, 96;
-	// inline asm
-	cvta.to.global.u64 %rd287, %rd288;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r323,%r324,%r325,%r326}, [%rd287];
-	// inline asm
-	mov.b32 	 %f1055, %r323;
-	mov.b32 	 %f1056, %r324;
-	mov.b32 	 %f1057, %r325;
-	add.s64 	%rd291, %rd294, 112;
-	// inline asm
-	cvta.to.global.u64 %rd290, %rd291;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r327,%r328,%r329,%r330}, [%rd290];
-	// inline asm
-	mov.f32 	%f1058, 0f3F800000;
-	sub.f32 	%f1059, %f1058, %f249;
-	mul.f32 	%f1060, %f249, %f1052;
-	mul.f32 	%f1061, %f249, %f1053;
-	mul.f32 	%f1062, %f249, %f1054;
-	fma.rn.f32 	%f1881, %f1059, %f1881, %f1060;
-	fma.rn.f32 	%f1882, %f1059, %f1882, %f1061;
-	fma.rn.f32 	%f1883, %f1059, %f1883, %f1062;
-	mul.f32 	%f1063, %f249, %f1055;
-	mul.f32 	%f1064, %f249, %f1056;
-	mul.f32 	%f1065, %f249, %f1057;
-	fma.rn.f32 	%f1878, %f1059, %f1878, %f1063;
-	fma.rn.f32 	%f1879, %f1059, %f1879, %f1064;
-	fma.rn.f32 	%f1880, %f1059, %f1880, %f1065;
-	mov.b32 	 %f1066, %r327;
-	mov.b32 	 %f1067, %r328;
-	mov.b32 	 %f1068, %r329;
-	mul.f32 	%f1069, %f249, %f1066;
-	mul.f32 	%f1070, %f249, %f1067;
-	mul.f32 	%f1071, %f249, %f1068;
-	fma.rn.f32 	%f1875, %f1059, %f1875, %f1069;
-	fma.rn.f32 	%f1876, %f1059, %f1876, %f1070;
-	fma.rn.f32 	%f1877, %f1059, %f1877, %f1071;
-	bra.uni 	BB3_37;
-
-BB3_26:
-	mov.f32 	%f1884, 0f00000000;
-	mov.f32 	%f1886, 0f3F800000;
-	setp.eq.s32	%p13, %r183, 4;
-	@%p13 bra 	BB3_29;
-	bra.uni 	BB3_27;
-
-BB3_29:
-	// inline asm
-	call (%rd668), _optix_get_instance_inverse_transform_from_handle, (%rd174);
-	// inline asm
-	bra.uni 	BB3_30;
-
-BB3_32:
-	// inline asm
-	call (%rd189), _optix_get_srt_motion_transform_from_handle, (%rd174);
-	// inline asm
-	// inline asm
-	cvta.to.global.u64 %rd191, %rd189;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r197,%r198,%r199,%r200}, [%rd191];
-	// inline asm
-	mov.b32	{%rs6, %rs7}, %r199;
-	add.s64 	%rd195, %rd189, 16;
-	// inline asm
+	// begin inline asm
+	call (%rd168), _optix_get_transform_list_handle, (%r656);
+	// end inline asm
+	// begin inline asm
+	call (%r190), _optix_get_transform_type_from_handle, (%rd168);
+	// end inline asm
+	or.b32  	%r191, %r190, 1;
+	setp.eq.s32 	%p14, %r191, 3;
+	@%p14 bra 	$L__BB3_32;
+	bra.uni 	$L__BB3_27;
+
+$L__BB3_32:
+	setp.eq.s32 	%p17, %r190, 2;
+	@%p17 bra 	$L__BB3_36;
+	bra.uni 	$L__BB3_33;
+
+$L__BB3_36:
+	// begin inline asm
+	call (%rd240), _optix_get_matrix_motion_transform_from_handle, (%rd168);
+	// end inline asm
+	// begin inline asm
+	cvta.to.global.u64 %rd242, %rd240;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r279,%r280,%r281,%r282}, [%rd242];
+	// end inline asm
+	add.s64 	%rd246, %rd240, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd245, %rd246;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r283,%r284,%r285,%r286}, [%rd245];
+	// end inline asm
+	add.s64 	%rd249, %rd240, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd248, %rd249;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r287,%r288,%r289,%r290}, [%rd248];
+	// end inline asm
+	add.s64 	%rd252, %rd240, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd251, %rd252;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r291,%r292,%r293,%r294}, [%rd251];
+	// end inline asm
+	add.s64 	%rd255, %rd240, 64;
+	// begin inline asm
+	cvta.to.global.u64 %rd254, %rd255;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r295,%r296,%r297,%r298}, [%rd254];
+	// end inline asm
+	add.s64 	%rd258, %rd240, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd257, %rd258;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r299,%r300,%r301,%r302}, [%rd257];
+	// end inline asm
+	add.s64 	%rd261, %rd240, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd260, %rd261;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r303,%r304,%r305,%r306}, [%rd260];
+	// end inline asm
+	add.s64 	%rd264, %rd240, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd263, %rd264;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r307,%r308,%r309,%r310}, [%rd263];
+	// end inline asm
+	mov.b32 	%f1078, %r282;
+	mov.b32 	%f1079, %r283;
+	and.b32  	%r323, %r281, 65535;
+	add.s32 	%r324, %r323, -1;
+	cvt.rn.f32.s32 	%f1080, %r324;
+	sub.f32 	%f1081, %f974, %f1078;
+	mul.f32 	%f1082, %f1081, %f1080;
+	sub.f32 	%f1083, %f1079, %f1078;
+	div.rn.f32 	%f1084, %f1082, %f1083;
+	min.f32 	%f1085, %f1080, %f1084;
+	mov.f32 	%f1086, 0f00000000;
+	max.f32 	%f1087, %f1086, %f1085;
+	cvt.rmi.f32.f32 	%f1088, %f1087;
+	sub.f32 	%f258, %f1087, %f1088;
+	cvt.rzi.s32.f32 	%r325, %f1088;
+	cvt.s64.s32 	%rd17, %r325;
+	mul.wide.s32 	%rd275, %r325, 48;
+	add.s64 	%rd267, %rd249, %rd275;
+	// begin inline asm
+	cvta.to.global.u64 %rd266, %rd267;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r311,%r312,%r313,%r314}, [%rd266];
+	// end inline asm
+	mov.b32 	%f1924, %r311;
+	mov.b32 	%f1925, %r312;
+	mov.b32 	%f1926, %r313;
+	add.s64 	%rd270, %rd267, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd269, %rd270;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r315,%r316,%r317,%r318}, [%rd269];
+	// end inline asm
+	mov.b32 	%f1921, %r315;
+	mov.b32 	%f1922, %r316;
+	mov.b32 	%f1923, %r317;
+	add.s64 	%rd273, %rd267, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd272, %rd273;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r319,%r320,%r321,%r322}, [%rd272];
+	// end inline asm
+	mov.b32 	%f1918, %r319;
+	mov.b32 	%f1919, %r320;
+	mov.b32 	%f1920, %r321;
+	setp.leu.f32 	%p19, %f258, 0f00000000;
+	@%p19 bra 	$L__BB3_38;
+
+	mov.f32 	%f1089, 0f3F800000;
+	sub.f32 	%f1090, %f1089, %f258;
+	mul.lo.s64 	%rd285, %rd17, 48;
+	add.s64 	%rd286, %rd240, %rd285;
+	add.s64 	%rd277, %rd286, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd276, %rd277;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r326,%r327,%r328,%r329}, [%rd276];
+	// end inline asm
+	mov.b32 	%f1091, %r326;
+	mov.b32 	%f1092, %r327;
+	mov.b32 	%f1093, %r328;
+	mul.f32 	%f1094, %f258, %f1091;
+	mul.f32 	%f1095, %f258, %f1092;
+	mul.f32 	%f1096, %f258, %f1093;
+	fma.rn.f32 	%f1924, %f1090, %f1924, %f1094;
+	fma.rn.f32 	%f1925, %f1090, %f1925, %f1095;
+	fma.rn.f32 	%f1926, %f1090, %f1926, %f1096;
+	add.s64 	%rd280, %rd286, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd279, %rd280;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r330,%r331,%r332,%r333}, [%rd279];
+	// end inline asm
+	mov.b32 	%f1097, %r330;
+	mov.b32 	%f1098, %r331;
+	mov.b32 	%f1099, %r332;
+	mul.f32 	%f1100, %f258, %f1097;
+	mul.f32 	%f1101, %f258, %f1098;
+	mul.f32 	%f1102, %f258, %f1099;
+	fma.rn.f32 	%f1921, %f1090, %f1921, %f1100;
+	fma.rn.f32 	%f1922, %f1090, %f1922, %f1101;
+	fma.rn.f32 	%f1923, %f1090, %f1923, %f1102;
+	add.s64 	%rd283, %rd286, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd282, %rd283;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r334,%r335,%r336,%r337}, [%rd282];
+	// end inline asm
+	mov.b32 	%f1103, %r334;
+	mov.b32 	%f1104, %r335;
+	mov.b32 	%f1105, %r336;
+	mul.f32 	%f1106, %f258, %f1103;
+	mul.f32 	%f1107, %f258, %f1104;
+	mul.f32 	%f1108, %f258, %f1105;
+	fma.rn.f32 	%f1918, %f1090, %f1918, %f1106;
+	fma.rn.f32 	%f1919, %f1090, %f1919, %f1107;
+	fma.rn.f32 	%f1920, %f1090, %f1920, %f1108;
+	bra.uni 	$L__BB3_38;
+
+$L__BB3_27:
+	mov.f32 	%f1927, 0f00000000;
+	mov.f32 	%f1929, 0f3F800000;
+	setp.eq.s32 	%p15, %r190, 4;
+	@%p15 bra 	$L__BB3_30;
+
+	setp.ne.s32 	%p16, %r190, 1;
+	mov.f32 	%f1928, %f1927;
+	mov.f32 	%f1930, %f1927;
+	mov.f32 	%f1931, %f1929;
+	mov.f32 	%f1932, %f1927;
+	mov.f32 	%f1933, %f1929;
+	mov.f32 	%f1934, %f1927;
+	mov.f32 	%f1935, %f1927;
+	@%p16 bra 	$L__BB3_39;
+
+	// begin inline asm
+	call (%rd170), _optix_get_static_transform_from_handle, (%rd168);
+	// end inline asm
+	add.s64 	%rd655, %rd170, 64;
+	bra.uni 	$L__BB3_31;
+
+$L__BB3_33:
+	// begin inline asm
+	call (%rd183), _optix_get_srt_motion_transform_from_handle, (%rd168);
+	// end inline asm
+	// begin inline asm
+	cvta.to.global.u64 %rd185, %rd183;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r204,%r205,%r206,%r207}, [%rd185];
+	// end inline asm
+	add.s64 	%rd189, %rd183, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd188, %rd189;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r208,%r209,%r210,%r211}, [%rd188];
+	// end inline asm
+	add.s64 	%rd192, %rd183, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd191, %rd192;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r212,%r213,%r214,%r215}, [%rd191];
+	// end inline asm
+	add.s64 	%rd195, %rd183, 48;
+	// begin inline asm
 	cvta.to.global.u64 %rd194, %rd195;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r201,%r202,%r203,%r204}, [%rd194];
-	// inline asm
-	add.s64 	%rd198, %rd189, 32;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r216,%r217,%r218,%r219}, [%rd194];
+	// end inline asm
+	add.s64 	%rd198, %rd183, 64;
+	// begin inline asm
 	cvta.to.global.u64 %rd197, %rd198;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r205,%r206,%r207,%r208}, [%rd197];
-	// inline asm
-	add.s64 	%rd201, %rd189, 48;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r220,%r221,%r222,%r223}, [%rd197];
+	// end inline asm
+	add.s64 	%rd201, %rd183, 80;
+	// begin inline asm
 	cvta.to.global.u64 %rd200, %rd201;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r209,%r210,%r211,%r212}, [%rd200];
-	// inline asm
-	add.s64 	%rd204, %rd189, 64;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r224,%r225,%r226,%r227}, [%rd200];
+	// end inline asm
+	add.s64 	%rd204, %rd183, 96;
+	// begin inline asm
 	cvta.to.global.u64 %rd203, %rd204;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r213,%r214,%r215,%r216}, [%rd203];
-	// inline asm
-	add.s64 	%rd207, %rd189, 80;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r228,%r229,%r230,%r231}, [%rd203];
+	// end inline asm
+	add.s64 	%rd207, %rd183, 112;
+	// begin inline asm
 	cvta.to.global.u64 %rd206, %rd207;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r217,%r218,%r219,%r220}, [%rd206];
-	// inline asm
-	add.s64 	%rd210, %rd189, 96;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r232,%r233,%r234,%r235}, [%rd206];
+	// end inline asm
+	add.s64 	%rd210, %rd183, 128;
+	// begin inline asm
 	cvta.to.global.u64 %rd209, %rd210;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r221,%r222,%r223,%r224}, [%rd209];
-	// inline asm
-	add.s64 	%rd213, %rd189, 112;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r236,%r237,%r238,%r239}, [%rd209];
+	// end inline asm
+	add.s64 	%rd213, %rd183, 144;
+	// begin inline asm
 	cvta.to.global.u64 %rd212, %rd213;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r225,%r226,%r227,%r228}, [%rd212];
-	// inline asm
-	add.s64 	%rd216, %rd189, 128;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r240,%r241,%r242,%r243}, [%rd212];
+	// end inline asm
+	mov.b32 	%f986, %r207;
+	mov.b32 	%f987, %r208;
+	and.b32  	%r260, %r206, 65535;
+	add.s32 	%r261, %r260, -1;
+	cvt.rn.f32.s32 	%f988, %r261;
+	sub.f32 	%f989, %f974, %f986;
+	mul.f32 	%f990, %f989, %f988;
+	sub.f32 	%f991, %f987, %f986;
+	div.rn.f32 	%f992, %f990, %f991;
+	min.f32 	%f993, %f988, %f992;
+	mov.f32 	%f994, 0f00000000;
+	max.f32 	%f995, %f994, %f993;
+	cvt.rmi.f32.f32 	%f996, %f995;
+	sub.f32 	%f218, %f995, %f996;
+	cvt.rzi.s32.f32 	%r262, %f996;
+	mul.wide.s32 	%rd227, %r262, 64;
+	add.s64 	%rd216, %rd192, %rd227;
+	// begin inline asm
 	cvta.to.global.u64 %rd215, %rd216;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r229,%r230,%r231,%r232}, [%rd215];
-	// inline asm
-	add.s64 	%rd219, %rd189, 144;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r244,%r245,%r246,%r247}, [%rd215];
+	// end inline asm
+	mov.b32 	%f1908, %r244;
+	mov.b32 	%f1909, %r245;
+	mov.b32 	%f1910, %r246;
+	add.s64 	%rd219, %rd216, 16;
+	// begin inline asm
 	cvta.to.global.u64 %rd218, %rd219;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r233,%r234,%r235,%r236}, [%rd218];
-	// inline asm
-	mov.b32 	 %f949, %r200;
-	mov.b32 	 %f950, %r201;
-	cvt.u32.u16	%r253, %rs6;
-	add.s32 	%r254, %r253, -1;
-	cvt.rn.f32.s32	%f951, %r254;
-	sub.f32 	%f952, %f938, %f949;
-	mul.f32 	%f953, %f952, %f951;
-	sub.f32 	%f954, %f950, %f949;
-	div.rn.f32 	%f955, %f953, %f954;
-	min.f32 	%f956, %f951, %f955;
-	mov.f32 	%f957, 0f00000000;
-	max.f32 	%f958, %f957, %f956;
-	cvt.rmi.f32.f32	%f959, %f958;
-	cvt.rzi.s32.f32	%r255, %f959;
-	cvt.s64.s32	%rd17, %r255;
-	mul.wide.s32 	%rd233, %r255, 64;
-	add.s64 	%rd222, %rd198, %rd233;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r248,%r249,%r250,%r251}, [%rd218];
+	// end inline asm
+	mov.b32 	%f1911, %r248;
+	mov.b32 	%f1912, %r249;
+	mov.b32 	%f1913, %r251;
+	add.s64 	%rd222, %rd216, 32;
+	// begin inline asm
 	cvta.to.global.u64 %rd221, %rd222;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r237,%r238,%r239,%r240}, [%rd221];
-	// inline asm
-	mov.b32 	 %f1865, %r237;
-	mov.b32 	 %f1866, %r238;
-	mov.b32 	 %f1867, %r239;
-	add.s64 	%rd225, %rd222, 16;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r252,%r253,%r254,%r255}, [%rd221];
+	// end inline asm
+	mov.b32 	%f1914, %r253;
+	mov.b32 	%f1915, %r254;
+	mov.b32 	%f1916, %r255;
+	add.s64 	%rd225, %rd216, 48;
+	// begin inline asm
 	cvta.to.global.u64 %rd224, %rd225;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r241,%r242,%r243,%r244}, [%rd224];
-	// inline asm
-	mov.b32 	 %f1868, %r241;
-	mov.b32 	 %f1869, %r242;
-	mov.b32 	 %f1870, %r244;
-	add.s64 	%rd228, %rd222, 32;
-	// inline asm
-	cvta.to.global.u64 %rd227, %rd228;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r245,%r246,%r247,%r248}, [%rd227];
-	// inline asm
-	sub.f32 	%f209, %f958, %f959;
-	mov.b32 	 %f1871, %r246;
-	mov.b32 	 %f1872, %r247;
-	mov.b32 	 %f1873, %r248;
-	add.s64 	%rd231, %rd222, 48;
-	// inline asm
-	cvta.to.global.u64 %rd230, %rd231;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r249,%r250,%r251,%r252}, [%rd230];
-	// inline asm
-	mov.b32 	 %f1874, %r249;
-	setp.leu.f32	%p16, %f209, 0f00000000;
-	@%p16 bra 	BB3_34;
-
-	shl.b64 	%rd246, %rd17, 6;
-	add.s64 	%rd247, %rd246, %rd189;
-	add.s64 	%rd235, %rd247, 96;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r256,%r257,%r258,%r259}, [%rd224];
+	// end inline asm
+	mov.b32 	%f1917, %r256;
+	setp.leu.f32 	%p18, %f218, 0f00000000;
+	@%p18 bra 	$L__BB3_35;
+
+	mov.f32 	%f997, 0f3F800000;
+	sub.f32 	%f998, %f997, %f218;
+	add.s64 	%rd229, %rd216, 64;
+	// begin inline asm
+	cvta.to.global.u64 %rd228, %rd229;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r263,%r264,%r265,%r266}, [%rd228];
+	// end inline asm
+	mov.b32 	%f999, %r263;
+	mov.b32 	%f1000, %r264;
+	mov.b32 	%f1001, %r265;
+	mul.f32 	%f1002, %f218, %f999;
+	mul.f32 	%f1003, %f218, %f1000;
+	mul.f32 	%f1004, %f218, %f1001;
+	fma.rn.f32 	%f1908, %f998, %f1908, %f1002;
+	fma.rn.f32 	%f1909, %f998, %f1909, %f1003;
+	fma.rn.f32 	%f1910, %f998, %f1910, %f1004;
+	add.s64 	%rd232, %rd216, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd231, %rd232;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r267,%r268,%r269,%r270}, [%rd231];
+	// end inline asm
+	mov.b32 	%f1005, %r267;
+	mov.b32 	%f1006, %r268;
+	mov.b32 	%f1007, %r270;
+	mul.f32 	%f1008, %f218, %f1005;
+	mul.f32 	%f1009, %f218, %f1006;
+	mul.f32 	%f1010, %f218, %f1007;
+	fma.rn.f32 	%f1911, %f998, %f1911, %f1008;
+	fma.rn.f32 	%f1912, %f998, %f1912, %f1009;
+	fma.rn.f32 	%f1913, %f998, %f1913, %f1010;
+	add.s64 	%rd235, %rd216, 96;
+	// begin inline asm
 	cvta.to.global.u64 %rd234, %rd235;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r256,%r257,%r258,%r259}, [%rd234];
-	// inline asm
-	mov.b32 	 %f960, %r256;
-	mov.b32 	 %f961, %r257;
-	mov.b32 	 %f962, %r258;
-	add.s64 	%rd238, %rd247, 112;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r271,%r272,%r273,%r274}, [%rd234];
+	// end inline asm
+	mov.b32 	%f1011, %r272;
+	mov.b32 	%f1012, %r273;
+	mov.b32 	%f1013, %r274;
+	mul.f32 	%f1014, %f218, %f1011;
+	mul.f32 	%f1015, %f218, %f1012;
+	mul.f32 	%f1016, %f218, %f1013;
+	fma.rn.f32 	%f1017, %f998, %f1914, %f1014;
+	fma.rn.f32 	%f1018, %f998, %f1915, %f1015;
+	fma.rn.f32 	%f1019, %f998, %f1916, %f1016;
+	add.s64 	%rd238, %rd216, 112;
+	// begin inline asm
 	cvta.to.global.u64 %rd237, %rd238;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r260,%r261,%r262,%r263}, [%rd237];
-	// inline asm
-	mov.b32 	 %f963, %r260;
-	mov.b32 	 %f964, %r261;
-	mov.b32 	 %f965, %r263;
-	add.s64 	%rd241, %rd247, 128;
-	// inline asm
-	cvta.to.global.u64 %rd240, %rd241;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r264,%r265,%r266,%r267}, [%rd240];
-	// inline asm
-	mov.b32 	 %f966, %r265;
-	mov.b32 	 %f967, %r266;
-	mov.b32 	 %f968, %r267;
-	add.s64 	%rd244, %rd247, 144;
-	// inline asm
-	cvta.to.global.u64 %rd243, %rd244;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r268,%r269,%r270,%r271}, [%rd243];
-	// inline asm
-	mov.f32 	%f969, 0f3F800000;
-	sub.f32 	%f970, %f969, %f209;
-	mul.f32 	%f971, %f209, %f960;
-	mul.f32 	%f972, %f209, %f961;
-	mul.f32 	%f973, %f209, %f962;
-	fma.rn.f32 	%f1865, %f970, %f1865, %f971;
-	fma.rn.f32 	%f1866, %f970, %f1866, %f972;
-	fma.rn.f32 	%f1867, %f970, %f1867, %f973;
-	mul.f32 	%f974, %f209, %f963;
-	mul.f32 	%f975, %f209, %f964;
-	mul.f32 	%f976, %f209, %f965;
-	fma.rn.f32 	%f1868, %f970, %f1868, %f974;
-	fma.rn.f32 	%f1869, %f970, %f1869, %f975;
-	fma.rn.f32 	%f1870, %f970, %f1870, %f976;
-	mul.f32 	%f977, %f209, %f966;
-	mul.f32 	%f978, %f209, %f967;
-	mul.f32 	%f979, %f209, %f968;
-	fma.rn.f32 	%f980, %f970, %f1871, %f977;
-	fma.rn.f32 	%f981, %f970, %f1872, %f978;
-	fma.rn.f32 	%f982, %f970, %f1873, %f979;
-	mov.b32 	 %f983, %r268;
-	mul.f32 	%f984, %f209, %f983;
-	fma.rn.f32 	%f985, %f970, %f1874, %f984;
-	mul.f32 	%f986, %f981, %f981;
-	fma.rn.f32 	%f987, %f980, %f980, %f986;
-	fma.rn.f32 	%f988, %f982, %f982, %f987;
-	fma.rn.f32 	%f989, %f985, %f985, %f988;
-	sqrt.rn.f32 	%f990, %f989;
-	rcp.rn.f32 	%f991, %f990;
-	mul.f32 	%f1871, %f980, %f991;
-	mul.f32 	%f1872, %f981, %f991;
-	mul.f32 	%f1873, %f982, %f991;
-	mul.f32 	%f1874, %f985, %f991;
-
-BB3_34:
-	mul.f32 	%f992, %f1872, %f1872;
-	fma.rn.f32 	%f993, %f1871, %f1871, %f992;
-	fma.rn.f32 	%f994, %f1873, %f1873, %f993;
-	fma.rn.f32 	%f995, %f1874, %f1874, %f994;
-	rcp.rn.f32 	%f996, %f995;
-	mul.f32 	%f997, %f1871, %f996;
-	mul.f32 	%f998, %f1872, %f996;
-	mul.f32 	%f999, %f1873, %f996;
-	mul.f32 	%f1000, %f1874, %f996;
-	mul.f32 	%f1001, %f1871, %f997;
-	mul.f32 	%f1002, %f1872, %f998;
-	mul.f32 	%f1003, %f1873, %f999;
-	mul.f32 	%f1004, %f1871, %f998;
-	mul.f32 	%f1005, %f1873, %f1000;
-	mul.f32 	%f1006, %f1871, %f999;
-	mul.f32 	%f1007, %f1872, %f1000;
-	mul.f32 	%f1008, %f1872, %f999;
-	mul.f32 	%f1009, %f1871, %f1000;
-	sub.f32 	%f1010, %f1001, %f1002;
-	sub.f32 	%f1011, %f1010, %f1003;
-	fma.rn.f32 	%f1012, %f1874, %f1000, %f1011;
-	sub.f32 	%f1013, %f1004, %f1005;
-	add.f32 	%f1014, %f1013, %f1013;
-	add.f32 	%f1015, %f1006, %f1007;
-	add.f32 	%f1016, %f1015, %f1015;
-	add.f32 	%f1017, %f1004, %f1005;
-	add.f32 	%f1018, %f1017, %f1017;
-	sub.f32 	%f1019, %f1002, %f1001;
-	sub.f32 	%f1020, %f1019, %f1003;
-	fma.rn.f32 	%f1021, %f1874, %f1000, %f1020;
-	sub.f32 	%f1022, %f1008, %f1009;
-	add.f32 	%f1023, %f1022, %f1022;
-	sub.f32 	%f1024, %f1006, %f1007;
-	add.f32 	%f1025, %f1024, %f1024;
-	add.f32 	%f1026, %f1008, %f1009;
-	add.f32 	%f1027, %f1026, %f1026;
-	neg.f32 	%f1028, %f1001;
-	sub.f32 	%f1029, %f1028, %f1002;
-	add.f32 	%f1030, %f1003, %f1029;
-	fma.rn.f32 	%f1031, %f1874, %f1000, %f1030;
-	mul.f32 	%f1032, %f1867, %f1012;
-	fma.rn.f32 	%f1033, %f1869, %f1014, %f1032;
-	fma.rn.f32 	%f1883, %f1870, %f1016, %f1033;
-	mul.f32 	%f1034, %f1869, %f1021;
-	fma.rn.f32 	%f1035, %f1867, %f1018, %f1034;
-	fma.rn.f32 	%f1880, %f1870, %f1023, %f1035;
-	mul.f32 	%f1036, %f1869, %f1027;
-	fma.rn.f32 	%f1037, %f1867, %f1025, %f1036;
-	fma.rn.f32 	%f1877, %f1870, %f1031, %f1037;
-	mul.f32 	%f1038, %f1866, %f1012;
-	fma.rn.f32 	%f1882, %f1868, %f1014, %f1038;
-	mul.f32 	%f1039, %f1868, %f1021;
-	fma.rn.f32 	%f1879, %f1866, %f1018, %f1039;
-	mul.f32 	%f1040, %f1868, %f1027;
-	fma.rn.f32 	%f1876, %f1866, %f1025, %f1040;
-	mul.f32 	%f1881, %f1865, %f1012;
-	mul.f32 	%f1878, %f1865, %f1018;
-	mul.f32 	%f1875, %f1865, %f1025;
-
-BB3_37:
-	mul.f32 	%f1072, %f1876, %f1880;
-	mul.f32 	%f1073, %f1877, %f1879;
-	sub.f32 	%f1074, %f1073, %f1072;
-	mul.f32 	%f1075, %f1881, %f1074;
-	mul.f32 	%f1076, %f1875, %f1880;
-	mul.f32 	%f1077, %f1877, %f1878;
-	sub.f32 	%f1078, %f1077, %f1076;
-	mul.f32 	%f1079, %f1078, %f1882;
-	sub.f32 	%f1080, %f1075, %f1079;
-	mul.f32 	%f1081, %f1875, %f1879;
-	mul.f32 	%f1082, %f1876, %f1878;
-	sub.f32 	%f1083, %f1082, %f1081;
-	fma.rn.f32 	%f1084, %f1083, %f1883, %f1080;
-	rcp.rn.f32 	%f1085, %f1084;
-	mul.f32 	%f1890, %f1074, %f1085;
-	mul.f32 	%f1086, %f1877, %f1882;
-	mul.f32 	%f1087, %f1876, %f1883;
-	sub.f32 	%f1088, %f1087, %f1086;
-	mul.f32 	%f1891, %f1085, %f1088;
-	mul.f32 	%f1089, %f1879, %f1883;
-	mul.f32 	%f1090, %f1880, %f1882;
-	sub.f32 	%f1091, %f1090, %f1089;
-	mul.f32 	%f1892, %f1085, %f1091;
-	sub.f32 	%f1092, %f1076, %f1077;
-	mul.f32 	%f1887, %f1092, %f1085;
-	mul.f32 	%f1093, %f1875, %f1883;
-	mul.f32 	%f1094, %f1877, %f1881;
-	sub.f32 	%f1095, %f1094, %f1093;
-	mul.f32 	%f1888, %f1085, %f1095;
-	mul.f32 	%f1096, %f1880, %f1881;
-	mul.f32 	%f1097, %f1878, %f1883;
-	sub.f32 	%f1098, %f1097, %f1096;
-	mul.f32 	%f1889, %f1085, %f1098;
-	mul.f32 	%f1884, %f1083, %f1085;
-	mul.f32 	%f1099, %f1876, %f1881;
-	mul.f32 	%f1100, %f1875, %f1882;
-	sub.f32 	%f1101, %f1100, %f1099;
-	mul.f32 	%f1885, %f1101, %f1085;
-	mul.f32 	%f1102, %f1878, %f1882;
-	mul.f32 	%f1103, %f1879, %f1881;
-	sub.f32 	%f1104, %f1103, %f1102;
-	mul.f32 	%f1886, %f1104, %f1085;
-	bra.uni 	BB3_38;
-
-BB3_27:
-	setp.ne.s32	%p14, %r183, 1;
-	mov.f32 	%f1885, %f1884;
-	mov.f32 	%f1887, %f1884;
-	mov.f32 	%f1888, %f1886;
-	mov.f32 	%f1889, %f1884;
-	mov.f32 	%f1890, %f1886;
-	mov.f32 	%f1891, %f1884;
-	mov.f32 	%f1892, %f1884;
-	@%p14 bra 	BB3_38;
-
-	// inline asm
-	call (%rd176), _optix_get_static_transform_from_handle, (%rd174);
-	// inline asm
-	add.s64 	%rd668, %rd176, 64;
-
-BB3_30:
-	// inline asm
-	cvta.to.global.u64 %rd180, %rd668;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r185,%r186,%r187,%r188}, [%rd180];
-	// inline asm
-	mov.b32 	 %f1890, %r185;
-	mov.b32 	 %f1891, %r186;
-	mov.b32 	 %f1892, %r187;
-	add.s64 	%rd184, %rd668, 16;
-	// inline asm
-	cvta.to.global.u64 %rd183, %rd184;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r189,%r190,%r191,%r192}, [%rd183];
-	// inline asm
-	mov.b32 	 %f1887, %r189;
-	mov.b32 	 %f1888, %r190;
-	mov.b32 	 %f1889, %r191;
-	add.s64 	%rd187, %rd668, 32;
-	// inline asm
-	cvta.to.global.u64 %rd186, %rd187;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r193,%r194,%r195,%r196}, [%rd186];
-	// inline asm
-	mov.b32 	 %f1884, %r193;
-	mov.b32 	 %f1885, %r194;
-	mov.b32 	 %f1886, %r195;
-
-BB3_38:
-	setp.eq.s32	%p18, %r647, 0;
-	@%p18 bra 	BB3_39;
-	bra.uni 	BB3_40;
-
-BB3_39:
-	mov.f32 	%f1864, %f1884;
-	mov.f32 	%f1863, %f1885;
-	mov.f32 	%f1862, %f1886;
-	mov.f32 	%f1861, %f1887;
-	mov.f32 	%f1860, %f1888;
-	mov.f32 	%f1859, %f1889;
-	mov.f32 	%f1858, %f1890;
-	mov.f32 	%f1857, %f1891;
-	mov.f32 	%f1856, %f1892;
-	bra.uni 	BB3_41;
-
-BB3_40:
-	mul.f32 	%f1105, %f1861, %f1891;
-	fma.rn.f32 	%f1106, %f1858, %f1890, %f1105;
-	fma.rn.f32 	%f289, %f1864, %f1892, %f1106;
-	mul.f32 	%f1107, %f1860, %f1891;
-	fma.rn.f32 	%f1108, %f1857, %f1890, %f1107;
-	fma.rn.f32 	%f290, %f1863, %f1892, %f1108;
-	mul.f32 	%f1109, %f1859, %f1891;
-	fma.rn.f32 	%f1110, %f1856, %f1890, %f1109;
-	fma.rn.f32 	%f291, %f1862, %f1892, %f1110;
-	mul.f32 	%f1111, %f1861, %f1888;
-	fma.rn.f32 	%f1112, %f1858, %f1887, %f1111;
-	fma.rn.f32 	%f292, %f1864, %f1889, %f1112;
-	mul.f32 	%f1113, %f1860, %f1888;
-	fma.rn.f32 	%f1114, %f1857, %f1887, %f1113;
-	fma.rn.f32 	%f293, %f1863, %f1889, %f1114;
-	mul.f32 	%f1115, %f1859, %f1888;
-	fma.rn.f32 	%f1116, %f1856, %f1887, %f1115;
-	fma.rn.f32 	%f294, %f1862, %f1889, %f1116;
-	mul.f32 	%f1117, %f1861, %f1885;
-	fma.rn.f32 	%f1118, %f1858, %f1884, %f1117;
-	fma.rn.f32 	%f1864, %f1864, %f1886, %f1118;
-	mul.f32 	%f1119, %f1860, %f1885;
-	fma.rn.f32 	%f1120, %f1857, %f1884, %f1119;
-	fma.rn.f32 	%f1863, %f1863, %f1886, %f1120;
-	mul.f32 	%f1121, %f1859, %f1885;
-	fma.rn.f32 	%f1122, %f1856, %f1884, %f1121;
-	fma.rn.f32 	%f1862, %f1862, %f1886, %f1122;
-	mov.f32 	%f1861, %f292;
-	mov.f32 	%f1860, %f293;
-	mov.f32 	%f1859, %f294;
-	mov.f32 	%f1858, %f289;
-	mov.f32 	%f1857, %f290;
-	mov.f32 	%f1856, %f291;
-
-BB3_41:
-	add.s32 	%r647, %r647, 1;
-	setp.lt.u32	%p19, %r647, %r30;
-	@%p19 bra 	BB3_25;
-
-	mul.f32 	%f1123, %f936, %f1857;
-	fma.rn.f32 	%f1124, %f935, %f1858, %f1123;
-	fma.rn.f32 	%f1902, %f1904, %f1856, %f1124;
-	mul.f32 	%f1125, %f936, %f1860;
-	fma.rn.f32 	%f1126, %f935, %f1861, %f1125;
-	fma.rn.f32 	%f1903, %f1904, %f1859, %f1126;
-	mul.f32 	%f1127, %f936, %f1863;
-	fma.rn.f32 	%f1128, %f935, %f1864, %f1127;
-	fma.rn.f32 	%f1904, %f1904, %f1862, %f1128;
-	bra.uni 	BB3_43;
-
-BB3_24:
-	mov.f32 	%f1902, %f935;
-	mov.f32 	%f1903, %f936;
-
-BB3_43:
-	ld.v4.f32 	{%f1131, %f1132, %f1133, %f1134}, [%rd3+208];
-	ld.v4.f32 	{%f1138, %f1139, %f1140, %f1141}, [%rd3+160];
-	fma.rn.f32 	%f1143, %f1855, %f1138, %f1131;
-	fma.rn.f32 	%f1145, %f1855, %f1139, %f1132;
-	fma.rn.f32 	%f1147, %f1855, %f1140, %f1133;
-	ld.v4.f32 	{%f1148, %f1149, %f1150, %f1151}, [%rd3+176];
-	fma.rn.f32 	%f1153, %f1854, %f1148, %f1143;
-	fma.rn.f32 	%f1155, %f1854, %f1149, %f1145;
-	fma.rn.f32 	%f1157, %f1854, %f1150, %f1147;
-	ld.v4.f32 	{%f1158, %f1159, %f1160, %f1161}, [%rd3+192];
-	fma.rn.f32 	%f1163, %f1853, %f1158, %f1153;
-	fma.rn.f32 	%f1165, %f1853, %f1159, %f1155;
-	fma.rn.f32 	%f1167, %f1853, %f1160, %f1157;
-	mul.f32 	%f1168, %f1902, %f1138;
-	mul.f32 	%f1169, %f1902, %f1139;
-	mul.f32 	%f1170, %f1902, %f1140;
-	fma.rn.f32 	%f1171, %f1903, %f1148, %f1168;
-	fma.rn.f32 	%f1172, %f1903, %f1149, %f1169;
-	fma.rn.f32 	%f1173, %f1903, %f1150, %f1170;
-	fma.rn.f32 	%f1174, %f1904, %f1158, %f1171;
-	fma.rn.f32 	%f1175, %f1904, %f1159, %f1172;
-	fma.rn.f32 	%f1176, %f1904, %f1160, %f1173;
-	rcp.rn.f32 	%f1177, %f1176;
-	mul.f32 	%f1178, %f1167, %f1177;
-	neg.f32 	%f313, %f1178;
-	fma.rn.f32 	%f314, %f313, %f1174, %f1163;
-	fma.rn.f32 	%f315, %f313, %f1175, %f1165;
-	ld.const.u64 	%rd21, [params+80];
-	setp.eq.s64	%p20, %rd21, 0;
-	@%p20 bra 	BB3_48;
-
-	ld.u64 	%rd295, [%rd52];
-	ld.const.u64 	%rd296, [params+328];
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r275,%r276,%r277,%r278}, [%rd237];
+	// end inline asm
+	mov.b32 	%f1020, %r275;
+	mul.f32 	%f1021, %f218, %f1020;
+	fma.rn.f32 	%f1022, %f998, %f1917, %f1021;
+	mul.f32 	%f1023, %f1018, %f1018;
+	fma.rn.f32 	%f1024, %f1017, %f1017, %f1023;
+	fma.rn.f32 	%f1025, %f1019, %f1019, %f1024;
+	fma.rn.f32 	%f1026, %f1022, %f1022, %f1025;
+	sqrt.rn.f32 	%f1027, %f1026;
+	rcp.rn.f32 	%f1028, %f1027;
+	mul.f32 	%f1914, %f1017, %f1028;
+	mul.f32 	%f1915, %f1018, %f1028;
+	mul.f32 	%f1916, %f1019, %f1028;
+	mul.f32 	%f1917, %f1028, %f1022;
+
+$L__BB3_35:
+	mul.f32 	%f1029, %f1915, %f1915;
+	fma.rn.f32 	%f1030, %f1914, %f1914, %f1029;
+	fma.rn.f32 	%f1031, %f1916, %f1916, %f1030;
+	fma.rn.f32 	%f1032, %f1917, %f1917, %f1031;
+	rcp.rn.f32 	%f1033, %f1032;
+	mul.f32 	%f1034, %f1914, %f1033;
+	mul.f32 	%f1035, %f1915, %f1033;
+	mul.f32 	%f1036, %f1916, %f1033;
+	mul.f32 	%f1037, %f1917, %f1033;
+	mul.f32 	%f1038, %f1914, %f1034;
+	mul.f32 	%f1039, %f1915, %f1035;
+	mul.f32 	%f1040, %f1916, %f1036;
+	mul.f32 	%f1041, %f1914, %f1035;
+	mul.f32 	%f1042, %f1916, %f1037;
+	mul.f32 	%f1043, %f1914, %f1036;
+	mul.f32 	%f1044, %f1915, %f1037;
+	mul.f32 	%f1045, %f1915, %f1036;
+	mul.f32 	%f1046, %f1914, %f1037;
+	sub.f32 	%f1047, %f1038, %f1039;
+	sub.f32 	%f1048, %f1047, %f1040;
+	fma.rn.f32 	%f1049, %f1917, %f1037, %f1048;
+	sub.f32 	%f1050, %f1041, %f1042;
+	add.f32 	%f1051, %f1050, %f1050;
+	add.f32 	%f1052, %f1043, %f1044;
+	add.f32 	%f1053, %f1052, %f1052;
+	add.f32 	%f1054, %f1041, %f1042;
+	add.f32 	%f1055, %f1054, %f1054;
+	sub.f32 	%f1056, %f1039, %f1038;
+	sub.f32 	%f1057, %f1056, %f1040;
+	fma.rn.f32 	%f1058, %f1917, %f1037, %f1057;
+	sub.f32 	%f1059, %f1045, %f1046;
+	add.f32 	%f1060, %f1059, %f1059;
+	sub.f32 	%f1061, %f1043, %f1044;
+	add.f32 	%f1062, %f1061, %f1061;
+	add.f32 	%f1063, %f1045, %f1046;
+	add.f32 	%f1064, %f1063, %f1063;
+	neg.f32 	%f1065, %f1038;
+	sub.f32 	%f1066, %f1065, %f1039;
+	add.f32 	%f1067, %f1040, %f1066;
+	fma.rn.f32 	%f1068, %f1917, %f1037, %f1067;
+	mul.f32 	%f1069, %f1910, %f1049;
+	fma.rn.f32 	%f1070, %f1912, %f1051, %f1069;
+	fma.rn.f32 	%f1926, %f1913, %f1053, %f1070;
+	mul.f32 	%f1071, %f1912, %f1058;
+	fma.rn.f32 	%f1072, %f1910, %f1055, %f1071;
+	fma.rn.f32 	%f1923, %f1913, %f1060, %f1072;
+	mul.f32 	%f1073, %f1912, %f1064;
+	fma.rn.f32 	%f1074, %f1910, %f1062, %f1073;
+	fma.rn.f32 	%f1920, %f1913, %f1068, %f1074;
+	mul.f32 	%f1075, %f1909, %f1049;
+	fma.rn.f32 	%f1925, %f1911, %f1051, %f1075;
+	mul.f32 	%f1076, %f1911, %f1058;
+	fma.rn.f32 	%f1922, %f1909, %f1055, %f1076;
+	mul.f32 	%f1077, %f1911, %f1064;
+	fma.rn.f32 	%f1919, %f1909, %f1062, %f1077;
+	mul.f32 	%f1924, %f1908, %f1049;
+	mul.f32 	%f1921, %f1908, %f1055;
+	mul.f32 	%f1918, %f1908, %f1062;
+
+$L__BB3_38:
+	mul.f32 	%f1109, %f1919, %f1923;
+	mul.f32 	%f1110, %f1920, %f1922;
+	sub.f32 	%f1111, %f1110, %f1109;
+	mul.f32 	%f1112, %f1924, %f1111;
+	mul.f32 	%f1113, %f1918, %f1923;
+	mul.f32 	%f1114, %f1920, %f1921;
+	sub.f32 	%f1115, %f1114, %f1113;
+	mul.f32 	%f1116, %f1115, %f1925;
+	sub.f32 	%f1117, %f1112, %f1116;
+	mul.f32 	%f1118, %f1918, %f1922;
+	mul.f32 	%f1119, %f1919, %f1921;
+	sub.f32 	%f1120, %f1119, %f1118;
+	fma.rn.f32 	%f1121, %f1120, %f1926, %f1117;
+	rcp.rn.f32 	%f1122, %f1121;
+	mul.f32 	%f1933, %f1111, %f1122;
+	mul.f32 	%f1123, %f1920, %f1925;
+	mul.f32 	%f1124, %f1919, %f1926;
+	sub.f32 	%f1125, %f1124, %f1123;
+	mul.f32 	%f1934, %f1125, %f1122;
+	mul.f32 	%f1126, %f1922, %f1926;
+	mul.f32 	%f1127, %f1923, %f1925;
+	sub.f32 	%f1128, %f1127, %f1126;
+	mul.f32 	%f1935, %f1128, %f1122;
+	sub.f32 	%f1129, %f1113, %f1114;
+	mul.f32 	%f1930, %f1129, %f1122;
+	mul.f32 	%f1130, %f1918, %f1926;
+	mul.f32 	%f1131, %f1920, %f1924;
+	sub.f32 	%f1132, %f1131, %f1130;
+	mul.f32 	%f1931, %f1132, %f1122;
+	mul.f32 	%f1133, %f1923, %f1924;
+	mul.f32 	%f1134, %f1921, %f1926;
+	sub.f32 	%f1135, %f1134, %f1133;
+	mul.f32 	%f1932, %f1135, %f1122;
+	mul.f32 	%f1927, %f1120, %f1122;
+	mul.f32 	%f1136, %f1919, %f1924;
+	mul.f32 	%f1137, %f1918, %f1925;
+	sub.f32 	%f1138, %f1137, %f1136;
+	mul.f32 	%f1928, %f1138, %f1122;
+	mul.f32 	%f1139, %f1921, %f1925;
+	mul.f32 	%f1140, %f1922, %f1924;
+	sub.f32 	%f1141, %f1140, %f1139;
+	mul.f32 	%f1929, %f1141, %f1122;
+	bra.uni 	$L__BB3_39;
+
+$L__BB3_30:
+	// begin inline asm
+	call (%rd655), _optix_get_instance_inverse_transform_from_handle, (%rd168);
+	// end inline asm
+
+$L__BB3_31:
+	// begin inline asm
+	cvta.to.global.u64 %rd174, %rd655;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r192,%r193,%r194,%r195}, [%rd174];
+	// end inline asm
+	mov.b32 	%f1933, %r192;
+	mov.b32 	%f1934, %r193;
+	mov.b32 	%f1935, %r194;
+	add.s64 	%rd178, %rd655, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd177, %rd178;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r196,%r197,%r198,%r199}, [%rd177];
+	// end inline asm
+	mov.b32 	%f1930, %r196;
+	mov.b32 	%f1931, %r197;
+	mov.b32 	%f1932, %r198;
+	add.s64 	%rd181, %rd655, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd180, %rd181;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r200,%r201,%r202,%r203}, [%rd180];
+	// end inline asm
+	mov.b32 	%f1927, %r200;
+	mov.b32 	%f1928, %r201;
+	mov.b32 	%f1929, %r202;
+
+$L__BB3_39:
+	setp.eq.s32 	%p20, %r656, 0;
+	@%p20 bra 	$L__BB3_41;
+
+	mul.f32 	%f1142, %f1904, %f1934;
+	fma.rn.f32 	%f1143, %f1901, %f1933, %f1142;
+	fma.rn.f32 	%f304, %f1907, %f1935, %f1143;
+	mul.f32 	%f1144, %f1903, %f1934;
+	fma.rn.f32 	%f1145, %f1900, %f1933, %f1144;
+	fma.rn.f32 	%f305, %f1906, %f1935, %f1145;
+	mul.f32 	%f1146, %f1902, %f1934;
+	fma.rn.f32 	%f1147, %f1899, %f1933, %f1146;
+	fma.rn.f32 	%f1935, %f1905, %f1935, %f1147;
+	mul.f32 	%f1148, %f1904, %f1931;
+	fma.rn.f32 	%f1149, %f1901, %f1930, %f1148;
+	fma.rn.f32 	%f307, %f1907, %f1932, %f1149;
+	mul.f32 	%f1150, %f1903, %f1931;
+	fma.rn.f32 	%f1151, %f1900, %f1930, %f1150;
+	fma.rn.f32 	%f308, %f1906, %f1932, %f1151;
+	mul.f32 	%f1152, %f1902, %f1931;
+	fma.rn.f32 	%f1153, %f1899, %f1930, %f1152;
+	fma.rn.f32 	%f1932, %f1905, %f1932, %f1153;
+	mul.f32 	%f1154, %f1904, %f1928;
+	fma.rn.f32 	%f1155, %f1901, %f1927, %f1154;
+	fma.rn.f32 	%f310, %f1907, %f1929, %f1155;
+	mul.f32 	%f1156, %f1903, %f1928;
+	fma.rn.f32 	%f1157, %f1900, %f1927, %f1156;
+	fma.rn.f32 	%f311, %f1906, %f1929, %f1157;
+	mul.f32 	%f1158, %f1902, %f1928;
+	fma.rn.f32 	%f1159, %f1899, %f1927, %f1158;
+	fma.rn.f32 	%f1929, %f1905, %f1929, %f1159;
+	mov.f32 	%f1927, %f310;
+	mov.f32 	%f1928, %f311;
+	mov.f32 	%f1930, %f307;
+	mov.f32 	%f1931, %f308;
+	mov.f32 	%f1933, %f304;
+	mov.f32 	%f1934, %f305;
+
+$L__BB3_41:
+	add.s32 	%r656, %r656, 1;
+	setp.lt.u32 	%p21, %r656, %r187;
+	mov.f32 	%f1899, %f1935;
+	mov.f32 	%f1900, %f1934;
+	mov.f32 	%f1901, %f1933;
+	mov.f32 	%f1902, %f1932;
+	mov.f32 	%f1903, %f1931;
+	mov.f32 	%f1904, %f1930;
+	mov.f32 	%f1905, %f1929;
+	mov.f32 	%f1906, %f1928;
+	mov.f32 	%f1907, %f1927;
+	@%p21 bra 	$L__BB3_26;
+
+$L__BB3_42:
+	mul.f32 	%f1160, %f1955, %f1934;
+	fma.rn.f32 	%f1161, %f1954, %f1933, %f1160;
+	mul.f32 	%f1162, %f1955, %f1931;
+	fma.rn.f32 	%f1163, %f1954, %f1930, %f1162;
+	mul.f32 	%f1164, %f1955, %f1928;
+	fma.rn.f32 	%f1165, %f1954, %f1927, %f1164;
+	fma.rn.f32 	%f1956, %f973, %f1929, %f1165;
+	fma.rn.f32 	%f1955, %f973, %f1932, %f1163;
+	fma.rn.f32 	%f1954, %f973, %f1935, %f1161;
+	bra.uni 	$L__BB3_44;
+
+$L__BB3_43:
+	mov.f32 	%f1956, %f973;
+
+$L__BB3_44:
+	add.s64 	%rd18, %rd3, 208;
+	ld.v4.f32 	{%f1169, %f1170, %f1171, %f1172}, [%rd3+208];
+	ld.f32 	%f1176, [%rd3+160];
+	fma.rn.f32 	%f1177, %f1896, %f1176, %f1169;
+	ld.f32 	%f1178, [%rd3+164];
+	fma.rn.f32 	%f1179, %f1896, %f1178, %f1170;
+	ld.f32 	%f1180, [%rd3+168];
+	fma.rn.f32 	%f1181, %f1896, %f1180, %f1171;
+	ld.f32 	%f1182, [%rd3+176];
+	fma.rn.f32 	%f1183, %f1897, %f1182, %f1177;
+	ld.f32 	%f1184, [%rd3+180];
+	fma.rn.f32 	%f1185, %f1897, %f1184, %f1179;
+	ld.f32 	%f1186, [%rd3+184];
+	fma.rn.f32 	%f1187, %f1897, %f1186, %f1181;
+	ld.f32 	%f1188, [%rd3+192];
+	fma.rn.f32 	%f1189, %f1898, %f1188, %f1183;
+	ld.f32 	%f1190, [%rd3+196];
+	fma.rn.f32 	%f1191, %f1898, %f1190, %f1185;
+	ld.f32 	%f1192, [%rd3+200];
+	fma.rn.f32 	%f1193, %f1898, %f1192, %f1187;
+	ld.v4.f32 	{%f1194, %f1195, %f1196, %f1197}, [%rd3+160];
+	mul.f32 	%f1201, %f1954, %f1194;
+	mul.f32 	%f1202, %f1954, %f1195;
+	mul.f32 	%f1203, %f1954, %f1196;
+	fma.rn.f32 	%f1204, %f1955, %f1182, %f1201;
+	fma.rn.f32 	%f1205, %f1955, %f1184, %f1202;
+	fma.rn.f32 	%f1206, %f1955, %f1186, %f1203;
+	fma.rn.f32 	%f1207, %f1956, %f1188, %f1204;
+	fma.rn.f32 	%f1208, %f1956, %f1190, %f1205;
+	fma.rn.f32 	%f1209, %f1956, %f1192, %f1206;
+	rcp.rn.f32 	%f1210, %f1209;
+	mul.f32 	%f1211, %f1193, %f1210;
+	neg.f32 	%f346, %f1211;
+	fma.rn.f32 	%f347, %f346, %f1207, %f1189;
+	fma.rn.f32 	%f348, %f346, %f1208, %f1191;
+	ld.const.u64 	%rd19, [params+80];
+	setp.eq.s64 	%p22, %rd19, 0;
+	@%p22 bra 	$L__BB3_49;
+
+	ld.u64 	%rd287, [%rd48];
+	ld.const.u64 	%rd288, [params+328];
+	cvta.to.global.u64 	%rd289, %rd288;
+	cvt.u64.u32 	%rd20, %r1;
+	mul.wide.u32 	%rd290, %r1, 8;
+	add.s64 	%rd291, %rd289, %rd290;
+	st.global.u64 	[%rd291], %rd287;
+	ld.const.u64 	%rd292, [params+336];
+	cvta.to.global.u64 	%rd293, %rd292;
+	mul.wide.u32 	%rd294, %r1, 4;
+	add.s64 	%rd295, %rd293, %rd294;
+	mov.u32 	%r338, 0;
+	st.global.u32 	[%rd295], %r338;
+	ld.const.u64 	%rd296, [params+344];
 	cvta.to.global.u64 	%rd297, %rd296;
-	cvt.u64.u32	%rd22, %r1;
-	mul.wide.u32 	%rd298, %r1, 8;
-	add.s64 	%rd299, %rd297, %rd298;
-	st.global.u64 	[%rd299], %rd295;
-	ld.const.u64 	%rd300, [params+336];
-	cvta.to.global.u64 	%rd301, %rd300;
-	mul.wide.u32 	%rd302, %r1, 4;
-	add.s64 	%rd303, %rd301, %rd302;
-	mov.u32 	%r331, 0;
-	st.global.u32 	[%rd303], %r331;
-	ld.const.u64 	%rd304, [params+344];
+	add.s64 	%rd21, %rd297, %rd294;
+	ld.global.u32 	%r10, [%rd21];
+	setp.eq.s32 	%p23, %r10, 0;
+	@%p23 bra 	$L__BB3_48;
+
+	// begin inline asm
+	call (%r339), _optix_read_instance_id, ();
+	// end inline asm
+	setp.ge.u32 	%p24, %r339, %r10;
+	@%p24 bra 	$L__BB3_48;
+
+	st.global.u32 	[%rd21], %r339;
+
+$L__BB3_48:
+	cvta.to.global.u64 	%rd298, %rd19;
+	shl.b64 	%rd299, %rd20, 2;
+	add.s64 	%rd300, %rd298, %rd299;
+	st.global.f32 	[%rd300], %f347;
+	ld.const.u64 	%rd301, [params+88];
+	cvta.to.global.u64 	%rd302, %rd301;
+	add.s64 	%rd303, %rd302, %rd299;
+	st.global.f32 	[%rd303], %f348;
+	ld.const.u64 	%rd304, [params+72];
 	cvta.to.global.u64 	%rd305, %rd304;
-	add.s64 	%rd23, %rd305, %rd302;
-	ld.global.u32 	%r9, [%rd23];
-	setp.eq.s32	%p21, %r9, 0;
-	@%p21 bra 	BB3_47;
-
-	// inline asm
-	call (%r332), _optix_read_instance_id, ();
-	// inline asm
-	setp.ge.u32	%p22, %r332, %r9;
-	@%p22 bra 	BB3_47;
-
-	st.global.u32 	[%rd23], %r332;
-
-BB3_47:
-	cvta.to.global.u64 	%rd306, %rd21;
-	shl.b64 	%rd307, %rd22, 2;
-	add.s64 	%rd308, %rd306, %rd307;
-	st.global.f32 	[%rd308], %f314;
-	ld.const.u64 	%rd309, [params+88];
-	cvta.to.global.u64 	%rd310, %rd309;
-	add.s64 	%rd311, %rd310, %rd307;
-	st.global.f32 	[%rd311], %f315;
-	ld.const.u64 	%rd312, [params+72];
-	cvta.to.global.u64 	%rd313, %rd312;
-	add.s64 	%rd314, %rd313, %rd307;
-	st.global.f32 	[%rd314], %f313;
-	bra.uni 	BB3_116;
-
-BB3_48:
-	fma.rn.f32 	%f2063, %f313, %f1902, %f1855;
-	fma.rn.f32 	%f2064, %f313, %f1903, %f1854;
-	fma.rn.f32 	%f2065, %f313, %f1904, %f1853;
-	ld.v4.f32 	{%f1180, %f1181, %f1182, %f1183}, [%rd3+96];
-	mul.f32 	%f1187, %f1180, 0f00000000;
-	mul.f32 	%f1188, %f1181, 0f00000000;
-	mul.f32 	%f1189, %f1182, 0f00000000;
-	ld.v4.f32 	{%f1190, %f1191, %f1192, %f1193}, [%rd3+112];
-	mov.f32 	%f2048, 0f00000000;
-	fma.rn.f32 	%f1198, %f2048, %f1190, %f1187;
-	fma.rn.f32 	%f1199, %f2048, %f1191, %f1188;
-	fma.rn.f32 	%f1200, %f2048, %f1192, %f1189;
-	ld.v4.f32 	{%f1201, %f1202, %f1203, %f1204}, [%rd3+128];
-	mov.f32 	%f1980, 0f3F800000;
-	fma.rn.f32 	%f1207, %f1980, %f1201, %f1198;
-	fma.rn.f32 	%f1209, %f1980, %f1202, %f1199;
-	fma.rn.f32 	%f1211, %f1980, %f1203, %f1200;
-	mul.f32 	%f1212, %f1207, %f1207;
-	fma.rn.f32 	%f1213, %f1209, %f1209, %f1212;
-	fma.rn.f32 	%f1214, %f1211, %f1211, %f1213;
-	sqrt.rn.f32 	%f1215, %f1214;
-	div.rn.f32 	%f2060, %f1207, %f1215;
-	div.rn.f32 	%f2061, %f1209, %f1215;
-	div.rn.f32 	%f2062, %f1211, %f1215;
-	ld.const.u64 	%rd24, [params+96];
-	setp.eq.s64	%p23, %rd24, 0;
-	@%p23 bra 	BB3_56;
-
-	mul.f32 	%f1216, %f314, %f314;
-	fma.rn.f32 	%f1217, %f315, %f315, %f1216;
-	sqrt.rn.f32 	%f1907, %f1217;
-	abs.f32 	%f323, %f314;
-	setp.eq.f32	%p24, %f323, 0f00000000;
-	abs.f32 	%f324, %f315;
-	setp.eq.f32	%p25, %f324, 0f00000000;
-	and.pred  	%p26, %p24, %p25;
-	mov.b32 	 %r11, %f314;
-	mov.b32 	 %r333, %f315;
-	and.b32  	%r12, %r333, -2147483648;
-	@%p26 bra 	BB3_53;
-	bra.uni 	BB3_50;
-
-BB3_53:
-	shr.s32 	%r340, %r11, 31;
-	and.b32  	%r341, %r340, 1078530011;
-	or.b32  	%r342, %r341, %r12;
-	mov.b32 	 %f1905, %r342;
-	bra.uni 	BB3_54;
-
-BB3_50:
-	setp.eq.f32	%p27, %f323, 0f7F800000;
-	setp.eq.f32	%p28, %f324, 0f7F800000;
-	and.pred  	%p29, %p27, %p28;
-	@%p29 bra 	BB3_52;
-	bra.uni 	BB3_51;
-
-BB3_52:
-	shr.s32 	%r336, %r11, 31;
-	and.b32  	%r337, %r336, 13483017;
-	add.s32 	%r338, %r337, 1061752795;
-	or.b32  	%r339, %r338, %r12;
-	mov.b32 	 %f1905, %r339;
-	bra.uni 	BB3_54;
-
-BB3_51:
-	max.f32 	%f1218, %f324, %f323;
-	min.f32 	%f1219, %f324, %f323;
-	div.rn.f32 	%f1220, %f1219, %f1218;
-	mul.rn.f32 	%f1221, %f1220, %f1220;
-	mov.f32 	%f1222, 0fC0B59883;
-	mov.f32 	%f1223, 0fBF52C7EA;
-	fma.rn.f32 	%f1224, %f1221, %f1223, %f1222;
-	mov.f32 	%f1225, 0fC0D21907;
-	fma.rn.f32 	%f1226, %f1224, %f1221, %f1225;
-	mul.f32 	%f1227, %f1221, %f1226;
-	mul.f32 	%f1228, %f1220, %f1227;
-	add.f32 	%f1229, %f1221, 0f41355DC0;
-	mov.f32 	%f1230, 0f41E6BD60;
-	fma.rn.f32 	%f1231, %f1229, %f1221, %f1230;
-	mov.f32 	%f1232, 0f419D92C8;
-	fma.rn.f32 	%f1233, %f1231, %f1221, %f1232;
-	rcp.rn.f32 	%f1234, %f1233;
-	fma.rn.f32 	%f1235, %f1228, %f1234, %f1220;
-	mov.f32 	%f1236, 0f3FC90FDB;
-	sub.f32 	%f1237, %f1236, %f1235;
-	setp.gt.f32	%p30, %f324, %f323;
-	selp.f32	%f1238, %f1237, %f1235, %p30;
-	mov.f32 	%f1239, 0f40490FDB;
-	sub.f32 	%f1240, %f1239, %f1238;
-	setp.lt.s32	%p31, %r11, 0;
-	selp.f32	%f1241, %f1240, %f1238, %p31;
-	mov.b32 	 %r334, %f1241;
-	or.b32  	%r335, %r334, %r12;
-	mov.b32 	 %f1242, %r335;
-	add.f32 	%f1243, %f323, %f324;
-	setp.gtu.f32	%p32, %f1243, 0f7F800000;
-	selp.f32	%f1905, %f1243, %f1242, %p32;
-
-BB3_54:
-	mul.f32 	%f1245, %f1905, 0f3E22F983;
-	setp.lt.f32	%p33, %f1245, 0f00000000;
-	add.f32 	%f1246, %f1245, 0f3F800000;
-	selp.f32	%f1906, %f1246, %f1245, %p33;
-	ld.const.u64 	%rd315, [params+184];
-	setp.eq.s64	%p34, %rd315, 0;
-	@%p34 bra 	BB3_56;
-
-	rcp.rn.f32 	%f1247, %f1907;
-	setp.neu.f32	%p35, %f1907, 0f00000000;
-	mul.f32 	%f1248, %f314, %f1247;
-	selp.f32	%f1249, %f1248, 0f3F800000, %p35;
-	mul.f32 	%f1250, %f315, %f1247;
-	selp.f32	%f1251, %f1250, 0f00000000, %p35;
-	ld.v4.f32 	{%f1252, %f1253, %f1254, %f1255}, [%rd3+32];
-	mul.f32 	%f1259, %f1249, %f1252;
-	mul.f32 	%f1260, %f1249, %f1253;
-	mul.f32 	%f1261, %f1249, %f1254;
-	ld.v4.f32 	{%f1262, %f1263, %f1264, %f1265}, [%rd3+48];
-	fma.rn.f32 	%f1269, %f1251, %f1262, %f1259;
-	fma.rn.f32 	%f1270, %f1251, %f1263, %f1260;
-	fma.rn.f32 	%f1271, %f1251, %f1264, %f1261;
-	ld.v4.f32 	{%f1272, %f1273, %f1274, %f1275}, [%rd3+64];
-	fma.rn.f32 	%f340, %f2048, %f1272, %f1269;
-	fma.rn.f32 	%f339, %f2048, %f1273, %f1270;
-	fma.rn.f32 	%f338, %f2048, %f1274, %f1271;
-	neg.f32 	%f1280, %f1251;
-	mul.f32 	%f1281, %f1252, %f1280;
-	mul.f32 	%f1282, %f1253, %f1280;
-	mul.f32 	%f1283, %f1254, %f1280;
-	fma.rn.f32 	%f1284, %f1249, %f1262, %f1281;
-	fma.rn.f32 	%f1285, %f1249, %f1263, %f1282;
-	fma.rn.f32 	%f1286, %f1249, %f1264, %f1283;
-	fma.rn.f32 	%f343, %f2048, %f1272, %f1284;
-	fma.rn.f32 	%f342, %f2048, %f1273, %f1285;
-	fma.rn.f32 	%f341, %f2048, %f1274, %f1286;
-
-BB3_56:
-	ld.u64 	%rd25, [%rd52];
-	ld.const.u64 	%rd316, [params+344];
-	cvta.to.global.u64 	%rd317, %rd316;
-	cvt.u64.u32	%rd26, %r1;
-	mul.wide.u32 	%rd318, %r1, 4;
-	add.s64 	%rd27, %rd317, %rd318;
-	ld.global.u32 	%r13, [%rd27];
-	setp.eq.s32	%p36, %r13, 0;
-	@%p36 bra 	BB3_57;
-
-	// inline asm
-	call (%r343), _optix_read_instance_id, ();
-	// inline asm
-	setp.ge.u32	%p37, %r343, %r13;
-	@%p37 bra 	BB3_57;
-
-	mov.f32 	%f1979, 0f00000000;
-	mov.f32 	%f1917, %f1980;
-	mov.f32 	%f1916, %f1979;
-	mov.f32 	%f1915, %f1979;
-	mov.f32 	%f1914, %f1979;
-	mov.f32 	%f1921, %f1979;
-	mov.f32 	%f1920, %f1980;
-	mov.f32 	%f1919, %f1979;
-	mov.f32 	%f1918, %f1979;
-	mov.f32 	%f1925, %f1979;
-	mov.f32 	%f1924, %f1979;
-	mov.f32 	%f1923, %f1980;
-	mov.f32 	%f1922, %f1979;
-	@%p2 bra 	BB3_77;
-
-	add.s32 	%r648, %r30, -1;
-	setp.lt.s32	%p39, %r648, 0;
-	@%p39 bra 	BB3_77;
-
-BB3_61:
+	add.s64 	%rd306, %rd305, %rd299;
+	st.global.f32 	[%rd306], %f346;
+	bra.uni 	$L__BB3_117;
+
+$L__BB3_49:
+	fma.rn.f32 	%f2115, %f346, %f1954, %f1896;
+	fma.rn.f32 	%f2116, %f346, %f1955, %f1897;
+	fma.rn.f32 	%f2117, %f346, %f1956, %f1898;
+	ld.v4.f32 	{%f1213, %f1214, %f1215, %f1216}, [%rd18+-112];
+	mul.f32 	%f1220, %f1213, 0f00000000;
+	mov.f32 	%f2100, 0f00000000;
+	mul.f32 	%f1222, %f1214, 0f00000000;
+	mul.f32 	%f1223, %f1215, 0f00000000;
+	ld.v4.f32 	{%f1224, %f1225, %f1226, %f1227}, [%rd18+-96];
+	fma.rn.f32 	%f1231, %f2100, %f1224, %f1220;
+	fma.rn.f32 	%f1232, %f2100, %f1225, %f1222;
+	fma.rn.f32 	%f1233, %f2100, %f1226, %f1223;
+	ld.f32 	%f1234, [%rd18+-80];
+	mov.f32 	%f2064, 0f3F800000;
+	fma.rn.f32 	%f1236, %f2064, %f1234, %f1231;
+	ld.f32 	%f1237, [%rd18+-76];
+	fma.rn.f32 	%f1238, %f2064, %f1237, %f1232;
+	ld.f32 	%f1239, [%rd18+-72];
+	fma.rn.f32 	%f1240, %f2064, %f1239, %f1233;
+	mul.f32 	%f1241, %f1236, %f1236;
+	fma.rn.f32 	%f1242, %f1238, %f1238, %f1241;
+	fma.rn.f32 	%f1243, %f1240, %f1240, %f1242;
+	sqrt.rn.f32 	%f1244, %f1243;
+	div.rn.f32 	%f2088, %f1236, %f1244;
+	div.rn.f32 	%f2089, %f1238, %f1244;
+	div.rn.f32 	%f2090, %f1240, %f1244;
+	ld.const.u64 	%rd22, [params+96];
+	setp.eq.s64 	%p25, %rd22, 0;
+	@%p25 bra 	$L__BB3_57;
+
+	mul.f32 	%f1245, %f347, %f347;
+	fma.rn.f32 	%f1246, %f348, %f348, %f1245;
+	sqrt.rn.f32 	%f1959, %f1246;
+	abs.f32 	%f356, %f347;
+	setp.eq.f32 	%p26, %f356, 0f00000000;
+	abs.f32 	%f357, %f348;
+	setp.eq.f32 	%p27, %f357, 0f00000000;
+	and.pred  	%p28, %p26, %p27;
+	mov.b32 	%r12, %f347;
+	mov.b32 	%r340, %f348;
+	and.b32  	%r13, %r340, -2147483648;
+	@%p28 bra 	$L__BB3_54;
+	bra.uni 	$L__BB3_51;
+
+$L__BB3_54:
+	shr.s32 	%r345, %r12, 31;
+	and.b32  	%r346, %r345, 1078530011;
+	or.b32  	%r347, %r346, %r13;
+	mov.b32 	%f1957, %r347;
+	bra.uni 	$L__BB3_55;
+
+$L__BB3_51:
+	setp.eq.f32 	%p29, %f356, 0f7F800000;
+	setp.eq.f32 	%p30, %f357, 0f7F800000;
+	and.pred  	%p31, %p29, %p30;
+	@%p31 bra 	$L__BB3_53;
+	bra.uni 	$L__BB3_52;
+
+$L__BB3_53:
+	setp.lt.s32 	%p35, %r12, 0;
+	selp.b32 	%r343, 1075235812, 1061752795, %p35;
+	or.b32  	%r344, %r343, %r13;
+	mov.b32 	%f1957, %r344;
+	bra.uni 	$L__BB3_55;
+
+$L__BB3_52:
+	setp.lt.s32 	%p32, %r12, 0;
+	min.f32 	%f1247, %f357, %f356;
+	max.f32 	%f1248, %f357, %f356;
+	div.rn.f32 	%f1249, %f1247, %f1248;
+	mul.rn.f32 	%f1250, %f1249, %f1249;
+	mov.f32 	%f1251, 0fC0B59883;
+	mov.f32 	%f1252, 0fBF52C7EA;
+	fma.rn.f32 	%f1253, %f1250, %f1252, %f1251;
+	mov.f32 	%f1254, 0fC0D21907;
+	fma.rn.f32 	%f1255, %f1253, %f1250, %f1254;
+	mul.f32 	%f1256, %f1250, %f1255;
+	mul.f32 	%f1257, %f1249, %f1256;
+	add.f32 	%f1258, %f1250, 0f41355DC0;
+	mov.f32 	%f1259, 0f41E6BD60;
+	fma.rn.f32 	%f1260, %f1258, %f1250, %f1259;
+	mov.f32 	%f1261, 0f419D92C8;
+	fma.rn.f32 	%f1262, %f1260, %f1250, %f1261;
+	rcp.rn.f32 	%f1263, %f1262;
+	fma.rn.f32 	%f1264, %f1257, %f1263, %f1249;
+	mov.f32 	%f1265, 0f3FC90FDB;
+	sub.f32 	%f1266, %f1265, %f1264;
+	setp.gt.f32 	%p33, %f357, %f356;
+	selp.f32 	%f1267, %f1266, %f1264, %p33;
+	mov.f32 	%f1268, 0f40490FDB;
+	sub.f32 	%f1269, %f1268, %f1267;
+	selp.f32 	%f1270, %f1269, %f1267, %p32;
+	mov.b32 	%r341, %f1270;
+	or.b32  	%r342, %r13, %r341;
+	mov.b32 	%f1271, %r342;
+	add.f32 	%f1272, %f356, %f357;
+	setp.le.f32 	%p34, %f1272, 0f7F800000;
+	selp.f32 	%f1957, %f1271, %f1272, %p34;
+
+$L__BB3_55:
+	mul.f32 	%f1274, %f1957, 0f3E22F983;
+	setp.lt.f32 	%p36, %f1274, 0f00000000;
+	add.f32 	%f1275, %f1274, 0f3F800000;
+	selp.f32 	%f1958, %f1275, %f1274, %p36;
+	ld.const.u64 	%rd307, [params+184];
+	setp.eq.s64 	%p37, %rd307, 0;
+	@%p37 bra 	$L__BB3_57;
+
+	rcp.rn.f32 	%f1276, %f1959;
+	setp.neu.f32 	%p38, %f1959, 0f00000000;
+	mul.f32 	%f1278, %f347, %f1276;
+	selp.f32 	%f1279, %f1278, 0f3F800000, %p38;
+	mul.f32 	%f1280, %f348, %f1276;
+	selp.f32 	%f1281, %f1280, 0f00000000, %p38;
+	ld.v4.f32 	{%f1282, %f1283, %f1284, %f1285}, [%rd18+-176];
+	mul.f32 	%f1289, %f1279, %f1282;
+	mul.f32 	%f1290, %f1279, %f1283;
+	mul.f32 	%f1291, %f1279, %f1284;
+	ld.v4.f32 	{%f1292, %f1293, %f1294, %f1295}, [%rd18+-160];
+	fma.rn.f32 	%f1299, %f1281, %f1292, %f1289;
+	fma.rn.f32 	%f1300, %f1281, %f1293, %f1290;
+	fma.rn.f32 	%f1301, %f1281, %f1294, %f1291;
+	ld.f32 	%f1302, [%rd18+-144];
+	fma.rn.f32 	%f2106, %f2100, %f1302, %f1299;
+	ld.f32 	%f1303, [%rd18+-140];
+	fma.rn.f32 	%f2107, %f2100, %f1303, %f1300;
+	ld.f32 	%f1304, [%rd18+-136];
+	fma.rn.f32 	%f2108, %f2100, %f1304, %f1301;
+	neg.f32 	%f1305, %f1281;
+	mul.f32 	%f1306, %f1282, %f1305;
+	mul.f32 	%f1307, %f1283, %f1305;
+	mul.f32 	%f1308, %f1284, %f1305;
+	fma.rn.f32 	%f1309, %f1279, %f1292, %f1306;
+	fma.rn.f32 	%f1310, %f1279, %f1293, %f1307;
+	fma.rn.f32 	%f1311, %f1279, %f1294, %f1308;
+	fma.rn.f32 	%f2103, %f2100, %f1302, %f1309;
+	fma.rn.f32 	%f2104, %f2100, %f1303, %f1310;
+	fma.rn.f32 	%f2105, %f2100, %f1304, %f1311;
+
+$L__BB3_57:
+	ld.u64 	%rd23, [%rd48];
+	ld.const.u64 	%rd308, [params+344];
+	cvta.to.global.u64 	%rd309, %rd308;
+	cvt.u64.u32 	%rd24, %r1;
+	mul.wide.u32 	%rd310, %r1, 4;
+	add.s64 	%rd25, %rd309, %rd310;
+	ld.global.u32 	%r14, [%rd25];
+	setp.eq.s32 	%p39, %r14, 0;
+	mov.f32 	%f2101, 0f00000000;
+	mov.f32 	%f2102, 0f00000000;
+	mov.f32 	%f2112, %f2088;
+	mov.f32 	%f2113, %f2089;
+	mov.f32 	%f2114, %f2090;
+	@%p39 bra 	$L__BB3_105;
+
+	// begin inline asm
+	call (%r348), _optix_read_instance_id, ();
+	// end inline asm
+	setp.ge.u32 	%p40, %r348, %r14;
+	mov.f32 	%f2112, %f2088;
+	mov.f32 	%f2113, %f2089;
+	mov.f32 	%f2114, %f2090;
+	@%p40 bra 	$L__BB3_105;
+
+	// begin inline asm
+	call (%r349), _optix_get_transform_list_size, ();
+	// end inline asm
+	setp.eq.s32 	%p41, %r349, 0;
+	mov.f32 	%f2065, 0f00000000;
+	mov.f32 	%f2002, %f2064;
+	mov.f32 	%f2003, %f2065;
+	mov.f32 	%f2004, %f2065;
+	mov.f32 	%f2005, %f2065;
+	mov.f32 	%f1998, %f2065;
+	mov.f32 	%f1999, %f2064;
+	mov.f32 	%f2000, %f2065;
+	mov.f32 	%f2001, %f2065;
+	mov.f32 	%f1994, %f2065;
+	mov.f32 	%f1995, %f2065;
+	mov.f32 	%f1996, %f2064;
+	mov.f32 	%f1997, %f2065;
+	@%p41 bra 	$L__BB3_77;
+
+	// begin inline asm
+	call (%r350), _optix_get_transform_list_size, ();
+	// end inline asm
+	// begin inline asm
+	call (%f1330), _optix_get_ray_time, ();
+	// end inline asm
+	setp.lt.s32 	%p42, %r350, 1;
+	@%p42 bra 	$L__BB3_77;
+
+	add.s32 	%r657, %r350, 1;
+	mov.u32 	%r658, 1;
+
+$L__BB3_62:
 	.pragma "nounroll";
-	// inline asm
-	call (%rd319), _optix_get_transform_list_handle, (%r648);
-	// inline asm
-	// inline asm
-	call (%r345), _optix_get_transform_type_from_handle, (%rd319);
-	// inline asm
-	and.b32  	%r346, %r345, -2;
-	setp.eq.s32	%p40, %r346, 2;
-	@%p40 bra 	BB3_67;
-	bra.uni 	BB3_62;
-
-BB3_67:
-	setp.eq.s32	%p43, %r345, 2;
-	@%p43 bra 	BB3_71;
-	bra.uni 	BB3_68;
-
-BB3_71:
-	// inline asm
-	call (%rd393), _optix_get_matrix_motion_transform_from_handle, (%rd319);
-	// inline asm
-	// inline asm
-	cvta.to.global.u64 %rd395, %rd393;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r434,%r435,%r436,%r437}, [%rd395];
-	// inline asm
-	mov.b32	{%rs12, %rs13}, %r436;
-	add.s64 	%rd399, %rd393, 16;
-	// inline asm
-	cvta.to.global.u64 %rd398, %rd399;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r438,%r439,%r440,%r441}, [%rd398];
-	// inline asm
-	add.s64 	%rd402, %rd393, 32;
-	// inline asm
-	cvta.to.global.u64 %rd401, %rd402;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r442,%r443,%r444,%r445}, [%rd401];
-	// inline asm
-	add.s64 	%rd405, %rd393, 48;
-	// inline asm
-	cvta.to.global.u64 %rd404, %rd405;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r446,%r447,%r448,%r449}, [%rd404];
-	// inline asm
-	add.s64 	%rd408, %rd393, 64;
-	// inline asm
-	cvta.to.global.u64 %rd407, %rd408;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r450,%r451,%r452,%r453}, [%rd407];
-	// inline asm
-	add.s64 	%rd411, %rd393, 80;
-	// inline asm
-	cvta.to.global.u64 %rd410, %rd411;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r454,%r455,%r456,%r457}, [%rd410];
-	// inline asm
-	add.s64 	%rd414, %rd393, 96;
-	// inline asm
-	cvta.to.global.u64 %rd413, %rd414;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r458,%r459,%r460,%r461}, [%rd413];
-	// inline asm
-	add.s64 	%rd417, %rd393, 112;
-	// inline asm
-	cvta.to.global.u64 %rd416, %rd417;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r462,%r463,%r464,%r465}, [%rd416];
-	// inline asm
-	mov.b32 	 %f1431, %r437;
-	mov.b32 	 %f1432, %r438;
-	cvt.u32.u16	%r478, %rs12;
-	add.s32 	%r479, %r478, -1;
-	cvt.rn.f32.s32	%f1433, %r479;
-	sub.f32 	%f1434, %f938, %f1431;
-	mul.f32 	%f1435, %f1434, %f1433;
-	sub.f32 	%f1436, %f1432, %f1431;
-	div.rn.f32 	%f1437, %f1435, %f1436;
-	min.f32 	%f1438, %f1433, %f1437;
-	mov.f32 	%f1439, 0f00000000;
-	max.f32 	%f1440, %f1439, %f1438;
-	cvt.rmi.f32.f32	%f1441, %f1440;
-	cvt.rzi.s32.f32	%r480, %f1441;
-	cvt.s64.s32	%rd35, %r480;
-	mul.wide.s32 	%rd428, %r480, 48;
-	add.s64 	%rd420, %rd402, %rd428;
-	// inline asm
+	add.s32 	%r352, %r657, -2;
+	// begin inline asm
+	call (%rd311), _optix_get_transform_list_handle, (%r352);
+	// end inline asm
+	// begin inline asm
+	call (%r353), _optix_get_transform_type_from_handle, (%rd311);
+	// end inline asm
+	or.b32  	%r354, %r353, 1;
+	setp.eq.s32 	%p43, %r354, 3;
+	@%p43 bra 	$L__BB3_68;
+	bra.uni 	$L__BB3_63;
+
+$L__BB3_68:
+	setp.eq.s32 	%p46, %r353, 2;
+	@%p46 bra 	$L__BB3_72;
+	bra.uni 	$L__BB3_69;
+
+$L__BB3_72:
+	// begin inline asm
+	call (%rd383), _optix_get_matrix_motion_transform_from_handle, (%rd311);
+	// end inline asm
+	// begin inline asm
+	cvta.to.global.u64 %rd385, %rd383;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r442,%r443,%r444,%r445}, [%rd385];
+	// end inline asm
+	add.s64 	%rd389, %rd383, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd388, %rd389;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r446,%r447,%r448,%r449}, [%rd388];
+	// end inline asm
+	add.s64 	%rd392, %rd383, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd391, %rd392;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r450,%r451,%r452,%r453}, [%rd391];
+	// end inline asm
+	add.s64 	%rd395, %rd383, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd394, %rd395;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r454,%r455,%r456,%r457}, [%rd394];
+	// end inline asm
+	add.s64 	%rd398, %rd383, 64;
+	// begin inline asm
+	cvta.to.global.u64 %rd397, %rd398;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r458,%r459,%r460,%r461}, [%rd397];
+	// end inline asm
+	add.s64 	%rd401, %rd383, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd400, %rd401;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r462,%r463,%r464,%r465}, [%rd400];
+	// end inline asm
+	add.s64 	%rd404, %rd383, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd403, %rd404;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r466,%r467,%r468,%r469}, [%rd403];
+	// end inline asm
+	add.s64 	%rd407, %rd383, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd406, %rd407;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r470,%r471,%r472,%r473}, [%rd406];
+	// end inline asm
+	mov.b32 	%f1458, %r445;
+	mov.b32 	%f1459, %r446;
+	and.b32  	%r486, %r444, 65535;
+	add.s32 	%r487, %r486, -1;
+	cvt.rn.f32.s32 	%f1460, %r487;
+	sub.f32 	%f1461, %f1330, %f1458;
+	mul.f32 	%f1462, %f1461, %f1460;
+	sub.f32 	%f1463, %f1459, %f1458;
+	div.rn.f32 	%f1464, %f1462, %f1463;
+	min.f32 	%f1465, %f1460, %f1464;
+	mov.f32 	%f1466, 0f00000000;
+	max.f32 	%f1467, %f1466, %f1465;
+	cvt.rmi.f32.f32 	%f1468, %f1467;
+	sub.f32 	%f463, %f1467, %f1468;
+	cvt.rzi.s32.f32 	%r488, %f1468;
+	cvt.s64.s32 	%rd32, %r488;
+	mul.wide.s32 	%rd418, %r488, 48;
+	add.s64 	%rd410, %rd392, %rd418;
+	// begin inline asm
+	cvta.to.global.u64 %rd409, %rd410;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r474,%r475,%r476,%r477}, [%rd409];
+	// end inline asm
+	mov.b32 	%f2002, %r474;
+	mov.b32 	%f2003, %r475;
+	mov.b32 	%f2004, %r476;
+	mov.b32 	%f2005, %r477;
+	add.s64 	%rd413, %rd410, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd412, %rd413;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r478,%r479,%r480,%r481}, [%rd412];
+	// end inline asm
+	mov.b32 	%f1998, %r478;
+	mov.b32 	%f1999, %r479;
+	mov.b32 	%f2000, %r480;
+	mov.b32 	%f2001, %r481;
+	add.s64 	%rd416, %rd410, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd415, %rd416;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r482,%r483,%r484,%r485}, [%rd415];
+	// end inline asm
+	mov.b32 	%f1994, %r482;
+	mov.b32 	%f1995, %r483;
+	mov.b32 	%f1996, %r484;
+	mov.b32 	%f1997, %r485;
+	setp.leu.f32 	%p48, %f463, 0f00000000;
+	@%p48 bra 	$L__BB3_74;
+
+	mov.f32 	%f1469, 0f3F800000;
+	sub.f32 	%f1470, %f1469, %f463;
+	mul.lo.s64 	%rd428, %rd32, 48;
+	add.s64 	%rd429, %rd383, %rd428;
+	add.s64 	%rd420, %rd429, 80;
+	// begin inline asm
 	cvta.to.global.u64 %rd419, %rd420;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r466,%r467,%r468,%r469}, [%rd419];
-	// inline asm
-	mov.b32 	 %f1950, %r466;
-	mov.b32 	 %f1951, %r467;
-	mov.b32 	 %f1952, %r468;
-	mov.b32 	 %f1953, %r469;
-	add.s64 	%rd423, %rd420, 16;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r489,%r490,%r491,%r492}, [%rd419];
+	// end inline asm
+	mov.b32 	%f1471, %r489;
+	mov.b32 	%f1472, %r490;
+	mov.b32 	%f1473, %r491;
+	mov.b32 	%f1474, %r492;
+	mul.f32 	%f1475, %f463, %f1471;
+	mul.f32 	%f1476, %f463, %f1472;
+	mul.f32 	%f1477, %f463, %f1473;
+	mul.f32 	%f1478, %f463, %f1474;
+	fma.rn.f32 	%f2002, %f1470, %f2002, %f1475;
+	fma.rn.f32 	%f2003, %f1470, %f2003, %f1476;
+	fma.rn.f32 	%f2004, %f1470, %f2004, %f1477;
+	fma.rn.f32 	%f2005, %f1470, %f2005, %f1478;
+	add.s64 	%rd423, %rd429, 96;
+	// begin inline asm
 	cvta.to.global.u64 %rd422, %rd423;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r470,%r471,%r472,%r473}, [%rd422];
-	// inline asm
-	mov.b32 	 %f1946, %r470;
-	mov.b32 	 %f1947, %r471;
-	mov.b32 	 %f1948, %r472;
-	mov.b32 	 %f1949, %r473;
-	add.s64 	%rd426, %rd420, 32;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r493,%r494,%r495,%r496}, [%rd422];
+	// end inline asm
+	mov.b32 	%f1479, %r493;
+	mov.b32 	%f1480, %r494;
+	mov.b32 	%f1481, %r495;
+	mov.b32 	%f1482, %r496;
+	mul.f32 	%f1483, %f463, %f1479;
+	mul.f32 	%f1484, %f463, %f1480;
+	mul.f32 	%f1485, %f463, %f1481;
+	mul.f32 	%f1486, %f463, %f1482;
+	fma.rn.f32 	%f1998, %f1470, %f1998, %f1483;
+	fma.rn.f32 	%f1999, %f1470, %f1999, %f1484;
+	fma.rn.f32 	%f2000, %f1470, %f2000, %f1485;
+	fma.rn.f32 	%f2001, %f1470, %f2001, %f1486;
+	add.s64 	%rd426, %rd429, 112;
+	// begin inline asm
 	cvta.to.global.u64 %rd425, %rd426;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r474,%r475,%r476,%r477}, [%rd425];
-	// inline asm
-	sub.f32 	%f437, %f1440, %f1441;
-	mov.b32 	 %f1942, %r474;
-	mov.b32 	 %f1943, %r475;
-	mov.b32 	 %f1944, %r476;
-	mov.b32 	 %f1945, %r477;
-	setp.leu.f32	%p45, %f437, 0f00000000;
-	@%p45 bra 	BB3_73;
-
-	mul.lo.s64 	%rd438, %rd35, 48;
-	add.s64 	%rd439, %rd393, %rd438;
-	add.s64 	%rd430, %rd439, 80;
-	// inline asm
-	cvta.to.global.u64 %rd429, %rd430;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r481,%r482,%r483,%r484}, [%rd429];
-	// inline asm
-	mov.b32 	 %f1442, %r481;
-	mov.b32 	 %f1443, %r482;
-	mov.b32 	 %f1444, %r483;
-	mov.b32 	 %f1445, %r484;
-	add.s64 	%rd433, %rd439, 96;
-	// inline asm
-	cvta.to.global.u64 %rd432, %rd433;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r485,%r486,%r487,%r488}, [%rd432];
-	// inline asm
-	mov.b32 	 %f1446, %r485;
-	mov.b32 	 %f1447, %r486;
-	mov.b32 	 %f1448, %r487;
-	mov.b32 	 %f1449, %r488;
-	add.s64 	%rd436, %rd439, 112;
-	// inline asm
-	cvta.to.global.u64 %rd435, %rd436;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r489,%r490,%r491,%r492}, [%rd435];
-	// inline asm
-	mov.f32 	%f1450, 0f3F800000;
-	sub.f32 	%f1451, %f1450, %f437;
-	mul.f32 	%f1452, %f437, %f1442;
-	mul.f32 	%f1453, %f437, %f1443;
-	mul.f32 	%f1454, %f437, %f1444;
-	mul.f32 	%f1455, %f437, %f1445;
-	fma.rn.f32 	%f1950, %f1451, %f1950, %f1452;
-	fma.rn.f32 	%f1951, %f1451, %f1951, %f1453;
-	fma.rn.f32 	%f1952, %f1451, %f1952, %f1454;
-	fma.rn.f32 	%f1953, %f1451, %f1953, %f1455;
-	mul.f32 	%f1456, %f437, %f1446;
-	mul.f32 	%f1457, %f437, %f1447;
-	mul.f32 	%f1458, %f437, %f1448;
-	mul.f32 	%f1459, %f437, %f1449;
-	fma.rn.f32 	%f1946, %f1451, %f1946, %f1456;
-	fma.rn.f32 	%f1947, %f1451, %f1947, %f1457;
-	fma.rn.f32 	%f1948, %f1451, %f1948, %f1458;
-	fma.rn.f32 	%f1949, %f1451, %f1949, %f1459;
-	mov.b32 	 %f1460, %r489;
-	mov.b32 	 %f1461, %r490;
-	mov.b32 	 %f1462, %r491;
-	mov.b32 	 %f1463, %r492;
-	mul.f32 	%f1464, %f437, %f1460;
-	mul.f32 	%f1465, %f437, %f1461;
-	mul.f32 	%f1466, %f437, %f1462;
-	mul.f32 	%f1467, %f437, %f1463;
-	fma.rn.f32 	%f1942, %f1451, %f1942, %f1464;
-	fma.rn.f32 	%f1943, %f1451, %f1943, %f1465;
-	fma.rn.f32 	%f1944, %f1451, %f1944, %f1466;
-	fma.rn.f32 	%f1945, %f1451, %f1945, %f1467;
-	bra.uni 	BB3_73;
-
-BB3_62:
-	mov.f32 	%f1942, 0f00000000;
-	mov.f32 	%f1944, 0f3F800000;
-	setp.eq.s32	%p41, %r345, 4;
-	@%p41 bra 	BB3_65;
-	bra.uni 	BB3_63;
-
-BB3_65:
-	// inline asm
-	call (%rd669), _optix_get_instance_transform_from_handle, (%rd319);
-	// inline asm
-	bra.uni 	BB3_66;
-
-BB3_68:
-	// inline asm
-	call (%rd334), _optix_get_srt_motion_transform_from_handle, (%rd319);
-	// inline asm
-	// inline asm
-	cvta.to.global.u64 %rd336, %rd334;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r359,%r360,%r361,%r362}, [%rd336];
-	// inline asm
-	mov.b32	{%rs10, %rs11}, %r361;
-	add.s64 	%rd340, %rd334, 16;
-	// inline asm
-	cvta.to.global.u64 %rd339, %rd340;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r363,%r364,%r365,%r366}, [%rd339];
-	// inline asm
-	add.s64 	%rd343, %rd334, 32;
-	// inline asm
-	cvta.to.global.u64 %rd342, %rd343;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r367,%r368,%r369,%r370}, [%rd342];
-	// inline asm
-	add.s64 	%rd346, %rd334, 48;
-	// inline asm
-	cvta.to.global.u64 %rd345, %rd346;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r371,%r372,%r373,%r374}, [%rd345];
-	// inline asm
-	add.s64 	%rd349, %rd334, 64;
-	// inline asm
-	cvta.to.global.u64 %rd348, %rd349;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r375,%r376,%r377,%r378}, [%rd348];
-	// inline asm
-	add.s64 	%rd352, %rd334, 80;
-	// inline asm
-	cvta.to.global.u64 %rd351, %rd352;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r379,%r380,%r381,%r382}, [%rd351];
-	// inline asm
-	add.s64 	%rd355, %rd334, 96;
-	// inline asm
-	cvta.to.global.u64 %rd354, %rd355;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r383,%r384,%r385,%r386}, [%rd354];
-	// inline asm
-	add.s64 	%rd358, %rd334, 112;
-	// inline asm
-	cvta.to.global.u64 %rd357, %rd358;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r387,%r388,%r389,%r390}, [%rd357];
-	// inline asm
-	add.s64 	%rd361, %rd334, 128;
-	// inline asm
-	cvta.to.global.u64 %rd360, %rd361;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r391,%r392,%r393,%r394}, [%rd360];
-	// inline asm
-	add.s64 	%rd364, %rd334, 144;
-	// inline asm
-	cvta.to.global.u64 %rd363, %rd364;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r395,%r396,%r397,%r398}, [%rd363];
-	// inline asm
-	mov.b32 	 %f1318, %r362;
-	mov.b32 	 %f1319, %r363;
-	cvt.u32.u16	%r415, %rs10;
-	add.s32 	%r416, %r415, -1;
-	cvt.rn.f32.s32	%f1320, %r416;
-	sub.f32 	%f1321, %f938, %f1318;
-	mul.f32 	%f1322, %f1321, %f1320;
-	sub.f32 	%f1323, %f1319, %f1318;
-	div.rn.f32 	%f1324, %f1322, %f1323;
-	min.f32 	%f1325, %f1320, %f1324;
-	mov.f32 	%f1326, 0f00000000;
-	max.f32 	%f1327, %f1326, %f1325;
-	cvt.rmi.f32.f32	%f1328, %f1327;
-	cvt.rzi.s32.f32	%r417, %f1328;
-	cvt.s64.s32	%rd33, %r417;
-	mul.wide.s32 	%rd378, %r417, 64;
-	add.s64 	%rd367, %rd343, %rd378;
-	// inline asm
-	cvta.to.global.u64 %rd366, %rd367;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r399,%r400,%r401,%r402}, [%rd366];
-	// inline asm
-	mov.b32 	 %f1926, %r399;
-	mov.b32 	 %f1927, %r400;
-	mov.b32 	 %f1928, %r401;
-	mov.b32 	 %f1929, %r402;
-	add.s64 	%rd370, %rd367, 16;
-	// inline asm
-	cvta.to.global.u64 %rd369, %rd370;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r403,%r404,%r405,%r406}, [%rd369];
-	// inline asm
-	mov.b32 	 %f1930, %r403;
-	mov.b32 	 %f1931, %r404;
-	mov.b32 	 %f1932, %r405;
-	mov.b32 	 %f1933, %r406;
-	add.s64 	%rd373, %rd367, 32;
-	// inline asm
-	cvta.to.global.u64 %rd372, %rd373;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r407,%r408,%r409,%r410}, [%rd372];
-	// inline asm
-	sub.f32 	%f376, %f1327, %f1328;
-	mov.b32 	 %f1934, %r407;
-	mov.b32 	 %f1935, %r408;
-	mov.b32 	 %f1936, %r409;
-	mov.b32 	 %f1937, %r410;
-	add.s64 	%rd376, %rd367, 48;
-	// inline asm
-	cvta.to.global.u64 %rd375, %rd376;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r411,%r412,%r413,%r414}, [%rd375];
-	// inline asm
-	mov.b32 	 %f1938, %r411;
-	mov.b32 	 %f1939, %r412;
-	mov.b32 	 %f1940, %r413;
-	mov.b32 	 %f1941, %r414;
-	setp.leu.f32	%p44, %f376, 0f00000000;
-	@%p44 bra 	BB3_70;
-
-	shl.b64 	%rd391, %rd33, 6;
-	add.s64 	%rd392, %rd391, %rd334;
-	add.s64 	%rd380, %rd392, 96;
-	// inline asm
-	cvta.to.global.u64 %rd379, %rd380;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r418,%r419,%r420,%r421}, [%rd379];
-	// inline asm
-	mov.b32 	 %f1329, %r418;
-	mov.b32 	 %f1330, %r419;
-	mov.b32 	 %f1331, %r420;
-	mov.b32 	 %f1332, %r421;
-	add.s64 	%rd383, %rd392, 112;
-	// inline asm
-	cvta.to.global.u64 %rd382, %rd383;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r422,%r423,%r424,%r425}, [%rd382];
-	// inline asm
-	mov.b32 	 %f1333, %r422;
-	mov.b32 	 %f1334, %r423;
-	mov.b32 	 %f1335, %r424;
-	mov.b32 	 %f1336, %r425;
-	add.s64 	%rd386, %rd392, 128;
-	// inline asm
-	cvta.to.global.u64 %rd385, %rd386;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r426,%r427,%r428,%r429}, [%rd385];
-	// inline asm
-	mov.b32 	 %f1337, %r426;
-	mov.b32 	 %f1338, %r427;
-	mov.b32 	 %f1339, %r428;
-	mov.b32 	 %f1340, %r429;
-	add.s64 	%rd389, %rd392, 144;
-	// inline asm
-	cvta.to.global.u64 %rd388, %rd389;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r430,%r431,%r432,%r433}, [%rd388];
-	// inline asm
-	mov.f32 	%f1341, 0f3F800000;
-	sub.f32 	%f1342, %f1341, %f376;
-	mul.f32 	%f1343, %f376, %f1329;
-	mul.f32 	%f1344, %f376, %f1330;
-	mul.f32 	%f1345, %f376, %f1331;
-	mul.f32 	%f1346, %f376, %f1332;
-	fma.rn.f32 	%f1926, %f1342, %f1926, %f1343;
-	fma.rn.f32 	%f1927, %f1342, %f1927, %f1344;
-	fma.rn.f32 	%f1928, %f1342, %f1928, %f1345;
-	fma.rn.f32 	%f1929, %f1342, %f1929, %f1346;
-	mul.f32 	%f1347, %f376, %f1333;
-	mul.f32 	%f1348, %f376, %f1334;
-	mul.f32 	%f1349, %f376, %f1335;
-	mul.f32 	%f1350, %f376, %f1336;
-	fma.rn.f32 	%f1930, %f1342, %f1930, %f1347;
-	fma.rn.f32 	%f1931, %f1342, %f1931, %f1348;
-	fma.rn.f32 	%f1932, %f1342, %f1932, %f1349;
-	fma.rn.f32 	%f1933, %f1342, %f1933, %f1350;
-	mul.f32 	%f1351, %f376, %f1337;
-	mul.f32 	%f1352, %f376, %f1338;
-	mul.f32 	%f1353, %f376, %f1339;
-	mul.f32 	%f1354, %f376, %f1340;
-	fma.rn.f32 	%f1934, %f1342, %f1934, %f1351;
-	fma.rn.f32 	%f1355, %f1342, %f1935, %f1352;
-	fma.rn.f32 	%f1356, %f1342, %f1936, %f1353;
-	fma.rn.f32 	%f1357, %f1342, %f1937, %f1354;
-	mov.b32 	 %f1358, %r430;
-	mov.b32 	 %f1359, %r431;
-	mov.b32 	 %f1360, %r432;
-	mov.b32 	 %f1361, %r433;
-	mul.f32 	%f1362, %f376, %f1358;
-	mul.f32 	%f1363, %f376, %f1359;
-	mul.f32 	%f1364, %f376, %f1360;
-	mul.f32 	%f1365, %f376, %f1361;
-	fma.rn.f32 	%f1366, %f1342, %f1938, %f1362;
-	fma.rn.f32 	%f1939, %f1342, %f1939, %f1363;
-	fma.rn.f32 	%f1940, %f1342, %f1940, %f1364;
-	fma.rn.f32 	%f1941, %f1342, %f1941, %f1365;
-	mul.f32 	%f1367, %f1356, %f1356;
-	fma.rn.f32 	%f1368, %f1355, %f1355, %f1367;
-	fma.rn.f32 	%f1369, %f1357, %f1357, %f1368;
-	fma.rn.f32 	%f1370, %f1366, %f1366, %f1369;
-	sqrt.rn.f32 	%f1371, %f1370;
-	rcp.rn.f32 	%f1372, %f1371;
-	mul.f32 	%f1935, %f1355, %f1372;
-	mul.f32 	%f1936, %f1356, %f1372;
-	mul.f32 	%f1937, %f1357, %f1372;
-	mul.f32 	%f1938, %f1366, %f1372;
-
-BB3_70:
-	mul.f32 	%f1373, %f1936, %f1936;
-	fma.rn.f32 	%f1374, %f1935, %f1935, %f1373;
-	fma.rn.f32 	%f1375, %f1937, %f1937, %f1374;
-	fma.rn.f32 	%f1376, %f1938, %f1938, %f1375;
-	rcp.rn.f32 	%f1377, %f1376;
-	mul.f32 	%f1378, %f1935, %f1377;
-	mul.f32 	%f1379, %f1936, %f1377;
-	mul.f32 	%f1380, %f1937, %f1377;
-	mul.f32 	%f1381, %f1938, %f1377;
-	mul.f32 	%f1382, %f1935, %f1378;
-	mul.f32 	%f1383, %f1936, %f1379;
-	mul.f32 	%f1384, %f1937, %f1380;
-	mul.f32 	%f1385, %f1935, %f1379;
-	mul.f32 	%f1386, %f1937, %f1381;
-	mul.f32 	%f1387, %f1935, %f1380;
-	mul.f32 	%f1388, %f1936, %f1381;
-	mul.f32 	%f1389, %f1936, %f1380;
-	mul.f32 	%f1390, %f1935, %f1381;
-	sub.f32 	%f1391, %f1382, %f1383;
-	sub.f32 	%f1392, %f1391, %f1384;
-	fma.rn.f32 	%f1393, %f1938, %f1381, %f1392;
-	sub.f32 	%f1394, %f1385, %f1386;
-	add.f32 	%f1395, %f1394, %f1394;
-	add.f32 	%f1396, %f1387, %f1388;
-	add.f32 	%f1397, %f1396, %f1396;
-	add.f32 	%f1398, %f1385, %f1386;
-	add.f32 	%f1399, %f1398, %f1398;
-	sub.f32 	%f1400, %f1383, %f1382;
-	sub.f32 	%f1401, %f1400, %f1384;
-	fma.rn.f32 	%f1402, %f1938, %f1381, %f1401;
-	sub.f32 	%f1403, %f1389, %f1390;
-	add.f32 	%f1404, %f1403, %f1403;
-	sub.f32 	%f1405, %f1387, %f1388;
-	add.f32 	%f1406, %f1405, %f1405;
-	add.f32 	%f1407, %f1389, %f1390;
-	add.f32 	%f1408, %f1407, %f1407;
-	neg.f32 	%f1409, %f1382;
-	sub.f32 	%f1410, %f1409, %f1383;
-	add.f32 	%f1411, %f1384, %f1410;
-	fma.rn.f32 	%f1412, %f1938, %f1381, %f1411;
-	mul.f32 	%f1413, %f1929, %f1393;
-	fma.rn.f32 	%f1414, %f1932, %f1395, %f1413;
-	fma.rn.f32 	%f1415, %f1934, %f1397, %f1414;
-	sub.f32 	%f1953, %f1939, %f1415;
-	mul.f32 	%f1416, %f1932, %f1402;
-	fma.rn.f32 	%f1417, %f1929, %f1399, %f1416;
-	fma.rn.f32 	%f1418, %f1934, %f1404, %f1417;
-	sub.f32 	%f1949, %f1940, %f1418;
-	mul.f32 	%f1419, %f1932, %f1408;
-	fma.rn.f32 	%f1420, %f1929, %f1406, %f1419;
-	fma.rn.f32 	%f1421, %f1934, %f1412, %f1420;
-	sub.f32 	%f1945, %f1941, %f1421;
-	mul.f32 	%f1422, %f1928, %f1393;
-	fma.rn.f32 	%f1423, %f1931, %f1395, %f1422;
-	fma.rn.f32 	%f1952, %f1933, %f1397, %f1423;
-	mul.f32 	%f1424, %f1931, %f1402;
-	fma.rn.f32 	%f1425, %f1928, %f1399, %f1424;
-	fma.rn.f32 	%f1948, %f1933, %f1404, %f1425;
-	mul.f32 	%f1426, %f1931, %f1408;
-	fma.rn.f32 	%f1427, %f1928, %f1406, %f1426;
-	fma.rn.f32 	%f1944, %f1933, %f1412, %f1427;
-	mul.f32 	%f1428, %f1927, %f1393;
-	fma.rn.f32 	%f1951, %f1930, %f1395, %f1428;
-	mul.f32 	%f1429, %f1930, %f1402;
-	fma.rn.f32 	%f1947, %f1927, %f1399, %f1429;
-	mul.f32 	%f1430, %f1930, %f1408;
-	fma.rn.f32 	%f1943, %f1927, %f1406, %f1430;
-	mul.f32 	%f1950, %f1926, %f1393;
-	mul.f32 	%f1946, %f1926, %f1399;
-	mul.f32 	%f1942, %f1926, %f1406;
-	bra.uni 	BB3_73;
-
-BB3_63:
-	setp.ne.s32	%p42, %r345, 1;
-	mov.f32 	%f1943, %f1942;
-	mov.f32 	%f1945, %f1942;
-	mov.f32 	%f1946, %f1942;
-	mov.f32 	%f1947, %f1944;
-	mov.f32 	%f1948, %f1942;
-	mov.f32 	%f1949, %f1942;
-	mov.f32 	%f1950, %f1944;
-	mov.f32 	%f1951, %f1942;
-	mov.f32 	%f1952, %f1942;
-	mov.f32 	%f1953, %f1942;
-	@%p42 bra 	BB3_73;
-
-	// inline asm
-	call (%rd321), _optix_get_static_transform_from_handle, (%rd319);
-	// inline asm
-	add.s64 	%rd669, %rd321, 16;
-
-BB3_66:
-	// inline asm
-	cvta.to.global.u64 %rd325, %rd669;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r347,%r348,%r349,%r350}, [%rd325];
-	// inline asm
-	mov.b32 	 %f1950, %r347;
-	mov.b32 	 %f1951, %r348;
-	mov.b32 	 %f1952, %r349;
-	mov.b32 	 %f1953, %r350;
-	add.s64 	%rd329, %rd669, 16;
-	// inline asm
-	cvta.to.global.u64 %rd328, %rd329;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r351,%r352,%r353,%r354}, [%rd328];
-	// inline asm
-	mov.b32 	 %f1946, %r351;
-	mov.b32 	 %f1947, %r352;
-	mov.b32 	 %f1948, %r353;
-	mov.b32 	 %f1949, %r354;
-	add.s64 	%rd332, %rd669, 32;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r497,%r498,%r499,%r500}, [%rd425];
+	// end inline asm
+	mov.b32 	%f1487, %r497;
+	mov.b32 	%f1488, %r498;
+	mov.b32 	%f1489, %r499;
+	mov.b32 	%f1490, %r500;
+	mul.f32 	%f1491, %f463, %f1487;
+	mul.f32 	%f1492, %f463, %f1488;
+	mul.f32 	%f1493, %f463, %f1489;
+	mul.f32 	%f1494, %f463, %f1490;
+	fma.rn.f32 	%f1994, %f1470, %f1994, %f1491;
+	fma.rn.f32 	%f1995, %f1470, %f1995, %f1492;
+	fma.rn.f32 	%f1996, %f1470, %f1996, %f1493;
+	fma.rn.f32 	%f1997, %f1470, %f1997, %f1494;
+	bra.uni 	$L__BB3_74;
+
+$L__BB3_63:
+	mov.f32 	%f1994, 0f00000000;
+	mov.f32 	%f1996, 0f3F800000;
+	setp.eq.s32 	%p44, %r353, 4;
+	@%p44 bra 	$L__BB3_66;
+
+	setp.ne.s32 	%p45, %r353, 1;
+	mov.f32 	%f1995, %f1994;
+	mov.f32 	%f1997, %f1994;
+	mov.f32 	%f1998, %f1994;
+	mov.f32 	%f1999, %f1996;
+	mov.f32 	%f2000, %f1994;
+	mov.f32 	%f2001, %f1994;
+	mov.f32 	%f2002, %f1996;
+	mov.f32 	%f2003, %f1994;
+	mov.f32 	%f2004, %f1994;
+	mov.f32 	%f2005, %f1994;
+	@%p45 bra 	$L__BB3_74;
+
+	// begin inline asm
+	call (%rd313), _optix_get_static_transform_from_handle, (%rd311);
+	// end inline asm
+	add.s64 	%rd656, %rd313, 16;
+	bra.uni 	$L__BB3_67;
+
+$L__BB3_69:
+	// begin inline asm
+	call (%rd326), _optix_get_srt_motion_transform_from_handle, (%rd311);
+	// end inline asm
+	// begin inline asm
+	cvta.to.global.u64 %rd328, %rd326;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r367,%r368,%r369,%r370}, [%rd328];
+	// end inline asm
+	add.s64 	%rd332, %rd326, 16;
+	// begin inline asm
 	cvta.to.global.u64 %rd331, %rd332;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r355,%r356,%r357,%r358}, [%rd331];
-	// inline asm
-	mov.b32 	 %f1942, %r355;
-	mov.b32 	 %f1943, %r356;
-	mov.b32 	 %f1944, %r357;
-	mov.b32 	 %f1945, %r358;
-
-BB3_73:
-	add.s32 	%r18, %r648, 1;
-	setp.eq.s32	%p46, %r18, %r30;
-	@%p46 bra 	BB3_74;
-	bra.uni 	BB3_75;
-
-BB3_74:
-	mov.f32 	%f1925, %f1942;
-	mov.f32 	%f1924, %f1943;
-	mov.f32 	%f1923, %f1944;
-	mov.f32 	%f1922, %f1945;
-	mov.f32 	%f1921, %f1946;
-	mov.f32 	%f1920, %f1947;
-	mov.f32 	%f1919, %f1948;
-	mov.f32 	%f1918, %f1949;
-	mov.f32 	%f1917, %f1950;
-	mov.f32 	%f1916, %f1951;
-	mov.f32 	%f1915, %f1952;
-	mov.f32 	%f1914, %f1953;
-	bra.uni 	BB3_76;
-
-BB3_75:
-	mul.f32 	%f1468, %f1921, %f1951;
-	fma.rn.f32 	%f1469, %f1917, %f1950, %f1468;
-	fma.rn.f32 	%f466, %f1925, %f1952, %f1469;
-	mul.f32 	%f1470, %f1920, %f1951;
-	fma.rn.f32 	%f1471, %f1916, %f1950, %f1470;
-	fma.rn.f32 	%f467, %f1924, %f1952, %f1471;
-	mul.f32 	%f1472, %f1919, %f1951;
-	fma.rn.f32 	%f1473, %f1915, %f1950, %f1472;
-	fma.rn.f32 	%f468, %f1923, %f1952, %f1473;
-	mul.f32 	%f1474, %f1918, %f1951;
-	fma.rn.f32 	%f1475, %f1914, %f1950, %f1474;
-	fma.rn.f32 	%f1476, %f1922, %f1952, %f1475;
-	add.f32 	%f469, %f1953, %f1476;
-	mul.f32 	%f1477, %f1921, %f1947;
-	fma.rn.f32 	%f1478, %f1917, %f1946, %f1477;
-	fma.rn.f32 	%f470, %f1925, %f1948, %f1478;
-	mul.f32 	%f1479, %f1920, %f1947;
-	fma.rn.f32 	%f1480, %f1916, %f1946, %f1479;
-	fma.rn.f32 	%f471, %f1924, %f1948, %f1480;
-	mul.f32 	%f1481, %f1919, %f1947;
-	fma.rn.f32 	%f1482, %f1915, %f1946, %f1481;
-	fma.rn.f32 	%f472, %f1923, %f1948, %f1482;
-	mul.f32 	%f1483, %f1918, %f1947;
-	fma.rn.f32 	%f1484, %f1914, %f1946, %f1483;
-	fma.rn.f32 	%f1485, %f1922, %f1948, %f1484;
-	add.f32 	%f473, %f1949, %f1485;
-	mul.f32 	%f1486, %f1921, %f1943;
-	fma.rn.f32 	%f1487, %f1917, %f1942, %f1486;
-	fma.rn.f32 	%f1925, %f1925, %f1944, %f1487;
-	mul.f32 	%f1488, %f1920, %f1943;
-	fma.rn.f32 	%f1489, %f1916, %f1942, %f1488;
-	fma.rn.f32 	%f1924, %f1924, %f1944, %f1489;
-	mul.f32 	%f1490, %f1919, %f1943;
-	fma.rn.f32 	%f1491, %f1915, %f1942, %f1490;
-	fma.rn.f32 	%f1923, %f1923, %f1944, %f1491;
-	mul.f32 	%f1492, %f1918, %f1943;
-	fma.rn.f32 	%f1493, %f1914, %f1942, %f1492;
-	fma.rn.f32 	%f1494, %f1922, %f1944, %f1493;
-	add.f32 	%f1922, %f1945, %f1494;
-	mov.f32 	%f1921, %f470;
-	mov.f32 	%f1920, %f471;
-	mov.f32 	%f1919, %f472;
-	mov.f32 	%f1918, %f473;
-	mov.f32 	%f1917, %f466;
-	mov.f32 	%f1916, %f467;
-	mov.f32 	%f1915, %f468;
-	mov.f32 	%f1914, %f469;
-
-BB3_76:
-	add.s32 	%r648, %r18, -2;
-	setp.gt.s32	%p47, %r648, -1;
-	@%p47 bra 	BB3_61;
-
-BB3_77:
-	mov.u32 	%r649, 0;
-	mov.f32 	%f1978, %f1979;
-	mov.f32 	%f1983, %f1979;
-	mov.f32 	%f1982, %f1980;
-	mov.f32 	%f1981, %f1979;
-	mov.f32 	%f1986, %f1979;
-	mov.f32 	%f1985, %f1979;
-	mov.f32 	%f1984, %f1980;
-	@%p2 bra 	BB3_95;
-
-BB3_78:
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r371,%r372,%r373,%r374}, [%rd331];
+	// end inline asm
+	add.s64 	%rd335, %rd326, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd334, %rd335;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r375,%r376,%r377,%r378}, [%rd334];
+	// end inline asm
+	add.s64 	%rd338, %rd326, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd337, %rd338;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r379,%r380,%r381,%r382}, [%rd337];
+	// end inline asm
+	add.s64 	%rd341, %rd326, 64;
+	// begin inline asm
+	cvta.to.global.u64 %rd340, %rd341;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r383,%r384,%r385,%r386}, [%rd340];
+	// end inline asm
+	add.s64 	%rd344, %rd326, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd343, %rd344;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r387,%r388,%r389,%r390}, [%rd343];
+	// end inline asm
+	add.s64 	%rd347, %rd326, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd346, %rd347;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r391,%r392,%r393,%r394}, [%rd346];
+	// end inline asm
+	add.s64 	%rd350, %rd326, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd349, %rd350;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r395,%r396,%r397,%r398}, [%rd349];
+	// end inline asm
+	add.s64 	%rd353, %rd326, 128;
+	// begin inline asm
+	cvta.to.global.u64 %rd352, %rd353;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r399,%r400,%r401,%r402}, [%rd352];
+	// end inline asm
+	add.s64 	%rd356, %rd326, 144;
+	// begin inline asm
+	cvta.to.global.u64 %rd355, %rd356;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r403,%r404,%r405,%r406}, [%rd355];
+	// end inline asm
+	mov.b32 	%f1345, %r370;
+	mov.b32 	%f1346, %r371;
+	and.b32  	%r423, %r369, 65535;
+	add.s32 	%r424, %r423, -1;
+	cvt.rn.f32.s32 	%f1347, %r424;
+	sub.f32 	%f1348, %f1330, %f1345;
+	mul.f32 	%f1349, %f1348, %f1347;
+	sub.f32 	%f1350, %f1346, %f1345;
+	div.rn.f32 	%f1351, %f1349, %f1350;
+	min.f32 	%f1352, %f1347, %f1351;
+	mov.f32 	%f1353, 0f00000000;
+	max.f32 	%f1354, %f1353, %f1352;
+	cvt.rmi.f32.f32 	%f1355, %f1354;
+	sub.f32 	%f402, %f1354, %f1355;
+	cvt.rzi.s32.f32 	%r425, %f1355;
+	mul.wide.s32 	%rd370, %r425, 64;
+	add.s64 	%rd359, %rd335, %rd370;
+	// begin inline asm
+	cvta.to.global.u64 %rd358, %rd359;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r407,%r408,%r409,%r410}, [%rd358];
+	// end inline asm
+	mov.b32 	%f1978, %r407;
+	mov.b32 	%f1979, %r408;
+	mov.b32 	%f1980, %r409;
+	mov.b32 	%f1981, %r410;
+	add.s64 	%rd362, %rd359, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd361, %rd362;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r411,%r412,%r413,%r414}, [%rd361];
+	// end inline asm
+	mov.b32 	%f1982, %r411;
+	mov.b32 	%f1983, %r412;
+	mov.b32 	%f1984, %r413;
+	mov.b32 	%f1985, %r414;
+	add.s64 	%rd365, %rd359, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd364, %rd365;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r415,%r416,%r417,%r418}, [%rd364];
+	// end inline asm
+	mov.b32 	%f1986, %r415;
+	mov.b32 	%f1987, %r416;
+	mov.b32 	%f1988, %r417;
+	mov.b32 	%f1989, %r418;
+	add.s64 	%rd368, %rd359, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd367, %rd368;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r419,%r420,%r421,%r422}, [%rd367];
+	// end inline asm
+	mov.b32 	%f1990, %r419;
+	mov.b32 	%f1991, %r420;
+	mov.b32 	%f1992, %r421;
+	mov.b32 	%f1993, %r422;
+	setp.leu.f32 	%p47, %f402, 0f00000000;
+	@%p47 bra 	$L__BB3_71;
+
+	mov.f32 	%f1356, 0f3F800000;
+	sub.f32 	%f1357, %f1356, %f402;
+	add.s64 	%rd372, %rd359, 64;
+	// begin inline asm
+	cvta.to.global.u64 %rd371, %rd372;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r426,%r427,%r428,%r429}, [%rd371];
+	// end inline asm
+	mov.b32 	%f1358, %r426;
+	mov.b32 	%f1359, %r427;
+	mov.b32 	%f1360, %r428;
+	mov.b32 	%f1361, %r429;
+	mul.f32 	%f1362, %f402, %f1358;
+	mul.f32 	%f1363, %f402, %f1359;
+	mul.f32 	%f1364, %f402, %f1360;
+	mul.f32 	%f1365, %f402, %f1361;
+	fma.rn.f32 	%f1978, %f1357, %f1978, %f1362;
+	fma.rn.f32 	%f1979, %f1357, %f1979, %f1363;
+	fma.rn.f32 	%f1980, %f1357, %f1980, %f1364;
+	fma.rn.f32 	%f1981, %f1357, %f1981, %f1365;
+	add.s64 	%rd375, %rd359, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd374, %rd375;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r430,%r431,%r432,%r433}, [%rd374];
+	// end inline asm
+	mov.b32 	%f1366, %r430;
+	mov.b32 	%f1367, %r431;
+	mov.b32 	%f1368, %r432;
+	mov.b32 	%f1369, %r433;
+	mul.f32 	%f1370, %f402, %f1366;
+	mul.f32 	%f1371, %f402, %f1367;
+	mul.f32 	%f1372, %f402, %f1368;
+	mul.f32 	%f1373, %f402, %f1369;
+	fma.rn.f32 	%f1982, %f1357, %f1982, %f1370;
+	fma.rn.f32 	%f1983, %f1357, %f1983, %f1371;
+	fma.rn.f32 	%f1984, %f1357, %f1984, %f1372;
+	fma.rn.f32 	%f1985, %f1357, %f1985, %f1373;
+	add.s64 	%rd378, %rd359, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd377, %rd378;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r434,%r435,%r436,%r437}, [%rd377];
+	// end inline asm
+	mov.b32 	%f1374, %r434;
+	mov.b32 	%f1375, %r435;
+	mov.b32 	%f1376, %r436;
+	mov.b32 	%f1377, %r437;
+	mul.f32 	%f1378, %f402, %f1374;
+	mul.f32 	%f1379, %f402, %f1375;
+	mul.f32 	%f1380, %f402, %f1376;
+	mul.f32 	%f1381, %f402, %f1377;
+	fma.rn.f32 	%f1986, %f1357, %f1986, %f1378;
+	fma.rn.f32 	%f1382, %f1357, %f1987, %f1379;
+	fma.rn.f32 	%f1383, %f1357, %f1988, %f1380;
+	fma.rn.f32 	%f1384, %f1357, %f1989, %f1381;
+	add.s64 	%rd381, %rd359, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd380, %rd381;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r438,%r439,%r440,%r441}, [%rd380];
+	// end inline asm
+	mov.b32 	%f1385, %r438;
+	mov.b32 	%f1386, %r439;
+	mov.b32 	%f1387, %r440;
+	mov.b32 	%f1388, %r441;
+	mul.f32 	%f1389, %f402, %f1385;
+	mul.f32 	%f1390, %f402, %f1386;
+	mul.f32 	%f1391, %f402, %f1387;
+	mul.f32 	%f1392, %f402, %f1388;
+	fma.rn.f32 	%f1393, %f1357, %f1990, %f1389;
+	fma.rn.f32 	%f1991, %f1357, %f1991, %f1390;
+	fma.rn.f32 	%f1992, %f1357, %f1992, %f1391;
+	fma.rn.f32 	%f1993, %f1357, %f1993, %f1392;
+	mul.f32 	%f1394, %f1383, %f1383;
+	fma.rn.f32 	%f1395, %f1382, %f1382, %f1394;
+	fma.rn.f32 	%f1396, %f1384, %f1384, %f1395;
+	fma.rn.f32 	%f1397, %f1393, %f1393, %f1396;
+	sqrt.rn.f32 	%f1398, %f1397;
+	rcp.rn.f32 	%f1399, %f1398;
+	mul.f32 	%f1987, %f1382, %f1399;
+	mul.f32 	%f1988, %f1383, %f1399;
+	mul.f32 	%f1989, %f1384, %f1399;
+	mul.f32 	%f1990, %f1399, %f1393;
+
+$L__BB3_71:
+	mul.f32 	%f1400, %f1988, %f1988;
+	fma.rn.f32 	%f1401, %f1987, %f1987, %f1400;
+	fma.rn.f32 	%f1402, %f1989, %f1989, %f1401;
+	fma.rn.f32 	%f1403, %f1990, %f1990, %f1402;
+	rcp.rn.f32 	%f1404, %f1403;
+	mul.f32 	%f1405, %f1987, %f1404;
+	mul.f32 	%f1406, %f1988, %f1404;
+	mul.f32 	%f1407, %f1989, %f1404;
+	mul.f32 	%f1408, %f1990, %f1404;
+	mul.f32 	%f1409, %f1987, %f1405;
+	mul.f32 	%f1410, %f1988, %f1406;
+	mul.f32 	%f1411, %f1989, %f1407;
+	mul.f32 	%f1412, %f1987, %f1406;
+	mul.f32 	%f1413, %f1989, %f1408;
+	mul.f32 	%f1414, %f1987, %f1407;
+	mul.f32 	%f1415, %f1988, %f1408;
+	mul.f32 	%f1416, %f1988, %f1407;
+	mul.f32 	%f1417, %f1987, %f1408;
+	sub.f32 	%f1418, %f1409, %f1410;
+	sub.f32 	%f1419, %f1418, %f1411;
+	fma.rn.f32 	%f1420, %f1990, %f1408, %f1419;
+	sub.f32 	%f1421, %f1412, %f1413;
+	add.f32 	%f1422, %f1421, %f1421;
+	add.f32 	%f1423, %f1414, %f1415;
+	add.f32 	%f1424, %f1423, %f1423;
+	add.f32 	%f1425, %f1412, %f1413;
+	add.f32 	%f1426, %f1425, %f1425;
+	sub.f32 	%f1427, %f1410, %f1409;
+	sub.f32 	%f1428, %f1427, %f1411;
+	fma.rn.f32 	%f1429, %f1990, %f1408, %f1428;
+	sub.f32 	%f1430, %f1416, %f1417;
+	add.f32 	%f1431, %f1430, %f1430;
+	sub.f32 	%f1432, %f1414, %f1415;
+	add.f32 	%f1433, %f1432, %f1432;
+	add.f32 	%f1434, %f1416, %f1417;
+	add.f32 	%f1435, %f1434, %f1434;
+	neg.f32 	%f1436, %f1409;
+	sub.f32 	%f1437, %f1436, %f1410;
+	add.f32 	%f1438, %f1411, %f1437;
+	fma.rn.f32 	%f1439, %f1990, %f1408, %f1438;
+	mul.f32 	%f1440, %f1981, %f1420;
+	fma.rn.f32 	%f1441, %f1984, %f1422, %f1440;
+	fma.rn.f32 	%f1442, %f1986, %f1424, %f1441;
+	sub.f32 	%f2005, %f1991, %f1442;
+	mul.f32 	%f1443, %f1984, %f1429;
+	fma.rn.f32 	%f1444, %f1981, %f1426, %f1443;
+	fma.rn.f32 	%f1445, %f1986, %f1431, %f1444;
+	sub.f32 	%f2001, %f1992, %f1445;
+	mul.f32 	%f1446, %f1984, %f1435;
+	fma.rn.f32 	%f1447, %f1981, %f1433, %f1446;
+	fma.rn.f32 	%f1448, %f1986, %f1439, %f1447;
+	sub.f32 	%f1997, %f1993, %f1448;
+	mul.f32 	%f1449, %f1980, %f1420;
+	fma.rn.f32 	%f1450, %f1983, %f1422, %f1449;
+	fma.rn.f32 	%f2004, %f1985, %f1424, %f1450;
+	mul.f32 	%f1451, %f1983, %f1429;
+	fma.rn.f32 	%f1452, %f1980, %f1426, %f1451;
+	fma.rn.f32 	%f2000, %f1985, %f1431, %f1452;
+	mul.f32 	%f1453, %f1983, %f1435;
+	fma.rn.f32 	%f1454, %f1980, %f1433, %f1453;
+	fma.rn.f32 	%f1996, %f1985, %f1439, %f1454;
+	mul.f32 	%f1455, %f1979, %f1420;
+	fma.rn.f32 	%f2003, %f1982, %f1422, %f1455;
+	mul.f32 	%f1456, %f1982, %f1429;
+	fma.rn.f32 	%f1999, %f1979, %f1426, %f1456;
+	mul.f32 	%f1457, %f1982, %f1435;
+	fma.rn.f32 	%f1995, %f1979, %f1433, %f1457;
+	mul.f32 	%f2002, %f1978, %f1420;
+	mul.f32 	%f1998, %f1978, %f1426;
+	mul.f32 	%f1994, %f1978, %f1433;
+	bra.uni 	$L__BB3_74;
+
+$L__BB3_66:
+	// begin inline asm
+	call (%rd656), _optix_get_instance_transform_from_handle, (%rd311);
+	// end inline asm
+
+$L__BB3_67:
+	// begin inline asm
+	cvta.to.global.u64 %rd317, %rd656;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r355,%r356,%r357,%r358}, [%rd317];
+	// end inline asm
+	mov.b32 	%f2002, %r355;
+	mov.b32 	%f2003, %r356;
+	mov.b32 	%f2004, %r357;
+	mov.b32 	%f2005, %r358;
+	add.s64 	%rd321, %rd656, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd320, %rd321;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r359,%r360,%r361,%r362}, [%rd320];
+	// end inline asm
+	mov.b32 	%f1998, %r359;
+	mov.b32 	%f1999, %r360;
+	mov.b32 	%f2000, %r361;
+	mov.b32 	%f2001, %r362;
+	add.s64 	%rd324, %rd656, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd323, %rd324;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r363,%r364,%r365,%r366}, [%rd323];
+	// end inline asm
+	mov.b32 	%f1994, %r363;
+	mov.b32 	%f1995, %r364;
+	mov.b32 	%f1996, %r365;
+	mov.b32 	%f1997, %r366;
+
+$L__BB3_74:
+	setp.eq.s32 	%p49, %r658, 1;
+	@%p49 bra 	$L__BB3_76;
+
+	mul.f32 	%f1495, %f1973, %f2003;
+	fma.rn.f32 	%f1496, %f1969, %f2002, %f1495;
+	fma.rn.f32 	%f500, %f1977, %f2004, %f1496;
+	mul.f32 	%f1497, %f1972, %f2003;
+	fma.rn.f32 	%f1498, %f1968, %f2002, %f1497;
+	fma.rn.f32 	%f501, %f1976, %f2004, %f1498;
+	mul.f32 	%f1499, %f1971, %f2003;
+	fma.rn.f32 	%f1500, %f1967, %f2002, %f1499;
+	fma.rn.f32 	%f502, %f1975, %f2004, %f1500;
+	mul.f32 	%f1501, %f1970, %f2003;
+	fma.rn.f32 	%f1502, %f1966, %f2002, %f1501;
+	fma.rn.f32 	%f1503, %f1974, %f2004, %f1502;
+	add.f32 	%f2005, %f2005, %f1503;
+	mul.f32 	%f1504, %f1973, %f1999;
+	fma.rn.f32 	%f1505, %f1969, %f1998, %f1504;
+	fma.rn.f32 	%f504, %f1977, %f2000, %f1505;
+	mul.f32 	%f1506, %f1972, %f1999;
+	fma.rn.f32 	%f1507, %f1968, %f1998, %f1506;
+	fma.rn.f32 	%f505, %f1976, %f2000, %f1507;
+	mul.f32 	%f1508, %f1971, %f1999;
+	fma.rn.f32 	%f1509, %f1967, %f1998, %f1508;
+	fma.rn.f32 	%f506, %f1975, %f2000, %f1509;
+	mul.f32 	%f1510, %f1970, %f1999;
+	fma.rn.f32 	%f1511, %f1966, %f1998, %f1510;
+	fma.rn.f32 	%f1512, %f1974, %f2000, %f1511;
+	add.f32 	%f2001, %f2001, %f1512;
+	mul.f32 	%f1513, %f1973, %f1995;
+	fma.rn.f32 	%f1514, %f1969, %f1994, %f1513;
+	fma.rn.f32 	%f508, %f1977, %f1996, %f1514;
+	mul.f32 	%f1515, %f1972, %f1995;
+	fma.rn.f32 	%f1516, %f1968, %f1994, %f1515;
+	fma.rn.f32 	%f509, %f1976, %f1996, %f1516;
+	mul.f32 	%f1517, %f1971, %f1995;
+	fma.rn.f32 	%f1518, %f1967, %f1994, %f1517;
+	fma.rn.f32 	%f510, %f1975, %f1996, %f1518;
+	mul.f32 	%f1519, %f1970, %f1995;
+	fma.rn.f32 	%f1520, %f1966, %f1994, %f1519;
+	fma.rn.f32 	%f1521, %f1974, %f1996, %f1520;
+	add.f32 	%f1997, %f1997, %f1521;
+	mov.f32 	%f1994, %f508;
+	mov.f32 	%f1995, %f509;
+	mov.f32 	%f1996, %f510;
+	mov.f32 	%f1998, %f504;
+	mov.f32 	%f1999, %f505;
+	mov.f32 	%f2000, %f506;
+	mov.f32 	%f2002, %f500;
+	mov.f32 	%f2003, %f501;
+	mov.f32 	%f2004, %f502;
+
+$L__BB3_76:
+	add.s32 	%r658, %r658, -1;
+	add.s32 	%r657, %r657, -1;
+	setp.gt.s32 	%p50, %r657, 1;
+	mov.f32 	%f1966, %f2005;
+	mov.f32 	%f1967, %f2004;
+	mov.f32 	%f1968, %f2003;
+	mov.f32 	%f1969, %f2002;
+	mov.f32 	%f1970, %f2001;
+	mov.f32 	%f1971, %f2000;
+	mov.f32 	%f1972, %f1999;
+	mov.f32 	%f1973, %f1998;
+	mov.f32 	%f1974, %f1997;
+	mov.f32 	%f1975, %f1996;
+	mov.f32 	%f1976, %f1995;
+	mov.f32 	%f1977, %f1994;
+	@%p50 bra 	$L__BB3_62;
+
+$L__BB3_77:
+	// begin inline asm
+	call (%r501), _optix_get_transform_list_size, ();
+	// end inline asm
+	setp.eq.s32 	%p51, %r501, 0;
+	mov.f32 	%f2066, %f2065;
+	mov.f32 	%f2061, %f2065;
+	mov.f32 	%f2062, %f2064;
+	mov.f32 	%f2063, %f2065;
+	mov.f32 	%f2058, %f2065;
+	mov.f32 	%f2059, %f2065;
+	mov.f32 	%f2060, %f2064;
+	@%p51 bra 	$L__BB3_96;
+
+	// begin inline asm
+	call (%r502), _optix_get_transform_list_size, ();
+	// end inline asm
+	// begin inline asm
+	call (%f1531), _optix_get_ray_time, ();
+	// end inline asm
+	setp.eq.s32 	%p52, %r502, 0;
+	@%p52 bra 	$L__BB3_96;
+
+	mov.u32 	%r659, 0;
+
+$L__BB3_80:
 	.pragma "nounroll";
-	// inline asm
-	call (%rd440), _optix_get_transform_list_handle, (%r649);
-	// inline asm
-	// inline asm
-	call (%r495), _optix_get_transform_type_from_handle, (%rd440);
-	// inline asm
-	and.b32  	%r496, %r495, -2;
-	setp.eq.s32	%p49, %r496, 2;
-	@%p49 bra 	BB3_84;
-	bra.uni 	BB3_79;
-
-BB3_84:
-	setp.eq.s32	%p52, %r495, 2;
-	@%p52 bra 	BB3_88;
-	bra.uni 	BB3_85;
-
-BB3_88:
-	// inline asm
-	call (%rd514), _optix_get_matrix_motion_transform_from_handle, (%rd440);
-	// inline asm
-	// inline asm
-	cvta.to.global.u64 %rd516, %rd514;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r584,%r585,%r586,%r587}, [%rd516];
-	// inline asm
-	mov.b32	{%rs16, %rs17}, %r586;
-	add.s64 	%rd520, %rd514, 16;
-	// inline asm
+	// begin inline asm
+	call (%rd430), _optix_get_transform_list_handle, (%r659);
+	// end inline asm
+	// begin inline asm
+	call (%r505), _optix_get_transform_type_from_handle, (%rd430);
+	// end inline asm
+	or.b32  	%r506, %r505, 1;
+	setp.eq.s32 	%p53, %r506, 3;
+	@%p53 bra 	$L__BB3_86;
+	bra.uni 	$L__BB3_81;
+
+$L__BB3_86:
+	setp.eq.s32 	%p56, %r505, 2;
+	@%p56 bra 	$L__BB3_90;
+	bra.uni 	$L__BB3_87;
+
+$L__BB3_90:
+	// begin inline asm
+	call (%rd502), _optix_get_matrix_motion_transform_from_handle, (%rd430);
+	// end inline asm
+	// begin inline asm
+	cvta.to.global.u64 %rd504, %rd502;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r594,%r595,%r596,%r597}, [%rd504];
+	// end inline asm
+	add.s64 	%rd508, %rd502, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd507, %rd508;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r598,%r599,%r600,%r601}, [%rd507];
+	// end inline asm
+	add.s64 	%rd511, %rd502, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd510, %rd511;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r602,%r603,%r604,%r605}, [%rd510];
+	// end inline asm
+	add.s64 	%rd514, %rd502, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd513, %rd514;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r606,%r607,%r608,%r609}, [%rd513];
+	// end inline asm
+	add.s64 	%rd517, %rd502, 64;
+	// begin inline asm
+	cvta.to.global.u64 %rd516, %rd517;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r610,%r611,%r612,%r613}, [%rd516];
+	// end inline asm
+	add.s64 	%rd520, %rd502, 80;
+	// begin inline asm
 	cvta.to.global.u64 %rd519, %rd520;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r588,%r589,%r590,%r591}, [%rd519];
-	// inline asm
-	add.s64 	%rd523, %rd514, 32;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r614,%r615,%r616,%r617}, [%rd519];
+	// end inline asm
+	add.s64 	%rd523, %rd502, 96;
+	// begin inline asm
 	cvta.to.global.u64 %rd522, %rd523;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r592,%r593,%r594,%r595}, [%rd522];
-	// inline asm
-	add.s64 	%rd526, %rd514, 48;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r618,%r619,%r620,%r621}, [%rd522];
+	// end inline asm
+	add.s64 	%rd526, %rd502, 112;
+	// begin inline asm
 	cvta.to.global.u64 %rd525, %rd526;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r596,%r597,%r598,%r599}, [%rd525];
-	// inline asm
-	add.s64 	%rd529, %rd514, 64;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r622,%r623,%r624,%r625}, [%rd525];
+	// end inline asm
+	mov.b32 	%f1635, %r597;
+	mov.b32 	%f1636, %r598;
+	and.b32  	%r638, %r596, 65535;
+	add.s32 	%r639, %r638, -1;
+	cvt.rn.f32.s32 	%f1637, %r639;
+	sub.f32 	%f1638, %f1531, %f1635;
+	mul.f32 	%f1639, %f1638, %f1637;
+	sub.f32 	%f1640, %f1636, %f1635;
+	div.rn.f32 	%f1641, %f1639, %f1640;
+	min.f32 	%f1642, %f1637, %f1641;
+	mov.f32 	%f1643, 0f00000000;
+	max.f32 	%f1644, %f1643, %f1642;
+	cvt.rmi.f32.f32 	%f1645, %f1644;
+	sub.f32 	%f595, %f1644, %f1645;
+	cvt.rzi.s32.f32 	%r640, %f1645;
+	cvt.s64.s32 	%rd39, %r640;
+	mul.wide.s32 	%rd537, %r640, 48;
+	add.s64 	%rd529, %rd511, %rd537;
+	// begin inline asm
 	cvta.to.global.u64 %rd528, %rd529;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r600,%r601,%r602,%r603}, [%rd528];
-	// inline asm
-	add.s64 	%rd532, %rd514, 80;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r626,%r627,%r628,%r629}, [%rd528];
+	// end inline asm
+	mov.b32 	%f2055, %r626;
+	mov.b32 	%f2056, %r627;
+	mov.b32 	%f2057, %r628;
+	add.s64 	%rd532, %rd529, 16;
+	// begin inline asm
 	cvta.to.global.u64 %rd531, %rd532;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r604,%r605,%r606,%r607}, [%rd531];
-	// inline asm
-	add.s64 	%rd535, %rd514, 96;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r630,%r631,%r632,%r633}, [%rd531];
+	// end inline asm
+	mov.b32 	%f2052, %r630;
+	mov.b32 	%f2053, %r631;
+	mov.b32 	%f2054, %r632;
+	add.s64 	%rd535, %rd529, 32;
+	// begin inline asm
 	cvta.to.global.u64 %rd534, %rd535;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r608,%r609,%r610,%r611}, [%rd534];
-	// inline asm
-	add.s64 	%rd538, %rd514, 112;
-	// inline asm
-	cvta.to.global.u64 %rd537, %rd538;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r612,%r613,%r614,%r615}, [%rd537];
-	// inline asm
-	mov.b32 	 %f1606, %r587;
-	mov.b32 	 %f1607, %r588;
-	cvt.u32.u16	%r628, %rs16;
-	add.s32 	%r629, %r628, -1;
-	cvt.rn.f32.s32	%f1608, %r629;
-	sub.f32 	%f1609, %f938, %f1606;
-	mul.f32 	%f1610, %f1609, %f1608;
-	sub.f32 	%f1611, %f1607, %f1606;
-	div.rn.f32 	%f1612, %f1610, %f1611;
-	min.f32 	%f1613, %f1608, %f1612;
-	mov.f32 	%f1614, 0f00000000;
-	max.f32 	%f1615, %f1614, %f1613;
-	cvt.rmi.f32.f32	%f1616, %f1615;
-	cvt.rzi.s32.f32	%r630, %f1616;
-	cvt.s64.s32	%rd43, %r630;
-	mul.wide.s32 	%rd549, %r630, 48;
-	add.s64 	%rd541, %rd523, %rd549;
-	// inline asm
-	cvta.to.global.u64 %rd540, %rd541;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r616,%r617,%r618,%r619}, [%rd540];
-	// inline asm
-	mov.b32 	 %f2003, %r616;
-	mov.b32 	 %f2004, %r617;
-	mov.b32 	 %f2005, %r618;
-	add.s64 	%rd544, %rd541, 16;
-	// inline asm
-	cvta.to.global.u64 %rd543, %rd544;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r620,%r621,%r622,%r623}, [%rd543];
-	// inline asm
-	mov.b32 	 %f2000, %r620;
-	mov.b32 	 %f2001, %r621;
-	mov.b32 	 %f2002, %r622;
-	add.s64 	%rd547, %rd541, 32;
-	// inline asm
-	cvta.to.global.u64 %rd546, %rd547;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r624,%r625,%r626,%r627}, [%rd546];
-	// inline asm
-	sub.f32 	%f566, %f1615, %f1616;
-	mov.b32 	 %f1997, %r624;
-	mov.b32 	 %f1998, %r625;
-	mov.b32 	 %f1999, %r626;
-	setp.leu.f32	%p54, %f566, 0f00000000;
-	@%p54 bra 	BB3_90;
-
-	mul.lo.s64 	%rd559, %rd43, 48;
-	add.s64 	%rd560, %rd514, %rd559;
-	add.s64 	%rd551, %rd560, 80;
-	// inline asm
-	cvta.to.global.u64 %rd550, %rd551;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r631,%r632,%r633,%r634}, [%rd550];
-	// inline asm
-	mov.b32 	 %f1617, %r631;
-	mov.b32 	 %f1618, %r632;
-	mov.b32 	 %f1619, %r633;
-	add.s64 	%rd554, %rd560, 96;
-	// inline asm
-	cvta.to.global.u64 %rd553, %rd554;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r635,%r636,%r637,%r638}, [%rd553];
-	// inline asm
-	mov.b32 	 %f1620, %r635;
-	mov.b32 	 %f1621, %r636;
-	mov.b32 	 %f1622, %r637;
-	add.s64 	%rd557, %rd560, 112;
-	// inline asm
-	cvta.to.global.u64 %rd556, %rd557;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r639,%r640,%r641,%r642}, [%rd556];
-	// inline asm
-	mov.f32 	%f1623, 0f3F800000;
-	sub.f32 	%f1624, %f1623, %f566;
-	mul.f32 	%f1625, %f566, %f1617;
-	mul.f32 	%f1626, %f566, %f1618;
-	mul.f32 	%f1627, %f566, %f1619;
-	fma.rn.f32 	%f2003, %f1624, %f2003, %f1625;
-	fma.rn.f32 	%f2004, %f1624, %f2004, %f1626;
-	fma.rn.f32 	%f2005, %f1624, %f2005, %f1627;
-	mul.f32 	%f1628, %f566, %f1620;
-	mul.f32 	%f1629, %f566, %f1621;
-	mul.f32 	%f1630, %f566, %f1622;
-	fma.rn.f32 	%f2000, %f1624, %f2000, %f1628;
-	fma.rn.f32 	%f2001, %f1624, %f2001, %f1629;
-	fma.rn.f32 	%f2002, %f1624, %f2002, %f1630;
-	mov.b32 	 %f1631, %r639;
-	mov.b32 	 %f1632, %r640;
-	mov.b32 	 %f1633, %r641;
-	mul.f32 	%f1634, %f566, %f1631;
-	mul.f32 	%f1635, %f566, %f1632;
-	mul.f32 	%f1636, %f566, %f1633;
-	fma.rn.f32 	%f1997, %f1624, %f1997, %f1634;
-	fma.rn.f32 	%f1998, %f1624, %f1998, %f1635;
-	fma.rn.f32 	%f1999, %f1624, %f1999, %f1636;
-	bra.uni 	BB3_90;
-
-BB3_79:
-	mov.f32 	%f2006, 0f00000000;
-	mov.f32 	%f2008, 0f3F800000;
-	setp.eq.s32	%p50, %r495, 4;
-	@%p50 bra 	BB3_82;
-	bra.uni 	BB3_80;
-
-BB3_82:
-	// inline asm
-	call (%rd670), _optix_get_instance_inverse_transform_from_handle, (%rd440);
-	// inline asm
-	bra.uni 	BB3_83;
-
-BB3_85:
-	// inline asm
-	call (%rd455), _optix_get_srt_motion_transform_from_handle, (%rd440);
-	// inline asm
-	// inline asm
-	cvta.to.global.u64 %rd457, %rd455;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r509,%r510,%r511,%r512}, [%rd457];
-	// inline asm
-	mov.b32	{%rs14, %rs15}, %r511;
-	add.s64 	%rd461, %rd455, 16;
-	// inline asm
-	cvta.to.global.u64 %rd460, %rd461;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r513,%r514,%r515,%r516}, [%rd460];
-	// inline asm
-	add.s64 	%rd464, %rd455, 32;
-	// inline asm
-	cvta.to.global.u64 %rd463, %rd464;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r517,%r518,%r519,%r520}, [%rd463];
-	// inline asm
-	add.s64 	%rd467, %rd455, 48;
-	// inline asm
-	cvta.to.global.u64 %rd466, %rd467;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r521,%r522,%r523,%r524}, [%rd466];
-	// inline asm
-	add.s64 	%rd470, %rd455, 64;
-	// inline asm
-	cvta.to.global.u64 %rd469, %rd470;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r525,%r526,%r527,%r528}, [%rd469];
-	// inline asm
-	add.s64 	%rd473, %rd455, 80;
-	// inline asm
-	cvta.to.global.u64 %rd472, %rd473;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r529,%r530,%r531,%r532}, [%rd472];
-	// inline asm
-	add.s64 	%rd476, %rd455, 96;
-	// inline asm
-	cvta.to.global.u64 %rd475, %rd476;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r533,%r534,%r535,%r536}, [%rd475];
-	// inline asm
-	add.s64 	%rd479, %rd455, 112;
-	// inline asm
-	cvta.to.global.u64 %rd478, %rd479;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r537,%r538,%r539,%r540}, [%rd478];
-	// inline asm
-	add.s64 	%rd482, %rd455, 128;
-	// inline asm
-	cvta.to.global.u64 %rd481, %rd482;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r541,%r542,%r543,%r544}, [%rd481];
-	// inline asm
-	add.s64 	%rd485, %rd455, 144;
-	// inline asm
-	cvta.to.global.u64 %rd484, %rd485;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r545,%r546,%r547,%r548}, [%rd484];
-	// inline asm
-	mov.b32 	 %f1514, %r512;
-	mov.b32 	 %f1515, %r513;
-	cvt.u32.u16	%r565, %rs14;
-	add.s32 	%r566, %r565, -1;
-	cvt.rn.f32.s32	%f1516, %r566;
-	sub.f32 	%f1517, %f938, %f1514;
-	mul.f32 	%f1518, %f1517, %f1516;
-	sub.f32 	%f1519, %f1515, %f1514;
-	div.rn.f32 	%f1520, %f1518, %f1519;
-	min.f32 	%f1521, %f1516, %f1520;
-	mov.f32 	%f1522, 0f00000000;
-	max.f32 	%f1523, %f1522, %f1521;
-	cvt.rmi.f32.f32	%f1524, %f1523;
-	cvt.rzi.s32.f32	%r567, %f1524;
-	cvt.s64.s32	%rd41, %r567;
-	mul.wide.s32 	%rd499, %r567, 64;
-	add.s64 	%rd488, %rd464, %rd499;
-	// inline asm
-	cvta.to.global.u64 %rd487, %rd488;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r549,%r550,%r551,%r552}, [%rd487];
-	// inline asm
-	mov.b32 	 %f1987, %r549;
-	mov.b32 	 %f1988, %r550;
-	mov.b32 	 %f1989, %r551;
-	add.s64 	%rd491, %rd488, 16;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r634,%r635,%r636,%r637}, [%rd534];
+	// end inline asm
+	mov.b32 	%f2049, %r634;
+	mov.b32 	%f2050, %r635;
+	mov.b32 	%f2051, %r636;
+	setp.leu.f32 	%p58, %f595, 0f00000000;
+	@%p58 bra 	$L__BB3_92;
+
+	mov.f32 	%f1646, 0f3F800000;
+	sub.f32 	%f1647, %f1646, %f595;
+	mul.lo.s64 	%rd547, %rd39, 48;
+	add.s64 	%rd548, %rd502, %rd547;
+	add.s64 	%rd539, %rd548, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd538, %rd539;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r641,%r642,%r643,%r644}, [%rd538];
+	// end inline asm
+	mov.b32 	%f1648, %r641;
+	mov.b32 	%f1649, %r642;
+	mov.b32 	%f1650, %r643;
+	mul.f32 	%f1651, %f595, %f1648;
+	mul.f32 	%f1652, %f595, %f1649;
+	mul.f32 	%f1653, %f595, %f1650;
+	fma.rn.f32 	%f2055, %f1647, %f2055, %f1651;
+	fma.rn.f32 	%f2056, %f1647, %f2056, %f1652;
+	fma.rn.f32 	%f2057, %f1647, %f2057, %f1653;
+	add.s64 	%rd542, %rd548, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd541, %rd542;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r645,%r646,%r647,%r648}, [%rd541];
+	// end inline asm
+	mov.b32 	%f1654, %r645;
+	mov.b32 	%f1655, %r646;
+	mov.b32 	%f1656, %r647;
+	mul.f32 	%f1657, %f595, %f1654;
+	mul.f32 	%f1658, %f595, %f1655;
+	mul.f32 	%f1659, %f595, %f1656;
+	fma.rn.f32 	%f2052, %f1647, %f2052, %f1657;
+	fma.rn.f32 	%f2053, %f1647, %f2053, %f1658;
+	fma.rn.f32 	%f2054, %f1647, %f2054, %f1659;
+	add.s64 	%rd545, %rd548, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd544, %rd545;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r649,%r650,%r651,%r652}, [%rd544];
+	// end inline asm
+	mov.b32 	%f1660, %r649;
+	mov.b32 	%f1661, %r650;
+	mov.b32 	%f1662, %r651;
+	mul.f32 	%f1663, %f595, %f1660;
+	mul.f32 	%f1664, %f595, %f1661;
+	mul.f32 	%f1665, %f595, %f1662;
+	fma.rn.f32 	%f2049, %f1647, %f2049, %f1663;
+	fma.rn.f32 	%f2050, %f1647, %f2050, %f1664;
+	fma.rn.f32 	%f2051, %f1647, %f2051, %f1665;
+	bra.uni 	$L__BB3_92;
+
+$L__BB3_81:
+	mov.f32 	%f2058, 0f00000000;
+	mov.f32 	%f2060, 0f3F800000;
+	setp.eq.s32 	%p54, %r505, 4;
+	@%p54 bra 	$L__BB3_84;
+
+	setp.ne.s32 	%p55, %r505, 1;
+	mov.f32 	%f2059, %f2058;
+	mov.f32 	%f2061, %f2058;
+	mov.f32 	%f2062, %f2060;
+	mov.f32 	%f2063, %f2058;
+	mov.f32 	%f2064, %f2060;
+	mov.f32 	%f2065, %f2058;
+	mov.f32 	%f2066, %f2058;
+	@%p55 bra 	$L__BB3_93;
+
+	// begin inline asm
+	call (%rd432), _optix_get_static_transform_from_handle, (%rd430);
+	// end inline asm
+	add.s64 	%rd657, %rd432, 64;
+	bra.uni 	$L__BB3_85;
+
+$L__BB3_87:
+	// begin inline asm
+	call (%rd445), _optix_get_srt_motion_transform_from_handle, (%rd430);
+	// end inline asm
+	// begin inline asm
+	cvta.to.global.u64 %rd447, %rd445;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r519,%r520,%r521,%r522}, [%rd447];
+	// end inline asm
+	add.s64 	%rd451, %rd445, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd450, %rd451;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r523,%r524,%r525,%r526}, [%rd450];
+	// end inline asm
+	add.s64 	%rd454, %rd445, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd453, %rd454;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r527,%r528,%r529,%r530}, [%rd453];
+	// end inline asm
+	add.s64 	%rd457, %rd445, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd456, %rd457;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r531,%r532,%r533,%r534}, [%rd456];
+	// end inline asm
+	add.s64 	%rd460, %rd445, 64;
+	// begin inline asm
+	cvta.to.global.u64 %rd459, %rd460;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r535,%r536,%r537,%r538}, [%rd459];
+	// end inline asm
+	add.s64 	%rd463, %rd445, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd462, %rd463;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r539,%r540,%r541,%r542}, [%rd462];
+	// end inline asm
+	add.s64 	%rd466, %rd445, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd465, %rd466;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r543,%r544,%r545,%r546}, [%rd465];
+	// end inline asm
+	add.s64 	%rd469, %rd445, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd468, %rd469;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r547,%r548,%r549,%r550}, [%rd468];
+	// end inline asm
+	add.s64 	%rd472, %rd445, 128;
+	// begin inline asm
+	cvta.to.global.u64 %rd471, %rd472;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r551,%r552,%r553,%r554}, [%rd471];
+	// end inline asm
+	add.s64 	%rd475, %rd445, 144;
+	// begin inline asm
+	cvta.to.global.u64 %rd474, %rd475;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r555,%r556,%r557,%r558}, [%rd474];
+	// end inline asm
+	mov.b32 	%f1543, %r522;
+	mov.b32 	%f1544, %r523;
+	and.b32  	%r575, %r521, 65535;
+	add.s32 	%r576, %r575, -1;
+	cvt.rn.f32.s32 	%f1545, %r576;
+	sub.f32 	%f1546, %f1531, %f1543;
+	mul.f32 	%f1547, %f1546, %f1545;
+	sub.f32 	%f1548, %f1544, %f1543;
+	div.rn.f32 	%f1549, %f1547, %f1548;
+	min.f32 	%f1550, %f1545, %f1549;
+	mov.f32 	%f1551, 0f00000000;
+	max.f32 	%f1552, %f1551, %f1550;
+	cvt.rmi.f32.f32 	%f1553, %f1552;
+	sub.f32 	%f555, %f1552, %f1553;
+	cvt.rzi.s32.f32 	%r577, %f1553;
+	mul.wide.s32 	%rd489, %r577, 64;
+	add.s64 	%rd478, %rd454, %rd489;
+	// begin inline asm
+	cvta.to.global.u64 %rd477, %rd478;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r559,%r560,%r561,%r562}, [%rd477];
+	// end inline asm
+	mov.b32 	%f2039, %r559;
+	mov.b32 	%f2040, %r560;
+	mov.b32 	%f2041, %r561;
+	add.s64 	%rd481, %rd478, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd480, %rd481;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r563,%r564,%r565,%r566}, [%rd480];
+	// end inline asm
+	mov.b32 	%f2042, %r563;
+	mov.b32 	%f2043, %r564;
+	mov.b32 	%f2044, %r566;
+	add.s64 	%rd484, %rd478, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd483, %rd484;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r567,%r568,%r569,%r570}, [%rd483];
+	// end inline asm
+	mov.b32 	%f2045, %r568;
+	mov.b32 	%f2046, %r569;
+	mov.b32 	%f2047, %r570;
+	add.s64 	%rd487, %rd478, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd486, %rd487;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r571,%r572,%r573,%r574}, [%rd486];
+	// end inline asm
+	mov.b32 	%f2048, %r571;
+	setp.leu.f32 	%p57, %f555, 0f00000000;
+	@%p57 bra 	$L__BB3_89;
+
+	mov.f32 	%f1554, 0f3F800000;
+	sub.f32 	%f1555, %f1554, %f555;
+	add.s64 	%rd491, %rd478, 64;
+	// begin inline asm
 	cvta.to.global.u64 %rd490, %rd491;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r553,%r554,%r555,%r556}, [%rd490];
-	// inline asm
-	mov.b32 	 %f1990, %r553;
-	mov.b32 	 %f1991, %r554;
-	mov.b32 	 %f1992, %r556;
-	add.s64 	%rd494, %rd488, 32;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r578,%r579,%r580,%r581}, [%rd490];
+	// end inline asm
+	mov.b32 	%f1556, %r578;
+	mov.b32 	%f1557, %r579;
+	mov.b32 	%f1558, %r580;
+	mul.f32 	%f1559, %f555, %f1556;
+	mul.f32 	%f1560, %f555, %f1557;
+	mul.f32 	%f1561, %f555, %f1558;
+	fma.rn.f32 	%f2039, %f1555, %f2039, %f1559;
+	fma.rn.f32 	%f2040, %f1555, %f2040, %f1560;
+	fma.rn.f32 	%f2041, %f1555, %f2041, %f1561;
+	add.s64 	%rd494, %rd478, 80;
+	// begin inline asm
 	cvta.to.global.u64 %rd493, %rd494;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r557,%r558,%r559,%r560}, [%rd493];
-	// inline asm
-	sub.f32 	%f526, %f1523, %f1524;
-	mov.b32 	 %f1993, %r558;
-	mov.b32 	 %f1994, %r559;
-	mov.b32 	 %f1995, %r560;
-	add.s64 	%rd497, %rd488, 48;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r582,%r583,%r584,%r585}, [%rd493];
+	// end inline asm
+	mov.b32 	%f1562, %r582;
+	mov.b32 	%f1563, %r583;
+	mov.b32 	%f1564, %r585;
+	mul.f32 	%f1565, %f555, %f1562;
+	mul.f32 	%f1566, %f555, %f1563;
+	mul.f32 	%f1567, %f555, %f1564;
+	fma.rn.f32 	%f2042, %f1555, %f2042, %f1565;
+	fma.rn.f32 	%f2043, %f1555, %f2043, %f1566;
+	fma.rn.f32 	%f2044, %f1555, %f2044, %f1567;
+	add.s64 	%rd497, %rd478, 96;
+	// begin inline asm
 	cvta.to.global.u64 %rd496, %rd497;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r561,%r562,%r563,%r564}, [%rd496];
-	// inline asm
-	mov.b32 	 %f1996, %r561;
-	setp.leu.f32	%p53, %f526, 0f00000000;
-	@%p53 bra 	BB3_87;
-
-	shl.b64 	%rd512, %rd41, 6;
-	add.s64 	%rd513, %rd512, %rd455;
-	add.s64 	%rd501, %rd513, 96;
-	// inline asm
-	cvta.to.global.u64 %rd500, %rd501;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r568,%r569,%r570,%r571}, [%rd500];
-	// inline asm
-	mov.b32 	 %f1525, %r568;
-	mov.b32 	 %f1526, %r569;
-	mov.b32 	 %f1527, %r570;
-	add.s64 	%rd504, %rd513, 112;
-	// inline asm
-	cvta.to.global.u64 %rd503, %rd504;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r572,%r573,%r574,%r575}, [%rd503];
-	// inline asm
-	mov.b32 	 %f1528, %r572;
-	mov.b32 	 %f1529, %r573;
-	mov.b32 	 %f1530, %r575;
-	add.s64 	%rd507, %rd513, 128;
-	// inline asm
-	cvta.to.global.u64 %rd506, %rd507;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r576,%r577,%r578,%r579}, [%rd506];
-	// inline asm
-	mov.b32 	 %f1531, %r577;
-	mov.b32 	 %f1532, %r578;
-	mov.b32 	 %f1533, %r579;
-	add.s64 	%rd510, %rd513, 144;
-	// inline asm
-	cvta.to.global.u64 %rd509, %rd510;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r580,%r581,%r582,%r583}, [%rd509];
-	// inline asm
-	mov.f32 	%f1534, 0f3F800000;
-	sub.f32 	%f1535, %f1534, %f526;
-	mul.f32 	%f1536, %f526, %f1525;
-	mul.f32 	%f1537, %f526, %f1526;
-	mul.f32 	%f1538, %f526, %f1527;
-	fma.rn.f32 	%f1987, %f1535, %f1987, %f1536;
-	fma.rn.f32 	%f1988, %f1535, %f1988, %f1537;
-	fma.rn.f32 	%f1989, %f1535, %f1989, %f1538;
-	mul.f32 	%f1539, %f526, %f1528;
-	mul.f32 	%f1540, %f526, %f1529;
-	mul.f32 	%f1541, %f526, %f1530;
-	fma.rn.f32 	%f1990, %f1535, %f1990, %f1539;
-	fma.rn.f32 	%f1991, %f1535, %f1991, %f1540;
-	fma.rn.f32 	%f1992, %f1535, %f1992, %f1541;
-	mul.f32 	%f1542, %f526, %f1531;
-	mul.f32 	%f1543, %f526, %f1532;
-	mul.f32 	%f1544, %f526, %f1533;
-	fma.rn.f32 	%f1545, %f1535, %f1993, %f1542;
-	fma.rn.f32 	%f1546, %f1535, %f1994, %f1543;
-	fma.rn.f32 	%f1547, %f1535, %f1995, %f1544;
-	mov.b32 	 %f1548, %r580;
-	mul.f32 	%f1549, %f526, %f1548;
-	fma.rn.f32 	%f1550, %f1535, %f1996, %f1549;
-	mul.f32 	%f1551, %f1546, %f1546;
-	fma.rn.f32 	%f1552, %f1545, %f1545, %f1551;
-	fma.rn.f32 	%f1553, %f1547, %f1547, %f1552;
-	fma.rn.f32 	%f1554, %f1550, %f1550, %f1553;
-	sqrt.rn.f32 	%f1555, %f1554;
-	rcp.rn.f32 	%f1556, %f1555;
-	mul.f32 	%f1993, %f1545, %f1556;
-	mul.f32 	%f1994, %f1546, %f1556;
-	mul.f32 	%f1995, %f1547, %f1556;
-	mul.f32 	%f1996, %f1550, %f1556;
-
-BB3_87:
-	mul.f32 	%f1557, %f1994, %f1994;
-	fma.rn.f32 	%f1558, %f1993, %f1993, %f1557;
-	fma.rn.f32 	%f1559, %f1995, %f1995, %f1558;
-	fma.rn.f32 	%f1560, %f1996, %f1996, %f1559;
-	rcp.rn.f32 	%f1561, %f1560;
-	mul.f32 	%f1562, %f1993, %f1561;
-	mul.f32 	%f1563, %f1994, %f1561;
-	mul.f32 	%f1564, %f1995, %f1561;
-	mul.f32 	%f1565, %f1996, %f1561;
-	mul.f32 	%f1566, %f1993, %f1562;
-	mul.f32 	%f1567, %f1994, %f1563;
-	mul.f32 	%f1568, %f1995, %f1564;
-	mul.f32 	%f1569, %f1993, %f1563;
-	mul.f32 	%f1570, %f1995, %f1565;
-	mul.f32 	%f1571, %f1993, %f1564;
-	mul.f32 	%f1572, %f1994, %f1565;
-	mul.f32 	%f1573, %f1994, %f1564;
-	mul.f32 	%f1574, %f1993, %f1565;
-	sub.f32 	%f1575, %f1566, %f1567;
-	sub.f32 	%f1576, %f1575, %f1568;
-	fma.rn.f32 	%f1577, %f1996, %f1565, %f1576;
-	sub.f32 	%f1578, %f1569, %f1570;
-	add.f32 	%f1579, %f1578, %f1578;
-	add.f32 	%f1580, %f1571, %f1572;
-	add.f32 	%f1581, %f1580, %f1580;
-	add.f32 	%f1582, %f1569, %f1570;
-	add.f32 	%f1583, %f1582, %f1582;
-	sub.f32 	%f1584, %f1567, %f1566;
-	sub.f32 	%f1585, %f1584, %f1568;
-	fma.rn.f32 	%f1586, %f1996, %f1565, %f1585;
-	sub.f32 	%f1587, %f1573, %f1574;
-	add.f32 	%f1588, %f1587, %f1587;
-	sub.f32 	%f1589, %f1571, %f1572;
-	add.f32 	%f1590, %f1589, %f1589;
-	add.f32 	%f1591, %f1573, %f1574;
-	add.f32 	%f1592, %f1591, %f1591;
-	neg.f32 	%f1593, %f1566;
-	sub.f32 	%f1594, %f1593, %f1567;
-	add.f32 	%f1595, %f1568, %f1594;
-	fma.rn.f32 	%f1596, %f1996, %f1565, %f1595;
-	mul.f32 	%f1597, %f1989, %f1577;
-	fma.rn.f32 	%f1598, %f1991, %f1579, %f1597;
-	fma.rn.f32 	%f2005, %f1992, %f1581, %f1598;
-	mul.f32 	%f1599, %f1991, %f1586;
-	fma.rn.f32 	%f1600, %f1989, %f1583, %f1599;
-	fma.rn.f32 	%f2002, %f1992, %f1588, %f1600;
-	mul.f32 	%f1601, %f1991, %f1592;
-	fma.rn.f32 	%f1602, %f1989, %f1590, %f1601;
-	fma.rn.f32 	%f1999, %f1992, %f1596, %f1602;
-	mul.f32 	%f1603, %f1988, %f1577;
-	fma.rn.f32 	%f2004, %f1990, %f1579, %f1603;
-	mul.f32 	%f1604, %f1990, %f1586;
-	fma.rn.f32 	%f2001, %f1988, %f1583, %f1604;
-	mul.f32 	%f1605, %f1990, %f1592;
-	fma.rn.f32 	%f1998, %f1988, %f1590, %f1605;
-	mul.f32 	%f2003, %f1987, %f1577;
-	mul.f32 	%f2000, %f1987, %f1583;
-	mul.f32 	%f1997, %f1987, %f1590;
-
-BB3_90:
-	mul.f32 	%f1637, %f1998, %f2002;
-	mul.f32 	%f1638, %f1999, %f2001;
-	sub.f32 	%f1639, %f1638, %f1637;
-	mul.f32 	%f1640, %f2003, %f1639;
-	mul.f32 	%f1641, %f1997, %f2002;
-	mul.f32 	%f1642, %f1999, %f2000;
-	sub.f32 	%f1643, %f1642, %f1641;
-	mul.f32 	%f1644, %f1643, %f2004;
-	sub.f32 	%f1645, %f1640, %f1644;
-	mul.f32 	%f1646, %f1997, %f2001;
-	mul.f32 	%f1647, %f1998, %f2000;
-	sub.f32 	%f1648, %f1647, %f1646;
-	fma.rn.f32 	%f1649, %f1648, %f2005, %f1645;
-	rcp.rn.f32 	%f1650, %f1649;
-	mul.f32 	%f2012, %f1639, %f1650;
-	mul.f32 	%f1651, %f1999, %f2004;
-	mul.f32 	%f1652, %f1998, %f2005;
-	sub.f32 	%f1653, %f1652, %f1651;
-	mul.f32 	%f2013, %f1650, %f1653;
-	mul.f32 	%f1654, %f2001, %f2005;
-	mul.f32 	%f1655, %f2002, %f2004;
-	sub.f32 	%f1656, %f1655, %f1654;
-	mul.f32 	%f2014, %f1650, %f1656;
-	sub.f32 	%f1657, %f1641, %f1642;
-	mul.f32 	%f2009, %f1657, %f1650;
-	mul.f32 	%f1658, %f1997, %f2005;
-	mul.f32 	%f1659, %f1999, %f2003;
-	sub.f32 	%f1660, %f1659, %f1658;
-	mul.f32 	%f2010, %f1650, %f1660;
-	mul.f32 	%f1661, %f2002, %f2003;
-	mul.f32 	%f1662, %f2000, %f2005;
-	sub.f32 	%f1663, %f1662, %f1661;
-	mul.f32 	%f2011, %f1650, %f1663;
-	mul.f32 	%f2006, %f1648, %f1650;
-	mul.f32 	%f1664, %f1998, %f2003;
-	mul.f32 	%f1665, %f1997, %f2004;
-	sub.f32 	%f1666, %f1665, %f1664;
-	mul.f32 	%f2007, %f1666, %f1650;
-	mul.f32 	%f1667, %f2000, %f2004;
-	mul.f32 	%f1668, %f2001, %f2003;
-	sub.f32 	%f1669, %f1668, %f1667;
-	mul.f32 	%f2008, %f1669, %f1650;
-	bra.uni 	BB3_91;
-
-BB3_80:
-	setp.ne.s32	%p51, %r495, 1;
-	mov.f32 	%f2007, %f2006;
-	mov.f32 	%f2009, %f2006;
-	mov.f32 	%f2010, %f2008;
-	mov.f32 	%f2011, %f2006;
-	mov.f32 	%f2012, %f2008;
-	mov.f32 	%f2013, %f2006;
-	mov.f32 	%f2014, %f2006;
-	@%p51 bra 	BB3_91;
-
-	// inline asm
-	call (%rd442), _optix_get_static_transform_from_handle, (%rd440);
-	// inline asm
-	add.s64 	%rd670, %rd442, 64;
-
-BB3_83:
-	// inline asm
-	cvta.to.global.u64 %rd446, %rd670;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r497,%r498,%r499,%r500}, [%rd446];
-	// inline asm
-	mov.b32 	 %f2012, %r497;
-	mov.b32 	 %f2013, %r498;
-	mov.b32 	 %f2014, %r499;
-	add.s64 	%rd450, %rd670, 16;
-	// inline asm
-	cvta.to.global.u64 %rd449, %rd450;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r501,%r502,%r503,%r504}, [%rd449];
-	// inline asm
-	mov.b32 	 %f2009, %r501;
-	mov.b32 	 %f2010, %r502;
-	mov.b32 	 %f2011, %r503;
-	add.s64 	%rd453, %rd670, 32;
-	// inline asm
-	cvta.to.global.u64 %rd452, %rd453;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r505,%r506,%r507,%r508}, [%rd452];
-	// inline asm
-	mov.b32 	 %f2006, %r505;
-	mov.b32 	 %f2007, %r506;
-	mov.b32 	 %f2008, %r507;
-
-BB3_91:
-	setp.eq.s32	%p55, %r649, 0;
-	@%p55 bra 	BB3_92;
-	bra.uni 	BB3_93;
-
-BB3_92:
-	mov.f32 	%f1986, %f2006;
-	mov.f32 	%f1985, %f2007;
-	mov.f32 	%f1984, %f2008;
-	mov.f32 	%f1983, %f2009;
-	mov.f32 	%f1982, %f2010;
-	mov.f32 	%f1981, %f2011;
-	mov.f32 	%f1980, %f2012;
-	mov.f32 	%f1979, %f2013;
-	mov.f32 	%f1978, %f2014;
-	bra.uni 	BB3_94;
-
-BB3_93:
-	mul.f32 	%f1670, %f1983, %f2013;
-	fma.rn.f32 	%f1671, %f1980, %f2012, %f1670;
-	fma.rn.f32 	%f606, %f1986, %f2014, %f1671;
-	mul.f32 	%f1672, %f1982, %f2013;
-	fma.rn.f32 	%f1673, %f1979, %f2012, %f1672;
-	fma.rn.f32 	%f607, %f1985, %f2014, %f1673;
-	mul.f32 	%f1674, %f1981, %f2013;
-	fma.rn.f32 	%f1675, %f1978, %f2012, %f1674;
-	fma.rn.f32 	%f608, %f1984, %f2014, %f1675;
-	mul.f32 	%f1676, %f1983, %f2010;
-	fma.rn.f32 	%f1677, %f1980, %f2009, %f1676;
-	fma.rn.f32 	%f609, %f1986, %f2011, %f1677;
-	mul.f32 	%f1678, %f1982, %f2010;
-	fma.rn.f32 	%f1679, %f1979, %f2009, %f1678;
-	fma.rn.f32 	%f610, %f1985, %f2011, %f1679;
-	mul.f32 	%f1680, %f1981, %f2010;
-	fma.rn.f32 	%f1681, %f1978, %f2009, %f1680;
-	fma.rn.f32 	%f611, %f1984, %f2011, %f1681;
-	mul.f32 	%f1682, %f1983, %f2007;
-	fma.rn.f32 	%f1683, %f1980, %f2006, %f1682;
-	fma.rn.f32 	%f1986, %f1986, %f2008, %f1683;
-	mul.f32 	%f1684, %f1982, %f2007;
-	fma.rn.f32 	%f1685, %f1979, %f2006, %f1684;
-	fma.rn.f32 	%f1985, %f1985, %f2008, %f1685;
-	mul.f32 	%f1686, %f1981, %f2007;
-	fma.rn.f32 	%f1687, %f1978, %f2006, %f1686;
-	fma.rn.f32 	%f1984, %f1984, %f2008, %f1687;
-	mov.f32 	%f1983, %f609;
-	mov.f32 	%f1982, %f610;
-	mov.f32 	%f1981, %f611;
-	mov.f32 	%f1980, %f606;
-	mov.f32 	%f1979, %f607;
-	mov.f32 	%f1978, %f608;
-
-BB3_94:
-	add.s32 	%r649, %r649, 1;
-	setp.lt.u32	%p56, %r649, %r30;
-	@%p56 bra 	BB3_78;
-
-BB3_95:
-	fma.rn.f32 	%f1688, %f2063, %f1917, %f1914;
-	fma.rn.f32 	%f1689, %f2064, %f1916, %f1688;
-	fma.rn.f32 	%f1690, %f2063, %f1921, %f1918;
-	fma.rn.f32 	%f1691, %f2064, %f1920, %f1690;
-	fma.rn.f32 	%f1692, %f2063, %f1925, %f1922;
-	fma.rn.f32 	%f1693, %f2064, %f1924, %f1692;
-	fma.rn.f32 	%f2063, %f2065, %f1915, %f1689;
-	fma.rn.f32 	%f2064, %f2065, %f1919, %f1691;
-	fma.rn.f32 	%f2065, %f2065, %f1923, %f1693;
-	ld.const.u64 	%rd561, [params+112];
-	setp.eq.s64	%p57, %rd561, 0;
-	mov.f32 	%f2057, %f2060;
-	mov.f32 	%f2058, %f2061;
-	mov.f32 	%f2059, %f2062;
-	@%p57 bra 	BB3_97;
-
-	mul.f32 	%f1694, %f2060, %f1980;
-	fma.rn.f32 	%f1695, %f2061, %f1983, %f1694;
-	mul.f32 	%f1696, %f2060, %f1979;
-	fma.rn.f32 	%f1697, %f2061, %f1982, %f1696;
-	mul.f32 	%f1698, %f2060, %f1978;
-	fma.rn.f32 	%f1699, %f2061, %f1981, %f1698;
-	fma.rn.f32 	%f1700, %f2062, %f1986, %f1695;
-	fma.rn.f32 	%f1701, %f2062, %f1985, %f1697;
-	fma.rn.f32 	%f1702, %f2062, %f1984, %f1699;
-	mul.f32 	%f1703, %f1700, %f1700;
-	fma.rn.f32 	%f1704, %f1701, %f1701, %f1703;
-	fma.rn.f32 	%f1705, %f1702, %f1702, %f1704;
-	sqrt.rn.f32 	%f1706, %f1705;
-	div.rn.f32 	%f2057, %f1700, %f1706;
-	div.rn.f32 	%f2058, %f1701, %f1706;
-	div.rn.f32 	%f2059, %f1702, %f1706;
-
-BB3_97:
-	ld.const.u64 	%rd562, [params+136];
-	setp.eq.s64	%p58, %rd562, 0;
-	@%p58 bra 	BB3_99;
-
-	mul.f32 	%f1707, %f2060, %f1980;
-	fma.rn.f32 	%f1708, %f2061, %f1983, %f1707;
-	mul.f32 	%f1709, %f2060, %f1979;
-	fma.rn.f32 	%f1710, %f2061, %f1982, %f1709;
-	mul.f32 	%f1711, %f2060, %f1978;
-	fma.rn.f32 	%f1712, %f2061, %f1981, %f1711;
-	fma.rn.f32 	%f1713, %f2062, %f1986, %f1708;
-	fma.rn.f32 	%f1714, %f2062, %f1985, %f1710;
-	fma.rn.f32 	%f1715, %f2062, %f1984, %f1712;
-	mul.f32 	%f1716, %f1713, %f1713;
-	fma.rn.f32 	%f1717, %f1714, %f1714, %f1716;
-	fma.rn.f32 	%f1718, %f1715, %f1715, %f1717;
-	sqrt.rn.f32 	%f1719, %f1718;
-	div.rn.f32 	%f2060, %f1713, %f1719;
-	div.rn.f32 	%f2061, %f1714, %f1719;
-	div.rn.f32 	%f2062, %f1715, %f1719;
-
-BB3_99:
-	ld.const.u64 	%rd563, [params+184];
-	setp.eq.s64	%p59, %rd563, 0;
-	@%p59 bra 	BB3_101;
-
-	mul.f32 	%f1720, %f340, %f1917;
-	fma.rn.f32 	%f1721, %f339, %f1916, %f1720;
-	mul.f32 	%f1722, %f340, %f1921;
-	fma.rn.f32 	%f1723, %f339, %f1920, %f1722;
-	mul.f32 	%f1724, %f340, %f1925;
-	fma.rn.f32 	%f1725, %f339, %f1924, %f1724;
-	fma.rn.f32 	%f340, %f338, %f1915, %f1721;
-	fma.rn.f32 	%f339, %f338, %f1919, %f1723;
-	fma.rn.f32 	%f338, %f338, %f1923, %f1725;
-	mul.f32 	%f1726, %f343, %f1917;
-	fma.rn.f32 	%f1727, %f342, %f1916, %f1726;
-	mul.f32 	%f1728, %f343, %f1921;
-	fma.rn.f32 	%f1729, %f342, %f1920, %f1728;
-	mul.f32 	%f1730, %f343, %f1925;
-	fma.rn.f32 	%f1731, %f342, %f1924, %f1730;
-	fma.rn.f32 	%f343, %f341, %f1915, %f1727;
-	fma.rn.f32 	%f342, %f341, %f1919, %f1729;
-	fma.rn.f32 	%f341, %f341, %f1923, %f1731;
-
-BB3_101:
-	ld.const.u64 	%rd564, [params+280];
-	ld.const.u64 	%rd565, [params+232];
-	mov.f32 	%f2048, 0f00000000;
-	or.b64  	%rd566, %rd564, %rd565;
-	setp.eq.s64	%p60, %rd566, 0;
-	mov.f32 	%f2049, %f2048;
-	mov.f32 	%f2050, %f2048;
-	@%p60 bra 	BB3_103;
-
-	mul.f32 	%f1735, %f2060, %f1917;
-	fma.rn.f32 	%f1736, %f2061, %f1921, %f1735;
-	mul.f32 	%f1737, %f2060, %f1916;
-	fma.rn.f32 	%f1738, %f2061, %f1920, %f1737;
-	mul.f32 	%f1739, %f2060, %f1915;
-	fma.rn.f32 	%f1740, %f2061, %f1919, %f1739;
-	fma.rn.f32 	%f1741, %f2062, %f1925, %f1736;
-	fma.rn.f32 	%f1742, %f2062, %f1924, %f1738;
-	fma.rn.f32 	%f1743, %f2062, %f1923, %f1740;
-	mul.f32 	%f1744, %f1741, %f1741;
-	fma.rn.f32 	%f1745, %f1742, %f1742, %f1744;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r586,%r587,%r588,%r589}, [%rd496];
+	// end inline asm
+	mov.b32 	%f1568, %r587;
+	mov.b32 	%f1569, %r588;
+	mov.b32 	%f1570, %r589;
+	mul.f32 	%f1571, %f555, %f1568;
+	mul.f32 	%f1572, %f555, %f1569;
+	mul.f32 	%f1573, %f555, %f1570;
+	fma.rn.f32 	%f1574, %f1555, %f2045, %f1571;
+	fma.rn.f32 	%f1575, %f1555, %f2046, %f1572;
+	fma.rn.f32 	%f1576, %f1555, %f2047, %f1573;
+	add.s64 	%rd500, %rd478, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd499, %rd500;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r590,%r591,%r592,%r593}, [%rd499];
+	// end inline asm
+	mov.b32 	%f1577, %r590;
+	mul.f32 	%f1578, %f555, %f1577;
+	fma.rn.f32 	%f1579, %f1555, %f2048, %f1578;
+	mul.f32 	%f1580, %f1575, %f1575;
+	fma.rn.f32 	%f1581, %f1574, %f1574, %f1580;
+	fma.rn.f32 	%f1582, %f1576, %f1576, %f1581;
+	fma.rn.f32 	%f1583, %f1579, %f1579, %f1582;
+	sqrt.rn.f32 	%f1584, %f1583;
+	rcp.rn.f32 	%f1585, %f1584;
+	mul.f32 	%f2045, %f1574, %f1585;
+	mul.f32 	%f2046, %f1575, %f1585;
+	mul.f32 	%f2047, %f1576, %f1585;
+	mul.f32 	%f2048, %f1585, %f1579;
+
+$L__BB3_89:
+	mul.f32 	%f1586, %f2046, %f2046;
+	fma.rn.f32 	%f1587, %f2045, %f2045, %f1586;
+	fma.rn.f32 	%f1588, %f2047, %f2047, %f1587;
+	fma.rn.f32 	%f1589, %f2048, %f2048, %f1588;
+	rcp.rn.f32 	%f1590, %f1589;
+	mul.f32 	%f1591, %f2045, %f1590;
+	mul.f32 	%f1592, %f2046, %f1590;
+	mul.f32 	%f1593, %f2047, %f1590;
+	mul.f32 	%f1594, %f2048, %f1590;
+	mul.f32 	%f1595, %f2045, %f1591;
+	mul.f32 	%f1596, %f2046, %f1592;
+	mul.f32 	%f1597, %f2047, %f1593;
+	mul.f32 	%f1598, %f2045, %f1592;
+	mul.f32 	%f1599, %f2047, %f1594;
+	mul.f32 	%f1600, %f2045, %f1593;
+	mul.f32 	%f1601, %f2046, %f1594;
+	mul.f32 	%f1602, %f2046, %f1593;
+	mul.f32 	%f1603, %f2045, %f1594;
+	sub.f32 	%f1604, %f1595, %f1596;
+	sub.f32 	%f1605, %f1604, %f1597;
+	fma.rn.f32 	%f1606, %f2048, %f1594, %f1605;
+	sub.f32 	%f1607, %f1598, %f1599;
+	add.f32 	%f1608, %f1607, %f1607;
+	add.f32 	%f1609, %f1600, %f1601;
+	add.f32 	%f1610, %f1609, %f1609;
+	add.f32 	%f1611, %f1598, %f1599;
+	add.f32 	%f1612, %f1611, %f1611;
+	sub.f32 	%f1613, %f1596, %f1595;
+	sub.f32 	%f1614, %f1613, %f1597;
+	fma.rn.f32 	%f1615, %f2048, %f1594, %f1614;
+	sub.f32 	%f1616, %f1602, %f1603;
+	add.f32 	%f1617, %f1616, %f1616;
+	sub.f32 	%f1618, %f1600, %f1601;
+	add.f32 	%f1619, %f1618, %f1618;
+	add.f32 	%f1620, %f1602, %f1603;
+	add.f32 	%f1621, %f1620, %f1620;
+	neg.f32 	%f1622, %f1595;
+	sub.f32 	%f1623, %f1622, %f1596;
+	add.f32 	%f1624, %f1597, %f1623;
+	fma.rn.f32 	%f1625, %f2048, %f1594, %f1624;
+	mul.f32 	%f1626, %f2041, %f1606;
+	fma.rn.f32 	%f1627, %f2043, %f1608, %f1626;
+	fma.rn.f32 	%f2057, %f2044, %f1610, %f1627;
+	mul.f32 	%f1628, %f2043, %f1615;
+	fma.rn.f32 	%f1629, %f2041, %f1612, %f1628;
+	fma.rn.f32 	%f2054, %f2044, %f1617, %f1629;
+	mul.f32 	%f1630, %f2043, %f1621;
+	fma.rn.f32 	%f1631, %f2041, %f1619, %f1630;
+	fma.rn.f32 	%f2051, %f2044, %f1625, %f1631;
+	mul.f32 	%f1632, %f2040, %f1606;
+	fma.rn.f32 	%f2056, %f2042, %f1608, %f1632;
+	mul.f32 	%f1633, %f2042, %f1615;
+	fma.rn.f32 	%f2053, %f2040, %f1612, %f1633;
+	mul.f32 	%f1634, %f2042, %f1621;
+	fma.rn.f32 	%f2050, %f2040, %f1619, %f1634;
+	mul.f32 	%f2055, %f2039, %f1606;
+	mul.f32 	%f2052, %f2039, %f1612;
+	mul.f32 	%f2049, %f2039, %f1619;
+
+$L__BB3_92:
+	mul.f32 	%f1666, %f2050, %f2054;
+	mul.f32 	%f1667, %f2051, %f2053;
+	sub.f32 	%f1668, %f1667, %f1666;
+	mul.f32 	%f1669, %f2055, %f1668;
+	mul.f32 	%f1670, %f2049, %f2054;
+	mul.f32 	%f1671, %f2051, %f2052;
+	sub.f32 	%f1672, %f1671, %f1670;
+	mul.f32 	%f1673, %f1672, %f2056;
+	sub.f32 	%f1674, %f1669, %f1673;
+	mul.f32 	%f1675, %f2049, %f2053;
+	mul.f32 	%f1676, %f2050, %f2052;
+	sub.f32 	%f1677, %f1676, %f1675;
+	fma.rn.f32 	%f1678, %f1677, %f2057, %f1674;
+	rcp.rn.f32 	%f1679, %f1678;
+	mul.f32 	%f2064, %f1668, %f1679;
+	mul.f32 	%f1680, %f2051, %f2056;
+	mul.f32 	%f1681, %f2050, %f2057;
+	sub.f32 	%f1682, %f1681, %f1680;
+	mul.f32 	%f2065, %f1682, %f1679;
+	mul.f32 	%f1683, %f2053, %f2057;
+	mul.f32 	%f1684, %f2054, %f2056;
+	sub.f32 	%f1685, %f1684, %f1683;
+	mul.f32 	%f2066, %f1685, %f1679;
+	sub.f32 	%f1686, %f1670, %f1671;
+	mul.f32 	%f2061, %f1686, %f1679;
+	mul.f32 	%f1687, %f2049, %f2057;
+	mul.f32 	%f1688, %f2051, %f2055;
+	sub.f32 	%f1689, %f1688, %f1687;
+	mul.f32 	%f2062, %f1689, %f1679;
+	mul.f32 	%f1690, %f2054, %f2055;
+	mul.f32 	%f1691, %f2052, %f2057;
+	sub.f32 	%f1692, %f1691, %f1690;
+	mul.f32 	%f2063, %f1692, %f1679;
+	mul.f32 	%f2058, %f1677, %f1679;
+	mul.f32 	%f1693, %f2050, %f2055;
+	mul.f32 	%f1694, %f2049, %f2056;
+	sub.f32 	%f1695, %f1694, %f1693;
+	mul.f32 	%f2059, %f1695, %f1679;
+	mul.f32 	%f1696, %f2052, %f2056;
+	mul.f32 	%f1697, %f2053, %f2055;
+	sub.f32 	%f1698, %f1697, %f1696;
+	mul.f32 	%f2060, %f1698, %f1679;
+	bra.uni 	$L__BB3_93;
+
+$L__BB3_84:
+	// begin inline asm
+	call (%rd657), _optix_get_instance_inverse_transform_from_handle, (%rd430);
+	// end inline asm
+
+$L__BB3_85:
+	// begin inline asm
+	cvta.to.global.u64 %rd436, %rd657;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r507,%r508,%r509,%r510}, [%rd436];
+	// end inline asm
+	mov.b32 	%f2064, %r507;
+	mov.b32 	%f2065, %r508;
+	mov.b32 	%f2066, %r509;
+	add.s64 	%rd440, %rd657, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd439, %rd440;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r511,%r512,%r513,%r514}, [%rd439];
+	// end inline asm
+	mov.b32 	%f2061, %r511;
+	mov.b32 	%f2062, %r512;
+	mov.b32 	%f2063, %r513;
+	add.s64 	%rd443, %rd657, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd442, %rd443;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r515,%r516,%r517,%r518}, [%rd442];
+	// end inline asm
+	mov.b32 	%f2058, %r515;
+	mov.b32 	%f2059, %r516;
+	mov.b32 	%f2060, %r517;
+
+$L__BB3_93:
+	setp.eq.s32 	%p59, %r659, 0;
+	@%p59 bra 	$L__BB3_95;
+
+	mul.f32 	%f1699, %f2035, %f2065;
+	fma.rn.f32 	%f1700, %f2032, %f2064, %f1699;
+	fma.rn.f32 	%f641, %f2038, %f2066, %f1700;
+	mul.f32 	%f1701, %f2034, %f2065;
+	fma.rn.f32 	%f1702, %f2031, %f2064, %f1701;
+	fma.rn.f32 	%f642, %f2037, %f2066, %f1702;
+	mul.f32 	%f1703, %f2033, %f2065;
+	fma.rn.f32 	%f1704, %f2030, %f2064, %f1703;
+	fma.rn.f32 	%f2066, %f2036, %f2066, %f1704;
+	mul.f32 	%f1705, %f2035, %f2062;
+	fma.rn.f32 	%f1706, %f2032, %f2061, %f1705;
+	fma.rn.f32 	%f644, %f2038, %f2063, %f1706;
+	mul.f32 	%f1707, %f2034, %f2062;
+	fma.rn.f32 	%f1708, %f2031, %f2061, %f1707;
+	fma.rn.f32 	%f645, %f2037, %f2063, %f1708;
+	mul.f32 	%f1709, %f2033, %f2062;
+	fma.rn.f32 	%f1710, %f2030, %f2061, %f1709;
+	fma.rn.f32 	%f2063, %f2036, %f2063, %f1710;
+	mul.f32 	%f1711, %f2035, %f2059;
+	fma.rn.f32 	%f1712, %f2032, %f2058, %f1711;
+	fma.rn.f32 	%f647, %f2038, %f2060, %f1712;
+	mul.f32 	%f1713, %f2034, %f2059;
+	fma.rn.f32 	%f1714, %f2031, %f2058, %f1713;
+	fma.rn.f32 	%f648, %f2037, %f2060, %f1714;
+	mul.f32 	%f1715, %f2033, %f2059;
+	fma.rn.f32 	%f1716, %f2030, %f2058, %f1715;
+	fma.rn.f32 	%f2060, %f2036, %f2060, %f1716;
+	mov.f32 	%f2058, %f647;
+	mov.f32 	%f2059, %f648;
+	mov.f32 	%f2061, %f644;
+	mov.f32 	%f2062, %f645;
+	mov.f32 	%f2064, %f641;
+	mov.f32 	%f2065, %f642;
+
+$L__BB3_95:
+	add.s32 	%r659, %r659, 1;
+	setp.lt.u32 	%p60, %r659, %r502;
+	mov.f32 	%f2030, %f2066;
+	mov.f32 	%f2031, %f2065;
+	mov.f32 	%f2032, %f2064;
+	mov.f32 	%f2033, %f2063;
+	mov.f32 	%f2034, %f2062;
+	mov.f32 	%f2035, %f2061;
+	mov.f32 	%f2036, %f2060;
+	mov.f32 	%f2037, %f2059;
+	mov.f32 	%f2038, %f2058;
+	@%p60 bra 	$L__BB3_80;
+
+$L__BB3_96:
+	fma.rn.f32 	%f1717, %f2115, %f2002, %f2005;
+	fma.rn.f32 	%f1718, %f2116, %f2003, %f1717;
+	fma.rn.f32 	%f1719, %f2115, %f1998, %f2001;
+	fma.rn.f32 	%f1720, %f2116, %f1999, %f1719;
+	fma.rn.f32 	%f1721, %f2115, %f1994, %f1997;
+	fma.rn.f32 	%f1722, %f2116, %f1995, %f1721;
+	fma.rn.f32 	%f2115, %f2117, %f2004, %f1718;
+	fma.rn.f32 	%f2116, %f2117, %f2000, %f1720;
+	fma.rn.f32 	%f2117, %f2117, %f1996, %f1722;
+	ld.const.u64 	%rd549, [params+112];
+	setp.eq.s64 	%p61, %rd549, 0;
+	mov.f32 	%f2085, %f2088;
+	mov.f32 	%f2086, %f2089;
+	mov.f32 	%f2087, %f2090;
+	@%p61 bra 	$L__BB3_98;
+
+	mul.f32 	%f1723, %f2088, %f2064;
+	fma.rn.f32 	%f1724, %f2089, %f2061, %f1723;
+	mul.f32 	%f1725, %f2088, %f2065;
+	fma.rn.f32 	%f1726, %f2089, %f2062, %f1725;
+	mul.f32 	%f1727, %f2088, %f2066;
+	fma.rn.f32 	%f1728, %f2089, %f2063, %f1727;
+	fma.rn.f32 	%f1729, %f2090, %f2058, %f1724;
+	fma.rn.f32 	%f1730, %f2090, %f2059, %f1726;
+	fma.rn.f32 	%f1731, %f2090, %f2060, %f1728;
+	mul.f32 	%f1732, %f1729, %f1729;
+	fma.rn.f32 	%f1733, %f1730, %f1730, %f1732;
+	fma.rn.f32 	%f1734, %f1731, %f1731, %f1733;
+	sqrt.rn.f32 	%f1735, %f1734;
+	div.rn.f32 	%f2085, %f1729, %f1735;
+	div.rn.f32 	%f2086, %f1730, %f1735;
+	div.rn.f32 	%f2087, %f1731, %f1735;
+
+$L__BB3_98:
+	ld.const.u64 	%rd550, [params+136];
+	setp.eq.s64 	%p62, %rd550, 0;
+	@%p62 bra 	$L__BB3_100;
+
+	mul.f32 	%f1736, %f2088, %f2064;
+	fma.rn.f32 	%f1737, %f2089, %f2061, %f1736;
+	mul.f32 	%f1738, %f2088, %f2065;
+	fma.rn.f32 	%f1739, %f2089, %f2062, %f1738;
+	mul.f32 	%f1740, %f2088, %f2066;
+	fma.rn.f32 	%f1741, %f2089, %f2063, %f1740;
+	fma.rn.f32 	%f1742, %f2090, %f2058, %f1737;
+	fma.rn.f32 	%f1743, %f2090, %f2059, %f1739;
+	fma.rn.f32 	%f1744, %f2090, %f2060, %f1741;
+	mul.f32 	%f1745, %f1742, %f1742;
 	fma.rn.f32 	%f1746, %f1743, %f1743, %f1745;
-	sqrt.rn.f32 	%f1747, %f1746;
-	div.rn.f32 	%f1748, %f1741, %f1747;
-	div.rn.f32 	%f1749, %f1742, %f1747;
-	div.rn.f32 	%f1750, %f1743, %f1747;
-	mul.f32 	%f1751, %f1748, %f1980;
-	mul.f32 	%f1752, %f1748, %f1979;
-	mul.f32 	%f1753, %f1748, %f1978;
-	fma.rn.f32 	%f1754, %f1749, %f1983, %f1751;
-	fma.rn.f32 	%f1755, %f1749, %f1982, %f1752;
-	fma.rn.f32 	%f1756, %f1749, %f1981, %f1753;
-	fma.rn.f32 	%f1757, %f1750, %f1986, %f1754;
-	fma.rn.f32 	%f1758, %f1750, %f1985, %f1755;
-	fma.rn.f32 	%f1759, %f1750, %f1984, %f1756;
-	mul.f32 	%f1760, %f1757, %f1757;
-	fma.rn.f32 	%f1761, %f1758, %f1758, %f1760;
-	fma.rn.f32 	%f1762, %f1759, %f1759, %f1761;
-	sqrt.rn.f32 	%f1763, %f1762;
-	rcp.rn.f32 	%f1764, %f1763;
-	mul.f32 	%f1765, %f1764, %f1757;
-	mul.f32 	%f1766, %f1764, %f1758;
-	mul.f32 	%f1767, %f1764, %f1759;
-	mul.f32 	%f1768, %f1980, 0f00000000;
-	mov.f32 	%f1769, 0f00000000;
-	fma.rn.f32 	%f1770, %f1769, %f1983, %f1768;
-	mul.f32 	%f1771, %f1979, 0f00000000;
-	fma.rn.f32 	%f1772, %f1769, %f1982, %f1771;
-	mul.f32 	%f1773, %f1978, 0f00000000;
-	fma.rn.f32 	%f1774, %f1769, %f1981, %f1773;
-	fma.rn.f32 	%f1775, %f1769, %f1986, %f1770;
-	fma.rn.f32 	%f1776, %f1769, %f1985, %f1772;
-	fma.rn.f32 	%f1777, %f1769, %f1984, %f1774;
-	mul.f32 	%f1778, %f1775, %f1764;
-	mul.f32 	%f1779, %f1776, %f1764;
-	mul.f32 	%f1780, %f1777, %f1764;
-	mul.f32 	%f1781, %f1765, %f1778;
-	fma.rn.f32 	%f1782, %f1766, %f1779, %f1781;
-	fma.rn.f32 	%f1783, %f1767, %f1780, %f1782;
-	mul.f32 	%f1784, %f1765, %f1783;
-	mul.f32 	%f1785, %f1766, %f1783;
-	mul.f32 	%f1786, %f1767, %f1783;
-	sub.f32 	%f2048, %f1778, %f1784;
-	sub.f32 	%f2049, %f1779, %f1785;
-	sub.f32 	%f2050, %f1780, %f1786;
-
-BB3_103:
-	st.global.u32 	[%rd27], %r343;
-	bra.uni 	BB3_104;
-
-BB3_57:
-	mov.f32 	%f2049, %f2048;
-	mov.f32 	%f2050, %f2048;
-	mov.f32 	%f2057, %f2060;
-	mov.f32 	%f2058, %f2061;
-	mov.f32 	%f2059, %f2062;
-
-BB3_104:
-	ld.const.u64 	%rd567, [params+328];
-	cvta.to.global.u64 	%rd568, %rd567;
-	shl.b64 	%rd569, %rd26, 3;
-	add.s64 	%rd570, %rd568, %rd569;
-	st.global.u64 	[%rd570], %rd25;
-	ld.const.u64 	%rd571, [params+336];
-	cvta.to.global.u64 	%rd572, %rd571;
-	shl.b64 	%rd573, %rd26, 2;
-	add.s64 	%rd574, %rd572, %rd573;
-	mov.u32 	%r643, 0;
-	st.global.u32 	[%rd574], %r643;
-	ld.const.u64 	%rd575, [params+160];
-	cvta.to.global.u64 	%rd576, %rd575;
-	add.s64 	%rd577, %rd576, %rd573;
-	st.global.f32 	[%rd577], %f2063;
-	ld.const.u64 	%rd578, [params+168];
+	fma.rn.f32 	%f1747, %f1744, %f1744, %f1746;
+	sqrt.rn.f32 	%f1748, %f1747;
+	div.rn.f32 	%f2088, %f1742, %f1748;
+	div.rn.f32 	%f2089, %f1743, %f1748;
+	div.rn.f32 	%f2090, %f1744, %f1748;
+
+$L__BB3_100:
+	mov.f32 	%f2114, %f2090;
+	mov.f32 	%f2113, %f2089;
+	mov.f32 	%f2112, %f2088;
+	ld.const.u64 	%rd551, [params+184];
+	setp.eq.s64 	%p63, %rd551, 0;
+	@%p63 bra 	$L__BB3_102;
+
+	mul.f32 	%f1749, %f2106, %f2002;
+	fma.rn.f32 	%f1750, %f2107, %f2003, %f1749;
+	mul.f32 	%f1751, %f2106, %f1998;
+	fma.rn.f32 	%f1752, %f2107, %f1999, %f1751;
+	mul.f32 	%f1753, %f2106, %f1994;
+	fma.rn.f32 	%f1754, %f2107, %f1995, %f1753;
+	fma.rn.f32 	%f2106, %f2108, %f2004, %f1750;
+	fma.rn.f32 	%f2107, %f2108, %f2000, %f1752;
+	fma.rn.f32 	%f2108, %f2108, %f1996, %f1754;
+	mul.f32 	%f1755, %f2103, %f2002;
+	fma.rn.f32 	%f1756, %f2104, %f2003, %f1755;
+	mul.f32 	%f1757, %f2103, %f1998;
+	fma.rn.f32 	%f1758, %f2104, %f1999, %f1757;
+	mul.f32 	%f1759, %f2103, %f1994;
+	fma.rn.f32 	%f1760, %f2104, %f1995, %f1759;
+	fma.rn.f32 	%f2103, %f2105, %f2004, %f1756;
+	fma.rn.f32 	%f2104, %f2105, %f2000, %f1758;
+	fma.rn.f32 	%f2105, %f2105, %f1996, %f1760;
+
+$L__BB3_102:
+	ld.const.u64 	%rd552, [params+232];
+	ld.const.u64 	%rd553, [params+280];
+	or.b64  	%rd554, %rd552, %rd553;
+	setp.eq.s64 	%p64, %rd554, 0;
+	mov.f32 	%f2100, 0f00000000;
+	mov.f32 	%f2101, %f2100;
+	mov.f32 	%f2102, %f2100;
+	@%p64 bra 	$L__BB3_104;
+
+	mul.f32 	%f1764, %f2112, %f2002;
+	fma.rn.f32 	%f1765, %f2113, %f1998, %f1764;
+	mul.f32 	%f1766, %f2112, %f2003;
+	fma.rn.f32 	%f1767, %f2113, %f1999, %f1766;
+	mul.f32 	%f1768, %f2112, %f2004;
+	fma.rn.f32 	%f1769, %f2113, %f2000, %f1768;
+	fma.rn.f32 	%f1770, %f2114, %f1994, %f1765;
+	fma.rn.f32 	%f1771, %f2114, %f1995, %f1767;
+	fma.rn.f32 	%f1772, %f2114, %f1996, %f1769;
+	mul.f32 	%f1773, %f1770, %f1770;
+	fma.rn.f32 	%f1774, %f1771, %f1771, %f1773;
+	fma.rn.f32 	%f1775, %f1772, %f1772, %f1774;
+	sqrt.rn.f32 	%f1776, %f1775;
+	div.rn.f32 	%f1777, %f1770, %f1776;
+	div.rn.f32 	%f1778, %f1771, %f1776;
+	div.rn.f32 	%f1779, %f1772, %f1776;
+	mul.f32 	%f1780, %f1777, %f2064;
+	mul.f32 	%f1781, %f1777, %f2065;
+	mul.f32 	%f1782, %f1777, %f2066;
+	fma.rn.f32 	%f1783, %f1778, %f2061, %f1780;
+	fma.rn.f32 	%f1784, %f1778, %f2062, %f1781;
+	fma.rn.f32 	%f1785, %f1778, %f2063, %f1782;
+	fma.rn.f32 	%f1786, %f1779, %f2058, %f1783;
+	fma.rn.f32 	%f1787, %f1779, %f2059, %f1784;
+	fma.rn.f32 	%f1788, %f1779, %f2060, %f1785;
+	mul.f32 	%f1789, %f1786, %f1786;
+	fma.rn.f32 	%f1790, %f1787, %f1787, %f1789;
+	fma.rn.f32 	%f1791, %f1788, %f1788, %f1790;
+	sqrt.rn.f32 	%f1792, %f1791;
+	rcp.rn.f32 	%f1793, %f1792;
+	mul.f32 	%f1794, %f1793, %f1786;
+	mul.f32 	%f1795, %f1793, %f1787;
+	mul.f32 	%f1796, %f1793, %f1788;
+	mul.f32 	%f1797, %f2064, 0f00000000;
+	mov.f32 	%f1798, 0f00000000;
+	fma.rn.f32 	%f1799, %f1798, %f2061, %f1797;
+	mul.f32 	%f1800, %f2065, 0f00000000;
+	fma.rn.f32 	%f1801, %f1798, %f2062, %f1800;
+	mul.f32 	%f1802, %f2066, 0f00000000;
+	fma.rn.f32 	%f1803, %f1798, %f2063, %f1802;
+	fma.rn.f32 	%f1804, %f1798, %f2058, %f1799;
+	fma.rn.f32 	%f1805, %f1798, %f2059, %f1801;
+	fma.rn.f32 	%f1806, %f1798, %f2060, %f1803;
+	mul.f32 	%f1807, %f1804, %f1793;
+	mul.f32 	%f1808, %f1805, %f1793;
+	mul.f32 	%f1809, %f1806, %f1793;
+	mul.f32 	%f1810, %f1794, %f1807;
+	fma.rn.f32 	%f1811, %f1795, %f1808, %f1810;
+	fma.rn.f32 	%f1812, %f1796, %f1809, %f1811;
+	mul.f32 	%f1813, %f1794, %f1812;
+	mul.f32 	%f1814, %f1795, %f1812;
+	mul.f32 	%f1815, %f1796, %f1812;
+	sub.f32 	%f2100, %f1807, %f1813;
+	sub.f32 	%f2101, %f1808, %f1814;
+	sub.f32 	%f2102, %f1809, %f1815;
+
+$L__BB3_104:
+	st.global.u32 	[%rd25], %r348;
+	mov.f32 	%f2088, %f2085;
+	mov.f32 	%f2089, %f2086;
+	mov.f32 	%f2090, %f2087;
+
+$L__BB3_105:
+	ld.const.u64 	%rd555, [params+328];
+	cvta.to.global.u64 	%rd556, %rd555;
+	shl.b64 	%rd557, %rd24, 3;
+	add.s64 	%rd558, %rd556, %rd557;
+	st.global.u64 	[%rd558], %rd23;
+	ld.const.u64 	%rd559, [params+336];
+	cvta.to.global.u64 	%rd560, %rd559;
+	shl.b64 	%rd561, %rd24, 2;
+	add.s64 	%rd562, %rd560, %rd561;
+	mov.u32 	%r653, 0;
+	st.global.u32 	[%rd562], %r653;
+	ld.const.u64 	%rd563, [params+160];
+	cvta.to.global.u64 	%rd564, %rd563;
+	add.s64 	%rd565, %rd564, %rd561;
+	st.global.f32 	[%rd565], %f2115;
+	ld.const.u64 	%rd566, [params+168];
+	cvta.to.global.u64 	%rd567, %rd566;
+	add.s64 	%rd568, %rd567, %rd561;
+	st.global.f32 	[%rd568], %f2116;
+	ld.const.u64 	%rd569, [params+176];
+	cvta.to.global.u64 	%rd570, %rd569;
+	add.s64 	%rd571, %rd570, %rd561;
+	st.global.f32 	[%rd571], %f2117;
+	ld.const.u64 	%rd572, [params+72];
+	cvta.to.global.u64 	%rd573, %rd572;
+	add.s64 	%rd574, %rd573, %rd561;
+	st.global.f32 	[%rd574], %f346;
+	@%p25 bra 	$L__BB3_107;
+
+	cvta.to.global.u64 	%rd575, %rd22;
+	add.s64 	%rd577, %rd575, %rd561;
+	st.global.f32 	[%rd577], %f1959;
+	ld.const.u64 	%rd578, [params+104];
 	cvta.to.global.u64 	%rd579, %rd578;
-	add.s64 	%rd580, %rd579, %rd573;
-	st.global.f32 	[%rd580], %f2064;
-	ld.const.u64 	%rd581, [params+176];
-	cvta.to.global.u64 	%rd582, %rd581;
-	add.s64 	%rd583, %rd582, %rd573;
-	st.global.f32 	[%rd583], %f2065;
-	ld.const.u64 	%rd584, [params+72];
+	add.s64 	%rd580, %rd579, %rd561;
+	st.global.f32 	[%rd580], %f1958;
+
+$L__BB3_107:
+	ld.const.u64 	%rd40, [params+112];
+	setp.eq.s64 	%p66, %rd40, 0;
+	@%p66 bra 	$L__BB3_109;
+
+	cvta.to.global.u64 	%rd581, %rd40;
+	add.s64 	%rd583, %rd581, %rd561;
+	st.global.f32 	[%rd583], %f2088;
+	ld.const.u64 	%rd584, [params+120];
 	cvta.to.global.u64 	%rd585, %rd584;
-	add.s64 	%rd586, %rd585, %rd573;
-	st.global.f32 	[%rd586], %f313;
-	@%p23 bra 	BB3_106;
-
-	cvta.to.global.u64 	%rd587, %rd24;
-	add.s64 	%rd589, %rd587, %rd573;
-	st.global.f32 	[%rd589], %f1907;
-	ld.const.u64 	%rd590, [params+104];
-	cvta.to.global.u64 	%rd591, %rd590;
-	add.s64 	%rd592, %rd591, %rd573;
-	st.global.f32 	[%rd592], %f1906;
-
-BB3_106:
-	ld.const.u64 	%rd44, [params+112];
-	setp.eq.s64	%p62, %rd44, 0;
-	@%p62 bra 	BB3_108;
-
-	cvta.to.global.u64 	%rd593, %rd44;
-	add.s64 	%rd595, %rd593, %rd573;
-	st.global.f32 	[%rd595], %f2057;
-	ld.const.u64 	%rd596, [params+120];
+	add.s64 	%rd586, %rd585, %rd561;
+	st.global.f32 	[%rd586], %f2089;
+	ld.const.u64 	%rd587, [params+128];
+	cvta.to.global.u64 	%rd588, %rd587;
+	add.s64 	%rd589, %rd588, %rd561;
+	st.global.f32 	[%rd589], %f2090;
+
+$L__BB3_109:
+	ld.const.u64 	%rd41, [params+136];
+	setp.eq.s64 	%p67, %rd41, 0;
+	@%p67 bra 	$L__BB3_111;
+
+	cvta.to.global.u64 	%rd590, %rd41;
+	add.s64 	%rd592, %rd590, %rd561;
+	st.global.f32 	[%rd592], %f2112;
+	ld.const.u64 	%rd593, [params+144];
+	cvta.to.global.u64 	%rd594, %rd593;
+	add.s64 	%rd595, %rd594, %rd561;
+	st.global.f32 	[%rd595], %f2113;
+	ld.const.u64 	%rd596, [params+152];
 	cvta.to.global.u64 	%rd597, %rd596;
-	add.s64 	%rd598, %rd597, %rd573;
-	st.global.f32 	[%rd598], %f2058;
-	ld.const.u64 	%rd599, [params+128];
-	cvta.to.global.u64 	%rd600, %rd599;
-	add.s64 	%rd601, %rd600, %rd573;
-	st.global.f32 	[%rd601], %f2059;
-
-BB3_108:
-	ld.const.u64 	%rd45, [params+136];
-	setp.eq.s64	%p63, %rd45, 0;
-	@%p63 bra 	BB3_110;
-
-	cvta.to.global.u64 	%rd602, %rd45;
-	add.s64 	%rd604, %rd602, %rd573;
-	st.global.f32 	[%rd604], %f2060;
-	ld.const.u64 	%rd605, [params+144];
+	add.s64 	%rd598, %rd597, %rd561;
+	st.global.f32 	[%rd598], %f2114;
+
+$L__BB3_111:
+	ld.const.u64 	%rd42, [params+184];
+	setp.eq.s64 	%p68, %rd42, 0;
+	@%p68 bra 	$L__BB3_113;
+
+	cvta.to.global.u64 	%rd599, %rd42;
+	add.s64 	%rd601, %rd599, %rd561;
+	st.global.f32 	[%rd601], %f2106;
+	ld.const.u64 	%rd602, [params+192];
+	cvta.to.global.u64 	%rd603, %rd602;
+	add.s64 	%rd604, %rd603, %rd561;
+	st.global.f32 	[%rd604], %f2107;
+	ld.const.u64 	%rd605, [params+200];
 	cvta.to.global.u64 	%rd606, %rd605;
-	add.s64 	%rd607, %rd606, %rd573;
-	st.global.f32 	[%rd607], %f2061;
-	ld.const.u64 	%rd608, [params+152];
+	add.s64 	%rd607, %rd606, %rd561;
+	st.global.f32 	[%rd607], %f2108;
+	ld.const.u64 	%rd608, [params+208];
 	cvta.to.global.u64 	%rd609, %rd608;
-	add.s64 	%rd610, %rd609, %rd573;
-	st.global.f32 	[%rd610], %f2062;
-
-BB3_110:
-	ld.const.u64 	%rd46, [params+184];
-	setp.eq.s64	%p64, %rd46, 0;
-	@%p64 bra 	BB3_112;
-
-	cvta.to.global.u64 	%rd611, %rd46;
-	add.s64 	%rd613, %rd611, %rd573;
-	st.global.f32 	[%rd613], %f340;
-	ld.const.u64 	%rd614, [params+192];
+	add.s64 	%rd610, %rd609, %rd561;
+	st.global.f32 	[%rd610], %f2103;
+	ld.const.u64 	%rd611, [params+216];
+	cvta.to.global.u64 	%rd612, %rd611;
+	add.s64 	%rd613, %rd612, %rd561;
+	st.global.f32 	[%rd613], %f2104;
+	ld.const.u64 	%rd614, [params+224];
 	cvta.to.global.u64 	%rd615, %rd614;
-	add.s64 	%rd616, %rd615, %rd573;
-	st.global.f32 	[%rd616], %f339;
-	ld.const.u64 	%rd617, [params+200];
-	cvta.to.global.u64 	%rd618, %rd617;
-	add.s64 	%rd619, %rd618, %rd573;
-	st.global.f32 	[%rd619], %f338;
-	ld.const.u64 	%rd620, [params+208];
+	add.s64 	%rd616, %rd615, %rd561;
+	st.global.f32 	[%rd616], %f2105;
+
+$L__BB3_113:
+	ld.const.u64 	%rd43, [params+232];
+	setp.eq.s64 	%p69, %rd43, 0;
+	@%p69 bra 	$L__BB3_115;
+
+	cvta.to.global.u64 	%rd617, %rd43;
+	add.s64 	%rd619, %rd617, %rd561;
+	st.global.f32 	[%rd619], %f2100;
+	ld.const.u64 	%rd620, [params+240];
 	cvta.to.global.u64 	%rd621, %rd620;
-	add.s64 	%rd622, %rd621, %rd573;
-	st.global.f32 	[%rd622], %f343;
-	ld.const.u64 	%rd623, [params+216];
+	add.s64 	%rd622, %rd621, %rd561;
+	st.global.f32 	[%rd622], %f2101;
+	ld.const.u64 	%rd623, [params+248];
 	cvta.to.global.u64 	%rd624, %rd623;
-	add.s64 	%rd625, %rd624, %rd573;
-	st.global.f32 	[%rd625], %f342;
-	ld.const.u64 	%rd626, [params+224];
+	add.s64 	%rd625, %rd624, %rd561;
+	st.global.f32 	[%rd625], %f2102;
+	ld.const.u64 	%rd626, [params+256];
 	cvta.to.global.u64 	%rd627, %rd626;
-	add.s64 	%rd628, %rd627, %rd573;
-	st.global.f32 	[%rd628], %f341;
-
-BB3_112:
-	ld.const.u64 	%rd47, [params+232];
-	setp.eq.s64	%p65, %rd47, 0;
-	@%p65 bra 	BB3_114;
-
-	cvta.to.global.u64 	%rd629, %rd47;
-	add.s64 	%rd631, %rd629, %rd573;
-	st.global.f32 	[%rd631], %f2048;
-	ld.const.u64 	%rd632, [params+240];
+	add.s64 	%rd628, %rd627, %rd561;
+	st.global.f32 	[%rd628], %f2100;
+	ld.const.u64 	%rd629, [params+264];
+	cvta.to.global.u64 	%rd630, %rd629;
+	add.s64 	%rd631, %rd630, %rd561;
+	st.global.f32 	[%rd631], %f2101;
+	ld.const.u64 	%rd632, [params+272];
 	cvta.to.global.u64 	%rd633, %rd632;
-	add.s64 	%rd634, %rd633, %rd573;
-	st.global.f32 	[%rd634], %f2049;
-	ld.const.u64 	%rd635, [params+248];
-	cvta.to.global.u64 	%rd636, %rd635;
-	add.s64 	%rd637, %rd636, %rd573;
-	st.global.f32 	[%rd637], %f2050;
-	ld.const.u64 	%rd638, [params+256];
+	add.s64 	%rd634, %rd633, %rd561;
+	st.global.f32 	[%rd634], %f2102;
+
+$L__BB3_115:
+	ld.const.u64 	%rd44, [params+280];
+	setp.eq.s64 	%p70, %rd44, 0;
+	@%p70 bra 	$L__BB3_117;
+
+	cvta.to.global.u64 	%rd635, %rd44;
+	add.s64 	%rd637, %rd635, %rd561;
+	st.global.f32 	[%rd637], %f2100;
+	ld.const.u64 	%rd638, [params+288];
 	cvta.to.global.u64 	%rd639, %rd638;
-	add.s64 	%rd640, %rd639, %rd573;
-	st.global.f32 	[%rd640], %f2048;
-	ld.const.u64 	%rd641, [params+264];
+	add.s64 	%rd640, %rd639, %rd561;
+	st.global.f32 	[%rd640], %f2101;
+	ld.const.u64 	%rd641, [params+296];
 	cvta.to.global.u64 	%rd642, %rd641;
-	add.s64 	%rd643, %rd642, %rd573;
-	st.global.f32 	[%rd643], %f2049;
-	ld.const.u64 	%rd644, [params+272];
+	add.s64 	%rd643, %rd642, %rd561;
+	st.global.f32 	[%rd643], %f2102;
+	ld.const.u64 	%rd644, [params+304];
 	cvta.to.global.u64 	%rd645, %rd644;
-	add.s64 	%rd646, %rd645, %rd573;
-	st.global.f32 	[%rd646], %f2050;
-
-BB3_114:
-	ld.const.u64 	%rd48, [params+280];
-	setp.eq.s64	%p66, %rd48, 0;
-	@%p66 bra 	BB3_116;
-
-	cvta.to.global.u64 	%rd647, %rd48;
-	add.s64 	%rd649, %rd647, %rd573;
-	st.global.f32 	[%rd649], %f2048;
-	ld.const.u64 	%rd650, [params+288];
+	add.s64 	%rd646, %rd645, %rd561;
+	st.global.f32 	[%rd646], %f2100;
+	ld.const.u64 	%rd647, [params+312];
+	cvta.to.global.u64 	%rd648, %rd647;
+	add.s64 	%rd649, %rd648, %rd561;
+	st.global.f32 	[%rd649], %f2101;
+	ld.const.u64 	%rd650, [params+320];
 	cvta.to.global.u64 	%rd651, %rd650;
-	add.s64 	%rd652, %rd651, %rd573;
-	st.global.f32 	[%rd652], %f2049;
-	ld.const.u64 	%rd653, [params+296];
-	cvta.to.global.u64 	%rd654, %rd653;
-	add.s64 	%rd655, %rd654, %rd573;
-	st.global.f32 	[%rd655], %f2050;
-	ld.const.u64 	%rd656, [params+304];
-	cvta.to.global.u64 	%rd657, %rd656;
-	add.s64 	%rd658, %rd657, %rd573;
-	st.global.f32 	[%rd658], %f2048;
-	ld.const.u64 	%rd659, [params+312];
-	cvta.to.global.u64 	%rd660, %rd659;
-	add.s64 	%rd661, %rd660, %rd573;
-	st.global.f32 	[%rd661], %f2049;
-	ld.const.u64 	%rd662, [params+320];
-	cvta.to.global.u64 	%rd663, %rd662;
-	add.s64 	%rd664, %rd663, %rd573;
-	st.global.f32 	[%rd664], %f2050;
-
-BB3_116:
+	add.s64 	%rd652, %rd651, %rd561;
+	st.global.f32 	[%rd652], %f2102;
+
+$L__BB3_117:
 	ret;
-}
 
+}
 	// .globl	__closesthit__mesh
-.visible .entry __closesthit__mesh(
-
-)
+.visible .entry __closesthit__mesh()
 {
 	.reg .pred 	%p<45>;
-	.reg .b16 	%rs<10>;
-	.reg .f32 	%f<1259>;
-	.reg .b32 	%r<336>;
-	.reg .b64 	%rd<450>;
-
-
-	// inline asm
-	call (%r16), _optix_get_launch_dimension_x, ();
-	// inline asm
-	// inline asm
-	call (%r17), _optix_get_launch_dimension_y, ();
-	// inline asm
-	// inline asm
-	call (%r19), _optix_get_launch_index_x, ();
-	// inline asm
-	// inline asm
-	call (%r20), _optix_get_launch_index_y, ();
-	// inline asm
-	// inline asm
-	call (%r21), _optix_get_launch_index_z, ();
-	// inline asm
-	mad.lo.s32 	%r22, %r21, %r17, %r20;
-	mad.lo.s32 	%r1, %r22, %r16, %r19;
+	.reg .b16 	%rs<2>;
+	.reg .f32 	%f<1286>;
+	.reg .b32 	%r<338>;
+	.reg .b64 	%rd<443>;
+
+
+	// begin inline asm
+	call (%r15), _optix_get_launch_dimension_x, ();
+	// end inline asm
+	// begin inline asm
+	call (%r16), _optix_get_launch_dimension_y, ();
+	// end inline asm
+	// begin inline asm
+	call (%r18), _optix_get_launch_index_x, ();
+	// end inline asm
+	// begin inline asm
+	call (%r19), _optix_get_launch_index_y, ();
+	// end inline asm
+	// begin inline asm
+	call (%r20), _optix_get_launch_index_z, ();
+	// end inline asm
+	mad.lo.s32 	%r21, %r20, %r16, %r19;
+	mad.lo.s32 	%r1, %r21, %r15, %r18;
 	ld.const.u64 	%rd1, [params+352];
-	setp.eq.s64	%p1, %rd1, 0;
-	@%p1 bra 	BB4_2;
+	setp.eq.s64 	%p1, %rd1, 0;
+	@%p1 bra 	$L__BB4_2;
 
-	cvta.to.global.u64 	%rd40, %rd1;
-	cvt.u64.u32	%rd41, %r1;
-	add.s64 	%rd42, %rd40, %rd41;
+	cvta.to.global.u64 	%rd39, %rd1;
+	cvt.u64.u32 	%rd40, %r1;
+	add.s64 	%rd41, %rd39, %rd40;
 	mov.u16 	%rs1, 1;
-	st.global.u8 	[%rd42], %rs1;
-	bra.uni 	BB4_77;
-
-BB4_2:
-	// inline asm
-	call (%rd43), _optix_get_sbt_data_ptr_64, ();
-	// inline asm
-	// inline asm
-	call (%r23), _optix_read_primitive_idx, ();
-	// inline asm
-	// inline asm
-	call (%f428), _optix_get_ray_tmax, ();
-	// inline asm
-	// inline asm
-	call (%f1099, %f1100), _optix_get_triangle_barycentrics, ();
-	// inline asm
-	ld.const.u64 	%rd3, [params+80];
-	setp.eq.s64	%p2, %rd3, 0;
-	@%p2 bra 	BB4_7;
-
-	ld.u64 	%rd44, [%rd43];
-	ld.const.u64 	%rd45, [params+328];
-	cvta.to.global.u64 	%rd46, %rd45;
-	cvt.u64.u32	%rd4, %r1;
-	mul.wide.u32 	%rd47, %r1, 8;
-	add.s64 	%rd48, %rd46, %rd47;
-	st.global.u64 	[%rd48], %rd44;
-	ld.const.u64 	%rd49, [params+336];
-	cvta.to.global.u64 	%rd50, %rd49;
-	mul.wide.u32 	%rd51, %r1, 4;
-	add.s64 	%rd52, %rd50, %rd51;
-	st.global.u32 	[%rd52], %r23;
-	ld.const.u64 	%rd53, [params+344];
-	cvta.to.global.u64 	%rd54, %rd53;
-	add.s64 	%rd5, %rd54, %rd51;
-	ld.global.u32 	%r3, [%rd5];
-	setp.eq.s32	%p3, %r3, 0;
-	@%p3 bra 	BB4_6;
-
-	// inline asm
-	call (%r24), _optix_read_instance_id, ();
-	// inline asm
-	setp.ge.u32	%p4, %r24, %r3;
-	@%p4 bra 	BB4_6;
-
-	st.global.u32 	[%rd5], %r24;
-
-BB4_6:
-	cvta.to.global.u64 	%rd55, %rd3;
-	shl.b64 	%rd56, %rd4, 2;
-	add.s64 	%rd57, %rd55, %rd56;
-	st.global.f32 	[%rd57], %f1099;
-	ld.const.u64 	%rd58, [params+88];
-	cvta.to.global.u64 	%rd59, %rd58;
-	add.s64 	%rd60, %rd59, %rd56;
-	st.global.f32 	[%rd60], %f1100;
-	ld.const.u64 	%rd61, [params+72];
-	cvta.to.global.u64 	%rd62, %rd61;
-	add.s64 	%rd63, %rd62, %rd56;
-	st.global.f32 	[%rd63], %f428;
-	bra.uni 	BB4_77;
-
-BB4_7:
-	ld.u64 	%rd6, [%rd43+8];
-	mov.f32 	%f437, 0f3F800000;
-	sub.f32 	%f438, %f437, %f1099;
-	sub.f32 	%f4, %f438, %f1100;
-	mul.wide.u32 	%rd64, %r23, 3;
-	ld.u64 	%rd65, [%rd6];
-	shl.b64 	%rd66, %rd64, 2;
-	add.s64 	%rd67, %rd65, %rd66;
-	ld.u32 	%r25, [%rd67];
-	mul.wide.u32 	%rd8, %r25, 3;
-	ld.u64 	%rd68, [%rd6+8];
-	shl.b64 	%rd69, %rd8, 2;
-	add.s64 	%rd70, %rd68, %rd69;
-	ld.u32 	%r26, [%rd67+4];
-	mul.wide.u32 	%rd10, %r26, 3;
-	shl.b64 	%rd71, %rd10, 2;
-	add.s64 	%rd72, %rd68, %rd71;
-	ld.u32 	%r27, [%rd67+8];
-	mul.wide.u32 	%rd12, %r27, 3;
-	shl.b64 	%rd73, %rd12, 2;
-	add.s64 	%rd74, %rd68, %rd73;
-	ld.f32 	%f439, [%rd70];
-	ld.f32 	%f440, [%rd70+4];
-	ld.f32 	%f441, [%rd70+8];
-	ld.f32 	%f442, [%rd72];
-	mul.f32 	%f443, %f442, %f1099;
-	ld.f32 	%f444, [%rd72+4];
-	mul.f32 	%f445, %f444, %f1099;
-	ld.f32 	%f446, [%rd72+8];
-	mul.f32 	%f447, %f446, %f1099;
-	fma.rn.f32 	%f448, %f439, %f4, %f443;
-	fma.rn.f32 	%f449, %f440, %f4, %f445;
-	fma.rn.f32 	%f450, %f441, %f4, %f447;
-	ld.f32 	%f451, [%rd74];
-	ld.f32 	%f452, [%rd74+4];
-	ld.f32 	%f453, [%rd74+8];
-	fma.rn.f32 	%f1256, %f451, %f1100, %f448;
-	fma.rn.f32 	%f1257, %f452, %f1100, %f449;
-	fma.rn.f32 	%f1258, %f453, %f1100, %f450;
-	sub.f32 	%f8, %f442, %f439;
-	sub.f32 	%f9, %f444, %f440;
-	sub.f32 	%f10, %f446, %f441;
-	sub.f32 	%f11, %f451, %f439;
-	sub.f32 	%f12, %f452, %f440;
-	sub.f32 	%f13, %f453, %f441;
-	mul.f32 	%f454, %f9, %f13;
-	mul.f32 	%f455, %f10, %f12;
-	sub.f32 	%f456, %f454, %f455;
-	mul.f32 	%f457, %f10, %f11;
-	mul.f32 	%f458, %f8, %f13;
-	sub.f32 	%f459, %f457, %f458;
-	mul.f32 	%f460, %f8, %f12;
-	mul.f32 	%f461, %f9, %f11;
-	sub.f32 	%f462, %f460, %f461;
-	mul.f32 	%f463, %f456, %f456;
-	fma.rn.f32 	%f464, %f459, %f459, %f463;
-	fma.rn.f32 	%f465, %f462, %f462, %f464;
-	sqrt.rn.f32 	%f466, %f465;
-	div.rn.f32 	%f1250, %f456, %f466;
-	div.rn.f32 	%f1251, %f459, %f466;
-	div.rn.f32 	%f1252, %f462, %f466;
-	ld.const.u64 	%rd13, [params+136];
-	setp.eq.s64	%p5, %rd13, 0;
-	mov.f32 	%f1084, 0f00000000;
-	mov.f32 	%f1085, %f1084;
-	mov.f32 	%f1086, %f1084;
-	mov.f32 	%f1087, %f1084;
-	mov.f32 	%f1088, %f1084;
-	mov.f32 	%f1089, %f1084;
-	mov.f32 	%f1090, %f1252;
-	mov.f32 	%f1091, %f1251;
-	mov.f32 	%f1092, %f1250;
-	@%p5 bra 	BB4_12;
-
-	mov.f32 	%f1084, 0f00000000;
-	ld.u64 	%rd14, [%rd6+16];
-	setp.eq.s64	%p6, %rd14, 0;
-	mov.f32 	%f1085, %f1084;
-	mov.f32 	%f1086, %f1084;
-	mov.f32 	%f1087, %f1084;
-	mov.f32 	%f1088, %f1084;
-	mov.f32 	%f1089, %f1084;
-	mov.f32 	%f1090, %f1252;
-	mov.f32 	%f1091, %f1251;
-	mov.f32 	%f1092, %f1250;
-	@%p6 bra 	BB4_12;
-
-	mul.wide.u32 	%rd444, %r25, 3;
-	shl.b64 	%rd443, %rd444, 2;
-	mov.f32 	%f1084, 0f00000000;
-	add.s64 	%rd76, %rd14, %rd443;
-	add.s64 	%rd78, %rd14, %rd71;
-	add.s64 	%rd80, %rd14, %rd73;
-	ld.f32 	%f17, [%rd76];
-	ld.f32 	%f18, [%rd76+4];
-	ld.f32 	%f19, [%rd76+8];
-	ld.f32 	%f20, [%rd78];
-	mul.f32 	%f479, %f20, %f1099;
-	ld.f32 	%f21, [%rd78+4];
-	mul.f32 	%f480, %f21, %f1099;
-	ld.f32 	%f22, [%rd78+8];
-	mul.f32 	%f481, %f22, %f1099;
-	fma.rn.f32 	%f482, %f17, %f4, %f479;
-	fma.rn.f32 	%f483, %f18, %f4, %f480;
-	fma.rn.f32 	%f484, %f19, %f4, %f481;
-	ld.f32 	%f23, [%rd80];
-	ld.f32 	%f24, [%rd80+4];
-	ld.f32 	%f25, [%rd80+8];
-	fma.rn.f32 	%f485, %f23, %f1100, %f482;
-	fma.rn.f32 	%f486, %f24, %f1100, %f483;
-	fma.rn.f32 	%f487, %f25, %f1100, %f484;
-	mul.f32 	%f488, %f485, %f485;
-	fma.rn.f32 	%f489, %f486, %f486, %f488;
-	fma.rn.f32 	%f490, %f487, %f487, %f489;
-	sqrt.rn.f32 	%f491, %f490;
-	div.rn.f32 	%f1092, %f485, %f491;
-	div.rn.f32 	%f1091, %f486, %f491;
-	div.rn.f32 	%f1090, %f487, %f491;
-	ld.const.u64 	%rd81, [params+280];
-	setp.eq.s64	%p7, %rd81, 0;
-	@%p7 bra 	BB4_10;
-
-	mul.f32 	%f492, %f1099, %f23;
-	fma.rn.f32 	%f493, %f4, %f20, %f492;
-	mul.f32 	%f494, %f1099, %f24;
-	fma.rn.f32 	%f495, %f4, %f21, %f494;
-	mul.f32 	%f496, %f1099, %f25;
-	fma.rn.f32 	%f497, %f4, %f22, %f496;
-	fma.rn.f32 	%f498, %f1100, %f17, %f493;
-	fma.rn.f32 	%f499, %f1100, %f18, %f495;
-	fma.rn.f32 	%f500, %f1100, %f19, %f497;
-	mul.f32 	%f501, %f498, %f498;
-	fma.rn.f32 	%f502, %f499, %f499, %f501;
-	fma.rn.f32 	%f503, %f500, %f500, %f502;
-	sqrt.rn.f32 	%f504, %f503;
-	rcp.rn.f32 	%f505, %f504;
-	mul.f32 	%f506, %f505, %f498;
-	mul.f32 	%f507, %f505, %f499;
-	mul.f32 	%f508, %f505, %f500;
-	sub.f32 	%f509, %f20, %f17;
-	mul.f32 	%f510, %f509, %f505;
-	sub.f32 	%f511, %f21, %f18;
-	mul.f32 	%f512, %f511, %f505;
-	sub.f32 	%f513, %f22, %f19;
-	mul.f32 	%f514, %f513, %f505;
-	sub.f32 	%f515, %f23, %f17;
-	mul.f32 	%f516, %f515, %f505;
-	sub.f32 	%f517, %f24, %f18;
-	mul.f32 	%f518, %f517, %f505;
-	sub.f32 	%f519, %f25, %f19;
-	mul.f32 	%f520, %f519, %f505;
-	mul.f32 	%f521, %f506, %f510;
-	fma.rn.f32 	%f522, %f507, %f512, %f521;
-	fma.rn.f32 	%f523, %f508, %f514, %f522;
-	neg.f32 	%f524, %f506;
-	neg.f32 	%f525, %f507;
-	neg.f32 	%f526, %f508;
-	fma.rn.f32 	%f1087, %f523, %f524, %f510;
-	fma.rn.f32 	%f1088, %f523, %f525, %f512;
-	fma.rn.f32 	%f1089, %f523, %f526, %f514;
-	mul.f32 	%f527, %f506, %f1087;
-	fma.rn.f32 	%f528, %f507, %f1088, %f527;
-	fma.rn.f32 	%f529, %f508, %f1089, %f528;
-	fma.rn.f32 	%f1084, %f529, %f524, %f516;
-	fma.rn.f32 	%f1085, %f529, %f525, %f518;
-	fma.rn.f32 	%f1086, %f529, %f526, %f520;
-	bra.uni 	BB4_12;
-
-BB4_10:
-	mov.f32 	%f1085, %f1084;
-	mov.f32 	%f1086, %f1084;
-	mov.f32 	%f1087, %f1084;
-	mov.f32 	%f1088, %f1084;
-	mov.f32 	%f1089, %f1084;
-
-BB4_12:
-	mov.b32 	 %r28, %f1252;
-	and.b32  	%r29, %r28, -2147483648;
-	or.b32  	%r30, %r29, 1065353216;
-	mov.b32 	 %f530, %r30;
-	add.f32 	%f531, %f1252, %f530;
-	mov.f32 	%f532, 0fBF800000;
-	div.rn.f32 	%f533, %f532, %f531;
-	mul.f32 	%f534, %f1250, %f1251;
-	mul.f32 	%f1244, %f534, %f533;
-	mul.f32 	%f535, %f1250, %f1250;
-	mul.f32 	%f536, %f535, %f533;
-	fma.rn.f32 	%f1247, %f530, %f536, 0f3F800000;
-	mul.f32 	%f1248, %f530, %f1244;
-	mul.f32 	%f537, %f1250, %f530;
-	neg.f32 	%f1249, %f537;
-	mul.f32 	%f538, %f1251, %f1251;
-	fma.rn.f32 	%f1245, %f538, %f533, %f530;
-	neg.f32 	%f1246, %f1251;
-	ld.const.u64 	%rd15, [params+96];
-	setp.eq.s64	%p8, %rd15, 0;
-	@%p8 bra 	BB4_17;
-
-	ld.u64 	%rd16, [%rd6+24];
-	setp.eq.s64	%p9, %rd16, 0;
-	@%p9 bra 	BB4_17;
-
-	mov.f32 	%f1083, 0f3F800000;
-	sub.f32 	%f1082, %f1083, %f1099;
-	sub.f32 	%f1081, %f1082, %f1100;
-	cvt.u64.u32	%rd447, %r27;
-	cvt.u64.u32	%rd446, %r26;
-	cvt.u64.u32	%rd445, %r25;
-	shl.b64 	%rd82, %rd445, 3;
-	add.s64 	%rd83, %rd16, %rd82;
-	shl.b64 	%rd84, %rd446, 3;
-	add.s64 	%rd85, %rd16, %rd84;
-	shl.b64 	%rd86, %rd447, 3;
-	add.s64 	%rd87, %rd16, %rd86;
-	ld.f32 	%f50, [%rd83];
-	ld.f32 	%f51, [%rd83+4];
-	ld.f32 	%f52, [%rd85];
-	mul.f32 	%f539, %f52, %f1099;
-	ld.f32 	%f53, [%rd85+4];
-	mul.f32 	%f540, %f53, %f1099;
-	fma.rn.f32 	%f541, %f50, %f1081, %f539;
-	fma.rn.f32 	%f542, %f51, %f1081, %f540;
-	ld.f32 	%f54, [%rd87];
-	ld.f32 	%f55, [%rd87+4];
-	fma.rn.f32 	%f1099, %f54, %f1100, %f541;
-	fma.rn.f32 	%f1100, %f55, %f1100, %f542;
-	ld.const.u64 	%rd88, [params+184];
-	setp.eq.s64	%p10, %rd88, 0;
-	@%p10 bra 	BB4_17;
-
-	sub.f32 	%f58, %f52, %f50;
-	sub.f32 	%f59, %f55, %f51;
-	mul.f32 	%f543, %f58, %f59;
-	sub.f32 	%f60, %f54, %f50;
-	sub.f32 	%f61, %f53, %f51;
-	mul.f32 	%f544, %f61, %f60;
-	sub.f32 	%f62, %f543, %f544;
-	setp.eq.f32	%p11, %f62, 0f00000000;
-	@%p11 bra 	BB4_17;
-
-	rcp.rn.f32 	%f545, %f62;
-	mul.f32 	%f546, %f61, %f11;
-	mul.f32 	%f547, %f59, %f8;
-	sub.f32 	%f548, %f547, %f546;
-	mul.f32 	%f549, %f61, %f12;
-	mul.f32 	%f550, %f59, %f9;
-	sub.f32 	%f551, %f550, %f549;
-	mul.f32 	%f552, %f61, %f13;
-	mul.f32 	%f553, %f59, %f10;
-	sub.f32 	%f554, %f553, %f552;
-	mul.f32 	%f1247, %f548, %f545;
-	mul.f32 	%f1248, %f551, %f545;
-	mul.f32 	%f1249, %f554, %f545;
-	mul.f32 	%f555, %f8, %f60;
-	mul.f32 	%f556, %f9, %f60;
-	mul.f32 	%f557, %f10, %f60;
-	mul.f32 	%f558, %f58, %f11;
-	sub.f32 	%f559, %f558, %f555;
-	mul.f32 	%f560, %f58, %f12;
-	sub.f32 	%f561, %f560, %f556;
-	mul.f32 	%f562, %f58, %f13;
-	sub.f32 	%f563, %f562, %f557;
-	mul.f32 	%f1244, %f559, %f545;
-	mul.f32 	%f1245, %f561, %f545;
-	mul.f32 	%f1246, %f563, %f545;
-
-BB4_17:
-	ld.u64 	%rd17, [%rd43];
-	ld.const.u64 	%rd89, [params+344];
-	cvta.to.global.u64 	%rd90, %rd89;
-	mul.wide.u32 	%rd91, %r1, 4;
-	add.s64 	%rd19, %rd90, %rd91;
-	ld.global.u32 	%r5, [%rd19];
-	setp.eq.s32	%p12, %r5, 0;
-	@%p12 bra 	BB4_65;
-
-	// inline asm
-	call (%r31), _optix_read_instance_id, ();
-	// inline asm
-	setp.ge.u32	%p13, %r31, %r5;
-	@%p13 bra 	BB4_65;
-
-	mov.f32 	%f1111, 0f3F800000;
-	// inline asm
-	call (%r32), _optix_get_transform_list_size, ();
-	// inline asm
-	setp.eq.s32	%p14, %r32, 0;
+	st.global.u8 	[%rd41], %rs1;
+	bra.uni 	$L__BB4_76;
+
+$L__BB4_2:
+	// begin inline asm
+	call (%rd42), _optix_get_sbt_data_ptr_64, ();
+	// end inline asm
+	ld.u64 	%rd3, [%rd42+8];
+	// begin inline asm
+	call (%r22), _optix_read_primitive_idx, ();
+	// end inline asm
+	// begin inline asm
+	call (%f437), _optix_get_ray_tmax, ();
+	// end inline asm
+	// begin inline asm
+	call (%f1120, %f1121), _optix_get_triangle_barycentrics, ();
+	// end inline asm
+	ld.const.u64 	%rd4, [params+80];
+	setp.eq.s64 	%p2, %rd4, 0;
+	mov.f32 	%f1111, 0f00000000;
+	mov.f32 	%f1112, 0f00000000;
+	mov.f32 	%f1113, 0f00000000;
+	mov.f32 	%f1114, 0f00000000;
+	mov.f32 	%f1115, 0f00000000;
+	mov.f32 	%f1116, 0f00000000;
+	@%p2 bra 	$L__BB4_7;
+
+	ld.u64 	%rd43, [%rd42];
+	ld.const.u64 	%rd44, [params+328];
+	cvta.to.global.u64 	%rd45, %rd44;
+	cvt.u64.u32 	%rd5, %r1;
+	mul.wide.u32 	%rd46, %r1, 8;
+	add.s64 	%rd47, %rd45, %rd46;
+	st.global.u64 	[%rd47], %rd43;
+	ld.const.u64 	%rd48, [params+336];
+	cvta.to.global.u64 	%rd49, %rd48;
+	mul.wide.u32 	%rd50, %r1, 4;
+	add.s64 	%rd51, %rd49, %rd50;
+	st.global.u32 	[%rd51], %r22;
+	ld.const.u64 	%rd52, [params+344];
+	cvta.to.global.u64 	%rd53, %rd52;
+	add.s64 	%rd6, %rd53, %rd50;
+	ld.global.u32 	%r3, [%rd6];
+	setp.eq.s32 	%p3, %r3, 0;
+	@%p3 bra 	$L__BB4_6;
+
+	// begin inline asm
+	call (%r23), _optix_read_instance_id, ();
+	// end inline asm
+	setp.ge.u32 	%p4, %r23, %r3;
+	@%p4 bra 	$L__BB4_6;
+
+	st.global.u32 	[%rd6], %r23;
+
+$L__BB4_6:
+	cvta.to.global.u64 	%rd54, %rd4;
+	shl.b64 	%rd55, %rd5, 2;
+	add.s64 	%rd56, %rd54, %rd55;
+	st.global.f32 	[%rd56], %f1120;
+	ld.const.u64 	%rd57, [params+88];
+	cvta.to.global.u64 	%rd58, %rd57;
+	add.s64 	%rd59, %rd58, %rd55;
+	st.global.f32 	[%rd59], %f1121;
+	ld.const.u64 	%rd60, [params+72];
+	cvta.to.global.u64 	%rd61, %rd60;
+	add.s64 	%rd62, %rd61, %rd55;
+	st.global.f32 	[%rd62], %f437;
+	bra.uni 	$L__BB4_76;
+
+$L__BB4_7:
+	mov.f32 	%f446, 0f3F800000;
+	sub.f32 	%f447, %f446, %f1120;
+	sub.f32 	%f4, %f447, %f1121;
+	ld.u64 	%rd63, [%rd3];
+	mul.wide.u32 	%rd64, %r22, 12;
+	add.s64 	%rd65, %rd63, %rd64;
+	ld.u32 	%r24, [%rd65];
+	mul.wide.u32 	%rd9, %r24, 3;
+	ld.u64 	%rd66, [%rd3+8];
+	shl.b64 	%rd67, %rd9, 2;
+	add.s64 	%rd68, %rd66, %rd67;
+	ld.u32 	%r25, [%rd65+4];
+	mul.wide.u32 	%rd11, %r25, 3;
+	shl.b64 	%rd69, %rd11, 2;
+	add.s64 	%rd70, %rd66, %rd69;
+	ld.u32 	%r26, [%rd65+8];
+	mul.wide.u32 	%rd13, %r26, 3;
+	shl.b64 	%rd71, %rd13, 2;
+	add.s64 	%rd72, %rd66, %rd71;
+	ld.f32 	%f448, [%rd68];
+	ld.f32 	%f449, [%rd68+4];
+	ld.f32 	%f450, [%rd68+8];
+	ld.f32 	%f451, [%rd70];
+	mul.f32 	%f452, %f451, %f1120;
+	ld.f32 	%f453, [%rd70+4];
+	mul.f32 	%f454, %f453, %f1120;
+	ld.f32 	%f455, [%rd70+8];
+	mul.f32 	%f456, %f455, %f1120;
+	fma.rn.f32 	%f457, %f448, %f4, %f452;
+	fma.rn.f32 	%f458, %f449, %f4, %f454;
+	fma.rn.f32 	%f459, %f450, %f4, %f456;
+	ld.f32 	%f460, [%rd72];
+	ld.f32 	%f461, [%rd72+4];
+	ld.f32 	%f462, [%rd72+8];
+	fma.rn.f32 	%f1283, %f460, %f1121, %f457;
+	fma.rn.f32 	%f1284, %f461, %f1121, %f458;
+	fma.rn.f32 	%f1285, %f462, %f1121, %f459;
+	sub.f32 	%f8, %f451, %f448;
+	sub.f32 	%f9, %f453, %f449;
+	sub.f32 	%f10, %f455, %f450;
+	sub.f32 	%f11, %f460, %f448;
+	sub.f32 	%f12, %f461, %f449;
+	sub.f32 	%f13, %f462, %f450;
+	mul.f32 	%f463, %f9, %f13;
+	mul.f32 	%f464, %f10, %f12;
+	sub.f32 	%f465, %f463, %f464;
+	mul.f32 	%f466, %f10, %f11;
+	mul.f32 	%f467, %f8, %f13;
+	sub.f32 	%f468, %f466, %f467;
+	mul.f32 	%f469, %f8, %f12;
+	mul.f32 	%f470, %f9, %f11;
+	sub.f32 	%f471, %f469, %f470;
+	mul.f32 	%f472, %f465, %f465;
+	fma.rn.f32 	%f473, %f468, %f468, %f472;
+	fma.rn.f32 	%f474, %f471, %f471, %f473;
+	sqrt.rn.f32 	%f475, %f474;
+	div.rn.f32 	%f1247, %f465, %f475;
+	div.rn.f32 	%f1248, %f468, %f475;
+	div.rn.f32 	%f1249, %f471, %f475;
+	ld.const.u64 	%rd14, [params+136];
+	setp.eq.s64 	%p5, %rd14, 0;
+	mov.f32 	%f1117, %f1247;
+	mov.f32 	%f1118, %f1248;
+	mov.f32 	%f1119, %f1249;
+	@%p5 bra 	$L__BB4_11;
+
+	mov.f32 	%f1116, 0f00000000;
+	mov.f32 	%f1115, 0f00000000;
+	mov.f32 	%f1114, 0f00000000;
+	mov.f32 	%f1113, 0f00000000;
+	mov.f32 	%f1112, 0f00000000;
+	mov.f32 	%f1111, 0f00000000;
+	ld.u64 	%rd15, [%rd3+16];
+	setp.eq.s64 	%p6, %rd15, 0;
+	mov.f32 	%f1117, %f1247;
+	mov.f32 	%f1118, %f1248;
+	mov.f32 	%f1119, %f1249;
+	@%p6 bra 	$L__BB4_11;
+
+	mul.wide.u32 	%rd437, %r24, 3;
+	shl.b64 	%rd436, %rd437, 2;
+	mov.f32 	%f1116, 0f00000000;
+	mov.f32 	%f1115, 0f00000000;
+	mov.f32 	%f1114, 0f00000000;
+	mov.f32 	%f1113, 0f00000000;
 	mov.f32 	%f1112, 0f00000000;
-	mov.f32 	%f1102, %f1112;
-	mov.f32 	%f1101, %f1112;
-	mov.f32 	%f1107, %f1112;
-	mov.f32 	%f1108, %f1111;
-	mov.f32 	%f1109, %f1112;
-	mov.f32 	%f1110, %f1112;
-	mov.f32 	%f1103, %f1112;
-	mov.f32 	%f1104, %f1112;
-	mov.f32 	%f1105, %f1111;
-	mov.f32 	%f1106, %f1112;
-	@%p14 bra 	BB4_37;
-
-	// inline asm
-	call (%f576), _optix_get_ray_time, ();
-	// inline asm
-	add.s32 	%r334, %r32, -1;
-	setp.lt.s32	%p15, %r334, 0;
-	@%p15 bra 	BB4_37;
-
-BB4_21:
+	mov.f32 	%f1111, 0f00000000;
+	add.s64 	%rd74, %rd15, %rd436;
+	add.s64 	%rd76, %rd15, %rd69;
+	add.s64 	%rd78, %rd15, %rd71;
+	ld.f32 	%f21, [%rd74];
+	ld.f32 	%f22, [%rd74+4];
+	ld.f32 	%f23, [%rd74+8];
+	ld.f32 	%f24, [%rd76];
+	mul.f32 	%f488, %f24, %f1120;
+	ld.f32 	%f25, [%rd76+4];
+	mul.f32 	%f489, %f25, %f1120;
+	ld.f32 	%f26, [%rd76+8];
+	mul.f32 	%f490, %f26, %f1120;
+	fma.rn.f32 	%f491, %f21, %f4, %f488;
+	fma.rn.f32 	%f492, %f22, %f4, %f489;
+	fma.rn.f32 	%f493, %f23, %f4, %f490;
+	ld.f32 	%f27, [%rd78];
+	ld.f32 	%f28, [%rd78+4];
+	ld.f32 	%f29, [%rd78+8];
+	fma.rn.f32 	%f494, %f27, %f1121, %f491;
+	fma.rn.f32 	%f495, %f28, %f1121, %f492;
+	fma.rn.f32 	%f496, %f29, %f1121, %f493;
+	mul.f32 	%f497, %f494, %f494;
+	fma.rn.f32 	%f498, %f495, %f495, %f497;
+	fma.rn.f32 	%f499, %f496, %f496, %f498;
+	sqrt.rn.f32 	%f500, %f499;
+	div.rn.f32 	%f1119, %f496, %f500;
+	div.rn.f32 	%f1118, %f495, %f500;
+	div.rn.f32 	%f1117, %f494, %f500;
+	ld.const.u64 	%rd79, [params+280];
+	setp.eq.s64 	%p7, %rd79, 0;
+	@%p7 bra 	$L__BB4_11;
+
+	mul.f32 	%f501, %f1120, %f27;
+	fma.rn.f32 	%f502, %f4, %f24, %f501;
+	mul.f32 	%f503, %f1120, %f28;
+	fma.rn.f32 	%f504, %f4, %f25, %f503;
+	mul.f32 	%f505, %f1120, %f29;
+	fma.rn.f32 	%f506, %f4, %f26, %f505;
+	fma.rn.f32 	%f507, %f1121, %f21, %f502;
+	fma.rn.f32 	%f508, %f1121, %f22, %f504;
+	fma.rn.f32 	%f509, %f1121, %f23, %f506;
+	mul.f32 	%f510, %f507, %f507;
+	fma.rn.f32 	%f511, %f508, %f508, %f510;
+	fma.rn.f32 	%f512, %f509, %f509, %f511;
+	sqrt.rn.f32 	%f513, %f512;
+	rcp.rn.f32 	%f514, %f513;
+	mul.f32 	%f515, %f514, %f507;
+	mul.f32 	%f516, %f514, %f508;
+	mul.f32 	%f517, %f514, %f509;
+	sub.f32 	%f518, %f24, %f21;
+	mul.f32 	%f519, %f518, %f514;
+	sub.f32 	%f520, %f25, %f22;
+	mul.f32 	%f521, %f520, %f514;
+	sub.f32 	%f522, %f26, %f23;
+	mul.f32 	%f523, %f522, %f514;
+	sub.f32 	%f524, %f27, %f21;
+	mul.f32 	%f525, %f524, %f514;
+	sub.f32 	%f526, %f28, %f22;
+	mul.f32 	%f527, %f526, %f514;
+	sub.f32 	%f528, %f29, %f23;
+	mul.f32 	%f529, %f528, %f514;
+	mul.f32 	%f530, %f515, %f519;
+	fma.rn.f32 	%f531, %f516, %f521, %f530;
+	fma.rn.f32 	%f532, %f517, %f523, %f531;
+	neg.f32 	%f533, %f515;
+	neg.f32 	%f534, %f516;
+	neg.f32 	%f535, %f517;
+	fma.rn.f32 	%f1114, %f532, %f533, %f519;
+	fma.rn.f32 	%f1115, %f532, %f534, %f521;
+	fma.rn.f32 	%f1116, %f532, %f535, %f523;
+	mul.f32 	%f536, %f515, %f1114;
+	fma.rn.f32 	%f537, %f516, %f1115, %f536;
+	fma.rn.f32 	%f538, %f517, %f1116, %f537;
+	fma.rn.f32 	%f1111, %f538, %f533, %f525;
+	fma.rn.f32 	%f1112, %f538, %f534, %f527;
+	fma.rn.f32 	%f1113, %f538, %f535, %f529;
+
+$L__BB4_11:
+	mov.b32 	%r27, %f1249;
+	and.b32  	%r28, %r27, -2147483648;
+	or.b32  	%r29, %r28, 1065353216;
+	mov.b32 	%f539, %r29;
+	add.f32 	%f540, %f1249, %f539;
+	mov.f32 	%f541, 0fBF800000;
+	div.rn.f32 	%f542, %f541, %f540;
+	mul.f32 	%f543, %f1247, %f1248;
+	mul.f32 	%f1127, %f543, %f542;
+	mul.f32 	%f544, %f1247, %f1247;
+	mul.f32 	%f545, %f544, %f542;
+	fma.rn.f32 	%f1124, %f545, %f539, 0f3F800000;
+	mul.f32 	%f1123, %f1127, %f539;
+	mul.f32 	%f546, %f1247, %f539;
+	neg.f32 	%f1122, %f546;
+	mul.f32 	%f547, %f1248, %f1248;
+	fma.rn.f32 	%f1126, %f547, %f542, %f539;
+	neg.f32 	%f1125, %f1248;
+	ld.const.u64 	%rd16, [params+96];
+	setp.eq.s64 	%p8, %rd16, 0;
+	@%p8 bra 	$L__BB4_16;
+
+	ld.u64 	%rd17, [%rd3+24];
+	setp.eq.s64 	%p9, %rd17, 0;
+	@%p9 bra 	$L__BB4_16;
+
+	mov.f32 	%f1109, 0f3F800000;
+	sub.f32 	%f1108, %f1109, %f1120;
+	sub.f32 	%f1107, %f1108, %f1121;
+	cvt.u64.u32 	%rd440, %r26;
+	cvt.u64.u32 	%rd439, %r25;
+	cvt.u64.u32 	%rd438, %r24;
+	shl.b64 	%rd80, %rd438, 3;
+	add.s64 	%rd81, %rd17, %rd80;
+	shl.b64 	%rd82, %rd439, 3;
+	add.s64 	%rd83, %rd17, %rd82;
+	shl.b64 	%rd84, %rd440, 3;
+	add.s64 	%rd85, %rd17, %rd84;
+	ld.f32 	%f56, [%rd81];
+	ld.f32 	%f57, [%rd81+4];
+	ld.f32 	%f58, [%rd83];
+	mul.f32 	%f548, %f58, %f1120;
+	ld.f32 	%f59, [%rd83+4];
+	mul.f32 	%f549, %f59, %f1120;
+	fma.rn.f32 	%f550, %f56, %f1107, %f548;
+	fma.rn.f32 	%f551, %f57, %f1107, %f549;
+	ld.f32 	%f60, [%rd85];
+	ld.f32 	%f61, [%rd85+4];
+	fma.rn.f32 	%f1120, %f60, %f1121, %f550;
+	fma.rn.f32 	%f1121, %f61, %f1121, %f551;
+	ld.const.u64 	%rd86, [params+184];
+	setp.eq.s64 	%p10, %rd86, 0;
+	@%p10 bra 	$L__BB4_16;
+
+	neg.f32 	%f1125, %f1248;
+	sub.f32 	%f64, %f58, %f56;
+	sub.f32 	%f65, %f61, %f57;
+	mul.f32 	%f552, %f64, %f65;
+	sub.f32 	%f66, %f60, %f56;
+	sub.f32 	%f67, %f59, %f57;
+	mul.f32 	%f553, %f67, %f66;
+	sub.f32 	%f68, %f552, %f553;
+	setp.eq.f32 	%p11, %f68, 0f00000000;
+	@%p11 bra 	$L__BB4_16;
+
+	rcp.rn.f32 	%f554, %f68;
+	mul.f32 	%f555, %f67, %f11;
+	mul.f32 	%f556, %f65, %f8;
+	sub.f32 	%f557, %f556, %f555;
+	mul.f32 	%f558, %f67, %f12;
+	mul.f32 	%f559, %f65, %f9;
+	sub.f32 	%f560, %f559, %f558;
+	mul.f32 	%f561, %f67, %f13;
+	mul.f32 	%f562, %f65, %f10;
+	sub.f32 	%f563, %f562, %f561;
+	mul.f32 	%f1124, %f557, %f554;
+	mul.f32 	%f1123, %f560, %f554;
+	mul.f32 	%f1122, %f563, %f554;
+	mul.f32 	%f564, %f8, %f66;
+	mul.f32 	%f565, %f9, %f66;
+	mul.f32 	%f566, %f10, %f66;
+	mul.f32 	%f567, %f64, %f11;
+	sub.f32 	%f568, %f567, %f564;
+	mul.f32 	%f569, %f64, %f12;
+	sub.f32 	%f570, %f569, %f565;
+	mul.f32 	%f571, %f64, %f13;
+	sub.f32 	%f572, %f571, %f566;
+	mul.f32 	%f1127, %f568, %f554;
+	mul.f32 	%f1126, %f570, %f554;
+	mul.f32 	%f1125, %f572, %f554;
+
+$L__BB4_16:
+	ld.u64 	%rd18, [%rd42];
+	ld.const.u64 	%rd87, [params+344];
+	cvta.to.global.u64 	%rd88, %rd87;
+	mul.wide.u32 	%rd89, %r1, 4;
+	add.s64 	%rd20, %rd88, %rd89;
+	ld.global.u32 	%r5, [%rd20];
+	setp.eq.s32 	%p12, %r5, 0;
+	@%p12 bra 	$L__BB4_64;
+
+	// begin inline asm
+	call (%r30), _optix_read_instance_id, ();
+	// end inline asm
+	setp.ge.u32 	%p13, %r30, %r5;
+	@%p13 bra 	$L__BB4_64;
+
+	mov.f32 	%f1164, 0f3F800000;
+	// begin inline asm
+	call (%r31), _optix_get_transform_list_size, ();
+	// end inline asm
+	setp.eq.s32 	%p14, %r31, 0;
+	mov.f32 	%f1165, 0f00000000;
+	mov.f32 	%f1166, %f1165;
+	mov.f32 	%f1167, %f1165;
+	mov.f32 	%f1160, %f1165;
+	mov.f32 	%f1161, %f1164;
+	mov.f32 	%f1162, %f1165;
+	mov.f32 	%f1163, %f1165;
+	mov.f32 	%f1156, %f1165;
+	mov.f32 	%f1157, %f1165;
+	mov.f32 	%f1158, %f1164;
+	mov.f32 	%f1159, %f1165;
+	@%p14 bra 	$L__BB4_36;
+
+	// begin inline asm
+	call (%r32), _optix_get_transform_list_size, ();
+	// end inline asm
+	// begin inline asm
+	call (%f585), _optix_get_ray_time, ();
+	// end inline asm
+	setp.lt.s32 	%p15, %r32, 1;
+	@%p15 bra 	$L__BB4_36;
+
+	mov.u32 	%r336, %r32;
+
+$L__BB4_21:
 	.pragma "nounroll";
-	// inline asm
-	call (%rd92), _optix_get_transform_list_handle, (%r334);
-	// inline asm
-	// inline asm
-	call (%r34), _optix_get_transform_type_from_handle, (%rd92);
-	// inline asm
-	and.b32  	%r35, %r34, -2;
-	setp.eq.s32	%p16, %r35, 2;
-	@%p16 bra 	BB4_27;
-	bra.uni 	BB4_22;
-
-BB4_27:
-	setp.eq.s32	%p19, %r34, 2;
-	@%p19 bra 	BB4_31;
-	bra.uni 	BB4_28;
-
-BB4_31:
-	// inline asm
-	call (%rd166), _optix_get_matrix_motion_transform_from_handle, (%rd92);
-	// inline asm
-	// inline asm
-	cvta.to.global.u64 %rd168, %rd166;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r123,%r124,%r125,%r126}, [%rd168];
-	// inline asm
-	mov.b32	{%rs4, %rs5}, %r125;
-	add.s64 	%rd172, %rd166, 16;
-	// inline asm
-	cvta.to.global.u64 %rd171, %rd172;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r127,%r128,%r129,%r130}, [%rd171];
-	// inline asm
-	add.s64 	%rd175, %rd166, 32;
-	// inline asm
-	cvta.to.global.u64 %rd174, %rd175;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r131,%r132,%r133,%r134}, [%rd174];
-	// inline asm
-	add.s64 	%rd178, %rd166, 48;
-	// inline asm
-	cvta.to.global.u64 %rd177, %rd178;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r135,%r136,%r137,%r138}, [%rd177];
-	// inline asm
-	add.s64 	%rd181, %rd166, 64;
-	// inline asm
-	cvta.to.global.u64 %rd180, %rd181;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r139,%r140,%r141,%r142}, [%rd180];
-	// inline asm
-	add.s64 	%rd184, %rd166, 80;
-	// inline asm
-	cvta.to.global.u64 %rd183, %rd184;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r143,%r144,%r145,%r146}, [%rd183];
-	// inline asm
-	add.s64 	%rd187, %rd166, 96;
-	// inline asm
-	cvta.to.global.u64 %rd186, %rd187;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r147,%r148,%r149,%r150}, [%rd186];
-	// inline asm
-	add.s64 	%rd190, %rd166, 112;
-	// inline asm
-	cvta.to.global.u64 %rd189, %rd190;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r151,%r152,%r153,%r154}, [%rd189];
-	// inline asm
-	mov.b32 	 %f703, %r126;
-	mov.b32 	 %f704, %r127;
-	cvt.u32.u16	%r167, %rs4;
+	add.s32 	%r33, %r336, -1;
+	// begin inline asm
+	call (%rd90), _optix_get_transform_list_handle, (%r33);
+	// end inline asm
+	// begin inline asm
+	call (%r34), _optix_get_transform_type_from_handle, (%rd90);
+	// end inline asm
+	or.b32  	%r35, %r34, 1;
+	setp.eq.s32 	%p16, %r35, 3;
+	@%p16 bra 	$L__BB4_27;
+	bra.uni 	$L__BB4_22;
+
+$L__BB4_27:
+	setp.eq.s32 	%p19, %r34, 2;
+	@%p19 bra 	$L__BB4_31;
+	bra.uni 	$L__BB4_28;
+
+$L__BB4_31:
+	// begin inline asm
+	call (%rd162), _optix_get_matrix_motion_transform_from_handle, (%rd90);
+	// end inline asm
+	// begin inline asm
+	cvta.to.global.u64 %rd164, %rd162;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r123,%r124,%r125,%r126}, [%rd164];
+	// end inline asm
+	add.s64 	%rd168, %rd162, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd167, %rd168;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r127,%r128,%r129,%r130}, [%rd167];
+	// end inline asm
+	add.s64 	%rd171, %rd162, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd170, %rd171;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r131,%r132,%r133,%r134}, [%rd170];
+	// end inline asm
+	add.s64 	%rd174, %rd162, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd173, %rd174;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r135,%r136,%r137,%r138}, [%rd173];
+	// end inline asm
+	add.s64 	%rd177, %rd162, 64;
+	// begin inline asm
+	cvta.to.global.u64 %rd176, %rd177;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r139,%r140,%r141,%r142}, [%rd176];
+	// end inline asm
+	add.s64 	%rd180, %rd162, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd179, %rd180;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r143,%r144,%r145,%r146}, [%rd179];
+	// end inline asm
+	add.s64 	%rd183, %rd162, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd182, %rd183;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r147,%r148,%r149,%r150}, [%rd182];
+	// end inline asm
+	add.s64 	%rd186, %rd162, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd185, %rd186;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r151,%r152,%r153,%r154}, [%rd185];
+	// end inline asm
+	mov.b32 	%f713, %r126;
+	mov.b32 	%f714, %r127;
+	and.b32  	%r167, %r125, 65535;
 	add.s32 	%r168, %r167, -1;
-	cvt.rn.f32.s32	%f705, %r168;
-	sub.f32 	%f706, %f576, %f703;
-	mul.f32 	%f707, %f706, %f705;
-	sub.f32 	%f708, %f704, %f703;
-	div.rn.f32 	%f709, %f707, %f708;
-	min.f32 	%f710, %f705, %f709;
-	mov.f32 	%f711, 0f00000000;
-	max.f32 	%f712, %f711, %f710;
-	cvt.rmi.f32.f32	%f713, %f712;
-	cvt.rzi.s32.f32	%r169, %f713;
-	mul.wide.s32 	%rd201, %r169, 48;
-	add.s64 	%rd193, %rd175, %rd201;
-	// inline asm
-	cvta.to.global.u64 %rd192, %rd193;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r155,%r156,%r157,%r158}, [%rd192];
-	// inline asm
-	mov.b32 	 %f1137, %r155;
-	mov.b32 	 %f1138, %r156;
-	mov.b32 	 %f1139, %r157;
-	mov.b32 	 %f1140, %r158;
-	add.s64 	%rd196, %rd193, 16;
-	// inline asm
-	cvta.to.global.u64 %rd195, %rd196;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r159,%r160,%r161,%r162}, [%rd195];
-	// inline asm
-	mov.b32 	 %f1133, %r159;
-	mov.b32 	 %f1134, %r160;
-	mov.b32 	 %f1135, %r161;
-	mov.b32 	 %f1136, %r162;
-	add.s64 	%rd199, %rd193, 32;
-	// inline asm
+	cvt.rn.f32.s32 	%f715, %r168;
+	sub.f32 	%f716, %f585, %f713;
+	mul.f32 	%f717, %f716, %f715;
+	sub.f32 	%f718, %f714, %f713;
+	div.rn.f32 	%f719, %f717, %f718;
+	min.f32 	%f720, %f715, %f719;
+	mov.f32 	%f721, 0f00000000;
+	max.f32 	%f722, %f721, %f720;
+	cvt.rmi.f32.f32 	%f723, %f722;
+	sub.f32 	%f172, %f722, %f723;
+	cvt.rzi.s32.f32 	%r169, %f723;
+	mul.wide.s32 	%rd197, %r169, 48;
+	add.s64 	%rd189, %rd171, %rd197;
+	// begin inline asm
+	cvta.to.global.u64 %rd188, %rd189;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r155,%r156,%r157,%r158}, [%rd188];
+	// end inline asm
+	mov.b32 	%f1164, %r155;
+	mov.b32 	%f1165, %r156;
+	mov.b32 	%f1166, %r157;
+	mov.b32 	%f1167, %r158;
+	add.s64 	%rd192, %rd189, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd191, %rd192;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r159,%r160,%r161,%r162}, [%rd191];
+	// end inline asm
+	mov.b32 	%f1160, %r159;
+	mov.b32 	%f1161, %r160;
+	mov.b32 	%f1162, %r161;
+	mov.b32 	%f1163, %r162;
+	add.s64 	%rd195, %rd189, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd194, %rd195;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r163,%r164,%r165,%r166}, [%rd194];
+	// end inline asm
+	mov.b32 	%f1156, %r163;
+	mov.b32 	%f1157, %r164;
+	mov.b32 	%f1158, %r165;
+	mov.b32 	%f1159, %r166;
+	setp.leu.f32 	%p21, %f172, 0f00000000;
+	@%p21 bra 	$L__BB4_33;
+
+	cvt.rmi.f32.f32 	%f1093, %f722;
+	cvt.rzi.s32.f32 	%r335, %f1093;
+	cvt.s64.s32 	%rd435, %r335;
+	mov.f32 	%f724, 0f3F800000;
+	sub.f32 	%f725, %f724, %f172;
+	mul.lo.s64 	%rd207, %rd435, 48;
+	add.s64 	%rd208, %rd162, %rd207;
+	add.s64 	%rd199, %rd208, 80;
+	// begin inline asm
 	cvta.to.global.u64 %rd198, %rd199;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r163,%r164,%r165,%r166}, [%rd198];
-	// inline asm
-	sub.f32 	%f171, %f712, %f713;
-	mov.b32 	 %f1129, %r163;
-	mov.b32 	 %f1130, %r164;
-	mov.b32 	 %f1131, %r165;
-	mov.b32 	 %f1132, %r166;
-	setp.leu.f32	%p21, %f171, 0f00000000;
-	@%p21 bra 	BB4_33;
-
-	cvt.rmi.f32.f32	%f1077, %f712;
-	cvt.rzi.s32.f32	%r333, %f1077;
-	cvt.s64.s32	%rd442, %r333;
-	mul.lo.s64 	%rd211, %rd442, 48;
-	add.s64 	%rd212, %rd166, %rd211;
-	add.s64 	%rd203, %rd212, 80;
-	// inline asm
-	cvta.to.global.u64 %rd202, %rd203;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r170,%r171,%r172,%r173}, [%rd202];
-	// inline asm
-	mov.b32 	 %f714, %r170;
-	mov.b32 	 %f715, %r171;
-	mov.b32 	 %f716, %r172;
-	mov.b32 	 %f717, %r173;
-	add.s64 	%rd206, %rd212, 96;
-	// inline asm
-	cvta.to.global.u64 %rd205, %rd206;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r174,%r175,%r176,%r177}, [%rd205];
-	// inline asm
-	mov.b32 	 %f718, %r174;
-	mov.b32 	 %f719, %r175;
-	mov.b32 	 %f720, %r176;
-	mov.b32 	 %f721, %r177;
-	add.s64 	%rd209, %rd212, 112;
-	// inline asm
-	cvta.to.global.u64 %rd208, %rd209;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r178,%r179,%r180,%r181}, [%rd208];
-	// inline asm
-	mov.f32 	%f722, 0f3F800000;
-	sub.f32 	%f723, %f722, %f171;
-	mul.f32 	%f724, %f171, %f714;
-	mul.f32 	%f725, %f171, %f715;
-	mul.f32 	%f726, %f171, %f716;
-	mul.f32 	%f727, %f171, %f717;
-	fma.rn.f32 	%f1137, %f723, %f1137, %f724;
-	fma.rn.f32 	%f1138, %f723, %f1138, %f725;
-	fma.rn.f32 	%f1139, %f723, %f1139, %f726;
-	fma.rn.f32 	%f1140, %f723, %f1140, %f727;
-	mul.f32 	%f728, %f171, %f718;
-	mul.f32 	%f729, %f171, %f719;
-	mul.f32 	%f730, %f171, %f720;
-	mul.f32 	%f731, %f171, %f721;
-	fma.rn.f32 	%f1133, %f723, %f1133, %f728;
-	fma.rn.f32 	%f1134, %f723, %f1134, %f729;
-	fma.rn.f32 	%f1135, %f723, %f1135, %f730;
-	fma.rn.f32 	%f1136, %f723, %f1136, %f731;
-	mov.b32 	 %f732, %r178;
-	mov.b32 	 %f733, %r179;
-	mov.b32 	 %f734, %r180;
-	mov.b32 	 %f735, %r181;
-	mul.f32 	%f736, %f171, %f732;
-	mul.f32 	%f737, %f171, %f733;
-	mul.f32 	%f738, %f171, %f734;
-	mul.f32 	%f739, %f171, %f735;
-	fma.rn.f32 	%f1129, %f723, %f1129, %f736;
-	fma.rn.f32 	%f1130, %f723, %f1130, %f737;
-	fma.rn.f32 	%f1131, %f723, %f1131, %f738;
-	fma.rn.f32 	%f1132, %f723, %f1132, %f739;
-	bra.uni 	BB4_33;
-
-BB4_22:
-	mov.f32 	%f1129, 0f00000000;
-	mov.f32 	%f1131, 0f3F800000;
-	setp.eq.s32	%p17, %r34, 4;
-	@%p17 bra 	BB4_25;
-	bra.uni 	BB4_23;
-
-BB4_25:
-	// inline asm
-	call (%rd448), _optix_get_instance_transform_from_handle, (%rd92);
-	// inline asm
-	bra.uni 	BB4_26;
-
-BB4_28:
-	// inline asm
-	call (%rd107), _optix_get_srt_motion_transform_from_handle, (%rd92);
-	// inline asm
-	// inline asm
-	cvta.to.global.u64 %rd109, %rd107;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r48,%r49,%r50,%r51}, [%rd109];
-	// inline asm
-	mov.b32	{%rs2, %rs3}, %r50;
-	add.s64 	%rd113, %rd107, 16;
-	// inline asm
-	cvta.to.global.u64 %rd112, %rd113;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r52,%r53,%r54,%r55}, [%rd112];
-	// inline asm
-	add.s64 	%rd116, %rd107, 32;
-	// inline asm
-	cvta.to.global.u64 %rd115, %rd116;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r56,%r57,%r58,%r59}, [%rd115];
-	// inline asm
-	add.s64 	%rd119, %rd107, 48;
-	// inline asm
-	cvta.to.global.u64 %rd118, %rd119;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r60,%r61,%r62,%r63}, [%rd118];
-	// inline asm
-	add.s64 	%rd122, %rd107, 64;
-	// inline asm
-	cvta.to.global.u64 %rd121, %rd122;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r64,%r65,%r66,%r67}, [%rd121];
-	// inline asm
-	add.s64 	%rd125, %rd107, 80;
-	// inline asm
-	cvta.to.global.u64 %rd124, %rd125;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r68,%r69,%r70,%r71}, [%rd124];
-	// inline asm
-	add.s64 	%rd128, %rd107, 96;
-	// inline asm
-	cvta.to.global.u64 %rd127, %rd128;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r72,%r73,%r74,%r75}, [%rd127];
-	// inline asm
-	add.s64 	%rd131, %rd107, 112;
-	// inline asm
-	cvta.to.global.u64 %rd130, %rd131;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r76,%r77,%r78,%r79}, [%rd130];
-	// inline asm
-	add.s64 	%rd134, %rd107, 128;
-	// inline asm
-	cvta.to.global.u64 %rd133, %rd134;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r80,%r81,%r82,%r83}, [%rd133];
-	// inline asm
-	add.s64 	%rd137, %rd107, 144;
-	// inline asm
-	cvta.to.global.u64 %rd136, %rd137;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r84,%r85,%r86,%r87}, [%rd136];
-	// inline asm
-	mov.b32 	 %f590, %r51;
-	mov.b32 	 %f591, %r52;
-	cvt.u32.u16	%r104, %rs2;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r170,%r171,%r172,%r173}, [%rd198];
+	// end inline asm
+	mov.b32 	%f726, %r170;
+	mov.b32 	%f727, %r171;
+	mov.b32 	%f728, %r172;
+	mov.b32 	%f729, %r173;
+	mul.f32 	%f730, %f172, %f726;
+	mul.f32 	%f731, %f172, %f727;
+	mul.f32 	%f732, %f172, %f728;
+	mul.f32 	%f733, %f172, %f729;
+	fma.rn.f32 	%f1164, %f725, %f1164, %f730;
+	fma.rn.f32 	%f1165, %f725, %f1165, %f731;
+	fma.rn.f32 	%f1166, %f725, %f1166, %f732;
+	fma.rn.f32 	%f1167, %f725, %f1167, %f733;
+	add.s64 	%rd202, %rd208, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd201, %rd202;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r174,%r175,%r176,%r177}, [%rd201];
+	// end inline asm
+	mov.b32 	%f734, %r174;
+	mov.b32 	%f735, %r175;
+	mov.b32 	%f736, %r176;
+	mov.b32 	%f737, %r177;
+	mul.f32 	%f738, %f172, %f734;
+	mul.f32 	%f739, %f172, %f735;
+	mul.f32 	%f740, %f172, %f736;
+	mul.f32 	%f741, %f172, %f737;
+	fma.rn.f32 	%f1160, %f725, %f1160, %f738;
+	fma.rn.f32 	%f1161, %f725, %f1161, %f739;
+	fma.rn.f32 	%f1162, %f725, %f1162, %f740;
+	fma.rn.f32 	%f1163, %f725, %f1163, %f741;
+	add.s64 	%rd205, %rd208, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd204, %rd205;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r178,%r179,%r180,%r181}, [%rd204];
+	// end inline asm
+	mov.b32 	%f742, %r178;
+	mov.b32 	%f743, %r179;
+	mov.b32 	%f744, %r180;
+	mov.b32 	%f745, %r181;
+	mul.f32 	%f746, %f172, %f742;
+	mul.f32 	%f747, %f172, %f743;
+	mul.f32 	%f748, %f172, %f744;
+	mul.f32 	%f749, %f172, %f745;
+	fma.rn.f32 	%f1156, %f725, %f1156, %f746;
+	fma.rn.f32 	%f1157, %f725, %f1157, %f747;
+	fma.rn.f32 	%f1158, %f725, %f1158, %f748;
+	fma.rn.f32 	%f1159, %f725, %f1159, %f749;
+	bra.uni 	$L__BB4_33;
+
+$L__BB4_22:
+	mov.f32 	%f1156, 0f00000000;
+	mov.f32 	%f1158, 0f3F800000;
+	setp.eq.s32 	%p17, %r34, 4;
+	@%p17 bra 	$L__BB4_25;
+
+	setp.ne.s32 	%p18, %r34, 1;
+	mov.f32 	%f1157, %f1156;
+	mov.f32 	%f1159, %f1156;
+	mov.f32 	%f1160, %f1156;
+	mov.f32 	%f1161, %f1158;
+	mov.f32 	%f1162, %f1156;
+	mov.f32 	%f1163, %f1156;
+	mov.f32 	%f1164, %f1158;
+	mov.f32 	%f1165, %f1156;
+	mov.f32 	%f1166, %f1156;
+	mov.f32 	%f1167, %f1156;
+	@%p18 bra 	$L__BB4_33;
+
+	// begin inline asm
+	call (%rd92), _optix_get_static_transform_from_handle, (%rd90);
+	// end inline asm
+	add.s64 	%rd441, %rd92, 16;
+	bra.uni 	$L__BB4_26;
+
+$L__BB4_28:
+	// begin inline asm
+	call (%rd105), _optix_get_srt_motion_transform_from_handle, (%rd90);
+	// end inline asm
+	// begin inline asm
+	cvta.to.global.u64 %rd107, %rd105;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r48,%r49,%r50,%r51}, [%rd107];
+	// end inline asm
+	add.s64 	%rd111, %rd105, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd110, %rd111;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r52,%r53,%r54,%r55}, [%rd110];
+	// end inline asm
+	add.s64 	%rd114, %rd105, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd113, %rd114;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r56,%r57,%r58,%r59}, [%rd113];
+	// end inline asm
+	add.s64 	%rd117, %rd105, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd116, %rd117;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r60,%r61,%r62,%r63}, [%rd116];
+	// end inline asm
+	add.s64 	%rd120, %rd105, 64;
+	// begin inline asm
+	cvta.to.global.u64 %rd119, %rd120;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r64,%r65,%r66,%r67}, [%rd119];
+	// end inline asm
+	add.s64 	%rd123, %rd105, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd122, %rd123;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r68,%r69,%r70,%r71}, [%rd122];
+	// end inline asm
+	add.s64 	%rd126, %rd105, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd125, %rd126;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r72,%r73,%r74,%r75}, [%rd125];
+	// end inline asm
+	add.s64 	%rd129, %rd105, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd128, %rd129;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r76,%r77,%r78,%r79}, [%rd128];
+	// end inline asm
+	add.s64 	%rd132, %rd105, 128;
+	// begin inline asm
+	cvta.to.global.u64 %rd131, %rd132;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r80,%r81,%r82,%r83}, [%rd131];
+	// end inline asm
+	add.s64 	%rd135, %rd105, 144;
+	// begin inline asm
+	cvta.to.global.u64 %rd134, %rd135;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r84,%r85,%r86,%r87}, [%rd134];
+	// end inline asm
+	mov.b32 	%f600, %r51;
+	mov.b32 	%f601, %r52;
+	and.b32  	%r104, %r50, 65535;
 	add.s32 	%r105, %r104, -1;
-	cvt.rn.f32.s32	%f592, %r105;
-	sub.f32 	%f593, %f576, %f590;
-	mul.f32 	%f594, %f593, %f592;
-	sub.f32 	%f595, %f591, %f590;
-	div.rn.f32 	%f596, %f594, %f595;
-	min.f32 	%f597, %f592, %f596;
-	mov.f32 	%f598, 0f00000000;
-	max.f32 	%f599, %f598, %f597;
-	cvt.rmi.f32.f32	%f600, %f599;
-	cvt.rzi.s32.f32	%r106, %f600;
-	mul.wide.s32 	%rd151, %r106, 64;
-	add.s64 	%rd140, %rd116, %rd151;
-	// inline asm
-	cvta.to.global.u64 %rd139, %rd140;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r88,%r89,%r90,%r91}, [%rd139];
-	// inline asm
-	mov.b32 	 %f1113, %r88;
-	mov.b32 	 %f1114, %r89;
-	mov.b32 	 %f1115, %r90;
-	mov.b32 	 %f1116, %r91;
-	add.s64 	%rd143, %rd140, 16;
-	// inline asm
-	cvta.to.global.u64 %rd142, %rd143;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r92,%r93,%r94,%r95}, [%rd142];
-	// inline asm
-	mov.b32 	 %f1117, %r92;
-	mov.b32 	 %f1118, %r93;
-	mov.b32 	 %f1119, %r94;
-	mov.b32 	 %f1120, %r95;
-	add.s64 	%rd146, %rd140, 32;
-	// inline asm
-	cvta.to.global.u64 %rd145, %rd146;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r96,%r97,%r98,%r99}, [%rd145];
-	// inline asm
-	sub.f32 	%f110, %f599, %f600;
-	mov.b32 	 %f1121, %r96;
-	mov.b32 	 %f1122, %r97;
-	mov.b32 	 %f1123, %r98;
-	mov.b32 	 %f1124, %r99;
-	add.s64 	%rd149, %rd140, 48;
-	// inline asm
-	cvta.to.global.u64 %rd148, %rd149;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r100,%r101,%r102,%r103}, [%rd148];
-	// inline asm
-	mov.b32 	 %f1125, %r100;
-	mov.b32 	 %f1126, %r101;
-	mov.b32 	 %f1127, %r102;
-	mov.b32 	 %f1128, %r103;
-	setp.leu.f32	%p20, %f110, 0f00000000;
-	@%p20 bra 	BB4_30;
-
-	cvt.rmi.f32.f32	%f1076, %f599;
-	cvt.rzi.s32.f32	%r332, %f1076;
-	cvt.s64.s32	%rd440, %r332;
-	shl.b64 	%rd164, %rd440, 6;
-	add.s64 	%rd165, %rd164, %rd107;
-	add.s64 	%rd153, %rd165, 96;
-	// inline asm
-	cvta.to.global.u64 %rd152, %rd153;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r107,%r108,%r109,%r110}, [%rd152];
-	// inline asm
-	mov.b32 	 %f601, %r107;
-	mov.b32 	 %f602, %r108;
-	mov.b32 	 %f603, %r109;
-	mov.b32 	 %f604, %r110;
-	add.s64 	%rd156, %rd165, 112;
-	// inline asm
-	cvta.to.global.u64 %rd155, %rd156;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r111,%r112,%r113,%r114}, [%rd155];
-	// inline asm
-	mov.b32 	 %f605, %r111;
-	mov.b32 	 %f606, %r112;
-	mov.b32 	 %f607, %r113;
-	mov.b32 	 %f608, %r114;
-	add.s64 	%rd159, %rd165, 128;
-	// inline asm
-	cvta.to.global.u64 %rd158, %rd159;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r115,%r116,%r117,%r118}, [%rd158];
-	// inline asm
-	mov.b32 	 %f609, %r115;
-	mov.b32 	 %f610, %r116;
-	mov.b32 	 %f611, %r117;
-	mov.b32 	 %f612, %r118;
-	add.s64 	%rd162, %rd165, 144;
-	// inline asm
-	cvta.to.global.u64 %rd161, %rd162;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r119,%r120,%r121,%r122}, [%rd161];
-	// inline asm
-	mov.f32 	%f613, 0f3F800000;
-	sub.f32 	%f614, %f613, %f110;
-	mul.f32 	%f615, %f110, %f601;
-	mul.f32 	%f616, %f110, %f602;
-	mul.f32 	%f617, %f110, %f603;
-	mul.f32 	%f618, %f110, %f604;
-	fma.rn.f32 	%f1113, %f614, %f1113, %f615;
-	fma.rn.f32 	%f1114, %f614, %f1114, %f616;
-	fma.rn.f32 	%f1115, %f614, %f1115, %f617;
-	fma.rn.f32 	%f1116, %f614, %f1116, %f618;
-	mul.f32 	%f619, %f110, %f605;
-	mul.f32 	%f620, %f110, %f606;
-	mul.f32 	%f621, %f110, %f607;
-	mul.f32 	%f622, %f110, %f608;
-	fma.rn.f32 	%f1117, %f614, %f1117, %f619;
-	fma.rn.f32 	%f1118, %f614, %f1118, %f620;
-	fma.rn.f32 	%f1119, %f614, %f1119, %f621;
-	fma.rn.f32 	%f1120, %f614, %f1120, %f622;
-	mul.f32 	%f623, %f110, %f609;
-	mul.f32 	%f624, %f110, %f610;
-	mul.f32 	%f625, %f110, %f611;
-	mul.f32 	%f626, %f110, %f612;
-	fma.rn.f32 	%f1121, %f614, %f1121, %f623;
-	fma.rn.f32 	%f627, %f614, %f1122, %f624;
-	fma.rn.f32 	%f628, %f614, %f1123, %f625;
-	fma.rn.f32 	%f629, %f614, %f1124, %f626;
-	mov.b32 	 %f630, %r119;
-	mov.b32 	 %f631, %r120;
-	mov.b32 	 %f632, %r121;
-	mov.b32 	 %f633, %r122;
-	mul.f32 	%f634, %f110, %f630;
-	mul.f32 	%f635, %f110, %f631;
-	mul.f32 	%f636, %f110, %f632;
-	mul.f32 	%f637, %f110, %f633;
-	fma.rn.f32 	%f638, %f614, %f1125, %f634;
-	fma.rn.f32 	%f1126, %f614, %f1126, %f635;
-	fma.rn.f32 	%f1127, %f614, %f1127, %f636;
-	fma.rn.f32 	%f1128, %f614, %f1128, %f637;
-	mul.f32 	%f639, %f628, %f628;
-	fma.rn.f32 	%f640, %f627, %f627, %f639;
-	fma.rn.f32 	%f641, %f629, %f629, %f640;
-	fma.rn.f32 	%f642, %f638, %f638, %f641;
-	sqrt.rn.f32 	%f643, %f642;
-	rcp.rn.f32 	%f644, %f643;
-	mul.f32 	%f1122, %f627, %f644;
-	mul.f32 	%f1123, %f628, %f644;
-	mul.f32 	%f1124, %f629, %f644;
-	mul.f32 	%f1125, %f638, %f644;
-
-BB4_30:
-	mul.f32 	%f645, %f1123, %f1123;
-	fma.rn.f32 	%f646, %f1122, %f1122, %f645;
-	fma.rn.f32 	%f647, %f1124, %f1124, %f646;
-	fma.rn.f32 	%f648, %f1125, %f1125, %f647;
-	rcp.rn.f32 	%f649, %f648;
-	mul.f32 	%f650, %f1122, %f649;
-	mul.f32 	%f651, %f1123, %f649;
-	mul.f32 	%f652, %f1124, %f649;
-	mul.f32 	%f653, %f1125, %f649;
-	mul.f32 	%f654, %f1122, %f650;
-	mul.f32 	%f655, %f1123, %f651;
-	mul.f32 	%f656, %f1124, %f652;
-	mul.f32 	%f657, %f1122, %f651;
-	mul.f32 	%f658, %f1124, %f653;
-	mul.f32 	%f659, %f1122, %f652;
-	mul.f32 	%f660, %f1123, %f653;
-	mul.f32 	%f661, %f1123, %f652;
-	mul.f32 	%f662, %f1122, %f653;
-	sub.f32 	%f663, %f654, %f655;
-	sub.f32 	%f664, %f663, %f656;
-	fma.rn.f32 	%f665, %f1125, %f653, %f664;
-	sub.f32 	%f666, %f657, %f658;
-	add.f32 	%f667, %f666, %f666;
-	add.f32 	%f668, %f659, %f660;
-	add.f32 	%f669, %f668, %f668;
-	add.f32 	%f670, %f657, %f658;
-	add.f32 	%f671, %f670, %f670;
-	sub.f32 	%f672, %f655, %f654;
-	sub.f32 	%f673, %f672, %f656;
-	fma.rn.f32 	%f674, %f1125, %f653, %f673;
-	sub.f32 	%f675, %f661, %f662;
-	add.f32 	%f676, %f675, %f675;
-	sub.f32 	%f677, %f659, %f660;
-	add.f32 	%f678, %f677, %f677;
-	add.f32 	%f679, %f661, %f662;
-	add.f32 	%f680, %f679, %f679;
-	neg.f32 	%f681, %f654;
-	sub.f32 	%f682, %f681, %f655;
-	add.f32 	%f683, %f656, %f682;
-	fma.rn.f32 	%f684, %f1125, %f653, %f683;
-	mul.f32 	%f685, %f1116, %f665;
-	fma.rn.f32 	%f686, %f1119, %f667, %f685;
-	fma.rn.f32 	%f687, %f1121, %f669, %f686;
-	sub.f32 	%f1140, %f1126, %f687;
-	mul.f32 	%f688, %f1119, %f674;
-	fma.rn.f32 	%f689, %f1116, %f671, %f688;
-	fma.rn.f32 	%f690, %f1121, %f676, %f689;
-	sub.f32 	%f1136, %f1127, %f690;
-	mul.f32 	%f691, %f1119, %f680;
-	fma.rn.f32 	%f692, %f1116, %f678, %f691;
-	fma.rn.f32 	%f693, %f1121, %f684, %f692;
-	sub.f32 	%f1132, %f1128, %f693;
-	mul.f32 	%f694, %f1115, %f665;
-	fma.rn.f32 	%f695, %f1118, %f667, %f694;
-	fma.rn.f32 	%f1139, %f1120, %f669, %f695;
-	mul.f32 	%f696, %f1118, %f674;
-	fma.rn.f32 	%f697, %f1115, %f671, %f696;
-	fma.rn.f32 	%f1135, %f1120, %f676, %f697;
-	mul.f32 	%f698, %f1118, %f680;
-	fma.rn.f32 	%f699, %f1115, %f678, %f698;
-	fma.rn.f32 	%f1131, %f1120, %f684, %f699;
-	mul.f32 	%f700, %f1114, %f665;
-	fma.rn.f32 	%f1138, %f1117, %f667, %f700;
-	mul.f32 	%f701, %f1117, %f674;
-	fma.rn.f32 	%f1134, %f1114, %f671, %f701;
-	mul.f32 	%f702, %f1117, %f680;
-	fma.rn.f32 	%f1130, %f1114, %f678, %f702;
-	mul.f32 	%f1137, %f1113, %f665;
-	mul.f32 	%f1133, %f1113, %f671;
-	mul.f32 	%f1129, %f1113, %f678;
-	bra.uni 	BB4_33;
-
-BB4_23:
-	setp.ne.s32	%p18, %r34, 1;
-	mov.f32 	%f1130, %f1129;
-	mov.f32 	%f1132, %f1129;
-	mov.f32 	%f1133, %f1129;
-	mov.f32 	%f1134, %f1131;
-	mov.f32 	%f1135, %f1129;
-	mov.f32 	%f1136, %f1129;
-	mov.f32 	%f1137, %f1131;
-	mov.f32 	%f1138, %f1129;
-	mov.f32 	%f1139, %f1129;
-	mov.f32 	%f1140, %f1129;
-	@%p18 bra 	BB4_33;
-
-	// inline asm
-	call (%rd94), _optix_get_static_transform_from_handle, (%rd92);
-	// inline asm
-	add.s64 	%rd448, %rd94, 16;
-
-BB4_26:
-	// inline asm
-	cvta.to.global.u64 %rd98, %rd448;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r36,%r37,%r38,%r39}, [%rd98];
-	// inline asm
-	mov.b32 	 %f1137, %r36;
-	mov.b32 	 %f1138, %r37;
-	mov.b32 	 %f1139, %r38;
-	mov.b32 	 %f1140, %r39;
-	add.s64 	%rd102, %rd448, 16;
-	// inline asm
-	cvta.to.global.u64 %rd101, %rd102;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r40,%r41,%r42,%r43}, [%rd101];
-	// inline asm
-	mov.b32 	 %f1133, %r40;
-	mov.b32 	 %f1134, %r41;
-	mov.b32 	 %f1135, %r42;
-	mov.b32 	 %f1136, %r43;
-	add.s64 	%rd105, %rd448, 32;
-	// inline asm
-	cvta.to.global.u64 %rd104, %rd105;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r44,%r45,%r46,%r47}, [%rd104];
-	// inline asm
-	mov.b32 	 %f1129, %r44;
-	mov.b32 	 %f1130, %r45;
-	mov.b32 	 %f1131, %r46;
-	mov.b32 	 %f1132, %r47;
-
-BB4_33:
-	add.s32 	%r11, %r334, 1;
-	setp.eq.s32	%p22, %r11, %r32;
-	@%p22 bra 	BB4_34;
-	bra.uni 	BB4_35;
-
-BB4_34:
-	mov.f32 	%f1112, %f1138;
-	mov.f32 	%f1111, %f1137;
-	mov.f32 	%f1110, %f1136;
-	mov.f32 	%f1109, %f1135;
-	mov.f32 	%f1108, %f1134;
-	mov.f32 	%f1107, %f1133;
-	mov.f32 	%f1106, %f1132;
-	mov.f32 	%f1105, %f1131;
-	mov.f32 	%f1104, %f1130;
-	mov.f32 	%f1103, %f1129;
-	mov.f32 	%f1102, %f1139;
-	mov.f32 	%f1101, %f1140;
-	bra.uni 	BB4_36;
-
-BB4_35:
-	mul.f32 	%f740, %f1107, %f1138;
-	fma.rn.f32 	%f741, %f1111, %f1137, %f740;
-	fma.rn.f32 	%f200, %f1103, %f1139, %f741;
-	mul.f32 	%f742, %f1108, %f1138;
-	fma.rn.f32 	%f743, %f1112, %f1137, %f742;
-	fma.rn.f32 	%f201, %f1104, %f1139, %f743;
-	mul.f32 	%f744, %f1109, %f1138;
-	fma.rn.f32 	%f745, %f1102, %f1137, %f744;
-	fma.rn.f32 	%f202, %f1105, %f1139, %f745;
-	mul.f32 	%f746, %f1110, %f1138;
-	fma.rn.f32 	%f747, %f1101, %f1137, %f746;
-	fma.rn.f32 	%f748, %f1106, %f1139, %f747;
-	add.f32 	%f203, %f1140, %f748;
-	mul.f32 	%f749, %f1107, %f1134;
-	fma.rn.f32 	%f750, %f1111, %f1133, %f749;
-	fma.rn.f32 	%f204, %f1103, %f1135, %f750;
-	mul.f32 	%f751, %f1108, %f1134;
-	fma.rn.f32 	%f752, %f1112, %f1133, %f751;
-	fma.rn.f32 	%f205, %f1104, %f1135, %f752;
-	mul.f32 	%f753, %f1109, %f1134;
-	fma.rn.f32 	%f754, %f1102, %f1133, %f753;
-	fma.rn.f32 	%f206, %f1105, %f1135, %f754;
-	mul.f32 	%f755, %f1110, %f1134;
-	fma.rn.f32 	%f756, %f1101, %f1133, %f755;
-	fma.rn.f32 	%f757, %f1106, %f1135, %f756;
-	add.f32 	%f207, %f1136, %f757;
-	mul.f32 	%f758, %f1107, %f1130;
-	fma.rn.f32 	%f759, %f1111, %f1129, %f758;
-	fma.rn.f32 	%f1103, %f1103, %f1131, %f759;
-	mul.f32 	%f760, %f1108, %f1130;
-	fma.rn.f32 	%f761, %f1112, %f1129, %f760;
-	fma.rn.f32 	%f1104, %f1104, %f1131, %f761;
-	mul.f32 	%f762, %f1109, %f1130;
-	fma.rn.f32 	%f763, %f1102, %f1129, %f762;
-	fma.rn.f32 	%f1105, %f1105, %f1131, %f763;
-	mul.f32 	%f764, %f1110, %f1130;
-	fma.rn.f32 	%f765, %f1101, %f1129, %f764;
-	fma.rn.f32 	%f766, %f1106, %f1131, %f765;
-	add.f32 	%f1106, %f1132, %f766;
-	mov.f32 	%f1112, %f201;
-	mov.f32 	%f1111, %f200;
-	mov.f32 	%f1110, %f207;
-	mov.f32 	%f1109, %f206;
-	mov.f32 	%f1108, %f205;
-	mov.f32 	%f1107, %f204;
-	mov.f32 	%f1102, %f202;
-	mov.f32 	%f1101, %f203;
-
-BB4_36:
-	add.s32 	%r334, %r11, -2;
-	setp.gt.s32	%p23, %r334, -1;
-	@%p23 bra 	BB4_21;
-
-BB4_37:
-	mov.f32 	%f1166, 0f00000000;
-	setp.eq.s32	%p43, %r32, 0;
-	mov.f32 	%f1167, 0f3F800000;
-	mov.f32 	%f1165, %f1166;
-	mov.f32 	%f1170, %f1166;
-	mov.f32 	%f1169, %f1167;
-	mov.f32 	%f1168, %f1166;
-	mov.f32 	%f1173, %f1166;
-	mov.f32 	%f1172, %f1166;
-	mov.f32 	%f1171, %f1167;
-	@%p43 bra 	BB4_56;
-
-	mov.u32 	%r335, 0;
-	// inline asm
-	call (%f776), _optix_get_ray_time, ();
-	// inline asm
-
-BB4_39:
+	cvt.rn.f32.s32 	%f602, %r105;
+	sub.f32 	%f603, %f585, %f600;
+	mul.f32 	%f604, %f603, %f602;
+	sub.f32 	%f605, %f601, %f600;
+	div.rn.f32 	%f606, %f604, %f605;
+	min.f32 	%f607, %f602, %f606;
+	mov.f32 	%f608, 0f00000000;
+	max.f32 	%f609, %f608, %f607;
+	cvt.rmi.f32.f32 	%f610, %f609;
+	sub.f32 	%f111, %f609, %f610;
+	cvt.rzi.s32.f32 	%r106, %f610;
+	mul.wide.s32 	%rd149, %r106, 64;
+	add.s64 	%rd138, %rd114, %rd149;
+	// begin inline asm
+	cvta.to.global.u64 %rd137, %rd138;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r88,%r89,%r90,%r91}, [%rd137];
+	// end inline asm
+	mov.b32 	%f1140, %r88;
+	mov.b32 	%f1141, %r89;
+	mov.b32 	%f1142, %r90;
+	mov.b32 	%f1143, %r91;
+	add.s64 	%rd141, %rd138, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd140, %rd141;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r92,%r93,%r94,%r95}, [%rd140];
+	// end inline asm
+	mov.b32 	%f1144, %r92;
+	mov.b32 	%f1145, %r93;
+	mov.b32 	%f1146, %r94;
+	mov.b32 	%f1147, %r95;
+	add.s64 	%rd144, %rd138, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd143, %rd144;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r96,%r97,%r98,%r99}, [%rd143];
+	// end inline asm
+	mov.b32 	%f1148, %r96;
+	mov.b32 	%f1149, %r97;
+	mov.b32 	%f1150, %r98;
+	mov.b32 	%f1151, %r99;
+	add.s64 	%rd147, %rd138, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd146, %rd147;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r100,%r101,%r102,%r103}, [%rd146];
+	// end inline asm
+	mov.b32 	%f1152, %r100;
+	mov.b32 	%f1153, %r101;
+	mov.b32 	%f1154, %r102;
+	mov.b32 	%f1155, %r103;
+	setp.leu.f32 	%p20, %f111, 0f00000000;
+	@%p20 bra 	$L__BB4_30;
+
+	mov.f32 	%f611, 0f3F800000;
+	sub.f32 	%f612, %f611, %f111;
+	add.s64 	%rd151, %rd138, 64;
+	// begin inline asm
+	cvta.to.global.u64 %rd150, %rd151;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r107,%r108,%r109,%r110}, [%rd150];
+	// end inline asm
+	mov.b32 	%f613, %r107;
+	mov.b32 	%f614, %r108;
+	mov.b32 	%f615, %r109;
+	mov.b32 	%f616, %r110;
+	mul.f32 	%f617, %f111, %f613;
+	mul.f32 	%f618, %f111, %f614;
+	mul.f32 	%f619, %f111, %f615;
+	mul.f32 	%f620, %f111, %f616;
+	fma.rn.f32 	%f1140, %f612, %f1140, %f617;
+	fma.rn.f32 	%f1141, %f612, %f1141, %f618;
+	fma.rn.f32 	%f1142, %f612, %f1142, %f619;
+	fma.rn.f32 	%f1143, %f612, %f1143, %f620;
+	add.s64 	%rd154, %rd138, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd153, %rd154;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r111,%r112,%r113,%r114}, [%rd153];
+	// end inline asm
+	mov.b32 	%f621, %r111;
+	mov.b32 	%f622, %r112;
+	mov.b32 	%f623, %r113;
+	mov.b32 	%f624, %r114;
+	mul.f32 	%f625, %f111, %f621;
+	mul.f32 	%f626, %f111, %f622;
+	mul.f32 	%f627, %f111, %f623;
+	mul.f32 	%f628, %f111, %f624;
+	fma.rn.f32 	%f1144, %f612, %f1144, %f625;
+	fma.rn.f32 	%f1145, %f612, %f1145, %f626;
+	fma.rn.f32 	%f1146, %f612, %f1146, %f627;
+	fma.rn.f32 	%f1147, %f612, %f1147, %f628;
+	add.s64 	%rd157, %rd138, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd156, %rd157;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r115,%r116,%r117,%r118}, [%rd156];
+	// end inline asm
+	mov.b32 	%f629, %r115;
+	mov.b32 	%f630, %r116;
+	mov.b32 	%f631, %r117;
+	mov.b32 	%f632, %r118;
+	mul.f32 	%f633, %f111, %f629;
+	mul.f32 	%f634, %f111, %f630;
+	mul.f32 	%f635, %f111, %f631;
+	mul.f32 	%f636, %f111, %f632;
+	fma.rn.f32 	%f1148, %f612, %f1148, %f633;
+	fma.rn.f32 	%f637, %f612, %f1149, %f634;
+	fma.rn.f32 	%f638, %f612, %f1150, %f635;
+	fma.rn.f32 	%f639, %f612, %f1151, %f636;
+	add.s64 	%rd160, %rd138, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd159, %rd160;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r119,%r120,%r121,%r122}, [%rd159];
+	// end inline asm
+	mov.b32 	%f640, %r119;
+	mov.b32 	%f641, %r120;
+	mov.b32 	%f642, %r121;
+	mov.b32 	%f643, %r122;
+	mul.f32 	%f644, %f111, %f640;
+	mul.f32 	%f645, %f111, %f641;
+	mul.f32 	%f646, %f111, %f642;
+	mul.f32 	%f647, %f111, %f643;
+	fma.rn.f32 	%f648, %f612, %f1152, %f644;
+	fma.rn.f32 	%f1153, %f612, %f1153, %f645;
+	fma.rn.f32 	%f1154, %f612, %f1154, %f646;
+	fma.rn.f32 	%f1155, %f612, %f1155, %f647;
+	mul.f32 	%f649, %f638, %f638;
+	fma.rn.f32 	%f650, %f637, %f637, %f649;
+	fma.rn.f32 	%f651, %f639, %f639, %f650;
+	fma.rn.f32 	%f652, %f648, %f648, %f651;
+	sqrt.rn.f32 	%f653, %f652;
+	rcp.rn.f32 	%f654, %f653;
+	mul.f32 	%f1149, %f637, %f654;
+	mul.f32 	%f1150, %f638, %f654;
+	mul.f32 	%f1151, %f639, %f654;
+	mul.f32 	%f1152, %f654, %f648;
+
+$L__BB4_30:
+	mul.f32 	%f655, %f1150, %f1150;
+	fma.rn.f32 	%f656, %f1149, %f1149, %f655;
+	fma.rn.f32 	%f657, %f1151, %f1151, %f656;
+	fma.rn.f32 	%f658, %f1152, %f1152, %f657;
+	rcp.rn.f32 	%f659, %f658;
+	mul.f32 	%f660, %f1149, %f659;
+	mul.f32 	%f661, %f1150, %f659;
+	mul.f32 	%f662, %f1151, %f659;
+	mul.f32 	%f663, %f1152, %f659;
+	mul.f32 	%f664, %f1149, %f660;
+	mul.f32 	%f665, %f1150, %f661;
+	mul.f32 	%f666, %f1151, %f662;
+	mul.f32 	%f667, %f1149, %f661;
+	mul.f32 	%f668, %f1151, %f663;
+	mul.f32 	%f669, %f1149, %f662;
+	mul.f32 	%f670, %f1150, %f663;
+	mul.f32 	%f671, %f1150, %f662;
+	mul.f32 	%f672, %f1149, %f663;
+	sub.f32 	%f673, %f664, %f665;
+	sub.f32 	%f674, %f673, %f666;
+	fma.rn.f32 	%f675, %f1152, %f663, %f674;
+	sub.f32 	%f676, %f667, %f668;
+	add.f32 	%f677, %f676, %f676;
+	add.f32 	%f678, %f669, %f670;
+	add.f32 	%f679, %f678, %f678;
+	add.f32 	%f680, %f667, %f668;
+	add.f32 	%f681, %f680, %f680;
+	sub.f32 	%f682, %f665, %f664;
+	sub.f32 	%f683, %f682, %f666;
+	fma.rn.f32 	%f684, %f1152, %f663, %f683;
+	sub.f32 	%f685, %f671, %f672;
+	add.f32 	%f686, %f685, %f685;
+	sub.f32 	%f687, %f669, %f670;
+	add.f32 	%f688, %f687, %f687;
+	add.f32 	%f689, %f671, %f672;
+	add.f32 	%f690, %f689, %f689;
+	neg.f32 	%f691, %f664;
+	sub.f32 	%f692, %f691, %f665;
+	add.f32 	%f693, %f666, %f692;
+	fma.rn.f32 	%f694, %f1152, %f663, %f693;
+	mul.f32 	%f695, %f1143, %f675;
+	fma.rn.f32 	%f696, %f1146, %f677, %f695;
+	fma.rn.f32 	%f697, %f1148, %f679, %f696;
+	sub.f32 	%f1167, %f1153, %f697;
+	mul.f32 	%f698, %f1146, %f684;
+	fma.rn.f32 	%f699, %f1143, %f681, %f698;
+	fma.rn.f32 	%f700, %f1148, %f686, %f699;
+	sub.f32 	%f1163, %f1154, %f700;
+	mul.f32 	%f701, %f1146, %f690;
+	fma.rn.f32 	%f702, %f1143, %f688, %f701;
+	fma.rn.f32 	%f703, %f1148, %f694, %f702;
+	sub.f32 	%f1159, %f1155, %f703;
+	mul.f32 	%f704, %f1142, %f675;
+	fma.rn.f32 	%f705, %f1145, %f677, %f704;
+	fma.rn.f32 	%f1166, %f1147, %f679, %f705;
+	mul.f32 	%f706, %f1145, %f684;
+	fma.rn.f32 	%f707, %f1142, %f681, %f706;
+	fma.rn.f32 	%f1162, %f1147, %f686, %f707;
+	mul.f32 	%f708, %f1145, %f690;
+	fma.rn.f32 	%f709, %f1142, %f688, %f708;
+	fma.rn.f32 	%f1158, %f1147, %f694, %f709;
+	mul.f32 	%f710, %f1141, %f675;
+	fma.rn.f32 	%f1165, %f1144, %f677, %f710;
+	mul.f32 	%f711, %f1144, %f684;
+	fma.rn.f32 	%f1161, %f1141, %f681, %f711;
+	mul.f32 	%f712, %f1144, %f690;
+	fma.rn.f32 	%f1157, %f1141, %f688, %f712;
+	mul.f32 	%f1164, %f1140, %f675;
+	mul.f32 	%f1160, %f1140, %f681;
+	mul.f32 	%f1156, %f1140, %f688;
+	bra.uni 	$L__BB4_33;
+
+$L__BB4_25:
+	// begin inline asm
+	call (%rd441), _optix_get_instance_transform_from_handle, (%rd90);
+	// end inline asm
+
+$L__BB4_26:
+	// begin inline asm
+	cvta.to.global.u64 %rd96, %rd441;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r36,%r37,%r38,%r39}, [%rd96];
+	// end inline asm
+	mov.b32 	%f1164, %r36;
+	mov.b32 	%f1165, %r37;
+	mov.b32 	%f1166, %r38;
+	mov.b32 	%f1167, %r39;
+	add.s64 	%rd100, %rd441, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd99, %rd100;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r40,%r41,%r42,%r43}, [%rd99];
+	// end inline asm
+	mov.b32 	%f1160, %r40;
+	mov.b32 	%f1161, %r41;
+	mov.b32 	%f1162, %r42;
+	mov.b32 	%f1163, %r43;
+	add.s64 	%rd103, %rd441, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd102, %rd103;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r44,%r45,%r46,%r47}, [%rd102];
+	// end inline asm
+	mov.b32 	%f1156, %r44;
+	mov.b32 	%f1157, %r45;
+	mov.b32 	%f1158, %r46;
+	mov.b32 	%f1159, %r47;
+
+$L__BB4_33:
+	setp.eq.s32 	%p22, %r336, %r32;
+	@%p22 bra 	$L__BB4_35;
+
+	mul.f32 	%f750, %f1132, %f1165;
+	fma.rn.f32 	%f751, %f1136, %f1164, %f750;
+	fma.rn.f32 	%f209, %f1128, %f1166, %f751;
+	mul.f32 	%f752, %f1133, %f1165;
+	fma.rn.f32 	%f753, %f1137, %f1164, %f752;
+	fma.rn.f32 	%f210, %f1129, %f1166, %f753;
+	mul.f32 	%f754, %f1134, %f1165;
+	fma.rn.f32 	%f755, %f1138, %f1164, %f754;
+	fma.rn.f32 	%f211, %f1130, %f1166, %f755;
+	mul.f32 	%f756, %f1135, %f1165;
+	fma.rn.f32 	%f757, %f1139, %f1164, %f756;
+	fma.rn.f32 	%f758, %f1131, %f1166, %f757;
+	add.f32 	%f1167, %f1167, %f758;
+	mul.f32 	%f759, %f1132, %f1161;
+	fma.rn.f32 	%f760, %f1136, %f1160, %f759;
+	fma.rn.f32 	%f213, %f1128, %f1162, %f760;
+	mul.f32 	%f761, %f1133, %f1161;
+	fma.rn.f32 	%f762, %f1137, %f1160, %f761;
+	fma.rn.f32 	%f214, %f1129, %f1162, %f762;
+	mul.f32 	%f763, %f1134, %f1161;
+	fma.rn.f32 	%f764, %f1138, %f1160, %f763;
+	fma.rn.f32 	%f215, %f1130, %f1162, %f764;
+	mul.f32 	%f765, %f1135, %f1161;
+	fma.rn.f32 	%f766, %f1139, %f1160, %f765;
+	fma.rn.f32 	%f767, %f1131, %f1162, %f766;
+	add.f32 	%f1163, %f1163, %f767;
+	mul.f32 	%f768, %f1132, %f1157;
+	fma.rn.f32 	%f769, %f1136, %f1156, %f768;
+	fma.rn.f32 	%f217, %f1128, %f1158, %f769;
+	mul.f32 	%f770, %f1133, %f1157;
+	fma.rn.f32 	%f771, %f1137, %f1156, %f770;
+	fma.rn.f32 	%f218, %f1129, %f1158, %f771;
+	mul.f32 	%f772, %f1134, %f1157;
+	fma.rn.f32 	%f773, %f1138, %f1156, %f772;
+	fma.rn.f32 	%f219, %f1130, %f1158, %f773;
+	mul.f32 	%f774, %f1135, %f1157;
+	fma.rn.f32 	%f775, %f1139, %f1156, %f774;
+	fma.rn.f32 	%f776, %f1131, %f1158, %f775;
+	add.f32 	%f1159, %f1159, %f776;
+	mov.f32 	%f1166, %f211;
+	mov.f32 	%f1165, %f210;
+	mov.f32 	%f1164, %f209;
+	mov.f32 	%f1162, %f215;
+	mov.f32 	%f1161, %f214;
+	mov.f32 	%f1160, %f213;
+	mov.f32 	%f1158, %f219;
+	mov.f32 	%f1157, %f218;
+	mov.f32 	%f1156, %f217;
+
+$L__BB4_35:
+	add.s32 	%r334, %r336, -1;
+	setp.gt.s32 	%p23, %r336, 1;
+	mov.u32 	%r336, %r334;
+	mov.f32 	%f1128, %f1156;
+	mov.f32 	%f1129, %f1157;
+	mov.f32 	%f1130, %f1158;
+	mov.f32 	%f1131, %f1159;
+	mov.f32 	%f1132, %f1160;
+	mov.f32 	%f1133, %f1161;
+	mov.f32 	%f1134, %f1162;
+	mov.f32 	%f1135, %f1163;
+	mov.f32 	%f1136, %f1164;
+	mov.f32 	%f1137, %f1165;
+	mov.f32 	%f1138, %f1166;
+	mov.f32 	%f1139, %f1167;
+	@%p23 bra 	$L__BB4_21;
+
+$L__BB4_36:
+	mov.f32 	%f1227, 0f00000000;
+	mov.f32 	%f1226, 0f3F800000;
+	// begin inline asm
+	call (%r182), _optix_get_transform_list_size, ();
+	// end inline asm
+	setp.eq.s32 	%p24, %r182, 0;
+	mov.f32 	%f1228, %f1227;
+	mov.f32 	%f1223, %f1227;
+	mov.f32 	%f1224, %f1226;
+	mov.f32 	%f1225, %f1227;
+	mov.f32 	%f1220, %f1227;
+	mov.f32 	%f1221, %f1227;
+	mov.f32 	%f1222, %f1226;
+	@%p24 bra 	$L__BB4_55;
+
+	// begin inline asm
+	call (%r183), _optix_get_transform_list_size, ();
+	// end inline asm
+	// begin inline asm
+	call (%f786), _optix_get_ray_time, ();
+	// end inline asm
+	setp.eq.s32 	%p25, %r183, 0;
+	@%p25 bra 	$L__BB4_55;
+
+	mov.u32 	%r337, 0;
+
+$L__BB4_39:
 	.pragma "nounroll";
-	// inline asm
-	call (%rd213), _optix_get_transform_list_handle, (%r335);
-	// inline asm
-	// inline asm
-	call (%r184), _optix_get_transform_type_from_handle, (%rd213);
-	// inline asm
-	and.b32  	%r185, %r184, -2;
-	setp.eq.s32	%p25, %r185, 2;
-	@%p25 bra 	BB4_45;
-	bra.uni 	BB4_40;
-
-BB4_45:
-	setp.eq.s32	%p28, %r184, 2;
-	@%p28 bra 	BB4_49;
-	bra.uni 	BB4_46;
-
-BB4_49:
-	// inline asm
-	call (%rd287), _optix_get_matrix_motion_transform_from_handle, (%rd213);
-	// inline asm
-	// inline asm
-	cvta.to.global.u64 %rd289, %rd287;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r273,%r274,%r275,%r276}, [%rd289];
-	// inline asm
-	mov.b32	{%rs8, %rs9}, %r275;
-	add.s64 	%rd293, %rd287, 16;
-	// inline asm
+	// begin inline asm
+	call (%rd209), _optix_get_transform_list_handle, (%r337);
+	// end inline asm
+	// begin inline asm
+	call (%r186), _optix_get_transform_type_from_handle, (%rd209);
+	// end inline asm
+	or.b32  	%r187, %r186, 1;
+	setp.eq.s32 	%p26, %r187, 3;
+	@%p26 bra 	$L__BB4_45;
+	bra.uni 	$L__BB4_40;
+
+$L__BB4_45:
+	setp.eq.s32 	%p29, %r186, 2;
+	@%p29 bra 	$L__BB4_49;
+	bra.uni 	$L__BB4_46;
+
+$L__BB4_49:
+	// begin inline asm
+	call (%rd281), _optix_get_matrix_motion_transform_from_handle, (%rd209);
+	// end inline asm
+	// begin inline asm
+	cvta.to.global.u64 %rd283, %rd281;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r275,%r276,%r277,%r278}, [%rd283];
+	// end inline asm
+	add.s64 	%rd287, %rd281, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd286, %rd287;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r279,%r280,%r281,%r282}, [%rd286];
+	// end inline asm
+	add.s64 	%rd290, %rd281, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd289, %rd290;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r283,%r284,%r285,%r286}, [%rd289];
+	// end inline asm
+	add.s64 	%rd293, %rd281, 48;
+	// begin inline asm
 	cvta.to.global.u64 %rd292, %rd293;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r277,%r278,%r279,%r280}, [%rd292];
-	// inline asm
-	add.s64 	%rd296, %rd287, 32;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r287,%r288,%r289,%r290}, [%rd292];
+	// end inline asm
+	add.s64 	%rd296, %rd281, 64;
+	// begin inline asm
 	cvta.to.global.u64 %rd295, %rd296;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r281,%r282,%r283,%r284}, [%rd295];
-	// inline asm
-	add.s64 	%rd299, %rd287, 48;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r291,%r292,%r293,%r294}, [%rd295];
+	// end inline asm
+	add.s64 	%rd299, %rd281, 80;
+	// begin inline asm
 	cvta.to.global.u64 %rd298, %rd299;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r285,%r286,%r287,%r288}, [%rd298];
-	// inline asm
-	add.s64 	%rd302, %rd287, 64;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r295,%r296,%r297,%r298}, [%rd298];
+	// end inline asm
+	add.s64 	%rd302, %rd281, 96;
+	// begin inline asm
 	cvta.to.global.u64 %rd301, %rd302;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r289,%r290,%r291,%r292}, [%rd301];
-	// inline asm
-	add.s64 	%rd305, %rd287, 80;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r299,%r300,%r301,%r302}, [%rd301];
+	// end inline asm
+	add.s64 	%rd305, %rd281, 112;
+	// begin inline asm
 	cvta.to.global.u64 %rd304, %rd305;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r293,%r294,%r295,%r296}, [%rd304];
-	// inline asm
-	add.s64 	%rd308, %rd287, 96;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r303,%r304,%r305,%r306}, [%rd304];
+	// end inline asm
+	mov.b32 	%f890, %r278;
+	mov.b32 	%f891, %r279;
+	and.b32  	%r319, %r277, 65535;
+	add.s32 	%r320, %r319, -1;
+	cvt.rn.f32.s32 	%f892, %r320;
+	sub.f32 	%f893, %f786, %f890;
+	mul.f32 	%f894, %f893, %f892;
+	sub.f32 	%f895, %f891, %f890;
+	div.rn.f32 	%f896, %f894, %f895;
+	min.f32 	%f897, %f892, %f896;
+	mov.f32 	%f898, 0f00000000;
+	max.f32 	%f899, %f898, %f897;
+	cvt.rmi.f32.f32 	%f900, %f899;
+	sub.f32 	%f304, %f899, %f900;
+	cvt.rzi.s32.f32 	%r321, %f900;
+	cvt.s64.s32 	%rd34, %r321;
+	mul.wide.s32 	%rd316, %r321, 48;
+	add.s64 	%rd308, %rd290, %rd316;
+	// begin inline asm
 	cvta.to.global.u64 %rd307, %rd308;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r297,%r298,%r299,%r300}, [%rd307];
-	// inline asm
-	add.s64 	%rd311, %rd287, 112;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r307,%r308,%r309,%r310}, [%rd307];
+	// end inline asm
+	mov.b32 	%f1217, %r307;
+	mov.b32 	%f1218, %r308;
+	mov.b32 	%f1219, %r309;
+	add.s64 	%rd311, %rd308, 16;
+	// begin inline asm
 	cvta.to.global.u64 %rd310, %rd311;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r301,%r302,%r303,%r304}, [%rd310];
-	// inline asm
-	mov.b32 	 %f879, %r276;
-	mov.b32 	 %f880, %r277;
-	cvt.u32.u16	%r317, %rs8;
-	add.s32 	%r318, %r317, -1;
-	cvt.rn.f32.s32	%f881, %r318;
-	sub.f32 	%f882, %f776, %f879;
-	mul.f32 	%f883, %f882, %f881;
-	sub.f32 	%f884, %f880, %f879;
-	div.rn.f32 	%f885, %f883, %f884;
-	min.f32 	%f886, %f881, %f885;
-	mov.f32 	%f887, 0f00000000;
-	max.f32 	%f888, %f887, %f886;
-	cvt.rmi.f32.f32	%f889, %f888;
-	cvt.rzi.s32.f32	%r319, %f889;
-	cvt.s64.s32	%rd35, %r319;
-	mul.wide.s32 	%rd322, %r319, 48;
-	add.s64 	%rd314, %rd296, %rd322;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r311,%r312,%r313,%r314}, [%rd310];
+	// end inline asm
+	mov.b32 	%f1214, %r311;
+	mov.b32 	%f1215, %r312;
+	mov.b32 	%f1216, %r313;
+	add.s64 	%rd314, %rd308, 32;
+	// begin inline asm
 	cvta.to.global.u64 %rd313, %rd314;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r305,%r306,%r307,%r308}, [%rd313];
-	// inline asm
-	mov.b32 	 %f1190, %r305;
-	mov.b32 	 %f1191, %r306;
-	mov.b32 	 %f1192, %r307;
-	add.s64 	%rd317, %rd314, 16;
-	// inline asm
-	cvta.to.global.u64 %rd316, %rd317;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r309,%r310,%r311,%r312}, [%rd316];
-	// inline asm
-	mov.b32 	 %f1187, %r309;
-	mov.b32 	 %f1188, %r310;
-	mov.b32 	 %f1189, %r311;
-	add.s64 	%rd320, %rd314, 32;
-	// inline asm
-	cvta.to.global.u64 %rd319, %rd320;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r313,%r314,%r315,%r316}, [%rd319];
-	// inline asm
-	sub.f32 	%f301, %f888, %f889;
-	mov.b32 	 %f1184, %r313;
-	mov.b32 	 %f1185, %r314;
-	mov.b32 	 %f1186, %r315;
-	setp.leu.f32	%p30, %f301, 0f00000000;
-	@%p30 bra 	BB4_51;
-
-	mul.lo.s64 	%rd332, %rd35, 48;
-	add.s64 	%rd333, %rd287, %rd332;
-	add.s64 	%rd324, %rd333, 80;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r315,%r316,%r317,%r318}, [%rd313];
+	// end inline asm
+	mov.b32 	%f1211, %r315;
+	mov.b32 	%f1212, %r316;
+	mov.b32 	%f1213, %r317;
+	setp.leu.f32 	%p31, %f304, 0f00000000;
+	@%p31 bra 	$L__BB4_51;
+
+	mov.f32 	%f901, 0f3F800000;
+	sub.f32 	%f902, %f901, %f304;
+	mul.lo.s64 	%rd326, %rd34, 48;
+	add.s64 	%rd327, %rd281, %rd326;
+	add.s64 	%rd318, %rd327, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd317, %rd318;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r322,%r323,%r324,%r325}, [%rd317];
+	// end inline asm
+	mov.b32 	%f903, %r322;
+	mov.b32 	%f904, %r323;
+	mov.b32 	%f905, %r324;
+	mul.f32 	%f906, %f304, %f903;
+	mul.f32 	%f907, %f304, %f904;
+	mul.f32 	%f908, %f304, %f905;
+	fma.rn.f32 	%f1217, %f902, %f1217, %f906;
+	fma.rn.f32 	%f1218, %f902, %f1218, %f907;
+	fma.rn.f32 	%f1219, %f902, %f1219, %f908;
+	add.s64 	%rd321, %rd327, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd320, %rd321;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r326,%r327,%r328,%r329}, [%rd320];
+	// end inline asm
+	mov.b32 	%f909, %r326;
+	mov.b32 	%f910, %r327;
+	mov.b32 	%f911, %r328;
+	mul.f32 	%f912, %f304, %f909;
+	mul.f32 	%f913, %f304, %f910;
+	mul.f32 	%f914, %f304, %f911;
+	fma.rn.f32 	%f1214, %f902, %f1214, %f912;
+	fma.rn.f32 	%f1215, %f902, %f1215, %f913;
+	fma.rn.f32 	%f1216, %f902, %f1216, %f914;
+	add.s64 	%rd324, %rd327, 112;
+	// begin inline asm
 	cvta.to.global.u64 %rd323, %rd324;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r320,%r321,%r322,%r323}, [%rd323];
-	// inline asm
-	mov.b32 	 %f890, %r320;
-	mov.b32 	 %f891, %r321;
-	mov.b32 	 %f892, %r322;
-	add.s64 	%rd327, %rd333, 96;
-	// inline asm
-	cvta.to.global.u64 %rd326, %rd327;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r324,%r325,%r326,%r327}, [%rd326];
-	// inline asm
-	mov.b32 	 %f893, %r324;
-	mov.b32 	 %f894, %r325;
-	mov.b32 	 %f895, %r326;
-	add.s64 	%rd330, %rd333, 112;
-	// inline asm
-	cvta.to.global.u64 %rd329, %rd330;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r328,%r329,%r330,%r331}, [%rd329];
-	// inline asm
-	mov.f32 	%f896, 0f3F800000;
-	sub.f32 	%f897, %f896, %f301;
-	mul.f32 	%f898, %f301, %f890;
-	mul.f32 	%f899, %f301, %f891;
-	mul.f32 	%f900, %f301, %f892;
-	fma.rn.f32 	%f1190, %f897, %f1190, %f898;
-	fma.rn.f32 	%f1191, %f897, %f1191, %f899;
-	fma.rn.f32 	%f1192, %f897, %f1192, %f900;
-	mul.f32 	%f901, %f301, %f893;
-	mul.f32 	%f902, %f301, %f894;
-	mul.f32 	%f903, %f301, %f895;
-	fma.rn.f32 	%f1187, %f897, %f1187, %f901;
-	fma.rn.f32 	%f1188, %f897, %f1188, %f902;
-	fma.rn.f32 	%f1189, %f897, %f1189, %f903;
-	mov.b32 	 %f904, %r328;
-	mov.b32 	 %f905, %r329;
-	mov.b32 	 %f906, %r330;
-	mul.f32 	%f907, %f301, %f904;
-	mul.f32 	%f908, %f301, %f905;
-	mul.f32 	%f909, %f301, %f906;
-	fma.rn.f32 	%f1184, %f897, %f1184, %f907;
-	fma.rn.f32 	%f1185, %f897, %f1185, %f908;
-	fma.rn.f32 	%f1186, %f897, %f1186, %f909;
-	bra.uni 	BB4_51;
-
-BB4_40:
-	mov.f32 	%f1193, 0f00000000;
-	mov.f32 	%f1195, 0f3F800000;
-	setp.eq.s32	%p26, %r184, 4;
-	@%p26 bra 	BB4_43;
-	bra.uni 	BB4_41;
-
-BB4_43:
-	// inline asm
-	call (%rd449), _optix_get_instance_inverse_transform_from_handle, (%rd213);
-	// inline asm
-	bra.uni 	BB4_44;
-
-BB4_46:
-	// inline asm
-	call (%rd228), _optix_get_srt_motion_transform_from_handle, (%rd213);
-	// inline asm
-	// inline asm
-	cvta.to.global.u64 %rd230, %rd228;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r198,%r199,%r200,%r201}, [%rd230];
-	// inline asm
-	mov.b32	{%rs6, %rs7}, %r200;
-	add.s64 	%rd234, %rd228, 16;
-	// inline asm
-	cvta.to.global.u64 %rd233, %rd234;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r202,%r203,%r204,%r205}, [%rd233];
-	// inline asm
-	add.s64 	%rd237, %rd228, 32;
-	// inline asm
-	cvta.to.global.u64 %rd236, %rd237;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r206,%r207,%r208,%r209}, [%rd236];
-	// inline asm
-	add.s64 	%rd240, %rd228, 48;
-	// inline asm
-	cvta.to.global.u64 %rd239, %rd240;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r210,%r211,%r212,%r213}, [%rd239];
-	// inline asm
-	add.s64 	%rd243, %rd228, 64;
-	// inline asm
-	cvta.to.global.u64 %rd242, %rd243;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r214,%r215,%r216,%r217}, [%rd242];
-	// inline asm
-	add.s64 	%rd246, %rd228, 80;
-	// inline asm
-	cvta.to.global.u64 %rd245, %rd246;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r218,%r219,%r220,%r221}, [%rd245];
-	// inline asm
-	add.s64 	%rd249, %rd228, 96;
-	// inline asm
-	cvta.to.global.u64 %rd248, %rd249;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r222,%r223,%r224,%r225}, [%rd248];
-	// inline asm
-	add.s64 	%rd252, %rd228, 112;
-	// inline asm
-	cvta.to.global.u64 %rd251, %rd252;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r226,%r227,%r228,%r229}, [%rd251];
-	// inline asm
-	add.s64 	%rd255, %rd228, 128;
-	// inline asm
-	cvta.to.global.u64 %rd254, %rd255;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r230,%r231,%r232,%r233}, [%rd254];
-	// inline asm
-	add.s64 	%rd258, %rd228, 144;
-	// inline asm
-	cvta.to.global.u64 %rd257, %rd258;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r234,%r235,%r236,%r237}, [%rd257];
-	// inline asm
-	mov.b32 	 %f787, %r201;
-	mov.b32 	 %f788, %r202;
-	cvt.u32.u16	%r254, %rs6;
-	add.s32 	%r255, %r254, -1;
-	cvt.rn.f32.s32	%f789, %r255;
-	sub.f32 	%f790, %f776, %f787;
-	mul.f32 	%f791, %f790, %f789;
-	sub.f32 	%f792, %f788, %f787;
-	div.rn.f32 	%f793, %f791, %f792;
-	min.f32 	%f794, %f789, %f793;
-	mov.f32 	%f795, 0f00000000;
-	max.f32 	%f796, %f795, %f794;
-	cvt.rmi.f32.f32	%f797, %f796;
-	cvt.rzi.s32.f32	%r256, %f797;
-	cvt.s64.s32	%rd33, %r256;
-	mul.wide.s32 	%rd272, %r256, 64;
-	add.s64 	%rd261, %rd237, %rd272;
-	// inline asm
-	cvta.to.global.u64 %rd260, %rd261;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r238,%r239,%r240,%r241}, [%rd260];
-	// inline asm
-	mov.b32 	 %f1174, %r238;
-	mov.b32 	 %f1175, %r239;
-	mov.b32 	 %f1176, %r240;
-	add.s64 	%rd264, %rd261, 16;
-	// inline asm
-	cvta.to.global.u64 %rd263, %rd264;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r242,%r243,%r244,%r245}, [%rd263];
-	// inline asm
-	mov.b32 	 %f1177, %r242;
-	mov.b32 	 %f1178, %r243;
-	mov.b32 	 %f1179, %r245;
-	add.s64 	%rd267, %rd261, 32;
-	// inline asm
-	cvta.to.global.u64 %rd266, %rd267;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r246,%r247,%r248,%r249}, [%rd266];
-	// inline asm
-	sub.f32 	%f261, %f796, %f797;
-	mov.b32 	 %f1180, %r247;
-	mov.b32 	 %f1181, %r248;
-	mov.b32 	 %f1182, %r249;
-	add.s64 	%rd270, %rd261, 48;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r330,%r331,%r332,%r333}, [%rd323];
+	// end inline asm
+	mov.b32 	%f915, %r330;
+	mov.b32 	%f916, %r331;
+	mov.b32 	%f917, %r332;
+	mul.f32 	%f918, %f304, %f915;
+	mul.f32 	%f919, %f304, %f916;
+	mul.f32 	%f920, %f304, %f917;
+	fma.rn.f32 	%f1211, %f902, %f1211, %f918;
+	fma.rn.f32 	%f1212, %f902, %f1212, %f919;
+	fma.rn.f32 	%f1213, %f902, %f1213, %f920;
+	bra.uni 	$L__BB4_51;
+
+$L__BB4_40:
+	mov.f32 	%f1220, 0f00000000;
+	mov.f32 	%f1222, 0f3F800000;
+	setp.eq.s32 	%p27, %r186, 4;
+	@%p27 bra 	$L__BB4_43;
+
+	setp.ne.s32 	%p28, %r186, 1;
+	mov.f32 	%f1221, %f1220;
+	mov.f32 	%f1223, %f1220;
+	mov.f32 	%f1224, %f1222;
+	mov.f32 	%f1225, %f1220;
+	mov.f32 	%f1226, %f1222;
+	mov.f32 	%f1227, %f1220;
+	mov.f32 	%f1228, %f1220;
+	@%p28 bra 	$L__BB4_52;
+
+	// begin inline asm
+	call (%rd211), _optix_get_static_transform_from_handle, (%rd209);
+	// end inline asm
+	add.s64 	%rd442, %rd211, 64;
+	bra.uni 	$L__BB4_44;
+
+$L__BB4_46:
+	// begin inline asm
+	call (%rd224), _optix_get_srt_motion_transform_from_handle, (%rd209);
+	// end inline asm
+	// begin inline asm
+	cvta.to.global.u64 %rd226, %rd224;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r200,%r201,%r202,%r203}, [%rd226];
+	// end inline asm
+	add.s64 	%rd230, %rd224, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd229, %rd230;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r204,%r205,%r206,%r207}, [%rd229];
+	// end inline asm
+	add.s64 	%rd233, %rd224, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd232, %rd233;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r208,%r209,%r210,%r211}, [%rd232];
+	// end inline asm
+	add.s64 	%rd236, %rd224, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd235, %rd236;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r212,%r213,%r214,%r215}, [%rd235];
+	// end inline asm
+	add.s64 	%rd239, %rd224, 64;
+	// begin inline asm
+	cvta.to.global.u64 %rd238, %rd239;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r216,%r217,%r218,%r219}, [%rd238];
+	// end inline asm
+	add.s64 	%rd242, %rd224, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd241, %rd242;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r220,%r221,%r222,%r223}, [%rd241];
+	// end inline asm
+	add.s64 	%rd245, %rd224, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd244, %rd245;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r224,%r225,%r226,%r227}, [%rd244];
+	// end inline asm
+	add.s64 	%rd248, %rd224, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd247, %rd248;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r228,%r229,%r230,%r231}, [%rd247];
+	// end inline asm
+	add.s64 	%rd251, %rd224, 128;
+	// begin inline asm
+	cvta.to.global.u64 %rd250, %rd251;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r232,%r233,%r234,%r235}, [%rd250];
+	// end inline asm
+	add.s64 	%rd254, %rd224, 144;
+	// begin inline asm
+	cvta.to.global.u64 %rd253, %rd254;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r236,%r237,%r238,%r239}, [%rd253];
+	// end inline asm
+	mov.b32 	%f798, %r203;
+	mov.b32 	%f799, %r204;
+	and.b32  	%r256, %r202, 65535;
+	add.s32 	%r257, %r256, -1;
+	cvt.rn.f32.s32 	%f800, %r257;
+	sub.f32 	%f801, %f786, %f798;
+	mul.f32 	%f802, %f801, %f800;
+	sub.f32 	%f803, %f799, %f798;
+	div.rn.f32 	%f804, %f802, %f803;
+	min.f32 	%f805, %f800, %f804;
+	mov.f32 	%f806, 0f00000000;
+	max.f32 	%f807, %f806, %f805;
+	cvt.rmi.f32.f32 	%f808, %f807;
+	sub.f32 	%f264, %f807, %f808;
+	cvt.rzi.s32.f32 	%r258, %f808;
+	mul.wide.s32 	%rd268, %r258, 64;
+	add.s64 	%rd257, %rd233, %rd268;
+	// begin inline asm
+	cvta.to.global.u64 %rd256, %rd257;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r240,%r241,%r242,%r243}, [%rd256];
+	// end inline asm
+	mov.b32 	%f1201, %r240;
+	mov.b32 	%f1202, %r241;
+	mov.b32 	%f1203, %r242;
+	add.s64 	%rd260, %rd257, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd259, %rd260;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r244,%r245,%r246,%r247}, [%rd259];
+	// end inline asm
+	mov.b32 	%f1204, %r244;
+	mov.b32 	%f1205, %r245;
+	mov.b32 	%f1206, %r247;
+	add.s64 	%rd263, %rd257, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd262, %rd263;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r248,%r249,%r250,%r251}, [%rd262];
+	// end inline asm
+	mov.b32 	%f1207, %r249;
+	mov.b32 	%f1208, %r250;
+	mov.b32 	%f1209, %r251;
+	add.s64 	%rd266, %rd257, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd265, %rd266;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r252,%r253,%r254,%r255}, [%rd265];
+	// end inline asm
+	mov.b32 	%f1210, %r252;
+	setp.leu.f32 	%p30, %f264, 0f00000000;
+	@%p30 bra 	$L__BB4_48;
+
+	mov.f32 	%f809, 0f3F800000;
+	sub.f32 	%f810, %f809, %f264;
+	add.s64 	%rd270, %rd257, 64;
+	// begin inline asm
 	cvta.to.global.u64 %rd269, %rd270;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r250,%r251,%r252,%r253}, [%rd269];
-	// inline asm
-	mov.b32 	 %f1183, %r250;
-	setp.leu.f32	%p29, %f261, 0f00000000;
-	@%p29 bra 	BB4_48;
-
-	shl.b64 	%rd285, %rd33, 6;
-	add.s64 	%rd286, %rd285, %rd228;
-	add.s64 	%rd274, %rd286, 96;
-	// inline asm
-	cvta.to.global.u64 %rd273, %rd274;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r257,%r258,%r259,%r260}, [%rd273];
-	// inline asm
-	mov.b32 	 %f798, %r257;
-	mov.b32 	 %f799, %r258;
-	mov.b32 	 %f800, %r259;
-	add.s64 	%rd277, %rd286, 112;
-	// inline asm
-	cvta.to.global.u64 %rd276, %rd277;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r261,%r262,%r263,%r264}, [%rd276];
-	// inline asm
-	mov.b32 	 %f801, %r261;
-	mov.b32 	 %f802, %r262;
-	mov.b32 	 %f803, %r264;
-	add.s64 	%rd280, %rd286, 128;
-	// inline asm
-	cvta.to.global.u64 %rd279, %rd280;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r265,%r266,%r267,%r268}, [%rd279];
-	// inline asm
-	mov.b32 	 %f804, %r266;
-	mov.b32 	 %f805, %r267;
-	mov.b32 	 %f806, %r268;
-	add.s64 	%rd283, %rd286, 144;
-	// inline asm
-	cvta.to.global.u64 %rd282, %rd283;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r269,%r270,%r271,%r272}, [%rd282];
-	// inline asm
-	mov.f32 	%f807, 0f3F800000;
-	sub.f32 	%f808, %f807, %f261;
-	mul.f32 	%f809, %f261, %f798;
-	mul.f32 	%f810, %f261, %f799;
-	mul.f32 	%f811, %f261, %f800;
-	fma.rn.f32 	%f1174, %f808, %f1174, %f809;
-	fma.rn.f32 	%f1175, %f808, %f1175, %f810;
-	fma.rn.f32 	%f1176, %f808, %f1176, %f811;
-	mul.f32 	%f812, %f261, %f801;
-	mul.f32 	%f813, %f261, %f802;
-	mul.f32 	%f814, %f261, %f803;
-	fma.rn.f32 	%f1177, %f808, %f1177, %f812;
-	fma.rn.f32 	%f1178, %f808, %f1178, %f813;
-	fma.rn.f32 	%f1179, %f808, %f1179, %f814;
-	mul.f32 	%f815, %f261, %f804;
-	mul.f32 	%f816, %f261, %f805;
-	mul.f32 	%f817, %f261, %f806;
-	fma.rn.f32 	%f818, %f808, %f1180, %f815;
-	fma.rn.f32 	%f819, %f808, %f1181, %f816;
-	fma.rn.f32 	%f820, %f808, %f1182, %f817;
-	mov.b32 	 %f821, %r269;
-	mul.f32 	%f822, %f261, %f821;
-	fma.rn.f32 	%f823, %f808, %f1183, %f822;
-	mul.f32 	%f824, %f819, %f819;
-	fma.rn.f32 	%f825, %f818, %f818, %f824;
-	fma.rn.f32 	%f826, %f820, %f820, %f825;
-	fma.rn.f32 	%f827, %f823, %f823, %f826;
-	sqrt.rn.f32 	%f828, %f827;
-	rcp.rn.f32 	%f829, %f828;
-	mul.f32 	%f1180, %f818, %f829;
-	mul.f32 	%f1181, %f819, %f829;
-	mul.f32 	%f1182, %f820, %f829;
-	mul.f32 	%f1183, %f823, %f829;
-
-BB4_48:
-	mul.f32 	%f830, %f1181, %f1181;
-	fma.rn.f32 	%f831, %f1180, %f1180, %f830;
-	fma.rn.f32 	%f832, %f1182, %f1182, %f831;
-	fma.rn.f32 	%f833, %f1183, %f1183, %f832;
-	rcp.rn.f32 	%f834, %f833;
-	mul.f32 	%f835, %f1180, %f834;
-	mul.f32 	%f836, %f1181, %f834;
-	mul.f32 	%f837, %f1182, %f834;
-	mul.f32 	%f838, %f1183, %f834;
-	mul.f32 	%f839, %f1180, %f835;
-	mul.f32 	%f840, %f1181, %f836;
-	mul.f32 	%f841, %f1182, %f837;
-	mul.f32 	%f842, %f1180, %f836;
-	mul.f32 	%f843, %f1182, %f838;
-	mul.f32 	%f844, %f1180, %f837;
-	mul.f32 	%f845, %f1181, %f838;
-	mul.f32 	%f846, %f1181, %f837;
-	mul.f32 	%f847, %f1180, %f838;
-	sub.f32 	%f848, %f839, %f840;
-	sub.f32 	%f849, %f848, %f841;
-	fma.rn.f32 	%f850, %f1183, %f838, %f849;
-	sub.f32 	%f851, %f842, %f843;
-	add.f32 	%f852, %f851, %f851;
-	add.f32 	%f853, %f844, %f845;
-	add.f32 	%f854, %f853, %f853;
-	add.f32 	%f855, %f842, %f843;
-	add.f32 	%f856, %f855, %f855;
-	sub.f32 	%f857, %f840, %f839;
-	sub.f32 	%f858, %f857, %f841;
-	fma.rn.f32 	%f859, %f1183, %f838, %f858;
-	sub.f32 	%f860, %f846, %f847;
-	add.f32 	%f861, %f860, %f860;
-	sub.f32 	%f862, %f844, %f845;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r259,%r260,%r261,%r262}, [%rd269];
+	// end inline asm
+	mov.b32 	%f811, %r259;
+	mov.b32 	%f812, %r260;
+	mov.b32 	%f813, %r261;
+	mul.f32 	%f814, %f264, %f811;
+	mul.f32 	%f815, %f264, %f812;
+	mul.f32 	%f816, %f264, %f813;
+	fma.rn.f32 	%f1201, %f810, %f1201, %f814;
+	fma.rn.f32 	%f1202, %f810, %f1202, %f815;
+	fma.rn.f32 	%f1203, %f810, %f1203, %f816;
+	add.s64 	%rd273, %rd257, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd272, %rd273;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r263,%r264,%r265,%r266}, [%rd272];
+	// end inline asm
+	mov.b32 	%f817, %r263;
+	mov.b32 	%f818, %r264;
+	mov.b32 	%f819, %r266;
+	mul.f32 	%f820, %f264, %f817;
+	mul.f32 	%f821, %f264, %f818;
+	mul.f32 	%f822, %f264, %f819;
+	fma.rn.f32 	%f1204, %f810, %f1204, %f820;
+	fma.rn.f32 	%f1205, %f810, %f1205, %f821;
+	fma.rn.f32 	%f1206, %f810, %f1206, %f822;
+	add.s64 	%rd276, %rd257, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd275, %rd276;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r267,%r268,%r269,%r270}, [%rd275];
+	// end inline asm
+	mov.b32 	%f823, %r268;
+	mov.b32 	%f824, %r269;
+	mov.b32 	%f825, %r270;
+	mul.f32 	%f826, %f264, %f823;
+	mul.f32 	%f827, %f264, %f824;
+	mul.f32 	%f828, %f264, %f825;
+	fma.rn.f32 	%f829, %f810, %f1207, %f826;
+	fma.rn.f32 	%f830, %f810, %f1208, %f827;
+	fma.rn.f32 	%f831, %f810, %f1209, %f828;
+	add.s64 	%rd279, %rd257, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd278, %rd279;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r271,%r272,%r273,%r274}, [%rd278];
+	// end inline asm
+	mov.b32 	%f832, %r271;
+	mul.f32 	%f833, %f264, %f832;
+	fma.rn.f32 	%f834, %f810, %f1210, %f833;
+	mul.f32 	%f835, %f830, %f830;
+	fma.rn.f32 	%f836, %f829, %f829, %f835;
+	fma.rn.f32 	%f837, %f831, %f831, %f836;
+	fma.rn.f32 	%f838, %f834, %f834, %f837;
+	sqrt.rn.f32 	%f839, %f838;
+	rcp.rn.f32 	%f840, %f839;
+	mul.f32 	%f1207, %f829, %f840;
+	mul.f32 	%f1208, %f830, %f840;
+	mul.f32 	%f1209, %f831, %f840;
+	mul.f32 	%f1210, %f840, %f834;
+
+$L__BB4_48:
+	mul.f32 	%f841, %f1208, %f1208;
+	fma.rn.f32 	%f842, %f1207, %f1207, %f841;
+	fma.rn.f32 	%f843, %f1209, %f1209, %f842;
+	fma.rn.f32 	%f844, %f1210, %f1210, %f843;
+	rcp.rn.f32 	%f845, %f844;
+	mul.f32 	%f846, %f1207, %f845;
+	mul.f32 	%f847, %f1208, %f845;
+	mul.f32 	%f848, %f1209, %f845;
+	mul.f32 	%f849, %f1210, %f845;
+	mul.f32 	%f850, %f1207, %f846;
+	mul.f32 	%f851, %f1208, %f847;
+	mul.f32 	%f852, %f1209, %f848;
+	mul.f32 	%f853, %f1207, %f847;
+	mul.f32 	%f854, %f1209, %f849;
+	mul.f32 	%f855, %f1207, %f848;
+	mul.f32 	%f856, %f1208, %f849;
+	mul.f32 	%f857, %f1208, %f848;
+	mul.f32 	%f858, %f1207, %f849;
+	sub.f32 	%f859, %f850, %f851;
+	sub.f32 	%f860, %f859, %f852;
+	fma.rn.f32 	%f861, %f1210, %f849, %f860;
+	sub.f32 	%f862, %f853, %f854;
 	add.f32 	%f863, %f862, %f862;
-	add.f32 	%f864, %f846, %f847;
+	add.f32 	%f864, %f855, %f856;
 	add.f32 	%f865, %f864, %f864;
-	neg.f32 	%f866, %f839;
-	sub.f32 	%f867, %f866, %f840;
-	add.f32 	%f868, %f841, %f867;
-	fma.rn.f32 	%f869, %f1183, %f838, %f868;
-	mul.f32 	%f870, %f1176, %f850;
-	fma.rn.f32 	%f871, %f1178, %f852, %f870;
-	fma.rn.f32 	%f1192, %f1179, %f854, %f871;
-	mul.f32 	%f872, %f1178, %f859;
-	fma.rn.f32 	%f873, %f1176, %f856, %f872;
-	fma.rn.f32 	%f1189, %f1179, %f861, %f873;
-	mul.f32 	%f874, %f1178, %f865;
-	fma.rn.f32 	%f875, %f1176, %f863, %f874;
-	fma.rn.f32 	%f1186, %f1179, %f869, %f875;
-	mul.f32 	%f876, %f1175, %f850;
-	fma.rn.f32 	%f1191, %f1177, %f852, %f876;
-	mul.f32 	%f877, %f1177, %f859;
-	fma.rn.f32 	%f1188, %f1175, %f856, %f877;
-	mul.f32 	%f878, %f1177, %f865;
-	fma.rn.f32 	%f1185, %f1175, %f863, %f878;
-	mul.f32 	%f1190, %f1174, %f850;
-	mul.f32 	%f1187, %f1174, %f856;
-	mul.f32 	%f1184, %f1174, %f863;
-
-BB4_51:
-	mul.f32 	%f910, %f1185, %f1189;
-	mul.f32 	%f911, %f1186, %f1188;
-	sub.f32 	%f912, %f911, %f910;
-	mul.f32 	%f913, %f1190, %f912;
-	mul.f32 	%f914, %f1184, %f1189;
-	mul.f32 	%f915, %f1186, %f1187;
-	sub.f32 	%f916, %f915, %f914;
-	mul.f32 	%f917, %f916, %f1191;
-	sub.f32 	%f918, %f913, %f917;
-	mul.f32 	%f919, %f1184, %f1188;
-	mul.f32 	%f920, %f1185, %f1187;
-	sub.f32 	%f921, %f920, %f919;
-	fma.rn.f32 	%f922, %f921, %f1192, %f918;
-	rcp.rn.f32 	%f923, %f922;
-	mul.f32 	%f1199, %f912, %f923;
-	mul.f32 	%f924, %f1186, %f1191;
-	mul.f32 	%f925, %f1185, %f1192;
-	sub.f32 	%f926, %f925, %f924;
-	mul.f32 	%f1200, %f923, %f926;
-	mul.f32 	%f927, %f1188, %f1192;
-	mul.f32 	%f928, %f1189, %f1191;
-	sub.f32 	%f929, %f928, %f927;
-	mul.f32 	%f1201, %f923, %f929;
-	sub.f32 	%f930, %f914, %f915;
-	mul.f32 	%f1196, %f930, %f923;
-	mul.f32 	%f931, %f1184, %f1192;
-	mul.f32 	%f932, %f1186, %f1190;
-	sub.f32 	%f933, %f932, %f931;
-	mul.f32 	%f1197, %f923, %f933;
-	mul.f32 	%f934, %f1189, %f1190;
-	mul.f32 	%f935, %f1187, %f1192;
-	sub.f32 	%f936, %f935, %f934;
-	mul.f32 	%f1198, %f923, %f936;
-	mul.f32 	%f1193, %f921, %f923;
-	mul.f32 	%f937, %f1185, %f1190;
-	mul.f32 	%f938, %f1184, %f1191;
-	sub.f32 	%f939, %f938, %f937;
-	mul.f32 	%f1194, %f939, %f923;
-	mul.f32 	%f940, %f1187, %f1191;
-	mul.f32 	%f941, %f1188, %f1190;
-	sub.f32 	%f942, %f941, %f940;
-	mul.f32 	%f1195, %f942, %f923;
-	bra.uni 	BB4_52;
-
-BB4_41:
-	setp.ne.s32	%p27, %r184, 1;
-	mov.f32 	%f1194, %f1193;
-	mov.f32 	%f1196, %f1193;
-	mov.f32 	%f1197, %f1195;
-	mov.f32 	%f1198, %f1193;
-	mov.f32 	%f1199, %f1195;
-	mov.f32 	%f1200, %f1193;
-	mov.f32 	%f1201, %f1193;
-	@%p27 bra 	BB4_52;
-
-	// inline asm
-	call (%rd215), _optix_get_static_transform_from_handle, (%rd213);
-	// inline asm
-	add.s64 	%rd449, %rd215, 64;
-
-BB4_44:
-	// inline asm
-	cvta.to.global.u64 %rd219, %rd449;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r186,%r187,%r188,%r189}, [%rd219];
-	// inline asm
-	mov.b32 	 %f1199, %r186;
-	mov.b32 	 %f1200, %r187;
-	mov.b32 	 %f1201, %r188;
-	add.s64 	%rd223, %rd449, 16;
-	// inline asm
-	cvta.to.global.u64 %rd222, %rd223;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r190,%r191,%r192,%r193}, [%rd222];
-	// inline asm
-	mov.b32 	 %f1196, %r190;
-	mov.b32 	 %f1197, %r191;
-	mov.b32 	 %f1198, %r192;
-	add.s64 	%rd226, %rd449, 32;
-	// inline asm
-	cvta.to.global.u64 %rd225, %rd226;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r194,%r195,%r196,%r197}, [%rd225];
-	// inline asm
-	mov.b32 	 %f1193, %r194;
-	mov.b32 	 %f1194, %r195;
-	mov.b32 	 %f1195, %r196;
-
-BB4_52:
-	setp.eq.s32	%p31, %r335, 0;
-	@%p31 bra 	BB4_53;
-	bra.uni 	BB4_54;
-
-BB4_53:
-	mov.f32 	%f1173, %f1193;
-	mov.f32 	%f1172, %f1194;
-	mov.f32 	%f1171, %f1195;
-	mov.f32 	%f1170, %f1196;
-	mov.f32 	%f1169, %f1197;
-	mov.f32 	%f1168, %f1198;
-	mov.f32 	%f1167, %f1199;
-	mov.f32 	%f1166, %f1200;
-	mov.f32 	%f1165, %f1201;
-	bra.uni 	BB4_55;
-
-BB4_54:
-	mul.f32 	%f943, %f1170, %f1200;
-	fma.rn.f32 	%f944, %f1167, %f1199, %f943;
-	fma.rn.f32 	%f341, %f1173, %f1201, %f944;
-	mul.f32 	%f945, %f1169, %f1200;
-	fma.rn.f32 	%f946, %f1166, %f1199, %f945;
-	fma.rn.f32 	%f342, %f1172, %f1201, %f946;
-	mul.f32 	%f947, %f1168, %f1200;
-	fma.rn.f32 	%f948, %f1165, %f1199, %f947;
-	fma.rn.f32 	%f343, %f1171, %f1201, %f948;
-	mul.f32 	%f949, %f1170, %f1197;
-	fma.rn.f32 	%f950, %f1167, %f1196, %f949;
-	fma.rn.f32 	%f344, %f1173, %f1198, %f950;
-	mul.f32 	%f951, %f1169, %f1197;
-	fma.rn.f32 	%f952, %f1166, %f1196, %f951;
-	fma.rn.f32 	%f345, %f1172, %f1198, %f952;
-	mul.f32 	%f953, %f1168, %f1197;
-	fma.rn.f32 	%f954, %f1165, %f1196, %f953;
-	fma.rn.f32 	%f346, %f1171, %f1198, %f954;
-	mul.f32 	%f955, %f1170, %f1194;
-	fma.rn.f32 	%f956, %f1167, %f1193, %f955;
-	fma.rn.f32 	%f1173, %f1173, %f1195, %f956;
-	mul.f32 	%f957, %f1169, %f1194;
-	fma.rn.f32 	%f958, %f1166, %f1193, %f957;
-	fma.rn.f32 	%f1172, %f1172, %f1195, %f958;
-	mul.f32 	%f959, %f1168, %f1194;
-	fma.rn.f32 	%f960, %f1165, %f1193, %f959;
-	fma.rn.f32 	%f1171, %f1171, %f1195, %f960;
-	mov.f32 	%f1170, %f344;
-	mov.f32 	%f1169, %f345;
-	mov.f32 	%f1168, %f346;
-	mov.f32 	%f1167, %f341;
-	mov.f32 	%f1166, %f342;
-	mov.f32 	%f1165, %f343;
-
-BB4_55:
-	add.s32 	%r335, %r335, 1;
-	setp.lt.u32	%p32, %r335, %r32;
-	@%p32 bra 	BB4_39;
-
-BB4_56:
-	fma.rn.f32 	%f961, %f1256, %f1111, %f1101;
-	fma.rn.f32 	%f962, %f1257, %f1112, %f961;
-	fma.rn.f32 	%f963, %f1256, %f1107, %f1110;
-	fma.rn.f32 	%f964, %f1257, %f1108, %f963;
-	fma.rn.f32 	%f965, %f1256, %f1103, %f1106;
-	fma.rn.f32 	%f966, %f1257, %f1104, %f965;
-	fma.rn.f32 	%f1256, %f1258, %f1102, %f962;
-	fma.rn.f32 	%f1257, %f1258, %f1109, %f964;
-	fma.rn.f32 	%f1258, %f1258, %f1105, %f966;
-	ld.const.u64 	%rd334, [params+112];
-	setp.eq.s64	%p33, %rd334, 0;
-	@%p33 bra 	BB4_58;
-
-	mul.f32 	%f967, %f1250, %f1167;
-	fma.rn.f32 	%f968, %f1251, %f1170, %f967;
-	mul.f32 	%f969, %f1250, %f1166;
-	fma.rn.f32 	%f970, %f1251, %f1169, %f969;
-	mul.f32 	%f971, %f1250, %f1165;
-	fma.rn.f32 	%f972, %f1251, %f1168, %f971;
-	fma.rn.f32 	%f973, %f1252, %f1173, %f968;
-	fma.rn.f32 	%f974, %f1252, %f1172, %f970;
-	fma.rn.f32 	%f975, %f1252, %f1171, %f972;
-	mul.f32 	%f976, %f973, %f973;
-	fma.rn.f32 	%f977, %f974, %f974, %f976;
-	fma.rn.f32 	%f978, %f975, %f975, %f977;
-	sqrt.rn.f32 	%f979, %f978;
-	div.rn.f32 	%f1250, %f973, %f979;
-	div.rn.f32 	%f1251, %f974, %f979;
-	div.rn.f32 	%f1252, %f975, %f979;
-
-BB4_58:
-	@%p5 bra 	BB4_60;
-
-	mul.f32 	%f980, %f1092, %f1167;
-	fma.rn.f32 	%f981, %f1091, %f1170, %f980;
-	mul.f32 	%f982, %f1092, %f1166;
-	fma.rn.f32 	%f983, %f1091, %f1169, %f982;
-	mul.f32 	%f984, %f1092, %f1165;
-	fma.rn.f32 	%f985, %f1091, %f1168, %f984;
-	fma.rn.f32 	%f986, %f1090, %f1173, %f981;
-	fma.rn.f32 	%f987, %f1090, %f1172, %f983;
-	fma.rn.f32 	%f988, %f1090, %f1171, %f985;
-	mul.f32 	%f989, %f986, %f986;
-	fma.rn.f32 	%f990, %f987, %f987, %f989;
-	fma.rn.f32 	%f991, %f988, %f988, %f990;
-	sqrt.rn.f32 	%f992, %f991;
-	div.rn.f32 	%f1092, %f986, %f992;
-	div.rn.f32 	%f1091, %f987, %f992;
-	div.rn.f32 	%f1090, %f988, %f992;
-
-BB4_60:
-	ld.const.u64 	%rd335, [params+184];
-	setp.eq.s64	%p35, %rd335, 0;
-	@%p35 bra 	BB4_62;
-
-	mul.f32 	%f993, %f1247, %f1111;
-	fma.rn.f32 	%f994, %f1248, %f1112, %f993;
-	mul.f32 	%f995, %f1247, %f1107;
-	fma.rn.f32 	%f996, %f1248, %f1108, %f995;
-	mul.f32 	%f997, %f1247, %f1103;
-	fma.rn.f32 	%f998, %f1248, %f1104, %f997;
-	fma.rn.f32 	%f1247, %f1249, %f1102, %f994;
-	fma.rn.f32 	%f1248, %f1249, %f1109, %f996;
-	fma.rn.f32 	%f1249, %f1249, %f1105, %f998;
-	mul.f32 	%f999, %f1244, %f1111;
-	fma.rn.f32 	%f1000, %f1245, %f1112, %f999;
-	mul.f32 	%f1001, %f1244, %f1107;
-	fma.rn.f32 	%f1002, %f1245, %f1108, %f1001;
-	mul.f32 	%f1003, %f1244, %f1103;
-	fma.rn.f32 	%f1004, %f1245, %f1104, %f1003;
-	fma.rn.f32 	%f1244, %f1246, %f1102, %f1000;
-	fma.rn.f32 	%f1245, %f1246, %f1109, %f1002;
-	fma.rn.f32 	%f1246, %f1246, %f1105, %f1004;
-
-BB4_62:
-	ld.const.u64 	%rd336, [params+280];
-	ld.const.u64 	%rd337, [params+232];
-	or.b64  	%rd338, %rd336, %rd337;
-	setp.eq.s64	%p36, %rd338, 0;
-	@%p36 bra 	BB4_64;
-
-	mul.f32 	%f1005, %f1092, %f1111;
-	fma.rn.f32 	%f1006, %f1091, %f1107, %f1005;
-	mul.f32 	%f1007, %f1092, %f1112;
-	fma.rn.f32 	%f1008, %f1091, %f1108, %f1007;
-	mul.f32 	%f1009, %f1092, %f1102;
-	fma.rn.f32 	%f1010, %f1091, %f1109, %f1009;
-	fma.rn.f32 	%f1011, %f1090, %f1103, %f1006;
-	fma.rn.f32 	%f1012, %f1090, %f1104, %f1008;
-	fma.rn.f32 	%f1013, %f1090, %f1105, %f1010;
-	mul.f32 	%f1014, %f1011, %f1011;
-	fma.rn.f32 	%f1015, %f1012, %f1012, %f1014;
-	fma.rn.f32 	%f1016, %f1013, %f1013, %f1015;
-	sqrt.rn.f32 	%f1017, %f1016;
-	div.rn.f32 	%f1018, %f1011, %f1017;
-	div.rn.f32 	%f1019, %f1012, %f1017;
-	div.rn.f32 	%f1020, %f1013, %f1017;
-	mul.f32 	%f1021, %f1018, %f1167;
-	mul.f32 	%f1022, %f1018, %f1166;
-	mul.f32 	%f1023, %f1018, %f1165;
-	fma.rn.f32 	%f1024, %f1019, %f1170, %f1021;
-	fma.rn.f32 	%f1025, %f1019, %f1169, %f1022;
-	fma.rn.f32 	%f1026, %f1019, %f1168, %f1023;
-	fma.rn.f32 	%f1027, %f1020, %f1173, %f1024;
-	fma.rn.f32 	%f1028, %f1020, %f1172, %f1025;
-	fma.rn.f32 	%f1029, %f1020, %f1171, %f1026;
-	mul.f32 	%f1030, %f1027, %f1027;
-	fma.rn.f32 	%f1031, %f1028, %f1028, %f1030;
-	fma.rn.f32 	%f1032, %f1029, %f1029, %f1031;
-	sqrt.rn.f32 	%f1033, %f1032;
-	rcp.rn.f32 	%f1034, %f1033;
-	mul.f32 	%f1035, %f1034, %f1027;
-	mul.f32 	%f1036, %f1034, %f1028;
-	mul.f32 	%f1037, %f1034, %f1029;
-	mul.f32 	%f1038, %f1087, %f1167;
-	fma.rn.f32 	%f1039, %f1088, %f1170, %f1038;
-	mul.f32 	%f1040, %f1087, %f1166;
-	fma.rn.f32 	%f1041, %f1088, %f1169, %f1040;
-	mul.f32 	%f1042, %f1087, %f1165;
-	fma.rn.f32 	%f1043, %f1088, %f1168, %f1042;
-	fma.rn.f32 	%f1044, %f1089, %f1173, %f1039;
-	fma.rn.f32 	%f1045, %f1089, %f1172, %f1041;
-	fma.rn.f32 	%f1046, %f1089, %f1171, %f1043;
-	mul.f32 	%f1047, %f1044, %f1034;
-	mul.f32 	%f1048, %f1045, %f1034;
-	mul.f32 	%f1049, %f1046, %f1034;
-	mul.f32 	%f1050, %f1084, %f1167;
-	fma.rn.f32 	%f1051, %f1085, %f1170, %f1050;
-	mul.f32 	%f1052, %f1084, %f1166;
-	fma.rn.f32 	%f1053, %f1085, %f1169, %f1052;
-	mul.f32 	%f1054, %f1084, %f1165;
-	fma.rn.f32 	%f1055, %f1085, %f1168, %f1054;
-	fma.rn.f32 	%f1056, %f1086, %f1173, %f1051;
-	fma.rn.f32 	%f1057, %f1086, %f1172, %f1053;
-	fma.rn.f32 	%f1058, %f1086, %f1171, %f1055;
-	mul.f32 	%f1059, %f1056, %f1034;
-	mul.f32 	%f1060, %f1057, %f1034;
-	mul.f32 	%f1061, %f1058, %f1034;
-	mul.f32 	%f1062, %f1035, %f1047;
-	fma.rn.f32 	%f1063, %f1036, %f1048, %f1062;
-	fma.rn.f32 	%f1064, %f1037, %f1049, %f1063;
-	mul.f32 	%f1065, %f1035, %f1064;
-	mul.f32 	%f1066, %f1036, %f1064;
-	mul.f32 	%f1067, %f1037, %f1064;
-	sub.f32 	%f1087, %f1047, %f1065;
-	sub.f32 	%f1088, %f1048, %f1066;
-	sub.f32 	%f1089, %f1049, %f1067;
-	mul.f32 	%f1068, %f1035, %f1059;
-	fma.rn.f32 	%f1069, %f1036, %f1060, %f1068;
-	fma.rn.f32 	%f1070, %f1037, %f1061, %f1069;
-	mul.f32 	%f1071, %f1035, %f1070;
-	mul.f32 	%f1072, %f1036, %f1070;
-	mul.f32 	%f1073, %f1037, %f1070;
-	sub.f32 	%f1084, %f1059, %f1071;
-	sub.f32 	%f1085, %f1060, %f1072;
-	sub.f32 	%f1086, %f1061, %f1073;
-
-BB4_64:
-	st.global.u32 	[%rd19], %r31;
-
-BB4_65:
-	ld.const.u64 	%rd441, [params+96];
-	setp.eq.s64	%p44, %rd441, 0;
-	cvt.u64.u32	%rd437, %r1;
-	ld.const.u64 	%rd339, [params+328];
-	cvta.to.global.u64 	%rd340, %rd339;
-	shl.b64 	%rd341, %rd437, 3;
-	add.s64 	%rd342, %rd340, %rd341;
-	st.global.u64 	[%rd342], %rd17;
-	ld.const.u64 	%rd343, [params+336];
-	cvta.to.global.u64 	%rd344, %rd343;
-	shl.b64 	%rd345, %rd437, 2;
-	add.s64 	%rd346, %rd344, %rd345;
-	st.global.u32 	[%rd346], %r23;
-	ld.const.u64 	%rd347, [params+160];
+	add.f32 	%f866, %f853, %f854;
+	add.f32 	%f867, %f866, %f866;
+	sub.f32 	%f868, %f851, %f850;
+	sub.f32 	%f869, %f868, %f852;
+	fma.rn.f32 	%f870, %f1210, %f849, %f869;
+	sub.f32 	%f871, %f857, %f858;
+	add.f32 	%f872, %f871, %f871;
+	sub.f32 	%f873, %f855, %f856;
+	add.f32 	%f874, %f873, %f873;
+	add.f32 	%f875, %f857, %f858;
+	add.f32 	%f876, %f875, %f875;
+	neg.f32 	%f877, %f850;
+	sub.f32 	%f878, %f877, %f851;
+	add.f32 	%f879, %f852, %f878;
+	fma.rn.f32 	%f880, %f1210, %f849, %f879;
+	mul.f32 	%f881, %f1203, %f861;
+	fma.rn.f32 	%f882, %f1205, %f863, %f881;
+	fma.rn.f32 	%f1219, %f1206, %f865, %f882;
+	mul.f32 	%f883, %f1205, %f870;
+	fma.rn.f32 	%f884, %f1203, %f867, %f883;
+	fma.rn.f32 	%f1216, %f1206, %f872, %f884;
+	mul.f32 	%f885, %f1205, %f876;
+	fma.rn.f32 	%f886, %f1203, %f874, %f885;
+	fma.rn.f32 	%f1213, %f1206, %f880, %f886;
+	mul.f32 	%f887, %f1202, %f861;
+	fma.rn.f32 	%f1218, %f1204, %f863, %f887;
+	mul.f32 	%f888, %f1204, %f870;
+	fma.rn.f32 	%f1215, %f1202, %f867, %f888;
+	mul.f32 	%f889, %f1204, %f876;
+	fma.rn.f32 	%f1212, %f1202, %f874, %f889;
+	mul.f32 	%f1217, %f1201, %f861;
+	mul.f32 	%f1214, %f1201, %f867;
+	mul.f32 	%f1211, %f1201, %f874;
+
+$L__BB4_51:
+	mul.f32 	%f921, %f1212, %f1216;
+	mul.f32 	%f922, %f1213, %f1215;
+	sub.f32 	%f923, %f922, %f921;
+	mul.f32 	%f924, %f1217, %f923;
+	mul.f32 	%f925, %f1211, %f1216;
+	mul.f32 	%f926, %f1213, %f1214;
+	sub.f32 	%f927, %f926, %f925;
+	mul.f32 	%f928, %f927, %f1218;
+	sub.f32 	%f929, %f924, %f928;
+	mul.f32 	%f930, %f1211, %f1215;
+	mul.f32 	%f931, %f1212, %f1214;
+	sub.f32 	%f932, %f931, %f930;
+	fma.rn.f32 	%f933, %f932, %f1219, %f929;
+	rcp.rn.f32 	%f934, %f933;
+	mul.f32 	%f1226, %f923, %f934;
+	mul.f32 	%f935, %f1213, %f1218;
+	mul.f32 	%f936, %f1212, %f1219;
+	sub.f32 	%f937, %f936, %f935;
+	mul.f32 	%f1227, %f937, %f934;
+	mul.f32 	%f938, %f1215, %f1219;
+	mul.f32 	%f939, %f1216, %f1218;
+	sub.f32 	%f940, %f939, %f938;
+	mul.f32 	%f1228, %f940, %f934;
+	sub.f32 	%f941, %f925, %f926;
+	mul.f32 	%f1223, %f941, %f934;
+	mul.f32 	%f942, %f1211, %f1219;
+	mul.f32 	%f943, %f1213, %f1217;
+	sub.f32 	%f944, %f943, %f942;
+	mul.f32 	%f1224, %f944, %f934;
+	mul.f32 	%f945, %f1216, %f1217;
+	mul.f32 	%f946, %f1214, %f1219;
+	sub.f32 	%f947, %f946, %f945;
+	mul.f32 	%f1225, %f947, %f934;
+	mul.f32 	%f1220, %f932, %f934;
+	mul.f32 	%f948, %f1212, %f1217;
+	mul.f32 	%f949, %f1211, %f1218;
+	sub.f32 	%f950, %f949, %f948;
+	mul.f32 	%f1221, %f950, %f934;
+	mul.f32 	%f951, %f1214, %f1218;
+	mul.f32 	%f952, %f1215, %f1217;
+	sub.f32 	%f953, %f952, %f951;
+	mul.f32 	%f1222, %f953, %f934;
+	bra.uni 	$L__BB4_52;
+
+$L__BB4_43:
+	// begin inline asm
+	call (%rd442), _optix_get_instance_inverse_transform_from_handle, (%rd209);
+	// end inline asm
+
+$L__BB4_44:
+	// begin inline asm
+	cvta.to.global.u64 %rd215, %rd442;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r188,%r189,%r190,%r191}, [%rd215];
+	// end inline asm
+	mov.b32 	%f1226, %r188;
+	mov.b32 	%f1227, %r189;
+	mov.b32 	%f1228, %r190;
+	add.s64 	%rd219, %rd442, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd218, %rd219;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r192,%r193,%r194,%r195}, [%rd218];
+	// end inline asm
+	mov.b32 	%f1223, %r192;
+	mov.b32 	%f1224, %r193;
+	mov.b32 	%f1225, %r194;
+	add.s64 	%rd222, %rd442, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd221, %rd222;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r196,%r197,%r198,%r199}, [%rd221];
+	// end inline asm
+	mov.b32 	%f1220, %r196;
+	mov.b32 	%f1221, %r197;
+	mov.b32 	%f1222, %r198;
+
+$L__BB4_52:
+	setp.eq.s32 	%p32, %r337, 0;
+	@%p32 bra 	$L__BB4_54;
+
+	mul.f32 	%f954, %f1197, %f1227;
+	fma.rn.f32 	%f955, %f1194, %f1226, %f954;
+	fma.rn.f32 	%f350, %f1200, %f1228, %f955;
+	mul.f32 	%f956, %f1196, %f1227;
+	fma.rn.f32 	%f957, %f1193, %f1226, %f956;
+	fma.rn.f32 	%f351, %f1199, %f1228, %f957;
+	mul.f32 	%f958, %f1195, %f1227;
+	fma.rn.f32 	%f959, %f1192, %f1226, %f958;
+	fma.rn.f32 	%f1228, %f1198, %f1228, %f959;
+	mul.f32 	%f960, %f1197, %f1224;
+	fma.rn.f32 	%f961, %f1194, %f1223, %f960;
+	fma.rn.f32 	%f353, %f1200, %f1225, %f961;
+	mul.f32 	%f962, %f1196, %f1224;
+	fma.rn.f32 	%f963, %f1193, %f1223, %f962;
+	fma.rn.f32 	%f354, %f1199, %f1225, %f963;
+	mul.f32 	%f964, %f1195, %f1224;
+	fma.rn.f32 	%f965, %f1192, %f1223, %f964;
+	fma.rn.f32 	%f1225, %f1198, %f1225, %f965;
+	mul.f32 	%f966, %f1197, %f1221;
+	fma.rn.f32 	%f967, %f1194, %f1220, %f966;
+	fma.rn.f32 	%f356, %f1200, %f1222, %f967;
+	mul.f32 	%f968, %f1196, %f1221;
+	fma.rn.f32 	%f969, %f1193, %f1220, %f968;
+	fma.rn.f32 	%f357, %f1199, %f1222, %f969;
+	mul.f32 	%f970, %f1195, %f1221;
+	fma.rn.f32 	%f971, %f1192, %f1220, %f970;
+	fma.rn.f32 	%f1222, %f1198, %f1222, %f971;
+	mov.f32 	%f1220, %f356;
+	mov.f32 	%f1221, %f357;
+	mov.f32 	%f1223, %f353;
+	mov.f32 	%f1224, %f354;
+	mov.f32 	%f1226, %f350;
+	mov.f32 	%f1227, %f351;
+
+$L__BB4_54:
+	add.s32 	%r337, %r337, 1;
+	setp.lt.u32 	%p33, %r337, %r183;
+	mov.f32 	%f1192, %f1228;
+	mov.f32 	%f1193, %f1227;
+	mov.f32 	%f1194, %f1226;
+	mov.f32 	%f1195, %f1225;
+	mov.f32 	%f1196, %f1224;
+	mov.f32 	%f1197, %f1223;
+	mov.f32 	%f1198, %f1222;
+	mov.f32 	%f1199, %f1221;
+	mov.f32 	%f1200, %f1220;
+	@%p33 bra 	$L__BB4_39;
+
+$L__BB4_55:
+	fma.rn.f32 	%f972, %f1283, %f1164, %f1167;
+	fma.rn.f32 	%f973, %f1284, %f1165, %f972;
+	fma.rn.f32 	%f974, %f1283, %f1160, %f1163;
+	fma.rn.f32 	%f975, %f1284, %f1161, %f974;
+	fma.rn.f32 	%f976, %f1283, %f1156, %f1159;
+	fma.rn.f32 	%f977, %f1284, %f1157, %f976;
+	fma.rn.f32 	%f1283, %f1285, %f1166, %f973;
+	fma.rn.f32 	%f1284, %f1285, %f1162, %f975;
+	fma.rn.f32 	%f1285, %f1285, %f1158, %f977;
+	ld.const.u64 	%rd328, [params+112];
+	setp.eq.s64 	%p34, %rd328, 0;
+	@%p34 bra 	$L__BB4_57;
+
+	mul.f32 	%f978, %f1247, %f1226;
+	fma.rn.f32 	%f979, %f1248, %f1223, %f978;
+	mul.f32 	%f980, %f1247, %f1227;
+	fma.rn.f32 	%f981, %f1248, %f1224, %f980;
+	mul.f32 	%f982, %f1247, %f1228;
+	fma.rn.f32 	%f983, %f1248, %f1225, %f982;
+	fma.rn.f32 	%f984, %f1249, %f1220, %f979;
+	fma.rn.f32 	%f985, %f1249, %f1221, %f981;
+	fma.rn.f32 	%f986, %f1249, %f1222, %f983;
+	mul.f32 	%f987, %f984, %f984;
+	fma.rn.f32 	%f988, %f985, %f985, %f987;
+	fma.rn.f32 	%f989, %f986, %f986, %f988;
+	sqrt.rn.f32 	%f990, %f989;
+	div.rn.f32 	%f1247, %f984, %f990;
+	div.rn.f32 	%f1248, %f985, %f990;
+	div.rn.f32 	%f1249, %f986, %f990;
+
+$L__BB4_57:
+	@%p5 bra 	$L__BB4_59;
+
+	mul.f32 	%f991, %f1117, %f1226;
+	fma.rn.f32 	%f992, %f1118, %f1223, %f991;
+	mul.f32 	%f993, %f1117, %f1227;
+	fma.rn.f32 	%f994, %f1118, %f1224, %f993;
+	mul.f32 	%f995, %f1117, %f1228;
+	fma.rn.f32 	%f996, %f1118, %f1225, %f995;
+	fma.rn.f32 	%f997, %f1119, %f1220, %f992;
+	fma.rn.f32 	%f998, %f1119, %f1221, %f994;
+	fma.rn.f32 	%f999, %f1119, %f1222, %f996;
+	mul.f32 	%f1000, %f997, %f997;
+	fma.rn.f32 	%f1001, %f998, %f998, %f1000;
+	fma.rn.f32 	%f1002, %f999, %f999, %f1001;
+	sqrt.rn.f32 	%f1003, %f1002;
+	div.rn.f32 	%f1117, %f997, %f1003;
+	div.rn.f32 	%f1118, %f998, %f1003;
+	div.rn.f32 	%f1119, %f999, %f1003;
+
+$L__BB4_59:
+	ld.const.u64 	%rd329, [params+184];
+	setp.eq.s64 	%p36, %rd329, 0;
+	@%p36 bra 	$L__BB4_61;
+
+	mul.f32 	%f1004, %f1124, %f1164;
+	fma.rn.f32 	%f1005, %f1123, %f1165, %f1004;
+	mul.f32 	%f1006, %f1124, %f1160;
+	fma.rn.f32 	%f1007, %f1123, %f1161, %f1006;
+	mul.f32 	%f1008, %f1124, %f1156;
+	fma.rn.f32 	%f1009, %f1123, %f1157, %f1008;
+	fma.rn.f32 	%f1124, %f1122, %f1166, %f1005;
+	fma.rn.f32 	%f1123, %f1122, %f1162, %f1007;
+	fma.rn.f32 	%f1122, %f1122, %f1158, %f1009;
+	mul.f32 	%f1010, %f1127, %f1164;
+	fma.rn.f32 	%f1011, %f1126, %f1165, %f1010;
+	mul.f32 	%f1012, %f1127, %f1160;
+	fma.rn.f32 	%f1013, %f1126, %f1161, %f1012;
+	mul.f32 	%f1014, %f1127, %f1156;
+	fma.rn.f32 	%f1015, %f1126, %f1157, %f1014;
+	fma.rn.f32 	%f1127, %f1125, %f1166, %f1011;
+	fma.rn.f32 	%f1126, %f1125, %f1162, %f1013;
+	fma.rn.f32 	%f1125, %f1125, %f1158, %f1015;
+
+$L__BB4_61:
+	ld.const.u64 	%rd330, [params+232];
+	ld.const.u64 	%rd331, [params+280];
+	or.b64  	%rd332, %rd330, %rd331;
+	setp.eq.s64 	%p37, %rd332, 0;
+	@%p37 bra 	$L__BB4_63;
+
+	mul.f32 	%f1016, %f1117, %f1164;
+	fma.rn.f32 	%f1017, %f1118, %f1160, %f1016;
+	mul.f32 	%f1018, %f1117, %f1165;
+	fma.rn.f32 	%f1019, %f1118, %f1161, %f1018;
+	mul.f32 	%f1020, %f1117, %f1166;
+	fma.rn.f32 	%f1021, %f1118, %f1162, %f1020;
+	fma.rn.f32 	%f1022, %f1119, %f1156, %f1017;
+	fma.rn.f32 	%f1023, %f1119, %f1157, %f1019;
+	fma.rn.f32 	%f1024, %f1119, %f1158, %f1021;
+	mul.f32 	%f1025, %f1022, %f1022;
+	fma.rn.f32 	%f1026, %f1023, %f1023, %f1025;
+	fma.rn.f32 	%f1027, %f1024, %f1024, %f1026;
+	sqrt.rn.f32 	%f1028, %f1027;
+	div.rn.f32 	%f1029, %f1022, %f1028;
+	div.rn.f32 	%f1030, %f1023, %f1028;
+	div.rn.f32 	%f1031, %f1024, %f1028;
+	mul.f32 	%f1032, %f1029, %f1226;
+	mul.f32 	%f1033, %f1029, %f1227;
+	mul.f32 	%f1034, %f1029, %f1228;
+	fma.rn.f32 	%f1035, %f1030, %f1223, %f1032;
+	fma.rn.f32 	%f1036, %f1030, %f1224, %f1033;
+	fma.rn.f32 	%f1037, %f1030, %f1225, %f1034;
+	fma.rn.f32 	%f1038, %f1031, %f1220, %f1035;
+	fma.rn.f32 	%f1039, %f1031, %f1221, %f1036;
+	fma.rn.f32 	%f1040, %f1031, %f1222, %f1037;
+	mul.f32 	%f1041, %f1038, %f1038;
+	fma.rn.f32 	%f1042, %f1039, %f1039, %f1041;
+	fma.rn.f32 	%f1043, %f1040, %f1040, %f1042;
+	sqrt.rn.f32 	%f1044, %f1043;
+	rcp.rn.f32 	%f1045, %f1044;
+	mul.f32 	%f1046, %f1045, %f1038;
+	mul.f32 	%f1047, %f1045, %f1039;
+	mul.f32 	%f1048, %f1045, %f1040;
+	mul.f32 	%f1049, %f1114, %f1226;
+	fma.rn.f32 	%f1050, %f1115, %f1223, %f1049;
+	mul.f32 	%f1051, %f1114, %f1227;
+	fma.rn.f32 	%f1052, %f1115, %f1224, %f1051;
+	mul.f32 	%f1053, %f1114, %f1228;
+	fma.rn.f32 	%f1054, %f1115, %f1225, %f1053;
+	fma.rn.f32 	%f1055, %f1116, %f1220, %f1050;
+	fma.rn.f32 	%f1056, %f1116, %f1221, %f1052;
+	fma.rn.f32 	%f1057, %f1116, %f1222, %f1054;
+	mul.f32 	%f1058, %f1055, %f1045;
+	mul.f32 	%f1059, %f1056, %f1045;
+	mul.f32 	%f1060, %f1057, %f1045;
+	mul.f32 	%f1061, %f1111, %f1226;
+	fma.rn.f32 	%f1062, %f1112, %f1223, %f1061;
+	mul.f32 	%f1063, %f1111, %f1227;
+	fma.rn.f32 	%f1064, %f1112, %f1224, %f1063;
+	mul.f32 	%f1065, %f1111, %f1228;
+	fma.rn.f32 	%f1066, %f1112, %f1225, %f1065;
+	fma.rn.f32 	%f1067, %f1113, %f1220, %f1062;
+	fma.rn.f32 	%f1068, %f1113, %f1221, %f1064;
+	fma.rn.f32 	%f1069, %f1113, %f1222, %f1066;
+	mul.f32 	%f1070, %f1067, %f1045;
+	mul.f32 	%f1071, %f1068, %f1045;
+	mul.f32 	%f1072, %f1069, %f1045;
+	mul.f32 	%f1073, %f1046, %f1058;
+	fma.rn.f32 	%f1074, %f1047, %f1059, %f1073;
+	fma.rn.f32 	%f1075, %f1048, %f1060, %f1074;
+	mul.f32 	%f1076, %f1046, %f1075;
+	mul.f32 	%f1077, %f1047, %f1075;
+	mul.f32 	%f1078, %f1048, %f1075;
+	sub.f32 	%f1114, %f1058, %f1076;
+	sub.f32 	%f1115, %f1059, %f1077;
+	sub.f32 	%f1116, %f1060, %f1078;
+	mul.f32 	%f1079, %f1046, %f1070;
+	fma.rn.f32 	%f1080, %f1047, %f1071, %f1079;
+	fma.rn.f32 	%f1081, %f1048, %f1072, %f1080;
+	mul.f32 	%f1082, %f1046, %f1081;
+	mul.f32 	%f1083, %f1047, %f1081;
+	mul.f32 	%f1084, %f1048, %f1081;
+	sub.f32 	%f1111, %f1070, %f1082;
+	sub.f32 	%f1112, %f1071, %f1083;
+	sub.f32 	%f1113, %f1072, %f1084;
+
+$L__BB4_63:
+	st.global.u32 	[%rd20], %r30;
+
+$L__BB4_64:
+	ld.const.u64 	%rd434, [params+96];
+	setp.eq.s64 	%p44, %rd434, 0;
+	cvt.u64.u32 	%rd431, %r1;
+	ld.const.u64 	%rd333, [params+328];
+	cvta.to.global.u64 	%rd334, %rd333;
+	shl.b64 	%rd335, %rd431, 3;
+	add.s64 	%rd336, %rd334, %rd335;
+	st.global.u64 	[%rd336], %rd18;
+	ld.const.u64 	%rd337, [params+336];
+	cvta.to.global.u64 	%rd338, %rd337;
+	shl.b64 	%rd339, %rd431, 2;
+	add.s64 	%rd340, %rd338, %rd339;
+	st.global.u32 	[%rd340], %r22;
+	ld.const.u64 	%rd341, [params+160];
+	cvta.to.global.u64 	%rd342, %rd341;
+	add.s64 	%rd343, %rd342, %rd339;
+	st.global.f32 	[%rd343], %f1283;
+	ld.const.u64 	%rd344, [params+168];
+	cvta.to.global.u64 	%rd345, %rd344;
+	add.s64 	%rd346, %rd345, %rd339;
+	st.global.f32 	[%rd346], %f1284;
+	ld.const.u64 	%rd347, [params+176];
 	cvta.to.global.u64 	%rd348, %rd347;
-	add.s64 	%rd349, %rd348, %rd345;
-	st.global.f32 	[%rd349], %f1256;
-	ld.const.u64 	%rd350, [params+168];
+	add.s64 	%rd349, %rd348, %rd339;
+	st.global.f32 	[%rd349], %f1285;
+	ld.const.u64 	%rd350, [params+72];
 	cvta.to.global.u64 	%rd351, %rd350;
-	add.s64 	%rd352, %rd351, %rd345;
-	st.global.f32 	[%rd352], %f1257;
-	ld.const.u64 	%rd353, [params+176];
-	cvta.to.global.u64 	%rd354, %rd353;
-	add.s64 	%rd355, %rd354, %rd345;
-	st.global.f32 	[%rd355], %f1258;
-	ld.const.u64 	%rd356, [params+72];
+	add.s64 	%rd352, %rd351, %rd339;
+	st.global.f32 	[%rd352], %f437;
+	@%p44 bra 	$L__BB4_66;
+
+	ld.const.u64 	%rd432, [params+96];
+	cvta.to.global.u64 	%rd353, %rd432;
+	add.s64 	%rd355, %rd353, %rd339;
+	st.global.f32 	[%rd355], %f1120;
+	ld.const.u64 	%rd356, [params+104];
 	cvta.to.global.u64 	%rd357, %rd356;
-	add.s64 	%rd358, %rd357, %rd345;
-	st.global.f32 	[%rd358], %f428;
-	@%p44 bra 	BB4_67;
-
-	ld.const.u64 	%rd438, [params+96];
-	cvta.to.global.u64 	%rd359, %rd438;
-	add.s64 	%rd361, %rd359, %rd345;
-	st.global.f32 	[%rd361], %f1099;
-	ld.const.u64 	%rd362, [params+104];
+	add.s64 	%rd358, %rd357, %rd339;
+	st.global.f32 	[%rd358], %f1121;
+
+$L__BB4_66:
+	ld.const.u64 	%rd35, [params+112];
+	setp.eq.s64 	%p39, %rd35, 0;
+	@%p39 bra 	$L__BB4_68;
+
+	cvta.to.global.u64 	%rd359, %rd35;
+	add.s64 	%rd361, %rd359, %rd339;
+	st.global.f32 	[%rd361], %f1247;
+	ld.const.u64 	%rd362, [params+120];
 	cvta.to.global.u64 	%rd363, %rd362;
-	add.s64 	%rd364, %rd363, %rd345;
-	st.global.f32 	[%rd364], %f1100;
-
-BB4_67:
-	ld.const.u64 	%rd36, [params+112];
-	setp.eq.s64	%p38, %rd36, 0;
-	@%p38 bra 	BB4_69;
-
-	cvta.to.global.u64 	%rd365, %rd36;
-	add.s64 	%rd367, %rd365, %rd345;
-	st.global.f32 	[%rd367], %f1250;
-	ld.const.u64 	%rd368, [params+120];
-	cvta.to.global.u64 	%rd369, %rd368;
-	add.s64 	%rd370, %rd369, %rd345;
-	st.global.f32 	[%rd370], %f1251;
-	ld.const.u64 	%rd371, [params+128];
+	add.s64 	%rd364, %rd363, %rd339;
+	st.global.f32 	[%rd364], %f1248;
+	ld.const.u64 	%rd365, [params+128];
+	cvta.to.global.u64 	%rd366, %rd365;
+	add.s64 	%rd367, %rd366, %rd339;
+	st.global.f32 	[%rd367], %f1249;
+
+$L__BB4_68:
+	@%p5 bra 	$L__BB4_70;
+
+	ld.const.u64 	%rd433, [params+136];
+	cvta.to.global.u64 	%rd368, %rd433;
+	add.s64 	%rd370, %rd368, %rd339;
+	st.global.f32 	[%rd370], %f1117;
+	ld.const.u64 	%rd371, [params+144];
 	cvta.to.global.u64 	%rd372, %rd371;
-	add.s64 	%rd373, %rd372, %rd345;
-	st.global.f32 	[%rd373], %f1252;
-
-BB4_69:
-	@%p5 bra 	BB4_71;
-
-	ld.const.u64 	%rd439, [params+136];
-	cvta.to.global.u64 	%rd374, %rd439;
-	add.s64 	%rd376, %rd374, %rd345;
-	st.global.f32 	[%rd376], %f1092;
-	ld.const.u64 	%rd377, [params+144];
-	cvta.to.global.u64 	%rd378, %rd377;
-	add.s64 	%rd379, %rd378, %rd345;
-	st.global.f32 	[%rd379], %f1091;
-	ld.const.u64 	%rd380, [params+152];
+	add.s64 	%rd373, %rd372, %rd339;
+	st.global.f32 	[%rd373], %f1118;
+	ld.const.u64 	%rd374, [params+152];
+	cvta.to.global.u64 	%rd375, %rd374;
+	add.s64 	%rd376, %rd375, %rd339;
+	st.global.f32 	[%rd376], %f1119;
+
+$L__BB4_70:
+	ld.const.u64 	%rd36, [params+184];
+	setp.eq.s64 	%p41, %rd36, 0;
+	@%p41 bra 	$L__BB4_72;
+
+	cvta.to.global.u64 	%rd377, %rd36;
+	add.s64 	%rd379, %rd377, %rd339;
+	st.global.f32 	[%rd379], %f1124;
+	ld.const.u64 	%rd380, [params+192];
 	cvta.to.global.u64 	%rd381, %rd380;
-	add.s64 	%rd382, %rd381, %rd345;
-	st.global.f32 	[%rd382], %f1090;
-
-BB4_71:
-	ld.const.u64 	%rd37, [params+184];
-	setp.eq.s64	%p40, %rd37, 0;
-	@%p40 bra 	BB4_73;
-
-	cvta.to.global.u64 	%rd383, %rd37;
-	add.s64 	%rd385, %rd383, %rd345;
-	st.global.f32 	[%rd385], %f1247;
-	ld.const.u64 	%rd386, [params+192];
+	add.s64 	%rd382, %rd381, %rd339;
+	st.global.f32 	[%rd382], %f1123;
+	ld.const.u64 	%rd383, [params+200];
+	cvta.to.global.u64 	%rd384, %rd383;
+	add.s64 	%rd385, %rd384, %rd339;
+	st.global.f32 	[%rd385], %f1122;
+	ld.const.u64 	%rd386, [params+208];
 	cvta.to.global.u64 	%rd387, %rd386;
-	add.s64 	%rd388, %rd387, %rd345;
-	st.global.f32 	[%rd388], %f1248;
-	ld.const.u64 	%rd389, [params+200];
+	add.s64 	%rd388, %rd387, %rd339;
+	st.global.f32 	[%rd388], %f1127;
+	ld.const.u64 	%rd389, [params+216];
 	cvta.to.global.u64 	%rd390, %rd389;
-	add.s64 	%rd391, %rd390, %rd345;
-	st.global.f32 	[%rd391], %f1249;
-	ld.const.u64 	%rd392, [params+208];
+	add.s64 	%rd391, %rd390, %rd339;
+	st.global.f32 	[%rd391], %f1126;
+	ld.const.u64 	%rd392, [params+224];
 	cvta.to.global.u64 	%rd393, %rd392;
-	add.s64 	%rd394, %rd393, %rd345;
-	st.global.f32 	[%rd394], %f1244;
-	ld.const.u64 	%rd395, [params+216];
-	cvta.to.global.u64 	%rd396, %rd395;
-	add.s64 	%rd397, %rd396, %rd345;
-	st.global.f32 	[%rd397], %f1245;
-	ld.const.u64 	%rd398, [params+224];
+	add.s64 	%rd394, %rd393, %rd339;
+	st.global.f32 	[%rd394], %f1125;
+
+$L__BB4_72:
+	ld.const.u64 	%rd37, [params+232];
+	setp.eq.s64 	%p42, %rd37, 0;
+	@%p42 bra 	$L__BB4_74;
+
+	cvta.to.global.u64 	%rd395, %rd37;
+	add.s64 	%rd397, %rd395, %rd339;
+	st.global.f32 	[%rd397], %f1114;
+	ld.const.u64 	%rd398, [params+240];
 	cvta.to.global.u64 	%rd399, %rd398;
-	add.s64 	%rd400, %rd399, %rd345;
-	st.global.f32 	[%rd400], %f1246;
-
-BB4_73:
-	ld.const.u64 	%rd38, [params+232];
-	setp.eq.s64	%p41, %rd38, 0;
-	@%p41 bra 	BB4_75;
-
-	cvta.to.global.u64 	%rd401, %rd38;
-	add.s64 	%rd403, %rd401, %rd345;
-	st.global.f32 	[%rd403], %f1087;
-	ld.const.u64 	%rd404, [params+240];
+	add.s64 	%rd400, %rd399, %rd339;
+	st.global.f32 	[%rd400], %f1115;
+	ld.const.u64 	%rd401, [params+248];
+	cvta.to.global.u64 	%rd402, %rd401;
+	add.s64 	%rd403, %rd402, %rd339;
+	st.global.f32 	[%rd403], %f1116;
+	ld.const.u64 	%rd404, [params+256];
 	cvta.to.global.u64 	%rd405, %rd404;
-	add.s64 	%rd406, %rd405, %rd345;
-	st.global.f32 	[%rd406], %f1088;
-	ld.const.u64 	%rd407, [params+248];
+	add.s64 	%rd406, %rd405, %rd339;
+	st.global.f32 	[%rd406], %f1111;
+	ld.const.u64 	%rd407, [params+264];
 	cvta.to.global.u64 	%rd408, %rd407;
-	add.s64 	%rd409, %rd408, %rd345;
-	st.global.f32 	[%rd409], %f1089;
-	ld.const.u64 	%rd410, [params+256];
+	add.s64 	%rd409, %rd408, %rd339;
+	st.global.f32 	[%rd409], %f1112;
+	ld.const.u64 	%rd410, [params+272];
 	cvta.to.global.u64 	%rd411, %rd410;
-	add.s64 	%rd412, %rd411, %rd345;
-	st.global.f32 	[%rd412], %f1084;
-	ld.const.u64 	%rd413, [params+264];
-	cvta.to.global.u64 	%rd414, %rd413;
-	add.s64 	%rd415, %rd414, %rd345;
-	st.global.f32 	[%rd415], %f1085;
-	ld.const.u64 	%rd416, [params+272];
+	add.s64 	%rd412, %rd411, %rd339;
+	st.global.f32 	[%rd412], %f1113;
+
+$L__BB4_74:
+	ld.const.u64 	%rd38, [params+280];
+	setp.eq.s64 	%p43, %rd38, 0;
+	@%p43 bra 	$L__BB4_76;
+
+	cvta.to.global.u64 	%rd413, %rd38;
+	add.s64 	%rd415, %rd413, %rd339;
+	st.global.f32 	[%rd415], %f1114;
+	ld.const.u64 	%rd416, [params+288];
 	cvta.to.global.u64 	%rd417, %rd416;
-	add.s64 	%rd418, %rd417, %rd345;
-	st.global.f32 	[%rd418], %f1086;
-
-BB4_75:
-	ld.const.u64 	%rd39, [params+280];
-	setp.eq.s64	%p42, %rd39, 0;
-	@%p42 bra 	BB4_77;
-
-	cvta.to.global.u64 	%rd419, %rd39;
-	add.s64 	%rd421, %rd419, %rd345;
-	st.global.f32 	[%rd421], %f1087;
-	ld.const.u64 	%rd422, [params+288];
+	add.s64 	%rd418, %rd417, %rd339;
+	st.global.f32 	[%rd418], %f1115;
+	ld.const.u64 	%rd419, [params+296];
+	cvta.to.global.u64 	%rd420, %rd419;
+	add.s64 	%rd421, %rd420, %rd339;
+	st.global.f32 	[%rd421], %f1116;
+	ld.const.u64 	%rd422, [params+304];
 	cvta.to.global.u64 	%rd423, %rd422;
-	add.s64 	%rd424, %rd423, %rd345;
-	st.global.f32 	[%rd424], %f1088;
-	ld.const.u64 	%rd425, [params+296];
+	add.s64 	%rd424, %rd423, %rd339;
+	st.global.f32 	[%rd424], %f1111;
+	ld.const.u64 	%rd425, [params+312];
 	cvta.to.global.u64 	%rd426, %rd425;
-	add.s64 	%rd427, %rd426, %rd345;
-	st.global.f32 	[%rd427], %f1089;
-	ld.const.u64 	%rd428, [params+304];
+	add.s64 	%rd427, %rd426, %rd339;
+	st.global.f32 	[%rd427], %f1112;
+	ld.const.u64 	%rd428, [params+320];
 	cvta.to.global.u64 	%rd429, %rd428;
-	add.s64 	%rd430, %rd429, %rd345;
-	st.global.f32 	[%rd430], %f1084;
-	ld.const.u64 	%rd431, [params+312];
-	cvta.to.global.u64 	%rd432, %rd431;
-	add.s64 	%rd433, %rd432, %rd345;
-	st.global.f32 	[%rd433], %f1085;
-	ld.const.u64 	%rd434, [params+320];
-	cvta.to.global.u64 	%rd435, %rd434;
-	add.s64 	%rd436, %rd435, %rd345;
-	st.global.f32 	[%rd436], %f1086;
-
-BB4_77:
+	add.s64 	%rd430, %rd429, %rd339;
+	st.global.f32 	[%rd430], %f1113;
+
+$L__BB4_76:
 	ret;
-}
 
+}
 	// .globl	__intersection__rectangle
-.visible .entry __intersection__rectangle(
-
-)
+.visible .entry __intersection__rectangle()
 {
-	.reg .pred 	%p<22>;
-	.reg .b16 	%rs<9>;
-	.reg .f32 	%f<932>;
-	.reg .b32 	%r<315>;
-	.reg .b64 	%rd<265>;
-
-
-	// inline asm
-	call (%rd18), _optix_get_sbt_data_ptr_64, ();
-	// inline asm
-	ld.u64 	%rd1, [%rd18+8];
-	// inline asm
-	call (%f315), _optix_get_world_ray_origin_x, ();
-	// inline asm
-	// inline asm
-	call (%f316), _optix_get_world_ray_origin_y, ();
-	// inline asm
-	// inline asm
-	call (%f880), _optix_get_world_ray_origin_z, ();
-	// inline asm
-	// inline asm
-	call (%r8), _optix_get_transform_list_size, ();
-	// inline asm
-	setp.eq.s32	%p1, %r8, 0;
-	@%p1 bra 	BB5_1;
-
-	mov.u32 	%r313, 0;
-	// inline asm
-	call (%f318), _optix_get_ray_time, ();
-	// inline asm
-
-BB5_3:
+	.reg .pred 	%p<23>;
+	.reg .f32 	%f<977>;
+	.reg .b32 	%r<318>;
+	.reg .b64 	%rd<258>;
+
+
+	// begin inline asm
+	call (%rd16), _optix_get_sbt_data_ptr_64, ();
+	// end inline asm
+	ld.u64 	%rd1, [%rd16+8];
+	// begin inline asm
+	call (%f916), _optix_get_world_ray_origin_x, ();
+	// end inline asm
+	// begin inline asm
+	call (%f917), _optix_get_world_ray_origin_y, ();
+	// end inline asm
+	// begin inline asm
+	call (%f918), _optix_get_world_ray_origin_z, ();
+	// end inline asm
+	// begin inline asm
+	call (%r9), _optix_get_transform_list_size, ();
+	// end inline asm
+	setp.eq.s32 	%p1, %r9, 0;
+	@%p1 bra 	$L__BB5_21;
+
+	// begin inline asm
+	call (%r10), _optix_get_transform_list_size, ();
+	// end inline asm
+	// begin inline asm
+	call (%f345), _optix_get_ray_time, ();
+	// end inline asm
+	setp.eq.s32 	%p2, %r10, 0;
+	@%p2 bra 	$L__BB5_19;
+
+	mov.u32 	%r316, 0;
+
+$L__BB5_3:
 	.pragma "nounroll";
-	// inline asm
-	call (%rd19), _optix_get_transform_list_handle, (%r313);
-	// inline asm
-	// inline asm
-	call (%r11), _optix_get_transform_type_from_handle, (%rd19);
-	// inline asm
-	and.b32  	%r12, %r11, -2;
-	setp.eq.s32	%p2, %r12, 2;
-	@%p2 bra 	BB5_9;
-	bra.uni 	BB5_4;
-
-BB5_9:
-	setp.eq.s32	%p5, %r11, 2;
-	@%p5 bra 	BB5_13;
-	bra.uni 	BB5_10;
-
-BB5_13:
-	// inline asm
-	call (%rd93), _optix_get_matrix_motion_transform_from_handle, (%rd19);
-	// inline asm
-	// inline asm
-	cvta.to.global.u64 %rd95, %rd93;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r100,%r101,%r102,%r103}, [%rd95];
-	// inline asm
-	mov.b32	{%rs3, %rs4}, %r102;
-	add.s64 	%rd99, %rd93, 16;
-	// inline asm
-	cvta.to.global.u64 %rd98, %rd99;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r104,%r105,%r106,%r107}, [%rd98];
-	// inline asm
-	add.s64 	%rd102, %rd93, 32;
-	// inline asm
-	cvta.to.global.u64 %rd101, %rd102;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r108,%r109,%r110,%r111}, [%rd101];
-	// inline asm
-	add.s64 	%rd105, %rd93, 48;
-	// inline asm
-	cvta.to.global.u64 %rd104, %rd105;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r112,%r113,%r114,%r115}, [%rd104];
-	// inline asm
-	add.s64 	%rd108, %rd93, 64;
-	// inline asm
-	cvta.to.global.u64 %rd107, %rd108;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r116,%r117,%r118,%r119}, [%rd107];
-	// inline asm
-	add.s64 	%rd111, %rd93, 80;
-	// inline asm
-	cvta.to.global.u64 %rd110, %rd111;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r120,%r121,%r122,%r123}, [%rd110];
-	// inline asm
-	add.s64 	%rd114, %rd93, 96;
-	// inline asm
-	cvta.to.global.u64 %rd113, %rd114;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r124,%r125,%r126,%r127}, [%rd113];
-	// inline asm
-	add.s64 	%rd117, %rd93, 112;
-	// inline asm
-	cvta.to.global.u64 %rd116, %rd117;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r128,%r129,%r130,%r131}, [%rd116];
-	// inline asm
-	mov.b32 	 %f445, %r103;
-	mov.b32 	 %f446, %r104;
-	cvt.u32.u16	%r144, %rs3;
-	add.s32 	%r145, %r144, -1;
-	cvt.rn.f32.s32	%f447, %r145;
-	sub.f32 	%f448, %f318, %f445;
-	mul.f32 	%f449, %f448, %f447;
-	sub.f32 	%f450, %f446, %f445;
-	div.rn.f32 	%f451, %f449, %f450;
-	min.f32 	%f452, %f447, %f451;
-	mov.f32 	%f453, 0f00000000;
-	max.f32 	%f454, %f453, %f452;
-	cvt.rmi.f32.f32	%f455, %f454;
-	cvt.rzi.s32.f32	%r146, %f455;
-	mul.wide.s32 	%rd128, %r146, 48;
-	add.s64 	%rd120, %rd102, %rd128;
-	// inline asm
-	cvta.to.global.u64 %rd119, %rd120;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r132,%r133,%r134,%r135}, [%rd119];
-	// inline asm
-	mov.b32 	 %f852, %r132;
-	mov.b32 	 %f853, %r133;
-	mov.b32 	 %f854, %r134;
-	mov.b32 	 %f855, %r135;
-	add.s64 	%rd123, %rd120, 16;
-	// inline asm
-	cvta.to.global.u64 %rd122, %rd123;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r136,%r137,%r138,%r139}, [%rd122];
-	// inline asm
-	mov.b32 	 %f848, %r136;
-	mov.b32 	 %f849, %r137;
-	mov.b32 	 %f850, %r138;
-	mov.b32 	 %f851, %r139;
-	add.s64 	%rd126, %rd120, 32;
-	// inline asm
+	// begin inline asm
+	call (%rd17), _optix_get_transform_list_handle, (%r316);
+	// end inline asm
+	// begin inline asm
+	call (%r13), _optix_get_transform_type_from_handle, (%rd17);
+	// end inline asm
+	or.b32  	%r14, %r13, 1;
+	setp.eq.s32 	%p3, %r14, 3;
+	@%p3 bra 	$L__BB5_9;
+	bra.uni 	$L__BB5_4;
+
+$L__BB5_9:
+	setp.eq.s32 	%p6, %r13, 2;
+	@%p6 bra 	$L__BB5_13;
+	bra.uni 	$L__BB5_10;
+
+$L__BB5_13:
+	// begin inline asm
+	call (%rd89), _optix_get_matrix_motion_transform_from_handle, (%rd17);
+	// end inline asm
+	// begin inline asm
+	cvta.to.global.u64 %rd91, %rd89;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r102,%r103,%r104,%r105}, [%rd91];
+	// end inline asm
+	add.s64 	%rd95, %rd89, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd94, %rd95;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r106,%r107,%r108,%r109}, [%rd94];
+	// end inline asm
+	add.s64 	%rd98, %rd89, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd97, %rd98;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r110,%r111,%r112,%r113}, [%rd97];
+	// end inline asm
+	add.s64 	%rd101, %rd89, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd100, %rd101;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r114,%r115,%r116,%r117}, [%rd100];
+	// end inline asm
+	add.s64 	%rd104, %rd89, 64;
+	// begin inline asm
+	cvta.to.global.u64 %rd103, %rd104;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r118,%r119,%r120,%r121}, [%rd103];
+	// end inline asm
+	add.s64 	%rd107, %rd89, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd106, %rd107;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r122,%r123,%r124,%r125}, [%rd106];
+	// end inline asm
+	add.s64 	%rd110, %rd89, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd109, %rd110;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r126,%r127,%r128,%r129}, [%rd109];
+	// end inline asm
+	add.s64 	%rd113, %rd89, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd112, %rd113;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r130,%r131,%r132,%r133}, [%rd112];
+	// end inline asm
+	mov.b32 	%f473, %r105;
+	mov.b32 	%f474, %r106;
+	and.b32  	%r146, %r104, 65535;
+	add.s32 	%r147, %r146, -1;
+	cvt.rn.f32.s32 	%f475, %r147;
+	sub.f32 	%f476, %f345, %f473;
+	mul.f32 	%f477, %f476, %f475;
+	sub.f32 	%f478, %f474, %f473;
+	div.rn.f32 	%f479, %f477, %f478;
+	min.f32 	%f480, %f475, %f479;
+	mov.f32 	%f481, 0f00000000;
+	max.f32 	%f482, %f481, %f480;
+	cvt.rmi.f32.f32 	%f483, %f482;
+	sub.f32 	%f90, %f482, %f483;
+	cvt.rzi.s32.f32 	%r148, %f483;
+	mul.wide.s32 	%rd124, %r148, 48;
+	add.s64 	%rd116, %rd98, %rd124;
+	// begin inline asm
+	cvta.to.global.u64 %rd115, %rd116;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r134,%r135,%r136,%r137}, [%rd115];
+	// end inline asm
+	mov.b32 	%f871, %r134;
+	mov.b32 	%f870, %r135;
+	mov.b32 	%f869, %r136;
+	mov.b32 	%f868, %r137;
+	add.s64 	%rd119, %rd116, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd118, %rd119;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r138,%r139,%r140,%r141}, [%rd118];
+	// end inline asm
+	mov.b32 	%f875, %r138;
+	mov.b32 	%f874, %r139;
+	mov.b32 	%f873, %r140;
+	mov.b32 	%f872, %r141;
+	add.s64 	%rd122, %rd116, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd121, %rd122;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r142,%r143,%r144,%r145}, [%rd121];
+	// end inline asm
+	mov.b32 	%f879, %r142;
+	mov.b32 	%f878, %r143;
+	mov.b32 	%f877, %r144;
+	mov.b32 	%f876, %r145;
+	setp.leu.f32 	%p8, %f90, 0f00000000;
+	@%p8 bra 	$L__BB5_15;
+
+	cvt.rmi.f32.f32 	%f839, %f482;
+	cvt.rzi.s32.f32 	%r315, %f839;
+	cvt.s64.s32 	%rd255, %r315;
+	mov.f32 	%f484, 0f3F800000;
+	sub.f32 	%f485, %f484, %f90;
+	mul.lo.s64 	%rd134, %rd255, 48;
+	add.s64 	%rd135, %rd89, %rd134;
+	add.s64 	%rd126, %rd135, 80;
+	// begin inline asm
 	cvta.to.global.u64 %rd125, %rd126;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r140,%r141,%r142,%r143}, [%rd125];
-	// inline asm
-	sub.f32 	%f98, %f454, %f455;
-	mov.b32 	 %f844, %r140;
-	mov.b32 	 %f845, %r141;
-	mov.b32 	 %f846, %r142;
-	mov.b32 	 %f847, %r143;
-	setp.leu.f32	%p7, %f98, 0f00000000;
-	@%p7 bra 	BB5_15;
-
-	cvt.rmi.f32.f32	%f815, %f454;
-	cvt.rzi.s32.f32	%r312, %f815;
-	cvt.s64.s32	%rd262, %r312;
-	mul.lo.s64 	%rd138, %rd262, 48;
-	add.s64 	%rd139, %rd93, %rd138;
-	add.s64 	%rd130, %rd139, 80;
-	// inline asm
-	cvta.to.global.u64 %rd129, %rd130;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r147,%r148,%r149,%r150}, [%rd129];
-	// inline asm
-	mov.b32 	 %f456, %r147;
-	mov.b32 	 %f457, %r148;
-	mov.b32 	 %f458, %r149;
-	mov.b32 	 %f459, %r150;
-	add.s64 	%rd133, %rd139, 96;
-	// inline asm
-	cvta.to.global.u64 %rd132, %rd133;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r151,%r152,%r153,%r154}, [%rd132];
-	// inline asm
-	mov.b32 	 %f460, %r151;
-	mov.b32 	 %f461, %r152;
-	mov.b32 	 %f462, %r153;
-	mov.b32 	 %f463, %r154;
-	add.s64 	%rd136, %rd139, 112;
-	// inline asm
-	cvta.to.global.u64 %rd135, %rd136;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r155,%r156,%r157,%r158}, [%rd135];
-	// inline asm
-	mov.f32 	%f464, 0f3F800000;
-	sub.f32 	%f465, %f464, %f98;
-	mul.f32 	%f466, %f98, %f456;
-	mul.f32 	%f467, %f98, %f457;
-	mul.f32 	%f468, %f98, %f458;
-	mul.f32 	%f469, %f98, %f459;
-	fma.rn.f32 	%f852, %f465, %f852, %f466;
-	fma.rn.f32 	%f853, %f465, %f853, %f467;
-	fma.rn.f32 	%f854, %f465, %f854, %f468;
-	fma.rn.f32 	%f855, %f465, %f855, %f469;
-	mul.f32 	%f470, %f98, %f460;
-	mul.f32 	%f471, %f98, %f461;
-	mul.f32 	%f472, %f98, %f462;
-	mul.f32 	%f473, %f98, %f463;
-	fma.rn.f32 	%f848, %f465, %f848, %f470;
-	fma.rn.f32 	%f849, %f465, %f849, %f471;
-	fma.rn.f32 	%f850, %f465, %f850, %f472;
-	fma.rn.f32 	%f851, %f465, %f851, %f473;
-	mov.b32 	 %f474, %r155;
-	mov.b32 	 %f475, %r156;
-	mov.b32 	 %f476, %r157;
-	mov.b32 	 %f477, %r158;
-	mul.f32 	%f478, %f98, %f474;
-	mul.f32 	%f479, %f98, %f475;
-	mul.f32 	%f480, %f98, %f476;
-	mul.f32 	%f481, %f98, %f477;
-	fma.rn.f32 	%f844, %f465, %f844, %f478;
-	fma.rn.f32 	%f845, %f465, %f845, %f479;
-	fma.rn.f32 	%f846, %f465, %f846, %f480;
-	fma.rn.f32 	%f847, %f465, %f847, %f481;
-	bra.uni 	BB5_15;
-
-BB5_4:
-	mov.f32 	%f856, 0f00000000;
-	mov.f32 	%f858, 0f3F800000;
-	setp.eq.s32	%p3, %r11, 4;
-	@%p3 bra 	BB5_7;
-	bra.uni 	BB5_5;
-
-BB5_7:
-	// inline asm
-	call (%rd263), _optix_get_instance_inverse_transform_from_handle, (%rd19);
-	// inline asm
-	bra.uni 	BB5_8;
-
-BB5_10:
-	// inline asm
-	call (%rd34), _optix_get_srt_motion_transform_from_handle, (%rd19);
-	// inline asm
-	// inline asm
-	cvta.to.global.u64 %rd36, %rd34;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r25,%r26,%r27,%r28}, [%rd36];
-	// inline asm
-	mov.b32	{%rs1, %rs2}, %r27;
-	add.s64 	%rd40, %rd34, 16;
-	// inline asm
-	cvta.to.global.u64 %rd39, %rd40;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r29,%r30,%r31,%r32}, [%rd39];
-	// inline asm
-	add.s64 	%rd43, %rd34, 32;
-	// inline asm
-	cvta.to.global.u64 %rd42, %rd43;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r33,%r34,%r35,%r36}, [%rd42];
-	// inline asm
-	add.s64 	%rd46, %rd34, 48;
-	// inline asm
-	cvta.to.global.u64 %rd45, %rd46;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r37,%r38,%r39,%r40}, [%rd45];
-	// inline asm
-	add.s64 	%rd49, %rd34, 64;
-	// inline asm
-	cvta.to.global.u64 %rd48, %rd49;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r41,%r42,%r43,%r44}, [%rd48];
-	// inline asm
-	add.s64 	%rd52, %rd34, 80;
-	// inline asm
-	cvta.to.global.u64 %rd51, %rd52;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r45,%r46,%r47,%r48}, [%rd51];
-	// inline asm
-	add.s64 	%rd55, %rd34, 96;
-	// inline asm
-	cvta.to.global.u64 %rd54, %rd55;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r49,%r50,%r51,%r52}, [%rd54];
-	// inline asm
-	add.s64 	%rd58, %rd34, 112;
-	// inline asm
-	cvta.to.global.u64 %rd57, %rd58;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r53,%r54,%r55,%r56}, [%rd57];
-	// inline asm
-	add.s64 	%rd61, %rd34, 128;
-	// inline asm
-	cvta.to.global.u64 %rd60, %rd61;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r57,%r58,%r59,%r60}, [%rd60];
-	// inline asm
-	add.s64 	%rd64, %rd34, 144;
-	// inline asm
-	cvta.to.global.u64 %rd63, %rd64;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r61,%r62,%r63,%r64}, [%rd63];
-	// inline asm
-	mov.b32 	 %f332, %r28;
-	mov.b32 	 %f333, %r29;
-	cvt.u32.u16	%r81, %rs1;
-	add.s32 	%r82, %r81, -1;
-	cvt.rn.f32.s32	%f334, %r82;
-	sub.f32 	%f335, %f318, %f332;
-	mul.f32 	%f336, %f335, %f334;
-	sub.f32 	%f337, %f333, %f332;
-	div.rn.f32 	%f338, %f336, %f337;
-	min.f32 	%f339, %f334, %f338;
-	mov.f32 	%f340, 0f00000000;
-	max.f32 	%f341, %f340, %f339;
-	cvt.rmi.f32.f32	%f342, %f341;
-	cvt.rzi.s32.f32	%r83, %f342;
-	mul.wide.s32 	%rd78, %r83, 64;
-	add.s64 	%rd67, %rd43, %rd78;
-	// inline asm
-	cvta.to.global.u64 %rd66, %rd67;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r65,%r66,%r67,%r68}, [%rd66];
-	// inline asm
-	mov.b32 	 %f828, %r65;
-	mov.b32 	 %f829, %r66;
-	mov.b32 	 %f830, %r67;
-	mov.b32 	 %f831, %r68;
-	add.s64 	%rd70, %rd67, 16;
-	// inline asm
-	cvta.to.global.u64 %rd69, %rd70;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r69,%r70,%r71,%r72}, [%rd69];
-	// inline asm
-	mov.b32 	 %f832, %r69;
-	mov.b32 	 %f833, %r70;
-	mov.b32 	 %f834, %r71;
-	mov.b32 	 %f835, %r72;
-	add.s64 	%rd73, %rd67, 32;
-	// inline asm
-	cvta.to.global.u64 %rd72, %rd73;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r73,%r74,%r75,%r76}, [%rd72];
-	// inline asm
-	sub.f32 	%f37, %f341, %f342;
-	mov.b32 	 %f836, %r73;
-	mov.b32 	 %f837, %r74;
-	mov.b32 	 %f838, %r75;
-	mov.b32 	 %f839, %r76;
-	add.s64 	%rd76, %rd67, 48;
-	// inline asm
-	cvta.to.global.u64 %rd75, %rd76;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r77,%r78,%r79,%r80}, [%rd75];
-	// inline asm
-	mov.b32 	 %f840, %r77;
-	mov.b32 	 %f841, %r78;
-	mov.b32 	 %f842, %r79;
-	mov.b32 	 %f843, %r80;
-	setp.leu.f32	%p6, %f37, 0f00000000;
-	@%p6 bra 	BB5_12;
-
-	cvt.rmi.f32.f32	%f814, %f341;
-	cvt.rzi.s32.f32	%r311, %f814;
-	cvt.s64.s32	%rd261, %r311;
-	shl.b64 	%rd91, %rd261, 6;
-	add.s64 	%rd92, %rd91, %rd34;
-	add.s64 	%rd80, %rd92, 96;
-	// inline asm
-	cvta.to.global.u64 %rd79, %rd80;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r84,%r85,%r86,%r87}, [%rd79];
-	// inline asm
-	mov.b32 	 %f343, %r84;
-	mov.b32 	 %f344, %r85;
-	mov.b32 	 %f345, %r86;
-	mov.b32 	 %f346, %r87;
-	add.s64 	%rd83, %rd92, 112;
-	// inline asm
-	cvta.to.global.u64 %rd82, %rd83;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r88,%r89,%r90,%r91}, [%rd82];
-	// inline asm
-	mov.b32 	 %f347, %r88;
-	mov.b32 	 %f348, %r89;
-	mov.b32 	 %f349, %r90;
-	mov.b32 	 %f350, %r91;
-	add.s64 	%rd86, %rd92, 128;
-	// inline asm
-	cvta.to.global.u64 %rd85, %rd86;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r92,%r93,%r94,%r95}, [%rd85];
-	// inline asm
-	mov.b32 	 %f351, %r92;
-	mov.b32 	 %f352, %r93;
-	mov.b32 	 %f353, %r94;
-	mov.b32 	 %f354, %r95;
-	add.s64 	%rd89, %rd92, 144;
-	// inline asm
-	cvta.to.global.u64 %rd88, %rd89;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r96,%r97,%r98,%r99}, [%rd88];
-	// inline asm
-	mov.f32 	%f355, 0f3F800000;
-	sub.f32 	%f356, %f355, %f37;
-	mul.f32 	%f357, %f37, %f343;
-	mul.f32 	%f358, %f37, %f344;
-	mul.f32 	%f359, %f37, %f345;
-	mul.f32 	%f360, %f37, %f346;
-	fma.rn.f32 	%f828, %f356, %f828, %f357;
-	fma.rn.f32 	%f829, %f356, %f829, %f358;
-	fma.rn.f32 	%f830, %f356, %f830, %f359;
-	fma.rn.f32 	%f831, %f356, %f831, %f360;
-	mul.f32 	%f361, %f37, %f347;
-	mul.f32 	%f362, %f37, %f348;
-	mul.f32 	%f363, %f37, %f349;
-	mul.f32 	%f364, %f37, %f350;
-	fma.rn.f32 	%f832, %f356, %f832, %f361;
-	fma.rn.f32 	%f833, %f356, %f833, %f362;
-	fma.rn.f32 	%f834, %f356, %f834, %f363;
-	fma.rn.f32 	%f835, %f356, %f835, %f364;
-	mul.f32 	%f365, %f37, %f351;
-	mul.f32 	%f366, %f37, %f352;
-	mul.f32 	%f367, %f37, %f353;
-	mul.f32 	%f368, %f37, %f354;
-	fma.rn.f32 	%f836, %f356, %f836, %f365;
-	fma.rn.f32 	%f369, %f356, %f837, %f366;
-	fma.rn.f32 	%f370, %f356, %f838, %f367;
-	fma.rn.f32 	%f371, %f356, %f839, %f368;
-	mov.b32 	 %f372, %r96;
-	mov.b32 	 %f373, %r97;
-	mov.b32 	 %f374, %r98;
-	mov.b32 	 %f375, %r99;
-	mul.f32 	%f376, %f37, %f372;
-	mul.f32 	%f377, %f37, %f373;
-	mul.f32 	%f378, %f37, %f374;
-	mul.f32 	%f379, %f37, %f375;
-	fma.rn.f32 	%f380, %f356, %f840, %f376;
-	fma.rn.f32 	%f841, %f356, %f841, %f377;
-	fma.rn.f32 	%f842, %f356, %f842, %f378;
-	fma.rn.f32 	%f843, %f356, %f843, %f379;
-	mul.f32 	%f381, %f370, %f370;
-	fma.rn.f32 	%f382, %f369, %f369, %f381;
-	fma.rn.f32 	%f383, %f371, %f371, %f382;
-	fma.rn.f32 	%f384, %f380, %f380, %f383;
-	sqrt.rn.f32 	%f385, %f384;
-	rcp.rn.f32 	%f386, %f385;
-	mul.f32 	%f837, %f369, %f386;
-	mul.f32 	%f838, %f370, %f386;
-	mul.f32 	%f839, %f371, %f386;
-	mul.f32 	%f840, %f380, %f386;
-
-BB5_12:
-	mul.f32 	%f387, %f838, %f838;
-	fma.rn.f32 	%f388, %f837, %f837, %f387;
-	fma.rn.f32 	%f389, %f839, %f839, %f388;
-	fma.rn.f32 	%f390, %f840, %f840, %f389;
-	rcp.rn.f32 	%f391, %f390;
-	mul.f32 	%f392, %f837, %f391;
-	mul.f32 	%f393, %f838, %f391;
-	mul.f32 	%f394, %f839, %f391;
-	mul.f32 	%f395, %f840, %f391;
-	mul.f32 	%f396, %f837, %f392;
-	mul.f32 	%f397, %f838, %f393;
-	mul.f32 	%f398, %f839, %f394;
-	mul.f32 	%f399, %f837, %f393;
-	mul.f32 	%f400, %f839, %f395;
-	mul.f32 	%f401, %f837, %f394;
-	mul.f32 	%f402, %f838, %f395;
-	mul.f32 	%f403, %f838, %f394;
-	mul.f32 	%f404, %f837, %f395;
-	sub.f32 	%f405, %f396, %f397;
-	sub.f32 	%f406, %f405, %f398;
-	fma.rn.f32 	%f407, %f840, %f395, %f406;
-	sub.f32 	%f408, %f399, %f400;
-	add.f32 	%f409, %f408, %f408;
-	add.f32 	%f410, %f401, %f402;
-	add.f32 	%f411, %f410, %f410;
-	add.f32 	%f412, %f399, %f400;
-	add.f32 	%f413, %f412, %f412;
-	sub.f32 	%f414, %f397, %f396;
-	sub.f32 	%f415, %f414, %f398;
-	fma.rn.f32 	%f416, %f840, %f395, %f415;
-	sub.f32 	%f417, %f403, %f404;
-	add.f32 	%f418, %f417, %f417;
-	sub.f32 	%f419, %f401, %f402;
-	add.f32 	%f420, %f419, %f419;
-	add.f32 	%f421, %f403, %f404;
-	add.f32 	%f422, %f421, %f421;
-	neg.f32 	%f423, %f396;
-	sub.f32 	%f424, %f423, %f397;
-	add.f32 	%f425, %f398, %f424;
-	fma.rn.f32 	%f426, %f840, %f395, %f425;
-	mul.f32 	%f427, %f831, %f407;
-	fma.rn.f32 	%f428, %f834, %f409, %f427;
-	fma.rn.f32 	%f429, %f836, %f411, %f428;
-	sub.f32 	%f855, %f841, %f429;
-	mul.f32 	%f430, %f834, %f416;
-	fma.rn.f32 	%f431, %f831, %f413, %f430;
-	fma.rn.f32 	%f432, %f836, %f418, %f431;
-	sub.f32 	%f851, %f842, %f432;
-	mul.f32 	%f433, %f834, %f422;
-	fma.rn.f32 	%f434, %f831, %f420, %f433;
-	fma.rn.f32 	%f435, %f836, %f426, %f434;
-	sub.f32 	%f847, %f843, %f435;
-	mul.f32 	%f436, %f830, %f407;
-	fma.rn.f32 	%f437, %f833, %f409, %f436;
-	fma.rn.f32 	%f854, %f835, %f411, %f437;
-	mul.f32 	%f438, %f833, %f416;
-	fma.rn.f32 	%f439, %f830, %f413, %f438;
-	fma.rn.f32 	%f850, %f835, %f418, %f439;
-	mul.f32 	%f440, %f833, %f422;
-	fma.rn.f32 	%f441, %f830, %f420, %f440;
-	fma.rn.f32 	%f846, %f835, %f426, %f441;
-	mul.f32 	%f442, %f829, %f407;
-	fma.rn.f32 	%f853, %f832, %f409, %f442;
-	mul.f32 	%f443, %f832, %f416;
-	fma.rn.f32 	%f849, %f829, %f413, %f443;
-	mul.f32 	%f444, %f832, %f422;
-	fma.rn.f32 	%f845, %f829, %f420, %f444;
-	mul.f32 	%f852, %f828, %f407;
-	mul.f32 	%f848, %f828, %f413;
-	mul.f32 	%f844, %f828, %f420;
-
-BB5_15:
-	mul.f32 	%f482, %f845, %f850;
-	mul.f32 	%f483, %f846, %f849;
-	sub.f32 	%f484, %f483, %f482;
-	mul.f32 	%f485, %f852, %f484;
-	mul.f32 	%f486, %f844, %f850;
-	mul.f32 	%f487, %f846, %f848;
-	sub.f32 	%f488, %f487, %f486;
-	mul.f32 	%f489, %f488, %f853;
-	sub.f32 	%f490, %f485, %f489;
-	mul.f32 	%f491, %f844, %f849;
-	mul.f32 	%f492, %f845, %f848;
-	sub.f32 	%f493, %f492, %f491;
-	fma.rn.f32 	%f494, %f493, %f854, %f490;
-	rcp.rn.f32 	%f495, %f494;
-	mul.f32 	%f864, %f484, %f495;
-	mul.f32 	%f496, %f846, %f853;
-	mul.f32 	%f497, %f845, %f854;
-	sub.f32 	%f498, %f497, %f496;
-	mul.f32 	%f865, %f495, %f498;
-	mul.f32 	%f499, %f849, %f854;
-	mul.f32 	%f500, %f850, %f853;
-	sub.f32 	%f501, %f500, %f499;
-	mul.f32 	%f866, %f495, %f501;
-	sub.f32 	%f502, %f486, %f487;
-	mul.f32 	%f860, %f502, %f495;
-	mul.f32 	%f503, %f844, %f854;
-	mul.f32 	%f504, %f846, %f852;
-	sub.f32 	%f505, %f504, %f503;
-	mul.f32 	%f861, %f495, %f505;
-	mul.f32 	%f506, %f850, %f852;
-	mul.f32 	%f507, %f848, %f854;
-	sub.f32 	%f508, %f507, %f506;
-	mul.f32 	%f862, %f495, %f508;
-	mul.f32 	%f856, %f493, %f495;
-	mul.f32 	%f509, %f845, %f852;
-	mul.f32 	%f510, %f844, %f853;
-	sub.f32 	%f511, %f510, %f509;
-	mul.f32 	%f857, %f511, %f495;
-	mul.f32 	%f512, %f848, %f853;
-	mul.f32 	%f513, %f849, %f852;
-	sub.f32 	%f514, %f513, %f512;
-	mul.f32 	%f858, %f514, %f495;
-	mul.f32 	%f515, %f855, %f864;
-	neg.f32 	%f516, %f515;
-	mul.f32 	%f517, %f851, %f865;
-	sub.f32 	%f518, %f516, %f517;
-	mul.f32 	%f519, %f847, %f866;
-	sub.f32 	%f867, %f518, %f519;
-	mul.f32 	%f520, %f855, %f860;
-	neg.f32 	%f521, %f520;
-	mul.f32 	%f522, %f851, %f861;
-	sub.f32 	%f523, %f521, %f522;
-	mul.f32 	%f524, %f847, %f862;
-	sub.f32 	%f863, %f523, %f524;
-	mul.f32 	%f525, %f855, %f856;
-	neg.f32 	%f526, %f525;
-	mul.f32 	%f527, %f851, %f857;
-	sub.f32 	%f528, %f526, %f527;
-	mul.f32 	%f529, %f847, %f858;
-	sub.f32 	%f859, %f528, %f529;
-	bra.uni 	BB5_16;
-
-BB5_5:
-	setp.ne.s32	%p4, %r11, 1;
-	mov.f32 	%f857, %f856;
-	mov.f32 	%f859, %f856;
-	mov.f32 	%f860, %f856;
-	mov.f32 	%f861, %f858;
-	mov.f32 	%f862, %f856;
-	mov.f32 	%f863, %f856;
-	mov.f32 	%f864, %f858;
-	mov.f32 	%f865, %f856;
-	mov.f32 	%f866, %f856;
-	mov.f32 	%f867, %f856;
-	@%p4 bra 	BB5_16;
-
-	// inline asm
-	call (%rd21), _optix_get_static_transform_from_handle, (%rd19);
-	// inline asm
-	add.s64 	%rd263, %rd21, 64;
-
-BB5_8:
-	// inline asm
-	cvta.to.global.u64 %rd25, %rd263;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r13,%r14,%r15,%r16}, [%rd25];
-	// inline asm
-	mov.b32 	 %f864, %r13;
-	mov.b32 	 %f865, %r14;
-	mov.b32 	 %f866, %r15;
-	mov.b32 	 %f867, %r16;
-	add.s64 	%rd29, %rd263, 16;
-	// inline asm
-	cvta.to.global.u64 %rd28, %rd29;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r17,%r18,%r19,%r20}, [%rd28];
-	// inline asm
-	mov.b32 	 %f860, %r17;
-	mov.b32 	 %f861, %r18;
-	mov.b32 	 %f862, %r19;
-	mov.b32 	 %f863, %r20;
-	add.s64 	%rd32, %rd263, 32;
-	// inline asm
-	cvta.to.global.u64 %rd31, %rd32;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r21,%r22,%r23,%r24}, [%rd31];
-	// inline asm
-	mov.b32 	 %f856, %r21;
-	mov.b32 	 %f857, %r22;
-	mov.b32 	 %f858, %r23;
-	mov.b32 	 %f859, %r24;
-
-BB5_16:
-	setp.eq.s32	%p8, %r313, 0;
-	@%p8 bra 	BB5_17;
-	bra.uni 	BB5_18;
-
-BB5_17:
-	mov.f32 	%f827, %f867;
-	mov.f32 	%f826, %f866;
-	mov.f32 	%f825, %f865;
-	mov.f32 	%f824, %f864;
-	mov.f32 	%f823, %f863;
-	mov.f32 	%f822, %f862;
-	mov.f32 	%f821, %f861;
-	mov.f32 	%f820, %f860;
-	mov.f32 	%f819, %f859;
-	mov.f32 	%f818, %f858;
-	mov.f32 	%f817, %f857;
-	mov.f32 	%f816, %f856;
-	bra.uni 	BB5_19;
-
-BB5_18:
-	mul.f32 	%f530, %f820, %f865;
-	fma.rn.f32 	%f531, %f824, %f864, %f530;
-	fma.rn.f32 	%f151, %f816, %f866, %f531;
-	mul.f32 	%f532, %f821, %f865;
-	fma.rn.f32 	%f533, %f825, %f864, %f532;
-	fma.rn.f32 	%f152, %f817, %f866, %f533;
-	mul.f32 	%f534, %f822, %f865;
-	fma.rn.f32 	%f535, %f826, %f864, %f534;
-	fma.rn.f32 	%f153, %f818, %f866, %f535;
-	mul.f32 	%f536, %f823, %f865;
-	fma.rn.f32 	%f537, %f827, %f864, %f536;
-	fma.rn.f32 	%f538, %f819, %f866, %f537;
-	add.f32 	%f154, %f867, %f538;
-	mul.f32 	%f539, %f820, %f861;
-	fma.rn.f32 	%f540, %f824, %f860, %f539;
-	fma.rn.f32 	%f155, %f816, %f862, %f540;
-	mul.f32 	%f541, %f821, %f861;
-	fma.rn.f32 	%f542, %f825, %f860, %f541;
-	fma.rn.f32 	%f156, %f817, %f862, %f542;
-	mul.f32 	%f543, %f822, %f861;
-	fma.rn.f32 	%f544, %f826, %f860, %f543;
-	fma.rn.f32 	%f157, %f818, %f862, %f544;
-	mul.f32 	%f545, %f823, %f861;
-	fma.rn.f32 	%f546, %f827, %f860, %f545;
-	fma.rn.f32 	%f547, %f819, %f862, %f546;
-	add.f32 	%f158, %f863, %f547;
-	mul.f32 	%f548, %f820, %f857;
-	fma.rn.f32 	%f549, %f824, %f856, %f548;
-	fma.rn.f32 	%f816, %f816, %f858, %f549;
-	mul.f32 	%f550, %f821, %f857;
-	fma.rn.f32 	%f551, %f825, %f856, %f550;
-	fma.rn.f32 	%f817, %f817, %f858, %f551;
-	mul.f32 	%f552, %f822, %f857;
-	fma.rn.f32 	%f553, %f826, %f856, %f552;
-	fma.rn.f32 	%f818, %f818, %f858, %f553;
-	mul.f32 	%f554, %f823, %f857;
-	fma.rn.f32 	%f555, %f827, %f856, %f554;
-	fma.rn.f32 	%f556, %f819, %f858, %f555;
-	add.f32 	%f819, %f859, %f556;
-	mov.f32 	%f827, %f154;
-	mov.f32 	%f826, %f153;
-	mov.f32 	%f825, %f152;
-	mov.f32 	%f824, %f151;
-	mov.f32 	%f823, %f158;
-	mov.f32 	%f822, %f157;
-	mov.f32 	%f821, %f156;
-	mov.f32 	%f820, %f155;
-
-BB5_19:
-	add.s32 	%r313, %r313, 1;
-	setp.lt.u32	%p9, %r313, %r8;
-	@%p9 bra 	BB5_3;
-
-	mul.f32 	%f557, %f315, %f824;
-	fma.rn.f32 	%f558, %f316, %f825, %f557;
-	fma.rn.f32 	%f559, %f880, %f826, %f558;
-	add.f32 	%f882, %f827, %f559;
-	mul.f32 	%f560, %f315, %f820;
-	fma.rn.f32 	%f561, %f316, %f821, %f560;
-	fma.rn.f32 	%f562, %f880, %f822, %f561;
-	add.f32 	%f881, %f823, %f562;
-	mul.f32 	%f563, %f315, %f816;
-	fma.rn.f32 	%f564, %f316, %f817, %f563;
-	fma.rn.f32 	%f565, %f880, %f818, %f564;
-	add.f32 	%f880, %f819, %f565;
-	bra.uni 	BB5_21;
-
-BB5_1:
-	mov.f32 	%f881, %f316;
-	mov.f32 	%f882, %f315;
-
-BB5_21:
-	setp.eq.s32	%p21, %r8, 0;
-	// inline asm
-	call (%f566), _optix_get_world_ray_direction_x, ();
-	// inline asm
-	// inline asm
-	call (%f567), _optix_get_world_ray_direction_y, ();
-	// inline asm
-	// inline asm
-	call (%f931), _optix_get_world_ray_direction_z, ();
-	// inline asm
-	// inline asm
-	call (%f569), _optix_get_ray_time, ();
-	// inline asm
-	mov.u32 	%r314, 0;
-	@%p21 bra 	BB5_22;
-
-BB5_23:
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r149,%r150,%r151,%r152}, [%rd125];
+	// end inline asm
+	mov.b32 	%f486, %r149;
+	mov.b32 	%f487, %r150;
+	mov.b32 	%f488, %r151;
+	mov.b32 	%f489, %r152;
+	mul.f32 	%f490, %f90, %f486;
+	mul.f32 	%f491, %f90, %f487;
+	mul.f32 	%f492, %f90, %f488;
+	mul.f32 	%f493, %f90, %f489;
+	fma.rn.f32 	%f871, %f485, %f871, %f490;
+	fma.rn.f32 	%f870, %f485, %f870, %f491;
+	fma.rn.f32 	%f869, %f485, %f869, %f492;
+	fma.rn.f32 	%f868, %f485, %f868, %f493;
+	add.s64 	%rd129, %rd135, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd128, %rd129;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r153,%r154,%r155,%r156}, [%rd128];
+	// end inline asm
+	mov.b32 	%f494, %r153;
+	mov.b32 	%f495, %r154;
+	mov.b32 	%f496, %r155;
+	mov.b32 	%f497, %r156;
+	mul.f32 	%f498, %f90, %f494;
+	mul.f32 	%f499, %f90, %f495;
+	mul.f32 	%f500, %f90, %f496;
+	mul.f32 	%f501, %f90, %f497;
+	fma.rn.f32 	%f875, %f485, %f875, %f498;
+	fma.rn.f32 	%f874, %f485, %f874, %f499;
+	fma.rn.f32 	%f873, %f485, %f873, %f500;
+	fma.rn.f32 	%f872, %f485, %f872, %f501;
+	add.s64 	%rd132, %rd135, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd131, %rd132;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r157,%r158,%r159,%r160}, [%rd131];
+	// end inline asm
+	mov.b32 	%f502, %r157;
+	mov.b32 	%f503, %r158;
+	mov.b32 	%f504, %r159;
+	mov.b32 	%f505, %r160;
+	mul.f32 	%f506, %f90, %f502;
+	mul.f32 	%f507, %f90, %f503;
+	mul.f32 	%f508, %f90, %f504;
+	mul.f32 	%f509, %f90, %f505;
+	fma.rn.f32 	%f879, %f485, %f879, %f506;
+	fma.rn.f32 	%f878, %f485, %f878, %f507;
+	fma.rn.f32 	%f877, %f485, %f877, %f508;
+	fma.rn.f32 	%f876, %f485, %f876, %f509;
+	bra.uni 	$L__BB5_15;
+
+$L__BB5_4:
+	mov.f32 	%f880, 0f00000000;
+	mov.f32 	%f883, 0f3F800000;
+	setp.eq.s32 	%p4, %r13, 4;
+	@%p4 bra 	$L__BB5_7;
+
+	setp.ne.s32 	%p5, %r13, 1;
+	mov.f32 	%f881, %f880;
+	mov.f32 	%f882, %f880;
+	mov.f32 	%f884, %f880;
+	mov.f32 	%f885, %f880;
+	mov.f32 	%f886, %f883;
+	mov.f32 	%f887, %f880;
+	mov.f32 	%f888, %f880;
+	mov.f32 	%f889, %f883;
+	mov.f32 	%f890, %f880;
+	mov.f32 	%f891, %f880;
+	@%p5 bra 	$L__BB5_16;
+
+	// begin inline asm
+	call (%rd19), _optix_get_static_transform_from_handle, (%rd17);
+	// end inline asm
+	add.s64 	%rd256, %rd19, 64;
+	bra.uni 	$L__BB5_8;
+
+$L__BB5_10:
+	// begin inline asm
+	call (%rd32), _optix_get_srt_motion_transform_from_handle, (%rd17);
+	// end inline asm
+	// begin inline asm
+	cvta.to.global.u64 %rd34, %rd32;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r27,%r28,%r29,%r30}, [%rd34];
+	// end inline asm
+	add.s64 	%rd38, %rd32, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd37, %rd38;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r31,%r32,%r33,%r34}, [%rd37];
+	// end inline asm
+	add.s64 	%rd41, %rd32, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd40, %rd41;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r35,%r36,%r37,%r38}, [%rd40];
+	// end inline asm
+	add.s64 	%rd44, %rd32, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd43, %rd44;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r39,%r40,%r41,%r42}, [%rd43];
+	// end inline asm
+	add.s64 	%rd47, %rd32, 64;
+	// begin inline asm
+	cvta.to.global.u64 %rd46, %rd47;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r43,%r44,%r45,%r46}, [%rd46];
+	// end inline asm
+	add.s64 	%rd50, %rd32, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd49, %rd50;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r47,%r48,%r49,%r50}, [%rd49];
+	// end inline asm
+	add.s64 	%rd53, %rd32, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd52, %rd53;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r51,%r52,%r53,%r54}, [%rd52];
+	// end inline asm
+	add.s64 	%rd56, %rd32, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd55, %rd56;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r55,%r56,%r57,%r58}, [%rd55];
+	// end inline asm
+	add.s64 	%rd59, %rd32, 128;
+	// begin inline asm
+	cvta.to.global.u64 %rd58, %rd59;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r59,%r60,%r61,%r62}, [%rd58];
+	// end inline asm
+	add.s64 	%rd62, %rd32, 144;
+	// begin inline asm
+	cvta.to.global.u64 %rd61, %rd62;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r63,%r64,%r65,%r66}, [%rd61];
+	// end inline asm
+	mov.b32 	%f360, %r30;
+	mov.b32 	%f361, %r31;
+	and.b32  	%r83, %r29, 65535;
+	add.s32 	%r84, %r83, -1;
+	cvt.rn.f32.s32 	%f362, %r84;
+	sub.f32 	%f363, %f345, %f360;
+	mul.f32 	%f364, %f363, %f362;
+	sub.f32 	%f365, %f361, %f360;
+	div.rn.f32 	%f366, %f364, %f365;
+	min.f32 	%f367, %f362, %f366;
+	mov.f32 	%f368, 0f00000000;
+	max.f32 	%f369, %f368, %f367;
+	cvt.rmi.f32.f32 	%f370, %f369;
+	sub.f32 	%f29, %f369, %f370;
+	cvt.rzi.s32.f32 	%r85, %f370;
+	mul.wide.s32 	%rd76, %r85, 64;
+	add.s64 	%rd65, %rd41, %rd76;
+	// begin inline asm
+	cvta.to.global.u64 %rd64, %rd65;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r67,%r68,%r69,%r70}, [%rd64];
+	// end inline asm
+	mov.b32 	%f852, %r67;
+	mov.b32 	%f853, %r68;
+	mov.b32 	%f854, %r69;
+	mov.b32 	%f855, %r70;
+	add.s64 	%rd68, %rd65, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd67, %rd68;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r71,%r72,%r73,%r74}, [%rd67];
+	// end inline asm
+	mov.b32 	%f856, %r71;
+	mov.b32 	%f857, %r72;
+	mov.b32 	%f858, %r73;
+	mov.b32 	%f859, %r74;
+	add.s64 	%rd71, %rd65, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd70, %rd71;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r75,%r76,%r77,%r78}, [%rd70];
+	// end inline asm
+	mov.b32 	%f860, %r75;
+	mov.b32 	%f861, %r76;
+	mov.b32 	%f862, %r77;
+	mov.b32 	%f863, %r78;
+	add.s64 	%rd74, %rd65, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd73, %rd74;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r79,%r80,%r81,%r82}, [%rd73];
+	// end inline asm
+	mov.b32 	%f864, %r79;
+	mov.b32 	%f865, %r80;
+	mov.b32 	%f866, %r81;
+	mov.b32 	%f867, %r82;
+	setp.leu.f32 	%p7, %f29, 0f00000000;
+	@%p7 bra 	$L__BB5_12;
+
+	mov.f32 	%f371, 0f3F800000;
+	sub.f32 	%f372, %f371, %f29;
+	add.s64 	%rd78, %rd65, 64;
+	// begin inline asm
+	cvta.to.global.u64 %rd77, %rd78;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r86,%r87,%r88,%r89}, [%rd77];
+	// end inline asm
+	mov.b32 	%f373, %r86;
+	mov.b32 	%f374, %r87;
+	mov.b32 	%f375, %r88;
+	mov.b32 	%f376, %r89;
+	mul.f32 	%f377, %f29, %f373;
+	mul.f32 	%f378, %f29, %f374;
+	mul.f32 	%f379, %f29, %f375;
+	mul.f32 	%f380, %f29, %f376;
+	fma.rn.f32 	%f852, %f372, %f852, %f377;
+	fma.rn.f32 	%f853, %f372, %f853, %f378;
+	fma.rn.f32 	%f854, %f372, %f854, %f379;
+	fma.rn.f32 	%f855, %f372, %f855, %f380;
+	add.s64 	%rd81, %rd65, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd80, %rd81;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r90,%r91,%r92,%r93}, [%rd80];
+	// end inline asm
+	mov.b32 	%f381, %r90;
+	mov.b32 	%f382, %r91;
+	mov.b32 	%f383, %r92;
+	mov.b32 	%f384, %r93;
+	mul.f32 	%f385, %f29, %f381;
+	mul.f32 	%f386, %f29, %f382;
+	mul.f32 	%f387, %f29, %f383;
+	mul.f32 	%f388, %f29, %f384;
+	fma.rn.f32 	%f856, %f372, %f856, %f385;
+	fma.rn.f32 	%f857, %f372, %f857, %f386;
+	fma.rn.f32 	%f858, %f372, %f858, %f387;
+	fma.rn.f32 	%f859, %f372, %f859, %f388;
+	add.s64 	%rd84, %rd65, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd83, %rd84;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r94,%r95,%r96,%r97}, [%rd83];
+	// end inline asm
+	mov.b32 	%f389, %r94;
+	mov.b32 	%f390, %r95;
+	mov.b32 	%f391, %r96;
+	mov.b32 	%f392, %r97;
+	mul.f32 	%f393, %f29, %f389;
+	mul.f32 	%f394, %f29, %f390;
+	mul.f32 	%f395, %f29, %f391;
+	mul.f32 	%f396, %f29, %f392;
+	fma.rn.f32 	%f860, %f372, %f860, %f393;
+	fma.rn.f32 	%f397, %f372, %f861, %f394;
+	fma.rn.f32 	%f398, %f372, %f862, %f395;
+	fma.rn.f32 	%f399, %f372, %f863, %f396;
+	add.s64 	%rd87, %rd65, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd86, %rd87;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r98,%r99,%r100,%r101}, [%rd86];
+	// end inline asm
+	mov.b32 	%f400, %r98;
+	mov.b32 	%f401, %r99;
+	mov.b32 	%f402, %r100;
+	mov.b32 	%f403, %r101;
+	mul.f32 	%f404, %f29, %f400;
+	mul.f32 	%f405, %f29, %f401;
+	mul.f32 	%f406, %f29, %f402;
+	mul.f32 	%f407, %f29, %f403;
+	fma.rn.f32 	%f408, %f372, %f864, %f404;
+	fma.rn.f32 	%f865, %f372, %f865, %f405;
+	fma.rn.f32 	%f866, %f372, %f866, %f406;
+	fma.rn.f32 	%f867, %f372, %f867, %f407;
+	mul.f32 	%f409, %f398, %f398;
+	fma.rn.f32 	%f410, %f397, %f397, %f409;
+	fma.rn.f32 	%f411, %f399, %f399, %f410;
+	fma.rn.f32 	%f412, %f408, %f408, %f411;
+	sqrt.rn.f32 	%f413, %f412;
+	rcp.rn.f32 	%f414, %f413;
+	mul.f32 	%f861, %f397, %f414;
+	mul.f32 	%f862, %f398, %f414;
+	mul.f32 	%f863, %f399, %f414;
+	mul.f32 	%f864, %f414, %f408;
+
+$L__BB5_12:
+	mul.f32 	%f415, %f862, %f862;
+	fma.rn.f32 	%f416, %f861, %f861, %f415;
+	fma.rn.f32 	%f417, %f863, %f863, %f416;
+	fma.rn.f32 	%f418, %f864, %f864, %f417;
+	rcp.rn.f32 	%f419, %f418;
+	mul.f32 	%f420, %f861, %f419;
+	mul.f32 	%f421, %f862, %f419;
+	mul.f32 	%f422, %f863, %f419;
+	mul.f32 	%f423, %f864, %f419;
+	mul.f32 	%f424, %f861, %f420;
+	mul.f32 	%f425, %f862, %f421;
+	mul.f32 	%f426, %f863, %f422;
+	mul.f32 	%f427, %f861, %f421;
+	mul.f32 	%f428, %f863, %f423;
+	mul.f32 	%f429, %f861, %f422;
+	mul.f32 	%f430, %f862, %f423;
+	mul.f32 	%f431, %f862, %f422;
+	mul.f32 	%f432, %f861, %f423;
+	sub.f32 	%f433, %f424, %f425;
+	sub.f32 	%f434, %f433, %f426;
+	fma.rn.f32 	%f435, %f864, %f423, %f434;
+	sub.f32 	%f436, %f427, %f428;
+	add.f32 	%f437, %f436, %f436;
+	add.f32 	%f438, %f429, %f430;
+	add.f32 	%f439, %f438, %f438;
+	add.f32 	%f440, %f427, %f428;
+	add.f32 	%f441, %f440, %f440;
+	sub.f32 	%f442, %f425, %f424;
+	sub.f32 	%f443, %f442, %f426;
+	fma.rn.f32 	%f444, %f864, %f423, %f443;
+	sub.f32 	%f445, %f431, %f432;
+	add.f32 	%f446, %f445, %f445;
+	sub.f32 	%f447, %f429, %f430;
+	add.f32 	%f448, %f447, %f447;
+	add.f32 	%f449, %f431, %f432;
+	add.f32 	%f450, %f449, %f449;
+	neg.f32 	%f451, %f424;
+	sub.f32 	%f452, %f451, %f425;
+	add.f32 	%f453, %f426, %f452;
+	fma.rn.f32 	%f454, %f864, %f423, %f453;
+	mul.f32 	%f455, %f855, %f435;
+	fma.rn.f32 	%f456, %f858, %f437, %f455;
+	fma.rn.f32 	%f457, %f860, %f439, %f456;
+	sub.f32 	%f868, %f865, %f457;
+	mul.f32 	%f458, %f858, %f444;
+	fma.rn.f32 	%f459, %f855, %f441, %f458;
+	fma.rn.f32 	%f460, %f860, %f446, %f459;
+	sub.f32 	%f872, %f866, %f460;
+	mul.f32 	%f461, %f858, %f450;
+	fma.rn.f32 	%f462, %f855, %f448, %f461;
+	fma.rn.f32 	%f463, %f860, %f454, %f462;
+	sub.f32 	%f876, %f867, %f463;
+	mul.f32 	%f464, %f854, %f435;
+	fma.rn.f32 	%f465, %f857, %f437, %f464;
+	fma.rn.f32 	%f869, %f859, %f439, %f465;
+	mul.f32 	%f466, %f857, %f444;
+	fma.rn.f32 	%f467, %f854, %f441, %f466;
+	fma.rn.f32 	%f873, %f859, %f446, %f467;
+	mul.f32 	%f468, %f857, %f450;
+	fma.rn.f32 	%f469, %f854, %f448, %f468;
+	fma.rn.f32 	%f877, %f859, %f454, %f469;
+	mul.f32 	%f470, %f853, %f435;
+	fma.rn.f32 	%f870, %f856, %f437, %f470;
+	mul.f32 	%f471, %f856, %f444;
+	fma.rn.f32 	%f874, %f853, %f441, %f471;
+	mul.f32 	%f472, %f856, %f450;
+	fma.rn.f32 	%f878, %f853, %f448, %f472;
+	mul.f32 	%f871, %f852, %f435;
+	mul.f32 	%f875, %f852, %f441;
+	mul.f32 	%f879, %f852, %f448;
+
+$L__BB5_15:
+	mul.f32 	%f510, %f873, %f878;
+	mul.f32 	%f511, %f874, %f877;
+	sub.f32 	%f512, %f511, %f510;
+	mul.f32 	%f513, %f871, %f512;
+	mul.f32 	%f514, %f873, %f879;
+	mul.f32 	%f515, %f875, %f877;
+	sub.f32 	%f516, %f515, %f514;
+	mul.f32 	%f517, %f870, %f516;
+	sub.f32 	%f518, %f513, %f517;
+	mul.f32 	%f519, %f874, %f879;
+	mul.f32 	%f520, %f875, %f878;
+	sub.f32 	%f521, %f520, %f519;
+	fma.rn.f32 	%f522, %f869, %f521, %f518;
+	rcp.rn.f32 	%f523, %f522;
+	mul.f32 	%f883, %f512, %f523;
+	mul.f32 	%f524, %f870, %f877;
+	mul.f32 	%f525, %f869, %f878;
+	sub.f32 	%f526, %f525, %f524;
+	mul.f32 	%f882, %f526, %f523;
+	mul.f32 	%f527, %f869, %f874;
+	mul.f32 	%f528, %f870, %f873;
+	sub.f32 	%f529, %f528, %f527;
+	mul.f32 	%f881, %f529, %f523;
+	sub.f32 	%f530, %f514, %f515;
+	mul.f32 	%f887, %f530, %f523;
+	mul.f32 	%f531, %f869, %f879;
+	mul.f32 	%f532, %f871, %f877;
+	sub.f32 	%f533, %f532, %f531;
+	mul.f32 	%f886, %f533, %f523;
+	mul.f32 	%f534, %f871, %f873;
+	mul.f32 	%f535, %f869, %f875;
+	sub.f32 	%f536, %f535, %f534;
+	mul.f32 	%f885, %f536, %f523;
+	mul.f32 	%f891, %f521, %f523;
+	mul.f32 	%f537, %f871, %f878;
+	mul.f32 	%f538, %f870, %f879;
+	sub.f32 	%f539, %f538, %f537;
+	mul.f32 	%f890, %f539, %f523;
+	mul.f32 	%f540, %f870, %f875;
+	mul.f32 	%f541, %f871, %f874;
+	sub.f32 	%f542, %f541, %f540;
+	mul.f32 	%f889, %f542, %f523;
+	mul.f32 	%f543, %f868, %f883;
+	neg.f32 	%f544, %f543;
+	mul.f32 	%f545, %f872, %f882;
+	sub.f32 	%f546, %f544, %f545;
+	mul.f32 	%f547, %f876, %f881;
+	sub.f32 	%f880, %f546, %f547;
+	mul.f32 	%f548, %f868, %f887;
+	neg.f32 	%f549, %f548;
+	mul.f32 	%f550, %f872, %f886;
+	sub.f32 	%f551, %f549, %f550;
+	mul.f32 	%f552, %f876, %f885;
+	sub.f32 	%f884, %f551, %f552;
+	mul.f32 	%f553, %f868, %f891;
+	neg.f32 	%f554, %f553;
+	mul.f32 	%f555, %f872, %f890;
+	sub.f32 	%f556, %f554, %f555;
+	mul.f32 	%f557, %f876, %f889;
+	sub.f32 	%f888, %f556, %f557;
+	bra.uni 	$L__BB5_16;
+
+$L__BB5_7:
+	// begin inline asm
+	call (%rd256), _optix_get_instance_inverse_transform_from_handle, (%rd17);
+	// end inline asm
+
+$L__BB5_8:
+	// begin inline asm
+	cvta.to.global.u64 %rd23, %rd256;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r15,%r16,%r17,%r18}, [%rd23];
+	// end inline asm
+	mov.b32 	%f883, %r15;
+	mov.b32 	%f882, %r16;
+	mov.b32 	%f881, %r17;
+	mov.b32 	%f880, %r18;
+	add.s64 	%rd27, %rd256, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd26, %rd27;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r19,%r20,%r21,%r22}, [%rd26];
+	// end inline asm
+	mov.b32 	%f887, %r19;
+	mov.b32 	%f886, %r20;
+	mov.b32 	%f885, %r21;
+	mov.b32 	%f884, %r22;
+	add.s64 	%rd30, %rd256, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd29, %rd30;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r23,%r24,%r25,%r26}, [%rd29];
+	// end inline asm
+	mov.b32 	%f891, %r23;
+	mov.b32 	%f890, %r24;
+	mov.b32 	%f889, %r25;
+	mov.b32 	%f888, %r26;
+
+$L__BB5_16:
+	setp.eq.s32 	%p9, %r316, 0;
+	@%p9 bra 	$L__BB5_18;
+
+	mul.f32 	%f558, %f848, %f883;
+	fma.rn.f32 	%f559, %f844, %f882, %f558;
+	fma.rn.f32 	%f151, %f840, %f881, %f559;
+	mul.f32 	%f560, %f849, %f883;
+	fma.rn.f32 	%f561, %f845, %f882, %f560;
+	fma.rn.f32 	%f152, %f841, %f881, %f561;
+	mul.f32 	%f562, %f850, %f883;
+	fma.rn.f32 	%f563, %f846, %f882, %f562;
+	fma.rn.f32 	%f153, %f842, %f881, %f563;
+	mul.f32 	%f564, %f851, %f883;
+	fma.rn.f32 	%f565, %f847, %f882, %f564;
+	fma.rn.f32 	%f566, %f843, %f881, %f565;
+	add.f32 	%f880, %f880, %f566;
+	mul.f32 	%f567, %f848, %f887;
+	fma.rn.f32 	%f568, %f844, %f886, %f567;
+	fma.rn.f32 	%f155, %f840, %f885, %f568;
+	mul.f32 	%f569, %f849, %f887;
+	fma.rn.f32 	%f570, %f845, %f886, %f569;
+	fma.rn.f32 	%f156, %f841, %f885, %f570;
+	mul.f32 	%f571, %f850, %f887;
+	fma.rn.f32 	%f572, %f846, %f886, %f571;
+	fma.rn.f32 	%f157, %f842, %f885, %f572;
+	mul.f32 	%f573, %f851, %f887;
+	fma.rn.f32 	%f574, %f847, %f886, %f573;
+	fma.rn.f32 	%f575, %f843, %f885, %f574;
+	add.f32 	%f884, %f884, %f575;
+	mul.f32 	%f576, %f848, %f891;
+	fma.rn.f32 	%f577, %f844, %f890, %f576;
+	fma.rn.f32 	%f159, %f840, %f889, %f577;
+	mul.f32 	%f578, %f849, %f891;
+	fma.rn.f32 	%f579, %f845, %f890, %f578;
+	fma.rn.f32 	%f160, %f841, %f889, %f579;
+	mul.f32 	%f580, %f850, %f891;
+	fma.rn.f32 	%f581, %f846, %f890, %f580;
+	fma.rn.f32 	%f161, %f842, %f889, %f581;
+	mul.f32 	%f582, %f851, %f891;
+	fma.rn.f32 	%f583, %f847, %f890, %f582;
+	fma.rn.f32 	%f584, %f843, %f889, %f583;
+	add.f32 	%f888, %f888, %f584;
+	mov.f32 	%f881, %f153;
+	mov.f32 	%f882, %f152;
+	mov.f32 	%f883, %f151;
+	mov.f32 	%f885, %f157;
+	mov.f32 	%f886, %f156;
+	mov.f32 	%f887, %f155;
+	mov.f32 	%f889, %f161;
+	mov.f32 	%f890, %f160;
+	mov.f32 	%f891, %f159;
+
+$L__BB5_18:
+	add.s32 	%r316, %r316, 1;
+	setp.lt.u32 	%p10, %r316, %r10;
+	mov.f32 	%f840, %f891;
+	mov.f32 	%f841, %f890;
+	mov.f32 	%f842, %f889;
+	mov.f32 	%f843, %f888;
+	mov.f32 	%f844, %f887;
+	mov.f32 	%f845, %f886;
+	mov.f32 	%f846, %f885;
+	mov.f32 	%f847, %f884;
+	mov.f32 	%f848, %f883;
+	mov.f32 	%f849, %f882;
+	mov.f32 	%f850, %f881;
+	mov.f32 	%f851, %f880;
+	@%p10 bra 	$L__BB5_3;
+
+$L__BB5_19:
+	mul.f32 	%f585, %f916, %f883;
+	fma.rn.f32 	%f586, %f917, %f882, %f585;
+	fma.rn.f32 	%f587, %f918, %f881, %f586;
+	mul.f32 	%f588, %f916, %f887;
+	fma.rn.f32 	%f589, %f917, %f886, %f588;
+	fma.rn.f32 	%f590, %f918, %f885, %f589;
+	mul.f32 	%f591, %f916, %f891;
+	fma.rn.f32 	%f592, %f917, %f890, %f591;
+	fma.rn.f32 	%f593, %f918, %f889, %f592;
+	add.f32 	%f918, %f888, %f593;
+	add.f32 	%f917, %f884, %f590;
+	add.f32 	%f916, %f880, %f587;
+
+$L__BB5_21:
+	// begin inline asm
+	call (%f974), _optix_get_world_ray_direction_x, ();
+	// end inline asm
+	// begin inline asm
+	call (%f975), _optix_get_world_ray_direction_y, ();
+	// end inline asm
+	// begin inline asm
+	call (%f596), _optix_get_world_ray_direction_z, ();
+	// end inline asm
+	// begin inline asm
+	call (%r161), _optix_get_transform_list_size, ();
+	// end inline asm
+	setp.eq.s32 	%p11, %r161, 0;
+	@%p11 bra 	$L__BB5_41;
+
+	// begin inline asm
+	call (%r162), _optix_get_transform_list_size, ();
+	// end inline asm
+	// begin inline asm
+	call (%f597), _optix_get_ray_time, ();
+	// end inline asm
+	setp.eq.s32 	%p12, %r162, 0;
+	@%p12 bra 	$L__BB5_40;
+
+	mov.u32 	%r317, 0;
+
+$L__BB5_24:
 	.pragma "nounroll";
-	// inline asm
-	call (%rd140), _optix_get_transform_list_handle, (%r314);
-	// inline asm
-	// inline asm
-	call (%r161), _optix_get_transform_type_from_handle, (%rd140);
-	// inline asm
-	and.b32  	%r162, %r161, -2;
-	setp.eq.s32	%p11, %r162, 2;
-	@%p11 bra 	BB5_29;
-	bra.uni 	BB5_24;
-
-BB5_29:
-	setp.eq.s32	%p14, %r161, 2;
-	@%p14 bra 	BB5_33;
-	bra.uni 	BB5_30;
-
-BB5_33:
-	// inline asm
-	call (%rd214), _optix_get_matrix_motion_transform_from_handle, (%rd140);
-	// inline asm
-	// inline asm
-	cvta.to.global.u64 %rd216, %rd214;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r250,%r251,%r252,%r253}, [%rd216];
-	// inline asm
-	mov.b32	{%rs7, %rs8}, %r252;
-	add.s64 	%rd220, %rd214, 16;
-	// inline asm
+	// begin inline asm
+	call (%rd136), _optix_get_transform_list_handle, (%r317);
+	// end inline asm
+	// begin inline asm
+	call (%r165), _optix_get_transform_type_from_handle, (%rd136);
+	// end inline asm
+	or.b32  	%r166, %r165, 1;
+	setp.eq.s32 	%p13, %r166, 3;
+	@%p13 bra 	$L__BB5_30;
+	bra.uni 	$L__BB5_25;
+
+$L__BB5_30:
+	setp.eq.s32 	%p16, %r165, 2;
+	@%p16 bra 	$L__BB5_34;
+	bra.uni 	$L__BB5_31;
+
+$L__BB5_34:
+	// begin inline asm
+	call (%rd208), _optix_get_matrix_motion_transform_from_handle, (%rd136);
+	// end inline asm
+	// begin inline asm
+	cvta.to.global.u64 %rd210, %rd208;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r254,%r255,%r256,%r257}, [%rd210];
+	// end inline asm
+	add.s64 	%rd214, %rd208, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd213, %rd214;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r258,%r259,%r260,%r261}, [%rd213];
+	// end inline asm
+	add.s64 	%rd217, %rd208, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd216, %rd217;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r262,%r263,%r264,%r265}, [%rd216];
+	// end inline asm
+	add.s64 	%rd220, %rd208, 48;
+	// begin inline asm
 	cvta.to.global.u64 %rd219, %rd220;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r254,%r255,%r256,%r257}, [%rd219];
-	// inline asm
-	add.s64 	%rd223, %rd214, 32;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r266,%r267,%r268,%r269}, [%rd219];
+	// end inline asm
+	add.s64 	%rd223, %rd208, 64;
+	// begin inline asm
 	cvta.to.global.u64 %rd222, %rd223;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r258,%r259,%r260,%r261}, [%rd222];
-	// inline asm
-	add.s64 	%rd226, %rd214, 48;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r270,%r271,%r272,%r273}, [%rd222];
+	// end inline asm
+	add.s64 	%rd226, %rd208, 80;
+	// begin inline asm
 	cvta.to.global.u64 %rd225, %rd226;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r262,%r263,%r264,%r265}, [%rd225];
-	// inline asm
-	add.s64 	%rd229, %rd214, 64;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r274,%r275,%r276,%r277}, [%rd225];
+	// end inline asm
+	add.s64 	%rd229, %rd208, 96;
+	// begin inline asm
 	cvta.to.global.u64 %rd228, %rd229;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r266,%r267,%r268,%r269}, [%rd228];
-	// inline asm
-	add.s64 	%rd232, %rd214, 80;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r278,%r279,%r280,%r281}, [%rd228];
+	// end inline asm
+	add.s64 	%rd232, %rd208, 112;
+	// begin inline asm
 	cvta.to.global.u64 %rd231, %rd232;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r270,%r271,%r272,%r273}, [%rd231];
-	// inline asm
-	add.s64 	%rd235, %rd214, 96;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r282,%r283,%r284,%r285}, [%rd231];
+	// end inline asm
+	mov.b32 	%f701, %r257;
+	mov.b32 	%f702, %r258;
+	and.b32  	%r298, %r256, 65535;
+	add.s32 	%r299, %r298, -1;
+	cvt.rn.f32.s32 	%f703, %r299;
+	sub.f32 	%f704, %f597, %f701;
+	mul.f32 	%f705, %f704, %f703;
+	sub.f32 	%f706, %f702, %f701;
+	div.rn.f32 	%f707, %f705, %f706;
+	min.f32 	%f708, %f703, %f707;
+	mov.f32 	%f709, 0f00000000;
+	max.f32 	%f710, %f709, %f708;
+	cvt.rmi.f32.f32 	%f711, %f710;
+	sub.f32 	%f258, %f710, %f711;
+	cvt.rzi.s32.f32 	%r300, %f711;
+	cvt.s64.s32 	%rd15, %r300;
+	mul.wide.s32 	%rd243, %r300, 48;
+	add.s64 	%rd235, %rd217, %rd243;
+	// begin inline asm
 	cvta.to.global.u64 %rd234, %rd235;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r274,%r275,%r276,%r277}, [%rd234];
-	// inline asm
-	add.s64 	%rd238, %rd214, 112;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r286,%r287,%r288,%r289}, [%rd234];
+	// end inline asm
+	mov.b32 	%f944, %r286;
+	mov.b32 	%f945, %r287;
+	mov.b32 	%f946, %r288;
+	add.s64 	%rd238, %rd235, 16;
+	// begin inline asm
 	cvta.to.global.u64 %rd237, %rd238;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r278,%r279,%r280,%r281}, [%rd237];
-	// inline asm
-	mov.b32 	 %f672, %r253;
-	mov.b32 	 %f673, %r254;
-	cvt.u32.u16	%r294, %rs7;
-	add.s32 	%r295, %r294, -1;
-	cvt.rn.f32.s32	%f674, %r295;
-	sub.f32 	%f675, %f569, %f672;
-	mul.f32 	%f676, %f675, %f674;
-	sub.f32 	%f677, %f673, %f672;
-	div.rn.f32 	%f678, %f676, %f677;
-	min.f32 	%f679, %f674, %f678;
-	mov.f32 	%f680, 0f00000000;
-	max.f32 	%f681, %f680, %f679;
-	cvt.rmi.f32.f32	%f682, %f681;
-	cvt.rzi.s32.f32	%r296, %f682;
-	cvt.s64.s32	%rd17, %r296;
-	mul.wide.s32 	%rd249, %r296, 48;
-	add.s64 	%rd241, %rd223, %rd249;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r290,%r291,%r292,%r293}, [%rd237];
+	// end inline asm
+	mov.b32 	%f941, %r290;
+	mov.b32 	%f942, %r291;
+	mov.b32 	%f943, %r292;
+	add.s64 	%rd241, %rd235, 32;
+	// begin inline asm
 	cvta.to.global.u64 %rd240, %rd241;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r282,%r283,%r284,%r285}, [%rd240];
-	// inline asm
-	mov.b32 	 %f908, %r282;
-	mov.b32 	 %f909, %r283;
-	mov.b32 	 %f910, %r284;
-	add.s64 	%rd244, %rd241, 16;
-	// inline asm
-	cvta.to.global.u64 %rd243, %rd244;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r286,%r287,%r288,%r289}, [%rd243];
-	// inline asm
-	mov.b32 	 %f905, %r286;
-	mov.b32 	 %f906, %r287;
-	mov.b32 	 %f907, %r288;
-	add.s64 	%rd247, %rd241, 32;
-	// inline asm
-	cvta.to.global.u64 %rd246, %rd247;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r290,%r291,%r292,%r293}, [%rd246];
-	// inline asm
-	sub.f32 	%f249, %f681, %f682;
-	mov.b32 	 %f902, %r290;
-	mov.b32 	 %f903, %r291;
-	mov.b32 	 %f904, %r292;
-	setp.leu.f32	%p16, %f249, 0f00000000;
-	@%p16 bra 	BB5_35;
-
-	mul.lo.s64 	%rd259, %rd17, 48;
-	add.s64 	%rd260, %rd214, %rd259;
-	add.s64 	%rd251, %rd260, 80;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r294,%r295,%r296,%r297}, [%rd240];
+	// end inline asm
+	mov.b32 	%f938, %r294;
+	mov.b32 	%f939, %r295;
+	mov.b32 	%f940, %r296;
+	setp.leu.f32 	%p18, %f258, 0f00000000;
+	@%p18 bra 	$L__BB5_36;
+
+	mov.f32 	%f712, 0f3F800000;
+	sub.f32 	%f713, %f712, %f258;
+	mul.lo.s64 	%rd253, %rd15, 48;
+	add.s64 	%rd254, %rd208, %rd253;
+	add.s64 	%rd245, %rd254, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd244, %rd245;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r301,%r302,%r303,%r304}, [%rd244];
+	// end inline asm
+	mov.b32 	%f714, %r301;
+	mov.b32 	%f715, %r302;
+	mov.b32 	%f716, %r303;
+	mul.f32 	%f717, %f258, %f714;
+	mul.f32 	%f718, %f258, %f715;
+	mul.f32 	%f719, %f258, %f716;
+	fma.rn.f32 	%f944, %f713, %f944, %f717;
+	fma.rn.f32 	%f945, %f713, %f945, %f718;
+	fma.rn.f32 	%f946, %f713, %f946, %f719;
+	add.s64 	%rd248, %rd254, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd247, %rd248;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r305,%r306,%r307,%r308}, [%rd247];
+	// end inline asm
+	mov.b32 	%f720, %r305;
+	mov.b32 	%f721, %r306;
+	mov.b32 	%f722, %r307;
+	mul.f32 	%f723, %f258, %f720;
+	mul.f32 	%f724, %f258, %f721;
+	mul.f32 	%f725, %f258, %f722;
+	fma.rn.f32 	%f941, %f713, %f941, %f723;
+	fma.rn.f32 	%f942, %f713, %f942, %f724;
+	fma.rn.f32 	%f943, %f713, %f943, %f725;
+	add.s64 	%rd251, %rd254, 112;
+	// begin inline asm
 	cvta.to.global.u64 %rd250, %rd251;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r297,%r298,%r299,%r300}, [%rd250];
-	// inline asm
-	mov.b32 	 %f683, %r297;
-	mov.b32 	 %f684, %r298;
-	mov.b32 	 %f685, %r299;
-	add.s64 	%rd254, %rd260, 96;
-	// inline asm
-	cvta.to.global.u64 %rd253, %rd254;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r301,%r302,%r303,%r304}, [%rd253];
-	// inline asm
-	mov.b32 	 %f686, %r301;
-	mov.b32 	 %f687, %r302;
-	mov.b32 	 %f688, %r303;
-	add.s64 	%rd257, %rd260, 112;
-	// inline asm
-	cvta.to.global.u64 %rd256, %rd257;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r305,%r306,%r307,%r308}, [%rd256];
-	// inline asm
-	mov.f32 	%f689, 0f3F800000;
-	sub.f32 	%f690, %f689, %f249;
-	mul.f32 	%f691, %f249, %f683;
-	mul.f32 	%f692, %f249, %f684;
-	mul.f32 	%f693, %f249, %f685;
-	fma.rn.f32 	%f908, %f690, %f908, %f691;
-	fma.rn.f32 	%f909, %f690, %f909, %f692;
-	fma.rn.f32 	%f910, %f690, %f910, %f693;
-	mul.f32 	%f694, %f249, %f686;
-	mul.f32 	%f695, %f249, %f687;
-	mul.f32 	%f696, %f249, %f688;
-	fma.rn.f32 	%f905, %f690, %f905, %f694;
-	fma.rn.f32 	%f906, %f690, %f906, %f695;
-	fma.rn.f32 	%f907, %f690, %f907, %f696;
-	mov.b32 	 %f697, %r305;
-	mov.b32 	 %f698, %r306;
-	mov.b32 	 %f699, %r307;
-	mul.f32 	%f700, %f249, %f697;
-	mul.f32 	%f701, %f249, %f698;
-	mul.f32 	%f702, %f249, %f699;
-	fma.rn.f32 	%f902, %f690, %f902, %f700;
-	fma.rn.f32 	%f903, %f690, %f903, %f701;
-	fma.rn.f32 	%f904, %f690, %f904, %f702;
-	bra.uni 	BB5_35;
-
-BB5_24:
-	mov.f32 	%f911, 0f00000000;
-	mov.f32 	%f913, 0f3F800000;
-	setp.eq.s32	%p12, %r161, 4;
-	@%p12 bra 	BB5_27;
-	bra.uni 	BB5_25;
-
-BB5_27:
-	// inline asm
-	call (%rd264), _optix_get_instance_inverse_transform_from_handle, (%rd140);
-	// inline asm
-	bra.uni 	BB5_28;
-
-BB5_30:
-	// inline asm
-	call (%rd155), _optix_get_srt_motion_transform_from_handle, (%rd140);
-	// inline asm
-	// inline asm
-	cvta.to.global.u64 %rd157, %rd155;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r175,%r176,%r177,%r178}, [%rd157];
-	// inline asm
-	mov.b32	{%rs5, %rs6}, %r177;
-	add.s64 	%rd161, %rd155, 16;
-	// inline asm
-	cvta.to.global.u64 %rd160, %rd161;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r179,%r180,%r181,%r182}, [%rd160];
-	// inline asm
-	add.s64 	%rd164, %rd155, 32;
-	// inline asm
-	cvta.to.global.u64 %rd163, %rd164;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r183,%r184,%r185,%r186}, [%rd163];
-	// inline asm
-	add.s64 	%rd167, %rd155, 48;
-	// inline asm
-	cvta.to.global.u64 %rd166, %rd167;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r187,%r188,%r189,%r190}, [%rd166];
-	// inline asm
-	add.s64 	%rd170, %rd155, 64;
-	// inline asm
-	cvta.to.global.u64 %rd169, %rd170;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r191,%r192,%r193,%r194}, [%rd169];
-	// inline asm
-	add.s64 	%rd173, %rd155, 80;
-	// inline asm
-	cvta.to.global.u64 %rd172, %rd173;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r195,%r196,%r197,%r198}, [%rd172];
-	// inline asm
-	add.s64 	%rd176, %rd155, 96;
-	// inline asm
-	cvta.to.global.u64 %rd175, %rd176;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r199,%r200,%r201,%r202}, [%rd175];
-	// inline asm
-	add.s64 	%rd179, %rd155, 112;
-	// inline asm
-	cvta.to.global.u64 %rd178, %rd179;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r203,%r204,%r205,%r206}, [%rd178];
-	// inline asm
-	add.s64 	%rd182, %rd155, 128;
-	// inline asm
-	cvta.to.global.u64 %rd181, %rd182;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r207,%r208,%r209,%r210}, [%rd181];
-	// inline asm
-	add.s64 	%rd185, %rd155, 144;
-	// inline asm
-	cvta.to.global.u64 %rd184, %rd185;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r211,%r212,%r213,%r214}, [%rd184];
-	// inline asm
-	mov.b32 	 %f580, %r178;
-	mov.b32 	 %f581, %r179;
-	cvt.u32.u16	%r231, %rs5;
-	add.s32 	%r232, %r231, -1;
-	cvt.rn.f32.s32	%f582, %r232;
-	sub.f32 	%f583, %f569, %f580;
-	mul.f32 	%f584, %f583, %f582;
-	sub.f32 	%f585, %f581, %f580;
-	div.rn.f32 	%f586, %f584, %f585;
-	min.f32 	%f587, %f582, %f586;
-	mov.f32 	%f588, 0f00000000;
-	max.f32 	%f589, %f588, %f587;
-	cvt.rmi.f32.f32	%f590, %f589;
-	cvt.rzi.s32.f32	%r233, %f590;
-	cvt.s64.s32	%rd15, %r233;
-	mul.wide.s32 	%rd199, %r233, 64;
-	add.s64 	%rd188, %rd164, %rd199;
-	// inline asm
-	cvta.to.global.u64 %rd187, %rd188;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r215,%r216,%r217,%r218}, [%rd187];
-	// inline asm
-	mov.b32 	 %f892, %r215;
-	mov.b32 	 %f893, %r216;
-	mov.b32 	 %f894, %r217;
-	add.s64 	%rd191, %rd188, 16;
-	// inline asm
-	cvta.to.global.u64 %rd190, %rd191;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r219,%r220,%r221,%r222}, [%rd190];
-	// inline asm
-	mov.b32 	 %f895, %r219;
-	mov.b32 	 %f896, %r220;
-	mov.b32 	 %f897, %r222;
-	add.s64 	%rd194, %rd188, 32;
-	// inline asm
-	cvta.to.global.u64 %rd193, %rd194;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r223,%r224,%r225,%r226}, [%rd193];
-	// inline asm
-	sub.f32 	%f209, %f589, %f590;
-	mov.b32 	 %f898, %r224;
-	mov.b32 	 %f899, %r225;
-	mov.b32 	 %f900, %r226;
-	add.s64 	%rd197, %rd188, 48;
-	// inline asm
-	cvta.to.global.u64 %rd196, %rd197;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r227,%r228,%r229,%r230}, [%rd196];
-	// inline asm
-	mov.b32 	 %f901, %r227;
-	setp.leu.f32	%p15, %f209, 0f00000000;
-	@%p15 bra 	BB5_32;
-
-	shl.b64 	%rd212, %rd15, 6;
-	add.s64 	%rd213, %rd212, %rd155;
-	add.s64 	%rd201, %rd213, 96;
-	// inline asm
-	cvta.to.global.u64 %rd200, %rd201;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r234,%r235,%r236,%r237}, [%rd200];
-	// inline asm
-	mov.b32 	 %f591, %r234;
-	mov.b32 	 %f592, %r235;
-	mov.b32 	 %f593, %r236;
-	add.s64 	%rd204, %rd213, 112;
-	// inline asm
-	cvta.to.global.u64 %rd203, %rd204;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r238,%r239,%r240,%r241}, [%rd203];
-	// inline asm
-	mov.b32 	 %f594, %r238;
-	mov.b32 	 %f595, %r239;
-	mov.b32 	 %f596, %r241;
-	add.s64 	%rd207, %rd213, 128;
-	// inline asm
-	cvta.to.global.u64 %rd206, %rd207;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r242,%r243,%r244,%r245}, [%rd206];
-	// inline asm
-	mov.b32 	 %f597, %r243;
-	mov.b32 	 %f598, %r244;
-	mov.b32 	 %f599, %r245;
-	add.s64 	%rd210, %rd213, 144;
-	// inline asm
-	cvta.to.global.u64 %rd209, %rd210;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r246,%r247,%r248,%r249}, [%rd209];
-	// inline asm
-	mov.f32 	%f600, 0f3F800000;
-	sub.f32 	%f601, %f600, %f209;
-	mul.f32 	%f602, %f209, %f591;
-	mul.f32 	%f603, %f209, %f592;
-	mul.f32 	%f604, %f209, %f593;
-	fma.rn.f32 	%f892, %f601, %f892, %f602;
-	fma.rn.f32 	%f893, %f601, %f893, %f603;
-	fma.rn.f32 	%f894, %f601, %f894, %f604;
-	mul.f32 	%f605, %f209, %f594;
-	mul.f32 	%f606, %f209, %f595;
-	mul.f32 	%f607, %f209, %f596;
-	fma.rn.f32 	%f895, %f601, %f895, %f605;
-	fma.rn.f32 	%f896, %f601, %f896, %f606;
-	fma.rn.f32 	%f897, %f601, %f897, %f607;
-	mul.f32 	%f608, %f209, %f597;
-	mul.f32 	%f609, %f209, %f598;
-	mul.f32 	%f610, %f209, %f599;
-	fma.rn.f32 	%f611, %f601, %f898, %f608;
-	fma.rn.f32 	%f612, %f601, %f899, %f609;
-	fma.rn.f32 	%f613, %f601, %f900, %f610;
-	mov.b32 	 %f614, %r246;
-	mul.f32 	%f615, %f209, %f614;
-	fma.rn.f32 	%f616, %f601, %f901, %f615;
-	mul.f32 	%f617, %f612, %f612;
-	fma.rn.f32 	%f618, %f611, %f611, %f617;
-	fma.rn.f32 	%f619, %f613, %f613, %f618;
-	fma.rn.f32 	%f620, %f616, %f616, %f619;
-	sqrt.rn.f32 	%f621, %f620;
-	rcp.rn.f32 	%f622, %f621;
-	mul.f32 	%f898, %f611, %f622;
-	mul.f32 	%f899, %f612, %f622;
-	mul.f32 	%f900, %f613, %f622;
-	mul.f32 	%f901, %f616, %f622;
-
-BB5_32:
-	mul.f32 	%f623, %f899, %f899;
-	fma.rn.f32 	%f624, %f898, %f898, %f623;
-	fma.rn.f32 	%f625, %f900, %f900, %f624;
-	fma.rn.f32 	%f626, %f901, %f901, %f625;
-	rcp.rn.f32 	%f627, %f626;
-	mul.f32 	%f628, %f898, %f627;
-	mul.f32 	%f629, %f899, %f627;
-	mul.f32 	%f630, %f900, %f627;
-	mul.f32 	%f631, %f901, %f627;
-	mul.f32 	%f632, %f898, %f628;
-	mul.f32 	%f633, %f899, %f629;
-	mul.f32 	%f634, %f900, %f630;
-	mul.f32 	%f635, %f898, %f629;
-	mul.f32 	%f636, %f900, %f631;
-	mul.f32 	%f637, %f898, %f630;
-	mul.f32 	%f638, %f899, %f631;
-	mul.f32 	%f639, %f899, %f630;
-	mul.f32 	%f640, %f898, %f631;
-	sub.f32 	%f641, %f632, %f633;
-	sub.f32 	%f642, %f641, %f634;
-	fma.rn.f32 	%f643, %f901, %f631, %f642;
-	sub.f32 	%f644, %f635, %f636;
-	add.f32 	%f645, %f644, %f644;
-	add.f32 	%f646, %f637, %f638;
-	add.f32 	%f647, %f646, %f646;
-	add.f32 	%f648, %f635, %f636;
-	add.f32 	%f649, %f648, %f648;
-	sub.f32 	%f650, %f633, %f632;
-	sub.f32 	%f651, %f650, %f634;
-	fma.rn.f32 	%f652, %f901, %f631, %f651;
-	sub.f32 	%f653, %f639, %f640;
-	add.f32 	%f654, %f653, %f653;
-	sub.f32 	%f655, %f637, %f638;
-	add.f32 	%f656, %f655, %f655;
-	add.f32 	%f657, %f639, %f640;
-	add.f32 	%f658, %f657, %f657;
-	neg.f32 	%f659, %f632;
-	sub.f32 	%f660, %f659, %f633;
-	add.f32 	%f661, %f634, %f660;
-	fma.rn.f32 	%f662, %f901, %f631, %f661;
-	mul.f32 	%f663, %f894, %f643;
-	fma.rn.f32 	%f664, %f896, %f645, %f663;
-	fma.rn.f32 	%f910, %f897, %f647, %f664;
-	mul.f32 	%f665, %f896, %f652;
-	fma.rn.f32 	%f666, %f894, %f649, %f665;
-	fma.rn.f32 	%f907, %f897, %f654, %f666;
-	mul.f32 	%f667, %f896, %f658;
-	fma.rn.f32 	%f668, %f894, %f656, %f667;
-	fma.rn.f32 	%f904, %f897, %f662, %f668;
-	mul.f32 	%f669, %f893, %f643;
-	fma.rn.f32 	%f909, %f895, %f645, %f669;
-	mul.f32 	%f670, %f895, %f652;
-	fma.rn.f32 	%f906, %f893, %f649, %f670;
-	mul.f32 	%f671, %f895, %f658;
-	fma.rn.f32 	%f903, %f893, %f656, %f671;
-	mul.f32 	%f908, %f892, %f643;
-	mul.f32 	%f905, %f892, %f649;
-	mul.f32 	%f902, %f892, %f656;
-
-BB5_35:
-	mul.f32 	%f703, %f903, %f907;
-	mul.f32 	%f704, %f904, %f906;
-	sub.f32 	%f705, %f704, %f703;
-	mul.f32 	%f706, %f908, %f705;
-	mul.f32 	%f707, %f902, %f907;
-	mul.f32 	%f708, %f904, %f905;
-	sub.f32 	%f709, %f708, %f707;
-	mul.f32 	%f710, %f709, %f909;
-	sub.f32 	%f711, %f706, %f710;
-	mul.f32 	%f712, %f902, %f906;
-	mul.f32 	%f713, %f903, %f905;
-	sub.f32 	%f714, %f713, %f712;
-	fma.rn.f32 	%f715, %f714, %f910, %f711;
-	rcp.rn.f32 	%f716, %f715;
-	mul.f32 	%f917, %f705, %f716;
-	mul.f32 	%f717, %f904, %f909;
-	mul.f32 	%f718, %f903, %f910;
-	sub.f32 	%f719, %f718, %f717;
-	mul.f32 	%f918, %f716, %f719;
-	mul.f32 	%f720, %f906, %f910;
-	mul.f32 	%f721, %f907, %f909;
-	sub.f32 	%f722, %f721, %f720;
-	mul.f32 	%f919, %f716, %f722;
-	sub.f32 	%f723, %f707, %f708;
-	mul.f32 	%f914, %f723, %f716;
-	mul.f32 	%f724, %f902, %f910;
-	mul.f32 	%f725, %f904, %f908;
-	sub.f32 	%f726, %f725, %f724;
-	mul.f32 	%f915, %f716, %f726;
-	mul.f32 	%f727, %f907, %f908;
-	mul.f32 	%f728, %f905, %f910;
-	sub.f32 	%f729, %f728, %f727;
-	mul.f32 	%f916, %f716, %f729;
-	mul.f32 	%f911, %f714, %f716;
-	mul.f32 	%f730, %f903, %f908;
-	mul.f32 	%f731, %f902, %f909;
-	sub.f32 	%f732, %f731, %f730;
-	mul.f32 	%f912, %f732, %f716;
-	mul.f32 	%f733, %f905, %f909;
-	mul.f32 	%f734, %f906, %f908;
-	sub.f32 	%f735, %f734, %f733;
-	mul.f32 	%f913, %f735, %f716;
-	bra.uni 	BB5_36;
-
-BB5_25:
-	setp.ne.s32	%p13, %r161, 1;
-	mov.f32 	%f912, %f911;
-	mov.f32 	%f914, %f911;
-	mov.f32 	%f915, %f913;
-	mov.f32 	%f916, %f911;
-	mov.f32 	%f917, %f913;
-	mov.f32 	%f918, %f911;
-	mov.f32 	%f919, %f911;
-	@%p13 bra 	BB5_36;
-
-	// inline asm
-	call (%rd142), _optix_get_static_transform_from_handle, (%rd140);
-	// inline asm
-	add.s64 	%rd264, %rd142, 64;
-
-BB5_28:
-	// inline asm
-	cvta.to.global.u64 %rd146, %rd264;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r163,%r164,%r165,%r166}, [%rd146];
-	// inline asm
-	mov.b32 	 %f917, %r163;
-	mov.b32 	 %f918, %r164;
-	mov.b32 	 %f919, %r165;
-	add.s64 	%rd150, %rd264, 16;
-	// inline asm
-	cvta.to.global.u64 %rd149, %rd150;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r167,%r168,%r169,%r170}, [%rd149];
-	// inline asm
-	mov.b32 	 %f914, %r167;
-	mov.b32 	 %f915, %r168;
-	mov.b32 	 %f916, %r169;
-	add.s64 	%rd153, %rd264, 32;
-	// inline asm
-	cvta.to.global.u64 %rd152, %rd153;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r171,%r172,%r173,%r174}, [%rd152];
-	// inline asm
-	mov.b32 	 %f911, %r171;
-	mov.b32 	 %f912, %r172;
-	mov.b32 	 %f913, %r173;
-
-BB5_36:
-	setp.eq.s32	%p17, %r314, 0;
-	@%p17 bra 	BB5_37;
-	bra.uni 	BB5_38;
-
-BB5_37:
-	mov.f32 	%f891, %f911;
-	mov.f32 	%f890, %f912;
-	mov.f32 	%f889, %f913;
-	mov.f32 	%f888, %f914;
-	mov.f32 	%f887, %f915;
-	mov.f32 	%f886, %f916;
-	mov.f32 	%f885, %f917;
-	mov.f32 	%f884, %f918;
-	mov.f32 	%f883, %f919;
-	bra.uni 	BB5_39;
-
-BB5_38:
-	mul.f32 	%f736, %f888, %f918;
-	fma.rn.f32 	%f737, %f885, %f917, %f736;
-	fma.rn.f32 	%f289, %f891, %f919, %f737;
-	mul.f32 	%f738, %f887, %f918;
-	fma.rn.f32 	%f739, %f884, %f917, %f738;
-	fma.rn.f32 	%f290, %f890, %f919, %f739;
-	mul.f32 	%f740, %f886, %f918;
-	fma.rn.f32 	%f741, %f883, %f917, %f740;
-	fma.rn.f32 	%f291, %f889, %f919, %f741;
-	mul.f32 	%f742, %f888, %f915;
-	fma.rn.f32 	%f743, %f885, %f914, %f742;
-	fma.rn.f32 	%f292, %f891, %f916, %f743;
-	mul.f32 	%f744, %f887, %f915;
-	fma.rn.f32 	%f745, %f884, %f914, %f744;
-	fma.rn.f32 	%f293, %f890, %f916, %f745;
-	mul.f32 	%f746, %f886, %f915;
-	fma.rn.f32 	%f747, %f883, %f914, %f746;
-	fma.rn.f32 	%f294, %f889, %f916, %f747;
-	mul.f32 	%f748, %f888, %f912;
-	fma.rn.f32 	%f749, %f885, %f911, %f748;
-	fma.rn.f32 	%f891, %f891, %f913, %f749;
-	mul.f32 	%f750, %f887, %f912;
-	fma.rn.f32 	%f751, %f884, %f911, %f750;
-	fma.rn.f32 	%f890, %f890, %f913, %f751;
-	mul.f32 	%f752, %f886, %f912;
-	fma.rn.f32 	%f753, %f883, %f911, %f752;
-	fma.rn.f32 	%f889, %f889, %f913, %f753;
-	mov.f32 	%f888, %f292;
-	mov.f32 	%f887, %f293;
-	mov.f32 	%f886, %f294;
-	mov.f32 	%f885, %f289;
-	mov.f32 	%f884, %f290;
-	mov.f32 	%f883, %f291;
-
-BB5_39:
-	add.s32 	%r314, %r314, 1;
-	setp.lt.u32	%p18, %r314, %r8;
-	@%p18 bra 	BB5_23;
-
-	mul.f32 	%f754, %f567, %f884;
-	fma.rn.f32 	%f755, %f566, %f885, %f754;
-	fma.rn.f32 	%f929, %f931, %f883, %f755;
-	mul.f32 	%f756, %f567, %f887;
-	fma.rn.f32 	%f757, %f566, %f888, %f756;
-	fma.rn.f32 	%f930, %f931, %f886, %f757;
-	mul.f32 	%f758, %f567, %f890;
-	fma.rn.f32 	%f759, %f566, %f891, %f758;
-	fma.rn.f32 	%f931, %f931, %f889, %f759;
-	bra.uni 	BB5_41;
-
-BB5_22:
-	mov.f32 	%f929, %f566;
-	mov.f32 	%f930, %f567;
-
-BB5_41:
-	ld.v4.f32 	{%f762, %f763, %f764, %f765}, [%rd1+80];
-	ld.v4.f32 	{%f769, %f770, %f771, %f772}, [%rd1+32];
-	fma.rn.f32 	%f774, %f882, %f769, %f762;
-	fma.rn.f32 	%f776, %f882, %f770, %f763;
-	fma.rn.f32 	%f778, %f882, %f771, %f764;
-	ld.v4.f32 	{%f779, %f780, %f781, %f782}, [%rd1+48];
-	fma.rn.f32 	%f784, %f881, %f779, %f774;
-	fma.rn.f32 	%f786, %f881, %f780, %f776;
-	fma.rn.f32 	%f788, %f881, %f781, %f778;
-	ld.v4.f32 	{%f789, %f790, %f791, %f792}, [%rd1+64];
-	fma.rn.f32 	%f794, %f880, %f789, %f784;
-	fma.rn.f32 	%f796, %f880, %f790, %f786;
-	fma.rn.f32 	%f798, %f880, %f791, %f788;
-	mul.f32 	%f799, %f929, %f769;
-	mul.f32 	%f800, %f929, %f770;
-	mul.f32 	%f801, %f929, %f771;
-	fma.rn.f32 	%f802, %f930, %f779, %f799;
-	fma.rn.f32 	%f803, %f930, %f780, %f800;
-	fma.rn.f32 	%f804, %f930, %f781, %f801;
-	fma.rn.f32 	%f805, %f931, %f789, %f802;
-	fma.rn.f32 	%f806, %f931, %f790, %f803;
-	fma.rn.f32 	%f807, %f931, %f791, %f804;
-	rcp.rn.f32 	%f808, %f807;
-	mul.f32 	%f809, %f798, %f808;
-	neg.f32 	%f313, %f809;
-	fma.rn.f32 	%f810, %f313, %f805, %f794;
-	fma.rn.f32 	%f314, %f313, %f806, %f796;
-	abs.f32 	%f811, %f810;
-	setp.gtu.f32	%p19, %f811, 0f3F800000;
-	@%p19 bra 	BB5_44;
-
-	abs.f32 	%f812, %f314;
-	setp.gtu.f32	%p20, %f812, 0f3F800000;
-	@%p20 bra 	BB5_44;
-
-	mov.u32 	%r310, 254;
-	// inline asm
-	call (%r309), _optix_report_intersection_0, (%f313, %r310);
-	// inline asm
-
-BB5_44:
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r309,%r310,%r311,%r312}, [%rd250];
+	// end inline asm
+	mov.b32 	%f726, %r309;
+	mov.b32 	%f727, %r310;
+	mov.b32 	%f728, %r311;
+	mul.f32 	%f729, %f258, %f726;
+	mul.f32 	%f730, %f258, %f727;
+	mul.f32 	%f731, %f258, %f728;
+	fma.rn.f32 	%f938, %f713, %f938, %f729;
+	fma.rn.f32 	%f939, %f713, %f939, %f730;
+	fma.rn.f32 	%f940, %f713, %f940, %f731;
+	bra.uni 	$L__BB5_36;
+
+$L__BB5_25:
+	mov.f32 	%f947, 0f00000000;
+	mov.f32 	%f949, 0f3F800000;
+	setp.eq.s32 	%p14, %r165, 4;
+	@%p14 bra 	$L__BB5_28;
+
+	setp.ne.s32 	%p15, %r165, 1;
+	mov.f32 	%f948, %f947;
+	mov.f32 	%f950, %f947;
+	mov.f32 	%f951, %f949;
+	mov.f32 	%f952, %f947;
+	mov.f32 	%f953, %f949;
+	mov.f32 	%f954, %f947;
+	mov.f32 	%f955, %f947;
+	@%p15 bra 	$L__BB5_37;
+
+	// begin inline asm
+	call (%rd138), _optix_get_static_transform_from_handle, (%rd136);
+	// end inline asm
+	add.s64 	%rd257, %rd138, 64;
+	bra.uni 	$L__BB5_29;
+
+$L__BB5_31:
+	// begin inline asm
+	call (%rd151), _optix_get_srt_motion_transform_from_handle, (%rd136);
+	// end inline asm
+	// begin inline asm
+	cvta.to.global.u64 %rd153, %rd151;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r179,%r180,%r181,%r182}, [%rd153];
+	// end inline asm
+	add.s64 	%rd157, %rd151, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd156, %rd157;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r183,%r184,%r185,%r186}, [%rd156];
+	// end inline asm
+	add.s64 	%rd160, %rd151, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd159, %rd160;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r187,%r188,%r189,%r190}, [%rd159];
+	// end inline asm
+	add.s64 	%rd163, %rd151, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd162, %rd163;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r191,%r192,%r193,%r194}, [%rd162];
+	// end inline asm
+	add.s64 	%rd166, %rd151, 64;
+	// begin inline asm
+	cvta.to.global.u64 %rd165, %rd166;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r195,%r196,%r197,%r198}, [%rd165];
+	// end inline asm
+	add.s64 	%rd169, %rd151, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd168, %rd169;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r199,%r200,%r201,%r202}, [%rd168];
+	// end inline asm
+	add.s64 	%rd172, %rd151, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd171, %rd172;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r203,%r204,%r205,%r206}, [%rd171];
+	// end inline asm
+	add.s64 	%rd175, %rd151, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd174, %rd175;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r207,%r208,%r209,%r210}, [%rd174];
+	// end inline asm
+	add.s64 	%rd178, %rd151, 128;
+	// begin inline asm
+	cvta.to.global.u64 %rd177, %rd178;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r211,%r212,%r213,%r214}, [%rd177];
+	// end inline asm
+	add.s64 	%rd181, %rd151, 144;
+	// begin inline asm
+	cvta.to.global.u64 %rd180, %rd181;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r215,%r216,%r217,%r218}, [%rd180];
+	// end inline asm
+	mov.b32 	%f609, %r182;
+	mov.b32 	%f610, %r183;
+	and.b32  	%r235, %r181, 65535;
+	add.s32 	%r236, %r235, -1;
+	cvt.rn.f32.s32 	%f611, %r236;
+	sub.f32 	%f612, %f597, %f609;
+	mul.f32 	%f613, %f612, %f611;
+	sub.f32 	%f614, %f610, %f609;
+	div.rn.f32 	%f615, %f613, %f614;
+	min.f32 	%f616, %f611, %f615;
+	mov.f32 	%f617, 0f00000000;
+	max.f32 	%f618, %f617, %f616;
+	cvt.rmi.f32.f32 	%f619, %f618;
+	sub.f32 	%f218, %f618, %f619;
+	cvt.rzi.s32.f32 	%r237, %f619;
+	mul.wide.s32 	%rd195, %r237, 64;
+	add.s64 	%rd184, %rd160, %rd195;
+	// begin inline asm
+	cvta.to.global.u64 %rd183, %rd184;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r219,%r220,%r221,%r222}, [%rd183];
+	// end inline asm
+	mov.b32 	%f928, %r219;
+	mov.b32 	%f929, %r220;
+	mov.b32 	%f930, %r221;
+	add.s64 	%rd187, %rd184, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd186, %rd187;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r223,%r224,%r225,%r226}, [%rd186];
+	// end inline asm
+	mov.b32 	%f931, %r223;
+	mov.b32 	%f932, %r224;
+	mov.b32 	%f933, %r226;
+	add.s64 	%rd190, %rd184, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd189, %rd190;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r227,%r228,%r229,%r230}, [%rd189];
+	// end inline asm
+	mov.b32 	%f934, %r228;
+	mov.b32 	%f935, %r229;
+	mov.b32 	%f936, %r230;
+	add.s64 	%rd193, %rd184, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd192, %rd193;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r231,%r232,%r233,%r234}, [%rd192];
+	// end inline asm
+	mov.b32 	%f937, %r231;
+	setp.leu.f32 	%p17, %f218, 0f00000000;
+	@%p17 bra 	$L__BB5_33;
+
+	mov.f32 	%f620, 0f3F800000;
+	sub.f32 	%f621, %f620, %f218;
+	add.s64 	%rd197, %rd184, 64;
+	// begin inline asm
+	cvta.to.global.u64 %rd196, %rd197;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r238,%r239,%r240,%r241}, [%rd196];
+	// end inline asm
+	mov.b32 	%f622, %r238;
+	mov.b32 	%f623, %r239;
+	mov.b32 	%f624, %r240;
+	mul.f32 	%f625, %f218, %f622;
+	mul.f32 	%f626, %f218, %f623;
+	mul.f32 	%f627, %f218, %f624;
+	fma.rn.f32 	%f928, %f621, %f928, %f625;
+	fma.rn.f32 	%f929, %f621, %f929, %f626;
+	fma.rn.f32 	%f930, %f621, %f930, %f627;
+	add.s64 	%rd200, %rd184, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd199, %rd200;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r242,%r243,%r244,%r245}, [%rd199];
+	// end inline asm
+	mov.b32 	%f628, %r242;
+	mov.b32 	%f629, %r243;
+	mov.b32 	%f630, %r245;
+	mul.f32 	%f631, %f218, %f628;
+	mul.f32 	%f632, %f218, %f629;
+	mul.f32 	%f633, %f218, %f630;
+	fma.rn.f32 	%f931, %f621, %f931, %f631;
+	fma.rn.f32 	%f932, %f621, %f932, %f632;
+	fma.rn.f32 	%f933, %f621, %f933, %f633;
+	add.s64 	%rd203, %rd184, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd202, %rd203;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r246,%r247,%r248,%r249}, [%rd202];
+	// end inline asm
+	mov.b32 	%f634, %r247;
+	mov.b32 	%f635, %r248;
+	mov.b32 	%f636, %r249;
+	mul.f32 	%f637, %f218, %f634;
+	mul.f32 	%f638, %f218, %f635;
+	mul.f32 	%f639, %f218, %f636;
+	fma.rn.f32 	%f640, %f621, %f934, %f637;
+	fma.rn.f32 	%f641, %f621, %f935, %f638;
+	fma.rn.f32 	%f642, %f621, %f936, %f639;
+	add.s64 	%rd206, %rd184, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd205, %rd206;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r250,%r251,%r252,%r253}, [%rd205];
+	// end inline asm
+	mov.b32 	%f643, %r250;
+	mul.f32 	%f644, %f218, %f643;
+	fma.rn.f32 	%f645, %f621, %f937, %f644;
+	mul.f32 	%f646, %f641, %f641;
+	fma.rn.f32 	%f647, %f640, %f640, %f646;
+	fma.rn.f32 	%f648, %f642, %f642, %f647;
+	fma.rn.f32 	%f649, %f645, %f645, %f648;
+	sqrt.rn.f32 	%f650, %f649;
+	rcp.rn.f32 	%f651, %f650;
+	mul.f32 	%f934, %f640, %f651;
+	mul.f32 	%f935, %f641, %f651;
+	mul.f32 	%f936, %f642, %f651;
+	mul.f32 	%f937, %f651, %f645;
+
+$L__BB5_33:
+	mul.f32 	%f652, %f935, %f935;
+	fma.rn.f32 	%f653, %f934, %f934, %f652;
+	fma.rn.f32 	%f654, %f936, %f936, %f653;
+	fma.rn.f32 	%f655, %f937, %f937, %f654;
+	rcp.rn.f32 	%f656, %f655;
+	mul.f32 	%f657, %f934, %f656;
+	mul.f32 	%f658, %f935, %f656;
+	mul.f32 	%f659, %f936, %f656;
+	mul.f32 	%f660, %f937, %f656;
+	mul.f32 	%f661, %f934, %f657;
+	mul.f32 	%f662, %f935, %f658;
+	mul.f32 	%f663, %f936, %f659;
+	mul.f32 	%f664, %f934, %f658;
+	mul.f32 	%f665, %f936, %f660;
+	mul.f32 	%f666, %f934, %f659;
+	mul.f32 	%f667, %f935, %f660;
+	mul.f32 	%f668, %f935, %f659;
+	mul.f32 	%f669, %f934, %f660;
+	sub.f32 	%f670, %f661, %f662;
+	sub.f32 	%f671, %f670, %f663;
+	fma.rn.f32 	%f672, %f937, %f660, %f671;
+	sub.f32 	%f673, %f664, %f665;
+	add.f32 	%f674, %f673, %f673;
+	add.f32 	%f675, %f666, %f667;
+	add.f32 	%f676, %f675, %f675;
+	add.f32 	%f677, %f664, %f665;
+	add.f32 	%f678, %f677, %f677;
+	sub.f32 	%f679, %f662, %f661;
+	sub.f32 	%f680, %f679, %f663;
+	fma.rn.f32 	%f681, %f937, %f660, %f680;
+	sub.f32 	%f682, %f668, %f669;
+	add.f32 	%f683, %f682, %f682;
+	sub.f32 	%f684, %f666, %f667;
+	add.f32 	%f685, %f684, %f684;
+	add.f32 	%f686, %f668, %f669;
+	add.f32 	%f687, %f686, %f686;
+	neg.f32 	%f688, %f661;
+	sub.f32 	%f689, %f688, %f662;
+	add.f32 	%f690, %f663, %f689;
+	fma.rn.f32 	%f691, %f937, %f660, %f690;
+	mul.f32 	%f692, %f930, %f672;
+	fma.rn.f32 	%f693, %f932, %f674, %f692;
+	fma.rn.f32 	%f946, %f933, %f676, %f693;
+	mul.f32 	%f694, %f932, %f681;
+	fma.rn.f32 	%f695, %f930, %f678, %f694;
+	fma.rn.f32 	%f943, %f933, %f683, %f695;
+	mul.f32 	%f696, %f932, %f687;
+	fma.rn.f32 	%f697, %f930, %f685, %f696;
+	fma.rn.f32 	%f940, %f933, %f691, %f697;
+	mul.f32 	%f698, %f929, %f672;
+	fma.rn.f32 	%f945, %f931, %f674, %f698;
+	mul.f32 	%f699, %f931, %f681;
+	fma.rn.f32 	%f942, %f929, %f678, %f699;
+	mul.f32 	%f700, %f931, %f687;
+	fma.rn.f32 	%f939, %f929, %f685, %f700;
+	mul.f32 	%f944, %f928, %f672;
+	mul.f32 	%f941, %f928, %f678;
+	mul.f32 	%f938, %f928, %f685;
+
+$L__BB5_36:
+	mul.f32 	%f732, %f939, %f943;
+	mul.f32 	%f733, %f940, %f942;
+	sub.f32 	%f734, %f733, %f732;
+	mul.f32 	%f735, %f944, %f734;
+	mul.f32 	%f736, %f938, %f943;
+	mul.f32 	%f737, %f940, %f941;
+	sub.f32 	%f738, %f737, %f736;
+	mul.f32 	%f739, %f738, %f945;
+	sub.f32 	%f740, %f735, %f739;
+	mul.f32 	%f741, %f938, %f942;
+	mul.f32 	%f742, %f939, %f941;
+	sub.f32 	%f743, %f742, %f741;
+	fma.rn.f32 	%f744, %f743, %f946, %f740;
+	rcp.rn.f32 	%f745, %f744;
+	mul.f32 	%f953, %f734, %f745;
+	mul.f32 	%f746, %f940, %f945;
+	mul.f32 	%f747, %f939, %f946;
+	sub.f32 	%f748, %f747, %f746;
+	mul.f32 	%f954, %f748, %f745;
+	mul.f32 	%f749, %f942, %f946;
+	mul.f32 	%f750, %f943, %f945;
+	sub.f32 	%f751, %f750, %f749;
+	mul.f32 	%f955, %f751, %f745;
+	sub.f32 	%f752, %f736, %f737;
+	mul.f32 	%f950, %f752, %f745;
+	mul.f32 	%f753, %f938, %f946;
+	mul.f32 	%f754, %f940, %f944;
+	sub.f32 	%f755, %f754, %f753;
+	mul.f32 	%f951, %f755, %f745;
+	mul.f32 	%f756, %f943, %f944;
+	mul.f32 	%f757, %f941, %f946;
+	sub.f32 	%f758, %f757, %f756;
+	mul.f32 	%f952, %f758, %f745;
+	mul.f32 	%f947, %f743, %f745;
+	mul.f32 	%f759, %f939, %f944;
+	mul.f32 	%f760, %f938, %f945;
+	sub.f32 	%f761, %f760, %f759;
+	mul.f32 	%f948, %f761, %f745;
+	mul.f32 	%f762, %f941, %f945;
+	mul.f32 	%f763, %f942, %f944;
+	sub.f32 	%f764, %f763, %f762;
+	mul.f32 	%f949, %f764, %f745;
+	bra.uni 	$L__BB5_37;
+
+$L__BB5_28:
+	// begin inline asm
+	call (%rd257), _optix_get_instance_inverse_transform_from_handle, (%rd136);
+	// end inline asm
+
+$L__BB5_29:
+	// begin inline asm
+	cvta.to.global.u64 %rd142, %rd257;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r167,%r168,%r169,%r170}, [%rd142];
+	// end inline asm
+	mov.b32 	%f953, %r167;
+	mov.b32 	%f954, %r168;
+	mov.b32 	%f955, %r169;
+	add.s64 	%rd146, %rd257, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd145, %rd146;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r171,%r172,%r173,%r174}, [%rd145];
+	// end inline asm
+	mov.b32 	%f950, %r171;
+	mov.b32 	%f951, %r172;
+	mov.b32 	%f952, %r173;
+	add.s64 	%rd149, %rd257, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd148, %rd149;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r175,%r176,%r177,%r178}, [%rd148];
+	// end inline asm
+	mov.b32 	%f947, %r175;
+	mov.b32 	%f948, %r176;
+	mov.b32 	%f949, %r177;
+
+$L__BB5_37:
+	setp.eq.s32 	%p19, %r317, 0;
+	@%p19 bra 	$L__BB5_39;
+
+	mul.f32 	%f765, %f924, %f954;
+	fma.rn.f32 	%f766, %f921, %f953, %f765;
+	fma.rn.f32 	%f304, %f927, %f955, %f766;
+	mul.f32 	%f767, %f923, %f954;
+	fma.rn.f32 	%f768, %f920, %f953, %f767;
+	fma.rn.f32 	%f305, %f926, %f955, %f768;
+	mul.f32 	%f769, %f922, %f954;
+	fma.rn.f32 	%f770, %f919, %f953, %f769;
+	fma.rn.f32 	%f955, %f925, %f955, %f770;
+	mul.f32 	%f771, %f924, %f951;
+	fma.rn.f32 	%f772, %f921, %f950, %f771;
+	fma.rn.f32 	%f307, %f927, %f952, %f772;
+	mul.f32 	%f773, %f923, %f951;
+	fma.rn.f32 	%f774, %f920, %f950, %f773;
+	fma.rn.f32 	%f308, %f926, %f952, %f774;
+	mul.f32 	%f775, %f922, %f951;
+	fma.rn.f32 	%f776, %f919, %f950, %f775;
+	fma.rn.f32 	%f952, %f925, %f952, %f776;
+	mul.f32 	%f777, %f924, %f948;
+	fma.rn.f32 	%f778, %f921, %f947, %f777;
+	fma.rn.f32 	%f310, %f927, %f949, %f778;
+	mul.f32 	%f779, %f923, %f948;
+	fma.rn.f32 	%f780, %f920, %f947, %f779;
+	fma.rn.f32 	%f311, %f926, %f949, %f780;
+	mul.f32 	%f781, %f922, %f948;
+	fma.rn.f32 	%f782, %f919, %f947, %f781;
+	fma.rn.f32 	%f949, %f925, %f949, %f782;
+	mov.f32 	%f947, %f310;
+	mov.f32 	%f948, %f311;
+	mov.f32 	%f950, %f307;
+	mov.f32 	%f951, %f308;
+	mov.f32 	%f953, %f304;
+	mov.f32 	%f954, %f305;
+
+$L__BB5_39:
+	add.s32 	%r317, %r317, 1;
+	setp.lt.u32 	%p20, %r317, %r162;
+	mov.f32 	%f919, %f955;
+	mov.f32 	%f920, %f954;
+	mov.f32 	%f921, %f953;
+	mov.f32 	%f922, %f952;
+	mov.f32 	%f923, %f951;
+	mov.f32 	%f924, %f950;
+	mov.f32 	%f925, %f949;
+	mov.f32 	%f926, %f948;
+	mov.f32 	%f927, %f947;
+	@%p20 bra 	$L__BB5_24;
+
+$L__BB5_40:
+	mul.f32 	%f783, %f975, %f954;
+	fma.rn.f32 	%f784, %f974, %f953, %f783;
+	mul.f32 	%f785, %f975, %f951;
+	fma.rn.f32 	%f786, %f974, %f950, %f785;
+	mul.f32 	%f787, %f975, %f948;
+	fma.rn.f32 	%f788, %f974, %f947, %f787;
+	fma.rn.f32 	%f976, %f596, %f949, %f788;
+	fma.rn.f32 	%f975, %f596, %f952, %f786;
+	fma.rn.f32 	%f974, %f596, %f955, %f784;
+	bra.uni 	$L__BB5_42;
+
+$L__BB5_41:
+	mov.f32 	%f976, %f596;
+
+$L__BB5_42:
+	ld.v4.f32 	{%f792, %f793, %f794, %f795}, [%rd1+80];
+	ld.f32 	%f799, [%rd1+32];
+	fma.rn.f32 	%f800, %f916, %f799, %f792;
+	ld.f32 	%f801, [%rd1+36];
+	fma.rn.f32 	%f802, %f916, %f801, %f793;
+	ld.f32 	%f803, [%rd1+40];
+	fma.rn.f32 	%f804, %f916, %f803, %f794;
+	ld.f32 	%f805, [%rd1+48];
+	fma.rn.f32 	%f806, %f917, %f805, %f800;
+	ld.f32 	%f807, [%rd1+52];
+	fma.rn.f32 	%f808, %f917, %f807, %f802;
+	ld.f32 	%f809, [%rd1+56];
+	fma.rn.f32 	%f810, %f917, %f809, %f804;
+	ld.f32 	%f811, [%rd1+64];
+	fma.rn.f32 	%f812, %f918, %f811, %f806;
+	ld.f32 	%f813, [%rd1+68];
+	fma.rn.f32 	%f814, %f918, %f813, %f808;
+	ld.f32 	%f815, [%rd1+72];
+	fma.rn.f32 	%f816, %f918, %f815, %f810;
+	ld.v4.f32 	{%f817, %f818, %f819, %f820}, [%rd1+32];
+	mul.f32 	%f824, %f974, %f817;
+	mul.f32 	%f825, %f974, %f818;
+	mul.f32 	%f826, %f974, %f819;
+	fma.rn.f32 	%f827, %f975, %f805, %f824;
+	fma.rn.f32 	%f828, %f975, %f807, %f825;
+	fma.rn.f32 	%f829, %f975, %f809, %f826;
+	fma.rn.f32 	%f830, %f976, %f811, %f827;
+	fma.rn.f32 	%f831, %f976, %f813, %f828;
+	fma.rn.f32 	%f832, %f976, %f815, %f829;
+	rcp.rn.f32 	%f833, %f832;
+	mul.f32 	%f834, %f816, %f833;
+	neg.f32 	%f340, %f834;
+	fma.rn.f32 	%f835, %f340, %f830, %f812;
+	fma.rn.f32 	%f341, %f340, %f831, %f814;
+	abs.f32 	%f836, %f835;
+	setp.gtu.f32 	%p21, %f836, 0f3F800000;
+	@%p21 bra 	$L__BB5_45;
+
+	abs.f32 	%f837, %f341;
+	setp.gtu.f32 	%p22, %f837, 0f3F800000;
+	@%p22 bra 	$L__BB5_45;
+
+	mov.u32 	%r314, 254;
+	// begin inline asm
+	call (%r313), _optix_report_intersection_0, (%f340, %r314);
+	// end inline asm
+
+$L__BB5_45:
 	ret;
-}
 
+}
 	// .globl	__closesthit__rectangle
-.visible .entry __closesthit__rectangle(
-
-)
+.visible .entry __closesthit__rectangle()
 {
-	.reg .pred 	%p<54>;
-	.reg .b16 	%rs<18>;
-	.reg .f32 	%f<1947>;
-	.reg .b32 	%r<638>;
-	.reg .b64 	%rd<670>;
-
-
-	// inline asm
-	call (%r21), _optix_get_launch_dimension_x, ();
-	// inline asm
-	// inline asm
-	call (%r22), _optix_get_launch_dimension_y, ();
-	// inline asm
-	// inline asm
-	call (%r24), _optix_get_launch_index_x, ();
-	// inline asm
-	// inline asm
-	call (%r25), _optix_get_launch_index_y, ();
-	// inline asm
-	// inline asm
-	call (%r26), _optix_get_launch_index_z, ();
-	// inline asm
-	mad.lo.s32 	%r27, %r26, %r22, %r25;
-	mad.lo.s32 	%r1, %r27, %r21, %r24;
+	.reg .pred 	%p<57>;
+	.reg .b16 	%rs<2>;
+	.reg .f32 	%f<2007>;
+	.reg .b32 	%r<650>;
+	.reg .b64 	%rd<656>;
+
+
+	// begin inline asm
+	call (%r25), _optix_get_launch_dimension_x, ();
+	// end inline asm
+	// begin inline asm
+	call (%r26), _optix_get_launch_dimension_y, ();
+	// end inline asm
+	// begin inline asm
+	call (%r28), _optix_get_launch_index_x, ();
+	// end inline asm
+	// begin inline asm
+	call (%r29), _optix_get_launch_index_y, ();
+	// end inline asm
+	// begin inline asm
+	call (%r30), _optix_get_launch_index_z, ();
+	// end inline asm
+	mad.lo.s32 	%r31, %r30, %r26, %r29;
+	mad.lo.s32 	%r1, %r31, %r25, %r28;
 	ld.const.u64 	%rd1, [params+352];
-	setp.eq.s64	%p1, %rd1, 0;
-	@%p1 bra 	BB6_2;
+	setp.eq.s64 	%p1, %rd1, 0;
+	@%p1 bra 	$L__BB6_2;
 
-	cvta.to.global.u64 	%rd49, %rd1;
-	cvt.u64.u32	%rd50, %r1;
-	add.s64 	%rd51, %rd49, %rd50;
+	cvta.to.global.u64 	%rd44, %rd1;
+	cvt.u64.u32 	%rd45, %r1;
+	add.s64 	%rd46, %rd44, %rd45;
 	mov.u16 	%rs1, 1;
-	st.global.u8 	[%rd51], %rs1;
-	bra.uni 	BB6_108;
-
-BB6_2:
-	// inline asm
-	call (%rd52), _optix_get_sbt_data_ptr_64, ();
-	// inline asm
-	ld.u64 	%rd3, [%rd52+8];
-	// inline asm
-	call (%f670), _optix_get_world_ray_origin_x, ();
-	// inline asm
-	// inline asm
-	call (%f671), _optix_get_world_ray_origin_y, ();
-	// inline asm
-	// inline asm
-	call (%f1743), _optix_get_world_ray_origin_z, ();
-	// inline asm
-	// inline asm
-	call (%r28), _optix_get_transform_list_size, ();
-	// inline asm
-	setp.eq.s32	%p2, %r28, 0;
-	@%p2 bra 	BB6_3;
-
-	mov.u32 	%r634, 0;
-	// inline asm
-	call (%f673), _optix_get_ray_time, ();
-	// inline asm
-
-BB6_5:
+	st.global.u8 	[%rd46], %rs1;
+	bra.uni 	$L__BB6_109;
+
+$L__BB6_2:
+	// begin inline asm
+	call (%rd47), _optix_get_sbt_data_ptr_64, ();
+	// end inline asm
+	ld.u64 	%rd3, [%rd47+8];
+	// begin inline asm
+	call (%f1794), _optix_get_world_ray_origin_x, ();
+	// end inline asm
+	// begin inline asm
+	call (%f1795), _optix_get_world_ray_origin_y, ();
+	// end inline asm
+	// begin inline asm
+	call (%f1796), _optix_get_world_ray_origin_z, ();
+	// end inline asm
+	// begin inline asm
+	call (%r32), _optix_get_transform_list_size, ();
+	// end inline asm
+	setp.eq.s32 	%p2, %r32, 0;
+	@%p2 bra 	$L__BB6_23;
+
+	// begin inline asm
+	call (%r33), _optix_get_transform_list_size, ();
+	// end inline asm
+	// begin inline asm
+	call (%f708), _optix_get_ray_time, ();
+	// end inline asm
+	setp.eq.s32 	%p3, %r33, 0;
+	@%p3 bra 	$L__BB6_21;
+
+	mov.u32 	%r645, 0;
+
+$L__BB6_5:
 	.pragma "nounroll";
-	// inline asm
-	call (%rd53), _optix_get_transform_list_handle, (%r634);
-	// inline asm
-	// inline asm
-	call (%r31), _optix_get_transform_type_from_handle, (%rd53);
-	// inline asm
-	and.b32  	%r32, %r31, -2;
-	setp.eq.s32	%p3, %r32, 2;
-	@%p3 bra 	BB6_11;
-	bra.uni 	BB6_6;
-
-BB6_11:
-	setp.eq.s32	%p6, %r31, 2;
-	@%p6 bra 	BB6_15;
-	bra.uni 	BB6_12;
-
-BB6_15:
-	// inline asm
-	call (%rd127), _optix_get_matrix_motion_transform_from_handle, (%rd53);
-	// inline asm
-	// inline asm
-	cvta.to.global.u64 %rd129, %rd127;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r120,%r121,%r122,%r123}, [%rd129];
-	// inline asm
-	mov.b32	{%rs4, %rs5}, %r122;
-	add.s64 	%rd133, %rd127, 16;
-	// inline asm
-	cvta.to.global.u64 %rd132, %rd133;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r124,%r125,%r126,%r127}, [%rd132];
-	// inline asm
-	add.s64 	%rd136, %rd127, 32;
-	// inline asm
-	cvta.to.global.u64 %rd135, %rd136;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r128,%r129,%r130,%r131}, [%rd135];
-	// inline asm
-	add.s64 	%rd139, %rd127, 48;
-	// inline asm
-	cvta.to.global.u64 %rd138, %rd139;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r132,%r133,%r134,%r135}, [%rd138];
-	// inline asm
-	add.s64 	%rd142, %rd127, 64;
-	// inline asm
-	cvta.to.global.u64 %rd141, %rd142;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r136,%r137,%r138,%r139}, [%rd141];
-	// inline asm
-	add.s64 	%rd145, %rd127, 80;
-	// inline asm
-	cvta.to.global.u64 %rd144, %rd145;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r140,%r141,%r142,%r143}, [%rd144];
-	// inline asm
-	add.s64 	%rd148, %rd127, 96;
-	// inline asm
-	cvta.to.global.u64 %rd147, %rd148;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r144,%r145,%r146,%r147}, [%rd147];
-	// inline asm
-	add.s64 	%rd151, %rd127, 112;
-	// inline asm
-	cvta.to.global.u64 %rd150, %rd151;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r148,%r149,%r150,%r151}, [%rd150];
-	// inline asm
-	mov.b32 	 %f800, %r123;
-	mov.b32 	 %f801, %r124;
-	cvt.u32.u16	%r164, %rs4;
-	add.s32 	%r165, %r164, -1;
-	cvt.rn.f32.s32	%f802, %r165;
-	sub.f32 	%f803, %f673, %f800;
-	mul.f32 	%f804, %f803, %f802;
-	sub.f32 	%f805, %f801, %f800;
-	div.rn.f32 	%f806, %f804, %f805;
-	min.f32 	%f807, %f802, %f806;
-	mov.f32 	%f808, 0f00000000;
-	max.f32 	%f809, %f808, %f807;
-	cvt.rmi.f32.f32	%f810, %f809;
-	cvt.rzi.s32.f32	%r166, %f810;
-	mul.wide.s32 	%rd162, %r166, 48;
-	add.s64 	%rd154, %rd136, %rd162;
-	// inline asm
-	cvta.to.global.u64 %rd153, %rd154;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r152,%r153,%r154,%r155}, [%rd153];
-	// inline asm
-	mov.b32 	 %f1715, %r152;
-	mov.b32 	 %f1716, %r153;
-	mov.b32 	 %f1717, %r154;
-	mov.b32 	 %f1718, %r155;
-	add.s64 	%rd157, %rd154, 16;
-	// inline asm
+	// begin inline asm
+	call (%rd48), _optix_get_transform_list_handle, (%r645);
+	// end inline asm
+	// begin inline asm
+	call (%r36), _optix_get_transform_type_from_handle, (%rd48);
+	// end inline asm
+	or.b32  	%r37, %r36, 1;
+	setp.eq.s32 	%p4, %r37, 3;
+	@%p4 bra 	$L__BB6_11;
+	bra.uni 	$L__BB6_6;
+
+$L__BB6_11:
+	setp.eq.s32 	%p7, %r36, 2;
+	@%p7 bra 	$L__BB6_15;
+	bra.uni 	$L__BB6_12;
+
+$L__BB6_15:
+	// begin inline asm
+	call (%rd120), _optix_get_matrix_motion_transform_from_handle, (%rd48);
+	// end inline asm
+	// begin inline asm
+	cvta.to.global.u64 %rd122, %rd120;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r125,%r126,%r127,%r128}, [%rd122];
+	// end inline asm
+	add.s64 	%rd126, %rd120, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd125, %rd126;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r129,%r130,%r131,%r132}, [%rd125];
+	// end inline asm
+	add.s64 	%rd129, %rd120, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd128, %rd129;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r133,%r134,%r135,%r136}, [%rd128];
+	// end inline asm
+	add.s64 	%rd132, %rd120, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd131, %rd132;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r137,%r138,%r139,%r140}, [%rd131];
+	// end inline asm
+	add.s64 	%rd135, %rd120, 64;
+	// begin inline asm
+	cvta.to.global.u64 %rd134, %rd135;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r141,%r142,%r143,%r144}, [%rd134];
+	// end inline asm
+	add.s64 	%rd138, %rd120, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd137, %rd138;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r145,%r146,%r147,%r148}, [%rd137];
+	// end inline asm
+	add.s64 	%rd141, %rd120, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd140, %rd141;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r149,%r150,%r151,%r152}, [%rd140];
+	// end inline asm
+	add.s64 	%rd144, %rd120, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd143, %rd144;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r153,%r154,%r155,%r156}, [%rd143];
+	// end inline asm
+	mov.b32 	%f836, %r128;
+	mov.b32 	%f837, %r129;
+	and.b32  	%r169, %r127, 65535;
+	add.s32 	%r170, %r169, -1;
+	cvt.rn.f32.s32 	%f838, %r170;
+	sub.f32 	%f839, %f708, %f836;
+	mul.f32 	%f840, %f839, %f838;
+	sub.f32 	%f841, %f837, %f836;
+	div.rn.f32 	%f842, %f840, %f841;
+	min.f32 	%f843, %f838, %f842;
+	mov.f32 	%f844, 0f00000000;
+	max.f32 	%f845, %f844, %f843;
+	cvt.rmi.f32.f32 	%f846, %f845;
+	sub.f32 	%f90, %f845, %f846;
+	cvt.rzi.s32.f32 	%r171, %f846;
+	mul.wide.s32 	%rd155, %r171, 48;
+	add.s64 	%rd147, %rd129, %rd155;
+	// begin inline asm
+	cvta.to.global.u64 %rd146, %rd147;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r157,%r158,%r159,%r160}, [%rd146];
+	// end inline asm
+	mov.b32 	%f1749, %r157;
+	mov.b32 	%f1748, %r158;
+	mov.b32 	%f1747, %r159;
+	mov.b32 	%f1746, %r160;
+	add.s64 	%rd150, %rd147, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd149, %rd150;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r161,%r162,%r163,%r164}, [%rd149];
+	// end inline asm
+	mov.b32 	%f1753, %r161;
+	mov.b32 	%f1752, %r162;
+	mov.b32 	%f1751, %r163;
+	mov.b32 	%f1750, %r164;
+	add.s64 	%rd153, %rd147, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd152, %rd153;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r165,%r166,%r167,%r168}, [%rd152];
+	// end inline asm
+	mov.b32 	%f1757, %r165;
+	mov.b32 	%f1756, %r166;
+	mov.b32 	%f1755, %r167;
+	mov.b32 	%f1754, %r168;
+	setp.leu.f32 	%p9, %f90, 0f00000000;
+	@%p9 bra 	$L__BB6_17;
+
+	cvt.rmi.f32.f32 	%f1717, %f845;
+	cvt.rzi.s32.f32 	%r644, %f1717;
+	cvt.s64.s32 	%rd651, %r644;
+	mov.f32 	%f847, 0f3F800000;
+	sub.f32 	%f848, %f847, %f90;
+	mul.lo.s64 	%rd165, %rd651, 48;
+	add.s64 	%rd166, %rd120, %rd165;
+	add.s64 	%rd157, %rd166, 80;
+	// begin inline asm
 	cvta.to.global.u64 %rd156, %rd157;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r156,%r157,%r158,%r159}, [%rd156];
-	// inline asm
-	mov.b32 	 %f1711, %r156;
-	mov.b32 	 %f1712, %r157;
-	mov.b32 	 %f1713, %r158;
-	mov.b32 	 %f1714, %r159;
-	add.s64 	%rd160, %rd154, 32;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r172,%r173,%r174,%r175}, [%rd156];
+	// end inline asm
+	mov.b32 	%f849, %r172;
+	mov.b32 	%f850, %r173;
+	mov.b32 	%f851, %r174;
+	mov.b32 	%f852, %r175;
+	mul.f32 	%f853, %f90, %f849;
+	mul.f32 	%f854, %f90, %f850;
+	mul.f32 	%f855, %f90, %f851;
+	mul.f32 	%f856, %f90, %f852;
+	fma.rn.f32 	%f1749, %f848, %f1749, %f853;
+	fma.rn.f32 	%f1748, %f848, %f1748, %f854;
+	fma.rn.f32 	%f1747, %f848, %f1747, %f855;
+	fma.rn.f32 	%f1746, %f848, %f1746, %f856;
+	add.s64 	%rd160, %rd166, 96;
+	// begin inline asm
 	cvta.to.global.u64 %rd159, %rd160;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r160,%r161,%r162,%r163}, [%rd159];
-	// inline asm
-	sub.f32 	%f98, %f809, %f810;
-	mov.b32 	 %f1710, %r160;
-	mov.b32 	 %f1709, %r161;
-	mov.b32 	 %f1708, %r162;
-	mov.b32 	 %f1707, %r163;
-	setp.leu.f32	%p8, %f98, 0f00000000;
-	@%p8 bra 	BB6_17;
-
-	cvt.rmi.f32.f32	%f1678, %f809;
-	cvt.rzi.s32.f32	%r633, %f1678;
-	cvt.s64.s32	%rd665, %r633;
-	mul.lo.s64 	%rd172, %rd665, 48;
-	add.s64 	%rd173, %rd127, %rd172;
-	add.s64 	%rd164, %rd173, 80;
-	// inline asm
-	cvta.to.global.u64 %rd163, %rd164;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r167,%r168,%r169,%r170}, [%rd163];
-	// inline asm
-	mov.b32 	 %f811, %r167;
-	mov.b32 	 %f812, %r168;
-	mov.b32 	 %f813, %r169;
-	mov.b32 	 %f814, %r170;
-	add.s64 	%rd167, %rd173, 96;
-	// inline asm
-	cvta.to.global.u64 %rd166, %rd167;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r171,%r172,%r173,%r174}, [%rd166];
-	// inline asm
-	mov.b32 	 %f815, %r171;
-	mov.b32 	 %f816, %r172;
-	mov.b32 	 %f817, %r173;
-	mov.b32 	 %f818, %r174;
-	add.s64 	%rd170, %rd173, 112;
-	// inline asm
-	cvta.to.global.u64 %rd169, %rd170;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r175,%r176,%r177,%r178}, [%rd169];
-	// inline asm
-	mov.f32 	%f819, 0f3F800000;
-	sub.f32 	%f820, %f819, %f98;
-	mul.f32 	%f821, %f98, %f811;
-	mul.f32 	%f822, %f98, %f812;
-	mul.f32 	%f823, %f98, %f813;
-	mul.f32 	%f824, %f98, %f814;
-	fma.rn.f32 	%f1715, %f820, %f1715, %f821;
-	fma.rn.f32 	%f1716, %f820, %f1716, %f822;
-	fma.rn.f32 	%f1717, %f820, %f1717, %f823;
-	fma.rn.f32 	%f1718, %f820, %f1718, %f824;
-	mul.f32 	%f825, %f98, %f815;
-	mul.f32 	%f826, %f98, %f816;
-	mul.f32 	%f827, %f98, %f817;
-	mul.f32 	%f828, %f98, %f818;
-	fma.rn.f32 	%f1711, %f820, %f1711, %f825;
-	fma.rn.f32 	%f1712, %f820, %f1712, %f826;
-	fma.rn.f32 	%f1713, %f820, %f1713, %f827;
-	fma.rn.f32 	%f1714, %f820, %f1714, %f828;
-	mov.b32 	 %f829, %r175;
-	mov.b32 	 %f830, %r176;
-	mov.b32 	 %f831, %r177;
-	mov.b32 	 %f832, %r178;
-	mul.f32 	%f833, %f98, %f829;
-	mul.f32 	%f834, %f98, %f830;
-	mul.f32 	%f835, %f98, %f831;
-	mul.f32 	%f836, %f98, %f832;
-	fma.rn.f32 	%f1710, %f820, %f1710, %f833;
-	fma.rn.f32 	%f1709, %f820, %f1709, %f834;
-	fma.rn.f32 	%f1708, %f820, %f1708, %f835;
-	fma.rn.f32 	%f1707, %f820, %f1707, %f836;
-	bra.uni 	BB6_17;
-
-BB6_6:
-	mov.f32 	%f1719, 0f00000000;
-	mov.f32 	%f1720, 0f3F800000;
-	setp.eq.s32	%p4, %r31, 4;
-	@%p4 bra 	BB6_9;
-	bra.uni 	BB6_7;
-
-BB6_9:
-	// inline asm
-	call (%rd666), _optix_get_instance_inverse_transform_from_handle, (%rd53);
-	// inline asm
-	bra.uni 	BB6_10;
-
-BB6_12:
-	// inline asm
-	call (%rd68), _optix_get_srt_motion_transform_from_handle, (%rd53);
-	// inline asm
-	// inline asm
-	cvta.to.global.u64 %rd70, %rd68;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r45,%r46,%r47,%r48}, [%rd70];
-	// inline asm
-	mov.b32	{%rs2, %rs3}, %r47;
-	add.s64 	%rd74, %rd68, 16;
-	// inline asm
-	cvta.to.global.u64 %rd73, %rd74;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r49,%r50,%r51,%r52}, [%rd73];
-	// inline asm
-	add.s64 	%rd77, %rd68, 32;
-	// inline asm
-	cvta.to.global.u64 %rd76, %rd77;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r53,%r54,%r55,%r56}, [%rd76];
-	// inline asm
-	add.s64 	%rd80, %rd68, 48;
-	// inline asm
-	cvta.to.global.u64 %rd79, %rd80;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r57,%r58,%r59,%r60}, [%rd79];
-	// inline asm
-	add.s64 	%rd83, %rd68, 64;
-	// inline asm
-	cvta.to.global.u64 %rd82, %rd83;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r61,%r62,%r63,%r64}, [%rd82];
-	// inline asm
-	add.s64 	%rd86, %rd68, 80;
-	// inline asm
-	cvta.to.global.u64 %rd85, %rd86;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r65,%r66,%r67,%r68}, [%rd85];
-	// inline asm
-	add.s64 	%rd89, %rd68, 96;
-	// inline asm
-	cvta.to.global.u64 %rd88, %rd89;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r69,%r70,%r71,%r72}, [%rd88];
-	// inline asm
-	add.s64 	%rd92, %rd68, 112;
-	// inline asm
-	cvta.to.global.u64 %rd91, %rd92;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r73,%r74,%r75,%r76}, [%rd91];
-	// inline asm
-	add.s64 	%rd95, %rd68, 128;
-	// inline asm
-	cvta.to.global.u64 %rd94, %rd95;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r77,%r78,%r79,%r80}, [%rd94];
-	// inline asm
-	add.s64 	%rd98, %rd68, 144;
-	// inline asm
-	cvta.to.global.u64 %rd97, %rd98;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r81,%r82,%r83,%r84}, [%rd97];
-	// inline asm
-	mov.b32 	 %f687, %r48;
-	mov.b32 	 %f688, %r49;
-	cvt.u32.u16	%r101, %rs2;
-	add.s32 	%r102, %r101, -1;
-	cvt.rn.f32.s32	%f689, %r102;
-	sub.f32 	%f690, %f673, %f687;
-	mul.f32 	%f691, %f690, %f689;
-	sub.f32 	%f692, %f688, %f687;
-	div.rn.f32 	%f693, %f691, %f692;
-	min.f32 	%f694, %f689, %f693;
-	mov.f32 	%f695, 0f00000000;
-	max.f32 	%f696, %f695, %f694;
-	cvt.rmi.f32.f32	%f697, %f696;
-	cvt.rzi.s32.f32	%r103, %f697;
-	mul.wide.s32 	%rd112, %r103, 64;
-	add.s64 	%rd101, %rd77, %rd112;
-	// inline asm
-	cvta.to.global.u64 %rd100, %rd101;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r85,%r86,%r87,%r88}, [%rd100];
-	// inline asm
-	mov.b32 	 %f1691, %r85;
-	mov.b32 	 %f1692, %r86;
-	mov.b32 	 %f1693, %r87;
-	mov.b32 	 %f1694, %r88;
-	add.s64 	%rd104, %rd101, 16;
-	// inline asm
-	cvta.to.global.u64 %rd103, %rd104;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r89,%r90,%r91,%r92}, [%rd103];
-	// inline asm
-	mov.b32 	 %f1695, %r89;
-	mov.b32 	 %f1696, %r90;
-	mov.b32 	 %f1697, %r91;
-	mov.b32 	 %f1698, %r92;
-	add.s64 	%rd107, %rd101, 32;
-	// inline asm
-	cvta.to.global.u64 %rd106, %rd107;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r93,%r94,%r95,%r96}, [%rd106];
-	// inline asm
-	sub.f32 	%f37, %f696, %f697;
-	mov.b32 	 %f1699, %r93;
-	mov.b32 	 %f1700, %r94;
-	mov.b32 	 %f1701, %r95;
-	mov.b32 	 %f1702, %r96;
-	add.s64 	%rd110, %rd101, 48;
-	// inline asm
-	cvta.to.global.u64 %rd109, %rd110;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r97,%r98,%r99,%r100}, [%rd109];
-	// inline asm
-	mov.b32 	 %f1703, %r97;
-	mov.b32 	 %f1704, %r98;
-	mov.b32 	 %f1705, %r99;
-	mov.b32 	 %f1706, %r100;
-	setp.leu.f32	%p7, %f37, 0f00000000;
-	@%p7 bra 	BB6_14;
-
-	cvt.rmi.f32.f32	%f1677, %f696;
-	cvt.rzi.s32.f32	%r632, %f1677;
-	cvt.s64.s32	%rd664, %r632;
-	shl.b64 	%rd125, %rd664, 6;
-	add.s64 	%rd126, %rd125, %rd68;
-	add.s64 	%rd114, %rd126, 96;
-	// inline asm
-	cvta.to.global.u64 %rd113, %rd114;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r104,%r105,%r106,%r107}, [%rd113];
-	// inline asm
-	mov.b32 	 %f698, %r104;
-	mov.b32 	 %f699, %r105;
-	mov.b32 	 %f700, %r106;
-	mov.b32 	 %f701, %r107;
-	add.s64 	%rd117, %rd126, 112;
-	// inline asm
-	cvta.to.global.u64 %rd116, %rd117;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r108,%r109,%r110,%r111}, [%rd116];
-	// inline asm
-	mov.b32 	 %f702, %r108;
-	mov.b32 	 %f703, %r109;
-	mov.b32 	 %f704, %r110;
-	mov.b32 	 %f705, %r111;
-	add.s64 	%rd120, %rd126, 128;
-	// inline asm
-	cvta.to.global.u64 %rd119, %rd120;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r112,%r113,%r114,%r115}, [%rd119];
-	// inline asm
-	mov.b32 	 %f706, %r112;
-	mov.b32 	 %f707, %r113;
-	mov.b32 	 %f708, %r114;
-	mov.b32 	 %f709, %r115;
-	add.s64 	%rd123, %rd126, 144;
-	// inline asm
-	cvta.to.global.u64 %rd122, %rd123;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r116,%r117,%r118,%r119}, [%rd122];
-	// inline asm
-	mov.f32 	%f710, 0f3F800000;
-	sub.f32 	%f711, %f710, %f37;
-	mul.f32 	%f712, %f37, %f698;
-	mul.f32 	%f713, %f37, %f699;
-	mul.f32 	%f714, %f37, %f700;
-	mul.f32 	%f715, %f37, %f701;
-	fma.rn.f32 	%f1691, %f711, %f1691, %f712;
-	fma.rn.f32 	%f1692, %f711, %f1692, %f713;
-	fma.rn.f32 	%f1693, %f711, %f1693, %f714;
-	fma.rn.f32 	%f1694, %f711, %f1694, %f715;
-	mul.f32 	%f716, %f37, %f702;
-	mul.f32 	%f717, %f37, %f703;
-	mul.f32 	%f718, %f37, %f704;
-	mul.f32 	%f719, %f37, %f705;
-	fma.rn.f32 	%f1695, %f711, %f1695, %f716;
-	fma.rn.f32 	%f1696, %f711, %f1696, %f717;
-	fma.rn.f32 	%f1697, %f711, %f1697, %f718;
-	fma.rn.f32 	%f1698, %f711, %f1698, %f719;
-	mul.f32 	%f720, %f37, %f706;
-	mul.f32 	%f721, %f37, %f707;
-	mul.f32 	%f722, %f37, %f708;
-	mul.f32 	%f723, %f37, %f709;
-	fma.rn.f32 	%f1699, %f711, %f1699, %f720;
-	fma.rn.f32 	%f724, %f711, %f1700, %f721;
-	fma.rn.f32 	%f725, %f711, %f1701, %f722;
-	fma.rn.f32 	%f726, %f711, %f1702, %f723;
-	mov.b32 	 %f727, %r116;
-	mov.b32 	 %f728, %r117;
-	mov.b32 	 %f729, %r118;
-	mov.b32 	 %f730, %r119;
-	mul.f32 	%f731, %f37, %f727;
-	mul.f32 	%f732, %f37, %f728;
-	mul.f32 	%f733, %f37, %f729;
-	mul.f32 	%f734, %f37, %f730;
-	fma.rn.f32 	%f735, %f711, %f1703, %f731;
-	fma.rn.f32 	%f1704, %f711, %f1704, %f732;
-	fma.rn.f32 	%f1705, %f711, %f1705, %f733;
-	fma.rn.f32 	%f1706, %f711, %f1706, %f734;
-	mul.f32 	%f736, %f725, %f725;
-	fma.rn.f32 	%f737, %f724, %f724, %f736;
-	fma.rn.f32 	%f738, %f726, %f726, %f737;
-	fma.rn.f32 	%f739, %f735, %f735, %f738;
-	sqrt.rn.f32 	%f740, %f739;
-	rcp.rn.f32 	%f741, %f740;
-	mul.f32 	%f1700, %f724, %f741;
-	mul.f32 	%f1701, %f725, %f741;
-	mul.f32 	%f1702, %f726, %f741;
-	mul.f32 	%f1703, %f735, %f741;
-
-BB6_14:
-	mul.f32 	%f742, %f1701, %f1701;
-	fma.rn.f32 	%f743, %f1700, %f1700, %f742;
-	fma.rn.f32 	%f744, %f1702, %f1702, %f743;
-	fma.rn.f32 	%f745, %f1703, %f1703, %f744;
-	rcp.rn.f32 	%f746, %f745;
-	mul.f32 	%f747, %f1700, %f746;
-	mul.f32 	%f748, %f1701, %f746;
-	mul.f32 	%f749, %f1702, %f746;
-	mul.f32 	%f750, %f1703, %f746;
-	mul.f32 	%f751, %f1700, %f747;
-	mul.f32 	%f752, %f1701, %f748;
-	mul.f32 	%f753, %f1702, %f749;
-	mul.f32 	%f754, %f1700, %f748;
-	mul.f32 	%f755, %f1702, %f750;
-	mul.f32 	%f756, %f1700, %f749;
-	mul.f32 	%f757, %f1701, %f750;
-	mul.f32 	%f758, %f1701, %f749;
-	mul.f32 	%f759, %f1700, %f750;
-	sub.f32 	%f760, %f751, %f752;
-	sub.f32 	%f761, %f760, %f753;
-	fma.rn.f32 	%f762, %f1703, %f750, %f761;
-	sub.f32 	%f763, %f754, %f755;
-	add.f32 	%f764, %f763, %f763;
-	add.f32 	%f765, %f756, %f757;
-	add.f32 	%f766, %f765, %f765;
-	add.f32 	%f767, %f754, %f755;
-	add.f32 	%f768, %f767, %f767;
-	sub.f32 	%f769, %f752, %f751;
-	sub.f32 	%f770, %f769, %f753;
-	fma.rn.f32 	%f771, %f1703, %f750, %f770;
-	sub.f32 	%f772, %f758, %f759;
-	add.f32 	%f773, %f772, %f772;
-	sub.f32 	%f774, %f756, %f757;
-	add.f32 	%f775, %f774, %f774;
-	add.f32 	%f776, %f758, %f759;
-	add.f32 	%f777, %f776, %f776;
-	neg.f32 	%f778, %f751;
-	sub.f32 	%f779, %f778, %f752;
-	add.f32 	%f780, %f753, %f779;
-	fma.rn.f32 	%f781, %f1703, %f750, %f780;
-	mul.f32 	%f782, %f1694, %f762;
-	fma.rn.f32 	%f783, %f1697, %f764, %f782;
-	fma.rn.f32 	%f784, %f1699, %f766, %f783;
-	sub.f32 	%f1718, %f1704, %f784;
-	mul.f32 	%f785, %f1697, %f771;
-	fma.rn.f32 	%f786, %f1694, %f768, %f785;
-	fma.rn.f32 	%f787, %f1699, %f773, %f786;
-	sub.f32 	%f1714, %f1705, %f787;
-	mul.f32 	%f788, %f1697, %f777;
-	fma.rn.f32 	%f789, %f1694, %f775, %f788;
-	fma.rn.f32 	%f790, %f1699, %f781, %f789;
-	sub.f32 	%f1707, %f1706, %f790;
-	mul.f32 	%f791, %f1693, %f762;
-	fma.rn.f32 	%f792, %f1696, %f764, %f791;
-	fma.rn.f32 	%f1717, %f1698, %f766, %f792;
-	mul.f32 	%f793, %f1696, %f771;
-	fma.rn.f32 	%f794, %f1693, %f768, %f793;
-	fma.rn.f32 	%f1713, %f1698, %f773, %f794;
-	mul.f32 	%f795, %f1696, %f777;
-	fma.rn.f32 	%f796, %f1693, %f775, %f795;
-	fma.rn.f32 	%f1708, %f1698, %f781, %f796;
-	mul.f32 	%f797, %f1692, %f762;
-	fma.rn.f32 	%f1716, %f1695, %f764, %f797;
-	mul.f32 	%f798, %f1695, %f771;
-	fma.rn.f32 	%f1712, %f1692, %f768, %f798;
-	mul.f32 	%f799, %f1695, %f777;
-	fma.rn.f32 	%f1709, %f1692, %f775, %f799;
-	mul.f32 	%f1715, %f1691, %f762;
-	mul.f32 	%f1711, %f1691, %f768;
-	mul.f32 	%f1710, %f1691, %f775;
-
-BB6_17:
-	mul.f32 	%f837, %f1709, %f1713;
-	mul.f32 	%f838, %f1708, %f1712;
-	sub.f32 	%f839, %f838, %f837;
-	mul.f32 	%f840, %f1715, %f839;
-	mul.f32 	%f841, %f1710, %f1713;
-	mul.f32 	%f842, %f1708, %f1711;
-	sub.f32 	%f843, %f842, %f841;
-	mul.f32 	%f844, %f843, %f1716;
-	sub.f32 	%f845, %f840, %f844;
-	mul.f32 	%f846, %f1710, %f1712;
-	mul.f32 	%f847, %f1709, %f1711;
-	sub.f32 	%f848, %f847, %f846;
-	fma.rn.f32 	%f849, %f848, %f1717, %f845;
-	rcp.rn.f32 	%f850, %f849;
-	mul.f32 	%f1727, %f839, %f850;
-	mul.f32 	%f851, %f1708, %f1716;
-	mul.f32 	%f852, %f1709, %f1717;
-	sub.f32 	%f853, %f852, %f851;
-	mul.f32 	%f1728, %f850, %f853;
-	mul.f32 	%f854, %f1712, %f1717;
-	mul.f32 	%f855, %f1713, %f1716;
-	sub.f32 	%f856, %f855, %f854;
-	mul.f32 	%f1729, %f850, %f856;
-	sub.f32 	%f857, %f841, %f842;
-	mul.f32 	%f1723, %f857, %f850;
-	mul.f32 	%f858, %f1710, %f1717;
-	mul.f32 	%f859, %f1708, %f1715;
-	sub.f32 	%f860, %f859, %f858;
-	mul.f32 	%f1724, %f850, %f860;
-	mul.f32 	%f861, %f1713, %f1715;
-	mul.f32 	%f862, %f1711, %f1717;
-	sub.f32 	%f863, %f862, %f861;
-	mul.f32 	%f1725, %f850, %f863;
-	mul.f32 	%f1722, %f848, %f850;
-	mul.f32 	%f864, %f1709, %f1715;
-	mul.f32 	%f865, %f1710, %f1716;
-	sub.f32 	%f866, %f865, %f864;
-	mul.f32 	%f1721, %f866, %f850;
-	mul.f32 	%f867, %f1711, %f1716;
-	mul.f32 	%f868, %f1712, %f1715;
-	sub.f32 	%f869, %f868, %f867;
-	mul.f32 	%f1720, %f869, %f850;
-	mul.f32 	%f870, %f1718, %f1727;
-	neg.f32 	%f871, %f870;
-	mul.f32 	%f872, %f1714, %f1728;
-	sub.f32 	%f873, %f871, %f872;
-	mul.f32 	%f874, %f1707, %f1729;
-	sub.f32 	%f1730, %f873, %f874;
-	mul.f32 	%f875, %f1718, %f1723;
-	neg.f32 	%f876, %f875;
-	mul.f32 	%f877, %f1714, %f1724;
-	sub.f32 	%f878, %f876, %f877;
-	mul.f32 	%f879, %f1707, %f1725;
-	sub.f32 	%f1726, %f878, %f879;
-	mul.f32 	%f880, %f1718, %f1722;
-	neg.f32 	%f881, %f880;
-	mul.f32 	%f882, %f1714, %f1721;
-	sub.f32 	%f883, %f881, %f882;
-	mul.f32 	%f884, %f1707, %f1720;
-	sub.f32 	%f1719, %f883, %f884;
-	bra.uni 	BB6_18;
-
-BB6_7:
-	setp.ne.s32	%p5, %r31, 1;
-	mov.f32 	%f1721, %f1719;
-	mov.f32 	%f1722, %f1719;
-	mov.f32 	%f1723, %f1719;
-	mov.f32 	%f1724, %f1720;
-	mov.f32 	%f1725, %f1719;
-	mov.f32 	%f1726, %f1719;
-	mov.f32 	%f1727, %f1720;
-	mov.f32 	%f1728, %f1719;
-	mov.f32 	%f1729, %f1719;
-	mov.f32 	%f1730, %f1719;
-	@%p5 bra 	BB6_18;
-
-	// inline asm
-	call (%rd55), _optix_get_static_transform_from_handle, (%rd53);
-	// inline asm
-	add.s64 	%rd666, %rd55, 64;
-
-BB6_10:
-	// inline asm
-	cvta.to.global.u64 %rd59, %rd666;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r33,%r34,%r35,%r36}, [%rd59];
-	// inline asm
-	mov.b32 	 %f1727, %r33;
-	mov.b32 	 %f1728, %r34;
-	mov.b32 	 %f1729, %r35;
-	mov.b32 	 %f1730, %r36;
-	add.s64 	%rd63, %rd666, 16;
-	// inline asm
-	cvta.to.global.u64 %rd62, %rd63;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r37,%r38,%r39,%r40}, [%rd62];
-	// inline asm
-	mov.b32 	 %f1723, %r37;
-	mov.b32 	 %f1724, %r38;
-	mov.b32 	 %f1725, %r39;
-	mov.b32 	 %f1726, %r40;
-	add.s64 	%rd66, %rd666, 32;
-	// inline asm
-	cvta.to.global.u64 %rd65, %rd66;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r41,%r42,%r43,%r44}, [%rd65];
-	// inline asm
-	mov.b32 	 %f1722, %r41;
-	mov.b32 	 %f1721, %r42;
-	mov.b32 	 %f1720, %r43;
-	mov.b32 	 %f1719, %r44;
-
-BB6_18:
-	setp.eq.s32	%p9, %r634, 0;
-	@%p9 bra 	BB6_19;
-	bra.uni 	BB6_20;
-
-BB6_19:
-	mov.f32 	%f1690, %f1730;
-	mov.f32 	%f1689, %f1729;
-	mov.f32 	%f1688, %f1728;
-	mov.f32 	%f1687, %f1727;
-	mov.f32 	%f1686, %f1726;
-	mov.f32 	%f1685, %f1725;
-	mov.f32 	%f1684, %f1724;
-	mov.f32 	%f1683, %f1723;
-	mov.f32 	%f1682, %f1719;
-	mov.f32 	%f1681, %f1720;
-	mov.f32 	%f1680, %f1721;
-	mov.f32 	%f1679, %f1722;
-	bra.uni 	BB6_21;
-
-BB6_20:
-	mul.f32 	%f885, %f1683, %f1728;
-	fma.rn.f32 	%f886, %f1687, %f1727, %f885;
-	fma.rn.f32 	%f151, %f1679, %f1729, %f886;
-	mul.f32 	%f887, %f1684, %f1728;
-	fma.rn.f32 	%f888, %f1688, %f1727, %f887;
-	fma.rn.f32 	%f152, %f1680, %f1729, %f888;
-	mul.f32 	%f889, %f1685, %f1728;
-	fma.rn.f32 	%f890, %f1689, %f1727, %f889;
-	fma.rn.f32 	%f153, %f1681, %f1729, %f890;
-	mul.f32 	%f891, %f1686, %f1728;
-	fma.rn.f32 	%f892, %f1690, %f1727, %f891;
-	fma.rn.f32 	%f893, %f1682, %f1729, %f892;
-	add.f32 	%f154, %f1730, %f893;
-	mul.f32 	%f894, %f1683, %f1724;
-	fma.rn.f32 	%f895, %f1687, %f1723, %f894;
-	fma.rn.f32 	%f155, %f1679, %f1725, %f895;
-	mul.f32 	%f896, %f1684, %f1724;
-	fma.rn.f32 	%f897, %f1688, %f1723, %f896;
-	fma.rn.f32 	%f156, %f1680, %f1725, %f897;
-	mul.f32 	%f898, %f1685, %f1724;
-	fma.rn.f32 	%f899, %f1689, %f1723, %f898;
-	fma.rn.f32 	%f157, %f1681, %f1725, %f899;
-	mul.f32 	%f900, %f1686, %f1724;
-	fma.rn.f32 	%f901, %f1690, %f1723, %f900;
-	fma.rn.f32 	%f902, %f1682, %f1725, %f901;
-	add.f32 	%f158, %f1726, %f902;
-	mul.f32 	%f903, %f1687, %f1722;
-	fma.rn.f32 	%f904, %f1683, %f1721, %f903;
-	fma.rn.f32 	%f1679, %f1679, %f1720, %f904;
-	mul.f32 	%f905, %f1688, %f1722;
-	fma.rn.f32 	%f906, %f1684, %f1721, %f905;
-	fma.rn.f32 	%f1680, %f1680, %f1720, %f906;
-	mul.f32 	%f907, %f1689, %f1722;
-	fma.rn.f32 	%f908, %f1685, %f1721, %f907;
-	fma.rn.f32 	%f1681, %f1681, %f1720, %f908;
-	mul.f32 	%f909, %f1690, %f1722;
-	fma.rn.f32 	%f910, %f1686, %f1721, %f909;
-	fma.rn.f32 	%f911, %f1682, %f1720, %f910;
-	add.f32 	%f1682, %f1719, %f911;
-	mov.f32 	%f1690, %f154;
-	mov.f32 	%f1689, %f153;
-	mov.f32 	%f1688, %f152;
-	mov.f32 	%f1687, %f151;
-	mov.f32 	%f1686, %f158;
-	mov.f32 	%f1685, %f157;
-	mov.f32 	%f1684, %f156;
-	mov.f32 	%f1683, %f155;
-
-BB6_21:
-	add.s32 	%r634, %r634, 1;
-	setp.lt.u32	%p10, %r634, %r28;
-	@%p10 bra 	BB6_5;
-
-	mul.f32 	%f912, %f670, %f1687;
-	fma.rn.f32 	%f913, %f671, %f1688, %f912;
-	fma.rn.f32 	%f914, %f1743, %f1689, %f913;
-	add.f32 	%f1745, %f1690, %f914;
-	mul.f32 	%f915, %f670, %f1683;
-	fma.rn.f32 	%f916, %f671, %f1684, %f915;
-	fma.rn.f32 	%f917, %f1743, %f1685, %f916;
-	add.f32 	%f1744, %f1686, %f917;
-	mul.f32 	%f918, %f670, %f1679;
-	fma.rn.f32 	%f919, %f671, %f1680, %f918;
-	fma.rn.f32 	%f920, %f1743, %f1681, %f919;
-	add.f32 	%f1743, %f1682, %f920;
-	bra.uni 	BB6_23;
-
-BB6_3:
-	mov.f32 	%f1744, %f671;
-	mov.f32 	%f1745, %f670;
-
-BB6_23:
-	// inline asm
-	call (%f921), _optix_get_world_ray_direction_x, ();
-	// inline asm
-	// inline asm
-	call (%f922), _optix_get_world_ray_direction_y, ();
-	// inline asm
-	// inline asm
-	call (%f1794), _optix_get_world_ray_direction_z, ();
-	// inline asm
-	// inline asm
-	call (%f924), _optix_get_ray_time, ();
-	// inline asm
-	mov.u32 	%r635, 0;
-	@%p2 bra 	BB6_24;
-
-BB6_25:
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r176,%r177,%r178,%r179}, [%rd159];
+	// end inline asm
+	mov.b32 	%f857, %r176;
+	mov.b32 	%f858, %r177;
+	mov.b32 	%f859, %r178;
+	mov.b32 	%f860, %r179;
+	mul.f32 	%f861, %f90, %f857;
+	mul.f32 	%f862, %f90, %f858;
+	mul.f32 	%f863, %f90, %f859;
+	mul.f32 	%f864, %f90, %f860;
+	fma.rn.f32 	%f1753, %f848, %f1753, %f861;
+	fma.rn.f32 	%f1752, %f848, %f1752, %f862;
+	fma.rn.f32 	%f1751, %f848, %f1751, %f863;
+	fma.rn.f32 	%f1750, %f848, %f1750, %f864;
+	add.s64 	%rd163, %rd166, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd162, %rd163;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r180,%r181,%r182,%r183}, [%rd162];
+	// end inline asm
+	mov.b32 	%f865, %r180;
+	mov.b32 	%f866, %r181;
+	mov.b32 	%f867, %r182;
+	mov.b32 	%f868, %r183;
+	mul.f32 	%f869, %f90, %f865;
+	mul.f32 	%f870, %f90, %f866;
+	mul.f32 	%f871, %f90, %f867;
+	mul.f32 	%f872, %f90, %f868;
+	fma.rn.f32 	%f1757, %f848, %f1757, %f869;
+	fma.rn.f32 	%f1756, %f848, %f1756, %f870;
+	fma.rn.f32 	%f1755, %f848, %f1755, %f871;
+	fma.rn.f32 	%f1754, %f848, %f1754, %f872;
+	bra.uni 	$L__BB6_17;
+
+$L__BB6_6:
+	mov.f32 	%f1758, 0f00000000;
+	mov.f32 	%f1761, 0f3F800000;
+	setp.eq.s32 	%p5, %r36, 4;
+	@%p5 bra 	$L__BB6_9;
+
+	setp.ne.s32 	%p6, %r36, 1;
+	mov.f32 	%f1759, %f1758;
+	mov.f32 	%f1760, %f1758;
+	mov.f32 	%f1762, %f1758;
+	mov.f32 	%f1763, %f1758;
+	mov.f32 	%f1764, %f1761;
+	mov.f32 	%f1765, %f1758;
+	mov.f32 	%f1766, %f1758;
+	mov.f32 	%f1767, %f1761;
+	mov.f32 	%f1768, %f1758;
+	mov.f32 	%f1769, %f1758;
+	@%p6 bra 	$L__BB6_18;
+
+	// begin inline asm
+	call (%rd50), _optix_get_static_transform_from_handle, (%rd48);
+	// end inline asm
+	add.s64 	%rd652, %rd50, 64;
+	bra.uni 	$L__BB6_10;
+
+$L__BB6_12:
+	// begin inline asm
+	call (%rd63), _optix_get_srt_motion_transform_from_handle, (%rd48);
+	// end inline asm
+	// begin inline asm
+	cvta.to.global.u64 %rd65, %rd63;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r50,%r51,%r52,%r53}, [%rd65];
+	// end inline asm
+	add.s64 	%rd69, %rd63, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd68, %rd69;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r54,%r55,%r56,%r57}, [%rd68];
+	// end inline asm
+	add.s64 	%rd72, %rd63, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd71, %rd72;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r58,%r59,%r60,%r61}, [%rd71];
+	// end inline asm
+	add.s64 	%rd75, %rd63, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd74, %rd75;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r62,%r63,%r64,%r65}, [%rd74];
+	// end inline asm
+	add.s64 	%rd78, %rd63, 64;
+	// begin inline asm
+	cvta.to.global.u64 %rd77, %rd78;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r66,%r67,%r68,%r69}, [%rd77];
+	// end inline asm
+	add.s64 	%rd81, %rd63, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd80, %rd81;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r70,%r71,%r72,%r73}, [%rd80];
+	// end inline asm
+	add.s64 	%rd84, %rd63, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd83, %rd84;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r74,%r75,%r76,%r77}, [%rd83];
+	// end inline asm
+	add.s64 	%rd87, %rd63, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd86, %rd87;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r78,%r79,%r80,%r81}, [%rd86];
+	// end inline asm
+	add.s64 	%rd90, %rd63, 128;
+	// begin inline asm
+	cvta.to.global.u64 %rd89, %rd90;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r82,%r83,%r84,%r85}, [%rd89];
+	// end inline asm
+	add.s64 	%rd93, %rd63, 144;
+	// begin inline asm
+	cvta.to.global.u64 %rd92, %rd93;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r86,%r87,%r88,%r89}, [%rd92];
+	// end inline asm
+	mov.b32 	%f723, %r53;
+	mov.b32 	%f724, %r54;
+	and.b32  	%r106, %r52, 65535;
+	add.s32 	%r107, %r106, -1;
+	cvt.rn.f32.s32 	%f725, %r107;
+	sub.f32 	%f726, %f708, %f723;
+	mul.f32 	%f727, %f726, %f725;
+	sub.f32 	%f728, %f724, %f723;
+	div.rn.f32 	%f729, %f727, %f728;
+	min.f32 	%f730, %f725, %f729;
+	mov.f32 	%f731, 0f00000000;
+	max.f32 	%f732, %f731, %f730;
+	cvt.rmi.f32.f32 	%f733, %f732;
+	sub.f32 	%f29, %f732, %f733;
+	cvt.rzi.s32.f32 	%r108, %f733;
+	mul.wide.s32 	%rd107, %r108, 64;
+	add.s64 	%rd96, %rd72, %rd107;
+	// begin inline asm
+	cvta.to.global.u64 %rd95, %rd96;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r90,%r91,%r92,%r93}, [%rd95];
+	// end inline asm
+	mov.b32 	%f1730, %r90;
+	mov.b32 	%f1731, %r91;
+	mov.b32 	%f1732, %r92;
+	mov.b32 	%f1733, %r93;
+	add.s64 	%rd99, %rd96, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd98, %rd99;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r94,%r95,%r96,%r97}, [%rd98];
+	// end inline asm
+	mov.b32 	%f1734, %r94;
+	mov.b32 	%f1735, %r95;
+	mov.b32 	%f1736, %r96;
+	mov.b32 	%f1737, %r97;
+	add.s64 	%rd102, %rd96, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd101, %rd102;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r98,%r99,%r100,%r101}, [%rd101];
+	// end inline asm
+	mov.b32 	%f1738, %r98;
+	mov.b32 	%f1739, %r99;
+	mov.b32 	%f1740, %r100;
+	mov.b32 	%f1741, %r101;
+	add.s64 	%rd105, %rd96, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd104, %rd105;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r102,%r103,%r104,%r105}, [%rd104];
+	// end inline asm
+	mov.b32 	%f1742, %r102;
+	mov.b32 	%f1743, %r103;
+	mov.b32 	%f1744, %r104;
+	mov.b32 	%f1745, %r105;
+	setp.leu.f32 	%p8, %f29, 0f00000000;
+	@%p8 bra 	$L__BB6_14;
+
+	mov.f32 	%f734, 0f3F800000;
+	sub.f32 	%f735, %f734, %f29;
+	add.s64 	%rd109, %rd96, 64;
+	// begin inline asm
+	cvta.to.global.u64 %rd108, %rd109;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r109,%r110,%r111,%r112}, [%rd108];
+	// end inline asm
+	mov.b32 	%f736, %r109;
+	mov.b32 	%f737, %r110;
+	mov.b32 	%f738, %r111;
+	mov.b32 	%f739, %r112;
+	mul.f32 	%f740, %f29, %f736;
+	mul.f32 	%f741, %f29, %f737;
+	mul.f32 	%f742, %f29, %f738;
+	mul.f32 	%f743, %f29, %f739;
+	fma.rn.f32 	%f1730, %f735, %f1730, %f740;
+	fma.rn.f32 	%f1731, %f735, %f1731, %f741;
+	fma.rn.f32 	%f1732, %f735, %f1732, %f742;
+	fma.rn.f32 	%f1733, %f735, %f1733, %f743;
+	add.s64 	%rd112, %rd96, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd111, %rd112;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r113,%r114,%r115,%r116}, [%rd111];
+	// end inline asm
+	mov.b32 	%f744, %r113;
+	mov.b32 	%f745, %r114;
+	mov.b32 	%f746, %r115;
+	mov.b32 	%f747, %r116;
+	mul.f32 	%f748, %f29, %f744;
+	mul.f32 	%f749, %f29, %f745;
+	mul.f32 	%f750, %f29, %f746;
+	mul.f32 	%f751, %f29, %f747;
+	fma.rn.f32 	%f1734, %f735, %f1734, %f748;
+	fma.rn.f32 	%f1735, %f735, %f1735, %f749;
+	fma.rn.f32 	%f1736, %f735, %f1736, %f750;
+	fma.rn.f32 	%f1737, %f735, %f1737, %f751;
+	add.s64 	%rd115, %rd96, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd114, %rd115;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r117,%r118,%r119,%r120}, [%rd114];
+	// end inline asm
+	mov.b32 	%f752, %r117;
+	mov.b32 	%f753, %r118;
+	mov.b32 	%f754, %r119;
+	mov.b32 	%f755, %r120;
+	mul.f32 	%f756, %f29, %f752;
+	mul.f32 	%f757, %f29, %f753;
+	mul.f32 	%f758, %f29, %f754;
+	mul.f32 	%f759, %f29, %f755;
+	fma.rn.f32 	%f1738, %f735, %f1738, %f756;
+	fma.rn.f32 	%f760, %f735, %f1739, %f757;
+	fma.rn.f32 	%f761, %f735, %f1740, %f758;
+	fma.rn.f32 	%f762, %f735, %f1741, %f759;
+	add.s64 	%rd118, %rd96, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd117, %rd118;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r121,%r122,%r123,%r124}, [%rd117];
+	// end inline asm
+	mov.b32 	%f763, %r121;
+	mov.b32 	%f764, %r122;
+	mov.b32 	%f765, %r123;
+	mov.b32 	%f766, %r124;
+	mul.f32 	%f767, %f29, %f763;
+	mul.f32 	%f768, %f29, %f764;
+	mul.f32 	%f769, %f29, %f765;
+	mul.f32 	%f770, %f29, %f766;
+	fma.rn.f32 	%f771, %f735, %f1742, %f767;
+	fma.rn.f32 	%f1743, %f735, %f1743, %f768;
+	fma.rn.f32 	%f1744, %f735, %f1744, %f769;
+	fma.rn.f32 	%f1745, %f735, %f1745, %f770;
+	mul.f32 	%f772, %f761, %f761;
+	fma.rn.f32 	%f773, %f760, %f760, %f772;
+	fma.rn.f32 	%f774, %f762, %f762, %f773;
+	fma.rn.f32 	%f775, %f771, %f771, %f774;
+	sqrt.rn.f32 	%f776, %f775;
+	rcp.rn.f32 	%f777, %f776;
+	mul.f32 	%f1739, %f760, %f777;
+	mul.f32 	%f1740, %f761, %f777;
+	mul.f32 	%f1741, %f762, %f777;
+	mul.f32 	%f1742, %f777, %f771;
+
+$L__BB6_14:
+	mul.f32 	%f778, %f1740, %f1740;
+	fma.rn.f32 	%f779, %f1739, %f1739, %f778;
+	fma.rn.f32 	%f780, %f1741, %f1741, %f779;
+	fma.rn.f32 	%f781, %f1742, %f1742, %f780;
+	rcp.rn.f32 	%f782, %f781;
+	mul.f32 	%f783, %f1739, %f782;
+	mul.f32 	%f784, %f1740, %f782;
+	mul.f32 	%f785, %f1741, %f782;
+	mul.f32 	%f786, %f1742, %f782;
+	mul.f32 	%f787, %f1739, %f783;
+	mul.f32 	%f788, %f1740, %f784;
+	mul.f32 	%f789, %f1741, %f785;
+	mul.f32 	%f790, %f1739, %f784;
+	mul.f32 	%f791, %f1741, %f786;
+	mul.f32 	%f792, %f1739, %f785;
+	mul.f32 	%f793, %f1740, %f786;
+	mul.f32 	%f794, %f1740, %f785;
+	mul.f32 	%f795, %f1739, %f786;
+	sub.f32 	%f796, %f787, %f788;
+	sub.f32 	%f797, %f796, %f789;
+	fma.rn.f32 	%f798, %f1742, %f786, %f797;
+	sub.f32 	%f799, %f790, %f791;
+	add.f32 	%f800, %f799, %f799;
+	add.f32 	%f801, %f792, %f793;
+	add.f32 	%f802, %f801, %f801;
+	add.f32 	%f803, %f790, %f791;
+	add.f32 	%f804, %f803, %f803;
+	sub.f32 	%f805, %f788, %f787;
+	sub.f32 	%f806, %f805, %f789;
+	fma.rn.f32 	%f807, %f1742, %f786, %f806;
+	sub.f32 	%f808, %f794, %f795;
+	add.f32 	%f809, %f808, %f808;
+	sub.f32 	%f810, %f792, %f793;
+	add.f32 	%f811, %f810, %f810;
+	add.f32 	%f812, %f794, %f795;
+	add.f32 	%f813, %f812, %f812;
+	neg.f32 	%f814, %f787;
+	sub.f32 	%f815, %f814, %f788;
+	add.f32 	%f816, %f789, %f815;
+	fma.rn.f32 	%f817, %f1742, %f786, %f816;
+	mul.f32 	%f818, %f1733, %f798;
+	fma.rn.f32 	%f819, %f1736, %f800, %f818;
+	fma.rn.f32 	%f820, %f1738, %f802, %f819;
+	sub.f32 	%f1746, %f1743, %f820;
+	mul.f32 	%f821, %f1736, %f807;
+	fma.rn.f32 	%f822, %f1733, %f804, %f821;
+	fma.rn.f32 	%f823, %f1738, %f809, %f822;
+	sub.f32 	%f1750, %f1744, %f823;
+	mul.f32 	%f824, %f1736, %f813;
+	fma.rn.f32 	%f825, %f1733, %f811, %f824;
+	fma.rn.f32 	%f826, %f1738, %f817, %f825;
+	sub.f32 	%f1754, %f1745, %f826;
+	mul.f32 	%f827, %f1732, %f798;
+	fma.rn.f32 	%f828, %f1735, %f800, %f827;
+	fma.rn.f32 	%f1747, %f1737, %f802, %f828;
+	mul.f32 	%f829, %f1735, %f807;
+	fma.rn.f32 	%f830, %f1732, %f804, %f829;
+	fma.rn.f32 	%f1751, %f1737, %f809, %f830;
+	mul.f32 	%f831, %f1735, %f813;
+	fma.rn.f32 	%f832, %f1732, %f811, %f831;
+	fma.rn.f32 	%f1755, %f1737, %f817, %f832;
+	mul.f32 	%f833, %f1731, %f798;
+	fma.rn.f32 	%f1748, %f1734, %f800, %f833;
+	mul.f32 	%f834, %f1734, %f807;
+	fma.rn.f32 	%f1752, %f1731, %f804, %f834;
+	mul.f32 	%f835, %f1734, %f813;
+	fma.rn.f32 	%f1756, %f1731, %f811, %f835;
+	mul.f32 	%f1749, %f1730, %f798;
+	mul.f32 	%f1753, %f1730, %f804;
+	mul.f32 	%f1757, %f1730, %f811;
+
+$L__BB6_17:
+	mul.f32 	%f873, %f1751, %f1756;
+	mul.f32 	%f874, %f1752, %f1755;
+	sub.f32 	%f875, %f874, %f873;
+	mul.f32 	%f876, %f1749, %f875;
+	mul.f32 	%f877, %f1751, %f1757;
+	mul.f32 	%f878, %f1753, %f1755;
+	sub.f32 	%f879, %f878, %f877;
+	mul.f32 	%f880, %f1748, %f879;
+	sub.f32 	%f881, %f876, %f880;
+	mul.f32 	%f882, %f1752, %f1757;
+	mul.f32 	%f883, %f1753, %f1756;
+	sub.f32 	%f884, %f883, %f882;
+	fma.rn.f32 	%f885, %f1747, %f884, %f881;
+	rcp.rn.f32 	%f886, %f885;
+	mul.f32 	%f1761, %f875, %f886;
+	mul.f32 	%f887, %f1748, %f1755;
+	mul.f32 	%f888, %f1747, %f1756;
+	sub.f32 	%f889, %f888, %f887;
+	mul.f32 	%f1760, %f889, %f886;
+	mul.f32 	%f890, %f1747, %f1752;
+	mul.f32 	%f891, %f1748, %f1751;
+	sub.f32 	%f892, %f891, %f890;
+	mul.f32 	%f1759, %f892, %f886;
+	sub.f32 	%f893, %f877, %f878;
+	mul.f32 	%f1765, %f893, %f886;
+	mul.f32 	%f894, %f1747, %f1757;
+	mul.f32 	%f895, %f1749, %f1755;
+	sub.f32 	%f896, %f895, %f894;
+	mul.f32 	%f1764, %f896, %f886;
+	mul.f32 	%f897, %f1749, %f1751;
+	mul.f32 	%f898, %f1747, %f1753;
+	sub.f32 	%f899, %f898, %f897;
+	mul.f32 	%f1763, %f899, %f886;
+	mul.f32 	%f1769, %f884, %f886;
+	mul.f32 	%f900, %f1749, %f1756;
+	mul.f32 	%f901, %f1748, %f1757;
+	sub.f32 	%f902, %f901, %f900;
+	mul.f32 	%f1768, %f902, %f886;
+	mul.f32 	%f903, %f1748, %f1753;
+	mul.f32 	%f904, %f1749, %f1752;
+	sub.f32 	%f905, %f904, %f903;
+	mul.f32 	%f1767, %f905, %f886;
+	mul.f32 	%f906, %f1746, %f1761;
+	neg.f32 	%f907, %f906;
+	mul.f32 	%f908, %f1750, %f1760;
+	sub.f32 	%f909, %f907, %f908;
+	mul.f32 	%f910, %f1754, %f1759;
+	sub.f32 	%f1758, %f909, %f910;
+	mul.f32 	%f911, %f1746, %f1765;
+	neg.f32 	%f912, %f911;
+	mul.f32 	%f913, %f1750, %f1764;
+	sub.f32 	%f914, %f912, %f913;
+	mul.f32 	%f915, %f1754, %f1763;
+	sub.f32 	%f1762, %f914, %f915;
+	mul.f32 	%f916, %f1746, %f1769;
+	neg.f32 	%f917, %f916;
+	mul.f32 	%f918, %f1750, %f1768;
+	sub.f32 	%f919, %f917, %f918;
+	mul.f32 	%f920, %f1754, %f1767;
+	sub.f32 	%f1766, %f919, %f920;
+	bra.uni 	$L__BB6_18;
+
+$L__BB6_9:
+	// begin inline asm
+	call (%rd652), _optix_get_instance_inverse_transform_from_handle, (%rd48);
+	// end inline asm
+
+$L__BB6_10:
+	// begin inline asm
+	cvta.to.global.u64 %rd54, %rd652;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r38,%r39,%r40,%r41}, [%rd54];
+	// end inline asm
+	mov.b32 	%f1761, %r38;
+	mov.b32 	%f1760, %r39;
+	mov.b32 	%f1759, %r40;
+	mov.b32 	%f1758, %r41;
+	add.s64 	%rd58, %rd652, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd57, %rd58;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r42,%r43,%r44,%r45}, [%rd57];
+	// end inline asm
+	mov.b32 	%f1765, %r42;
+	mov.b32 	%f1764, %r43;
+	mov.b32 	%f1763, %r44;
+	mov.b32 	%f1762, %r45;
+	add.s64 	%rd61, %rd652, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd60, %rd61;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r46,%r47,%r48,%r49}, [%rd60];
+	// end inline asm
+	mov.b32 	%f1769, %r46;
+	mov.b32 	%f1768, %r47;
+	mov.b32 	%f1767, %r48;
+	mov.b32 	%f1766, %r49;
+
+$L__BB6_18:
+	setp.eq.s32 	%p10, %r645, 0;
+	@%p10 bra 	$L__BB6_20;
+
+	mul.f32 	%f921, %f1726, %f1761;
+	fma.rn.f32 	%f922, %f1722, %f1760, %f921;
+	fma.rn.f32 	%f151, %f1718, %f1759, %f922;
+	mul.f32 	%f923, %f1727, %f1761;
+	fma.rn.f32 	%f924, %f1723, %f1760, %f923;
+	fma.rn.f32 	%f152, %f1719, %f1759, %f924;
+	mul.f32 	%f925, %f1728, %f1761;
+	fma.rn.f32 	%f926, %f1724, %f1760, %f925;
+	fma.rn.f32 	%f153, %f1720, %f1759, %f926;
+	mul.f32 	%f927, %f1729, %f1761;
+	fma.rn.f32 	%f928, %f1725, %f1760, %f927;
+	fma.rn.f32 	%f929, %f1721, %f1759, %f928;
+	add.f32 	%f1758, %f1758, %f929;
+	mul.f32 	%f930, %f1726, %f1765;
+	fma.rn.f32 	%f931, %f1722, %f1764, %f930;
+	fma.rn.f32 	%f155, %f1718, %f1763, %f931;
+	mul.f32 	%f932, %f1727, %f1765;
+	fma.rn.f32 	%f933, %f1723, %f1764, %f932;
+	fma.rn.f32 	%f156, %f1719, %f1763, %f933;
+	mul.f32 	%f934, %f1728, %f1765;
+	fma.rn.f32 	%f935, %f1724, %f1764, %f934;
+	fma.rn.f32 	%f157, %f1720, %f1763, %f935;
+	mul.f32 	%f936, %f1729, %f1765;
+	fma.rn.f32 	%f937, %f1725, %f1764, %f936;
+	fma.rn.f32 	%f938, %f1721, %f1763, %f937;
+	add.f32 	%f1762, %f1762, %f938;
+	mul.f32 	%f939, %f1726, %f1769;
+	fma.rn.f32 	%f940, %f1722, %f1768, %f939;
+	fma.rn.f32 	%f159, %f1718, %f1767, %f940;
+	mul.f32 	%f941, %f1727, %f1769;
+	fma.rn.f32 	%f942, %f1723, %f1768, %f941;
+	fma.rn.f32 	%f160, %f1719, %f1767, %f942;
+	mul.f32 	%f943, %f1728, %f1769;
+	fma.rn.f32 	%f944, %f1724, %f1768, %f943;
+	fma.rn.f32 	%f161, %f1720, %f1767, %f944;
+	mul.f32 	%f945, %f1729, %f1769;
+	fma.rn.f32 	%f946, %f1725, %f1768, %f945;
+	fma.rn.f32 	%f947, %f1721, %f1767, %f946;
+	add.f32 	%f1766, %f1766, %f947;
+	mov.f32 	%f1759, %f153;
+	mov.f32 	%f1760, %f152;
+	mov.f32 	%f1761, %f151;
+	mov.f32 	%f1763, %f157;
+	mov.f32 	%f1764, %f156;
+	mov.f32 	%f1765, %f155;
+	mov.f32 	%f1767, %f161;
+	mov.f32 	%f1768, %f160;
+	mov.f32 	%f1769, %f159;
+
+$L__BB6_20:
+	add.s32 	%r645, %r645, 1;
+	setp.lt.u32 	%p11, %r645, %r33;
+	mov.f32 	%f1718, %f1769;
+	mov.f32 	%f1719, %f1768;
+	mov.f32 	%f1720, %f1767;
+	mov.f32 	%f1721, %f1766;
+	mov.f32 	%f1722, %f1765;
+	mov.f32 	%f1723, %f1764;
+	mov.f32 	%f1724, %f1763;
+	mov.f32 	%f1725, %f1762;
+	mov.f32 	%f1726, %f1761;
+	mov.f32 	%f1727, %f1760;
+	mov.f32 	%f1728, %f1759;
+	mov.f32 	%f1729, %f1758;
+	@%p11 bra 	$L__BB6_5;
+
+$L__BB6_21:
+	mul.f32 	%f948, %f1794, %f1761;
+	fma.rn.f32 	%f949, %f1795, %f1760, %f948;
+	fma.rn.f32 	%f950, %f1796, %f1759, %f949;
+	mul.f32 	%f951, %f1794, %f1765;
+	fma.rn.f32 	%f952, %f1795, %f1764, %f951;
+	fma.rn.f32 	%f953, %f1796, %f1763, %f952;
+	mul.f32 	%f954, %f1794, %f1769;
+	fma.rn.f32 	%f955, %f1795, %f1768, %f954;
+	fma.rn.f32 	%f956, %f1796, %f1767, %f955;
+	add.f32 	%f1796, %f1766, %f956;
+	add.f32 	%f1795, %f1762, %f953;
+	add.f32 	%f1794, %f1758, %f950;
+
+$L__BB6_23:
+	// begin inline asm
+	call (%f1852), _optix_get_world_ray_direction_x, ();
+	// end inline asm
+	// begin inline asm
+	call (%f1853), _optix_get_world_ray_direction_y, ();
+	// end inline asm
+	// begin inline asm
+	call (%f959), _optix_get_world_ray_direction_z, ();
+	// end inline asm
+	// begin inline asm
+	call (%r184), _optix_get_transform_list_size, ();
+	// end inline asm
+	setp.eq.s32 	%p12, %r184, 0;
+	@%p12 bra 	$L__BB6_43;
+
+	// begin inline asm
+	call (%r185), _optix_get_transform_list_size, ();
+	// end inline asm
+	// begin inline asm
+	call (%f960), _optix_get_ray_time, ();
+	// end inline asm
+	setp.eq.s32 	%p13, %r185, 0;
+	@%p13 bra 	$L__BB6_42;
+
+	mov.u32 	%r646, 0;
+
+$L__BB6_26:
 	.pragma "nounroll";
-	// inline asm
-	call (%rd174), _optix_get_transform_list_handle, (%r635);
-	// inline asm
-	// inline asm
-	call (%r181), _optix_get_transform_type_from_handle, (%rd174);
-	// inline asm
-	and.b32  	%r182, %r181, -2;
-	setp.eq.s32	%p12, %r182, 2;
-	@%p12 bra 	BB6_31;
-	bra.uni 	BB6_26;
-
-BB6_31:
-	setp.eq.s32	%p15, %r181, 2;
-	@%p15 bra 	BB6_35;
-	bra.uni 	BB6_32;
-
-BB6_35:
-	// inline asm
-	call (%rd248), _optix_get_matrix_motion_transform_from_handle, (%rd174);
-	// inline asm
-	// inline asm
-	cvta.to.global.u64 %rd250, %rd248;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r270,%r271,%r272,%r273}, [%rd250];
-	// inline asm
-	mov.b32	{%rs8, %rs9}, %r272;
-	add.s64 	%rd254, %rd248, 16;
-	// inline asm
+	// begin inline asm
+	call (%rd167), _optix_get_transform_list_handle, (%r646);
+	// end inline asm
+	// begin inline asm
+	call (%r188), _optix_get_transform_type_from_handle, (%rd167);
+	// end inline asm
+	or.b32  	%r189, %r188, 1;
+	setp.eq.s32 	%p14, %r189, 3;
+	@%p14 bra 	$L__BB6_32;
+	bra.uni 	$L__BB6_27;
+
+$L__BB6_32:
+	setp.eq.s32 	%p17, %r188, 2;
+	@%p17 bra 	$L__BB6_36;
+	bra.uni 	$L__BB6_33;
+
+$L__BB6_36:
+	// begin inline asm
+	call (%rd239), _optix_get_matrix_motion_transform_from_handle, (%rd167);
+	// end inline asm
+	// begin inline asm
+	cvta.to.global.u64 %rd241, %rd239;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r277,%r278,%r279,%r280}, [%rd241];
+	// end inline asm
+	add.s64 	%rd245, %rd239, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd244, %rd245;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r281,%r282,%r283,%r284}, [%rd244];
+	// end inline asm
+	add.s64 	%rd248, %rd239, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd247, %rd248;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r285,%r286,%r287,%r288}, [%rd247];
+	// end inline asm
+	add.s64 	%rd251, %rd239, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd250, %rd251;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r289,%r290,%r291,%r292}, [%rd250];
+	// end inline asm
+	add.s64 	%rd254, %rd239, 64;
+	// begin inline asm
 	cvta.to.global.u64 %rd253, %rd254;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r274,%r275,%r276,%r277}, [%rd253];
-	// inline asm
-	add.s64 	%rd257, %rd248, 32;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r293,%r294,%r295,%r296}, [%rd253];
+	// end inline asm
+	add.s64 	%rd257, %rd239, 80;
+	// begin inline asm
 	cvta.to.global.u64 %rd256, %rd257;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r278,%r279,%r280,%r281}, [%rd256];
-	// inline asm
-	add.s64 	%rd260, %rd248, 48;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r297,%r298,%r299,%r300}, [%rd256];
+	// end inline asm
+	add.s64 	%rd260, %rd239, 96;
+	// begin inline asm
 	cvta.to.global.u64 %rd259, %rd260;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r282,%r283,%r284,%r285}, [%rd259];
-	// inline asm
-	add.s64 	%rd263, %rd248, 64;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r301,%r302,%r303,%r304}, [%rd259];
+	// end inline asm
+	add.s64 	%rd263, %rd239, 112;
+	// begin inline asm
 	cvta.to.global.u64 %rd262, %rd263;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r286,%r287,%r288,%r289}, [%rd262];
-	// inline asm
-	add.s64 	%rd266, %rd248, 80;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r305,%r306,%r307,%r308}, [%rd262];
+	// end inline asm
+	mov.b32 	%f1064, %r280;
+	mov.b32 	%f1065, %r281;
+	and.b32  	%r321, %r279, 65535;
+	add.s32 	%r322, %r321, -1;
+	cvt.rn.f32.s32 	%f1066, %r322;
+	sub.f32 	%f1067, %f960, %f1064;
+	mul.f32 	%f1068, %f1067, %f1066;
+	sub.f32 	%f1069, %f1065, %f1064;
+	div.rn.f32 	%f1070, %f1068, %f1069;
+	min.f32 	%f1071, %f1066, %f1070;
+	mov.f32 	%f1072, 0f00000000;
+	max.f32 	%f1073, %f1072, %f1071;
+	cvt.rmi.f32.f32 	%f1074, %f1073;
+	sub.f32 	%f258, %f1073, %f1074;
+	cvt.rzi.s32.f32 	%r323, %f1074;
+	cvt.s64.s32 	%rd17, %r323;
+	mul.wide.s32 	%rd274, %r323, 48;
+	add.s64 	%rd266, %rd248, %rd274;
+	// begin inline asm
 	cvta.to.global.u64 %rd265, %rd266;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r290,%r291,%r292,%r293}, [%rd265];
-	// inline asm
-	add.s64 	%rd269, %rd248, 96;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r309,%r310,%r311,%r312}, [%rd265];
+	// end inline asm
+	mov.b32 	%f1822, %r309;
+	mov.b32 	%f1823, %r310;
+	mov.b32 	%f1824, %r311;
+	add.s64 	%rd269, %rd266, 16;
+	// begin inline asm
 	cvta.to.global.u64 %rd268, %rd269;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r294,%r295,%r296,%r297}, [%rd268];
-	// inline asm
-	add.s64 	%rd272, %rd248, 112;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r313,%r314,%r315,%r316}, [%rd268];
+	// end inline asm
+	mov.b32 	%f1819, %r313;
+	mov.b32 	%f1820, %r314;
+	mov.b32 	%f1821, %r315;
+	add.s64 	%rd272, %rd266, 32;
+	// begin inline asm
 	cvta.to.global.u64 %rd271, %rd272;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r298,%r299,%r300,%r301}, [%rd271];
-	// inline asm
-	mov.b32 	 %f1027, %r273;
-	mov.b32 	 %f1028, %r274;
-	cvt.u32.u16	%r314, %rs8;
-	add.s32 	%r315, %r314, -1;
-	cvt.rn.f32.s32	%f1029, %r315;
-	sub.f32 	%f1030, %f924, %f1027;
-	mul.f32 	%f1031, %f1030, %f1029;
-	sub.f32 	%f1032, %f1028, %f1027;
-	div.rn.f32 	%f1033, %f1031, %f1032;
-	min.f32 	%f1034, %f1029, %f1033;
-	mov.f32 	%f1035, 0f00000000;
-	max.f32 	%f1036, %f1035, %f1034;
-	cvt.rmi.f32.f32	%f1037, %f1036;
-	cvt.rzi.s32.f32	%r316, %f1037;
-	cvt.s64.s32	%rd19, %r316;
-	mul.wide.s32 	%rd283, %r316, 48;
-	add.s64 	%rd275, %rd257, %rd283;
-	// inline asm
-	cvta.to.global.u64 %rd274, %rd275;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r302,%r303,%r304,%r305}, [%rd274];
-	// inline asm
-	mov.b32 	 %f1771, %r302;
-	mov.b32 	 %f1772, %r303;
-	mov.b32 	 %f1773, %r304;
-	add.s64 	%rd278, %rd275, 16;
-	// inline asm
-	cvta.to.global.u64 %rd277, %rd278;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r306,%r307,%r308,%r309}, [%rd277];
-	// inline asm
-	mov.b32 	 %f1768, %r306;
-	mov.b32 	 %f1769, %r307;
-	mov.b32 	 %f1770, %r308;
-	add.s64 	%rd281, %rd275, 32;
-	// inline asm
-	cvta.to.global.u64 %rd280, %rd281;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r310,%r311,%r312,%r313}, [%rd280];
-	// inline asm
-	sub.f32 	%f249, %f1036, %f1037;
-	mov.b32 	 %f1765, %r310;
-	mov.b32 	 %f1766, %r311;
-	mov.b32 	 %f1767, %r312;
-	setp.leu.f32	%p17, %f249, 0f00000000;
-	@%p17 bra 	BB6_37;
-
-	mul.lo.s64 	%rd293, %rd19, 48;
-	add.s64 	%rd294, %rd248, %rd293;
-	add.s64 	%rd285, %rd294, 80;
-	// inline asm
-	cvta.to.global.u64 %rd284, %rd285;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r317,%r318,%r319,%r320}, [%rd284];
-	// inline asm
-	mov.b32 	 %f1038, %r317;
-	mov.b32 	 %f1039, %r318;
-	mov.b32 	 %f1040, %r319;
-	add.s64 	%rd288, %rd294, 96;
-	// inline asm
-	cvta.to.global.u64 %rd287, %rd288;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r321,%r322,%r323,%r324}, [%rd287];
-	// inline asm
-	mov.b32 	 %f1041, %r321;
-	mov.b32 	 %f1042, %r322;
-	mov.b32 	 %f1043, %r323;
-	add.s64 	%rd291, %rd294, 112;
-	// inline asm
-	cvta.to.global.u64 %rd290, %rd291;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r325,%r326,%r327,%r328}, [%rd290];
-	// inline asm
-	mov.f32 	%f1044, 0f3F800000;
-	sub.f32 	%f1045, %f1044, %f249;
-	mul.f32 	%f1046, %f249, %f1038;
-	mul.f32 	%f1047, %f249, %f1039;
-	mul.f32 	%f1048, %f249, %f1040;
-	fma.rn.f32 	%f1771, %f1045, %f1771, %f1046;
-	fma.rn.f32 	%f1772, %f1045, %f1772, %f1047;
-	fma.rn.f32 	%f1773, %f1045, %f1773, %f1048;
-	mul.f32 	%f1049, %f249, %f1041;
-	mul.f32 	%f1050, %f249, %f1042;
-	mul.f32 	%f1051, %f249, %f1043;
-	fma.rn.f32 	%f1768, %f1045, %f1768, %f1049;
-	fma.rn.f32 	%f1769, %f1045, %f1769, %f1050;
-	fma.rn.f32 	%f1770, %f1045, %f1770, %f1051;
-	mov.b32 	 %f1052, %r325;
-	mov.b32 	 %f1053, %r326;
-	mov.b32 	 %f1054, %r327;
-	mul.f32 	%f1055, %f249, %f1052;
-	mul.f32 	%f1056, %f249, %f1053;
-	mul.f32 	%f1057, %f249, %f1054;
-	fma.rn.f32 	%f1765, %f1045, %f1765, %f1055;
-	fma.rn.f32 	%f1766, %f1045, %f1766, %f1056;
-	fma.rn.f32 	%f1767, %f1045, %f1767, %f1057;
-	bra.uni 	BB6_37;
-
-BB6_26:
-	mov.f32 	%f1774, 0f00000000;
-	mov.f32 	%f1776, 0f3F800000;
-	setp.eq.s32	%p13, %r181, 4;
-	@%p13 bra 	BB6_29;
-	bra.uni 	BB6_27;
-
-BB6_29:
-	// inline asm
-	call (%rd667), _optix_get_instance_inverse_transform_from_handle, (%rd174);
-	// inline asm
-	bra.uni 	BB6_30;
-
-BB6_32:
-	// inline asm
-	call (%rd189), _optix_get_srt_motion_transform_from_handle, (%rd174);
-	// inline asm
-	// inline asm
-	cvta.to.global.u64 %rd191, %rd189;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r195,%r196,%r197,%r198}, [%rd191];
-	// inline asm
-	mov.b32	{%rs6, %rs7}, %r197;
-	add.s64 	%rd195, %rd189, 16;
-	// inline asm
-	cvta.to.global.u64 %rd194, %rd195;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r199,%r200,%r201,%r202}, [%rd194];
-	// inline asm
-	add.s64 	%rd198, %rd189, 32;
-	// inline asm
-	cvta.to.global.u64 %rd197, %rd198;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r203,%r204,%r205,%r206}, [%rd197];
-	// inline asm
-	add.s64 	%rd201, %rd189, 48;
-	// inline asm
-	cvta.to.global.u64 %rd200, %rd201;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r207,%r208,%r209,%r210}, [%rd200];
-	// inline asm
-	add.s64 	%rd204, %rd189, 64;
-	// inline asm
-	cvta.to.global.u64 %rd203, %rd204;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r211,%r212,%r213,%r214}, [%rd203];
-	// inline asm
-	add.s64 	%rd207, %rd189, 80;
-	// inline asm
-	cvta.to.global.u64 %rd206, %rd207;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r215,%r216,%r217,%r218}, [%rd206];
-	// inline asm
-	add.s64 	%rd210, %rd189, 96;
-	// inline asm
-	cvta.to.global.u64 %rd209, %rd210;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r219,%r220,%r221,%r222}, [%rd209];
-	// inline asm
-	add.s64 	%rd213, %rd189, 112;
-	// inline asm
-	cvta.to.global.u64 %rd212, %rd213;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r223,%r224,%r225,%r226}, [%rd212];
-	// inline asm
-	add.s64 	%rd216, %rd189, 128;
-	// inline asm
-	cvta.to.global.u64 %rd215, %rd216;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r227,%r228,%r229,%r230}, [%rd215];
-	// inline asm
-	add.s64 	%rd219, %rd189, 144;
-	// inline asm
-	cvta.to.global.u64 %rd218, %rd219;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r231,%r232,%r233,%r234}, [%rd218];
-	// inline asm
-	mov.b32 	 %f935, %r198;
-	mov.b32 	 %f936, %r199;
-	cvt.u32.u16	%r251, %rs6;
-	add.s32 	%r252, %r251, -1;
-	cvt.rn.f32.s32	%f937, %r252;
-	sub.f32 	%f938, %f924, %f935;
-	mul.f32 	%f939, %f938, %f937;
-	sub.f32 	%f940, %f936, %f935;
-	div.rn.f32 	%f941, %f939, %f940;
-	min.f32 	%f942, %f937, %f941;
-	mov.f32 	%f943, 0f00000000;
-	max.f32 	%f944, %f943, %f942;
-	cvt.rmi.f32.f32	%f945, %f944;
-	cvt.rzi.s32.f32	%r253, %f945;
-	cvt.s64.s32	%rd17, %r253;
-	mul.wide.s32 	%rd233, %r253, 64;
-	add.s64 	%rd222, %rd198, %rd233;
-	// inline asm
-	cvta.to.global.u64 %rd221, %rd222;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r235,%r236,%r237,%r238}, [%rd221];
-	// inline asm
-	mov.b32 	 %f1755, %r235;
-	mov.b32 	 %f1756, %r236;
-	mov.b32 	 %f1757, %r237;
-	add.s64 	%rd225, %rd222, 16;
-	// inline asm
-	cvta.to.global.u64 %rd224, %rd225;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r239,%r240,%r241,%r242}, [%rd224];
-	// inline asm
-	mov.b32 	 %f1758, %r239;
-	mov.b32 	 %f1759, %r240;
-	mov.b32 	 %f1760, %r242;
-	add.s64 	%rd228, %rd222, 32;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r317,%r318,%r319,%r320}, [%rd271];
+	// end inline asm
+	mov.b32 	%f1816, %r317;
+	mov.b32 	%f1817, %r318;
+	mov.b32 	%f1818, %r319;
+	setp.leu.f32 	%p19, %f258, 0f00000000;
+	@%p19 bra 	$L__BB6_38;
+
+	mov.f32 	%f1075, 0f3F800000;
+	sub.f32 	%f1076, %f1075, %f258;
+	mul.lo.s64 	%rd284, %rd17, 48;
+	add.s64 	%rd285, %rd239, %rd284;
+	add.s64 	%rd276, %rd285, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd275, %rd276;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r324,%r325,%r326,%r327}, [%rd275];
+	// end inline asm
+	mov.b32 	%f1077, %r324;
+	mov.b32 	%f1078, %r325;
+	mov.b32 	%f1079, %r326;
+	mul.f32 	%f1080, %f258, %f1077;
+	mul.f32 	%f1081, %f258, %f1078;
+	mul.f32 	%f1082, %f258, %f1079;
+	fma.rn.f32 	%f1822, %f1076, %f1822, %f1080;
+	fma.rn.f32 	%f1823, %f1076, %f1823, %f1081;
+	fma.rn.f32 	%f1824, %f1076, %f1824, %f1082;
+	add.s64 	%rd279, %rd285, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd278, %rd279;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r328,%r329,%r330,%r331}, [%rd278];
+	// end inline asm
+	mov.b32 	%f1083, %r328;
+	mov.b32 	%f1084, %r329;
+	mov.b32 	%f1085, %r330;
+	mul.f32 	%f1086, %f258, %f1083;
+	mul.f32 	%f1087, %f258, %f1084;
+	mul.f32 	%f1088, %f258, %f1085;
+	fma.rn.f32 	%f1819, %f1076, %f1819, %f1086;
+	fma.rn.f32 	%f1820, %f1076, %f1820, %f1087;
+	fma.rn.f32 	%f1821, %f1076, %f1821, %f1088;
+	add.s64 	%rd282, %rd285, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd281, %rd282;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r332,%r333,%r334,%r335}, [%rd281];
+	// end inline asm
+	mov.b32 	%f1089, %r332;
+	mov.b32 	%f1090, %r333;
+	mov.b32 	%f1091, %r334;
+	mul.f32 	%f1092, %f258, %f1089;
+	mul.f32 	%f1093, %f258, %f1090;
+	mul.f32 	%f1094, %f258, %f1091;
+	fma.rn.f32 	%f1816, %f1076, %f1816, %f1092;
+	fma.rn.f32 	%f1817, %f1076, %f1817, %f1093;
+	fma.rn.f32 	%f1818, %f1076, %f1818, %f1094;
+	bra.uni 	$L__BB6_38;
+
+$L__BB6_27:
+	mov.f32 	%f1825, 0f00000000;
+	mov.f32 	%f1827, 0f3F800000;
+	setp.eq.s32 	%p15, %r188, 4;
+	@%p15 bra 	$L__BB6_30;
+
+	setp.ne.s32 	%p16, %r188, 1;
+	mov.f32 	%f1826, %f1825;
+	mov.f32 	%f1828, %f1825;
+	mov.f32 	%f1829, %f1827;
+	mov.f32 	%f1830, %f1825;
+	mov.f32 	%f1831, %f1827;
+	mov.f32 	%f1832, %f1825;
+	mov.f32 	%f1833, %f1825;
+	@%p16 bra 	$L__BB6_39;
+
+	// begin inline asm
+	call (%rd169), _optix_get_static_transform_from_handle, (%rd167);
+	// end inline asm
+	add.s64 	%rd653, %rd169, 64;
+	bra.uni 	$L__BB6_31;
+
+$L__BB6_33:
+	// begin inline asm
+	call (%rd182), _optix_get_srt_motion_transform_from_handle, (%rd167);
+	// end inline asm
+	// begin inline asm
+	cvta.to.global.u64 %rd184, %rd182;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r202,%r203,%r204,%r205}, [%rd184];
+	// end inline asm
+	add.s64 	%rd188, %rd182, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd187, %rd188;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r206,%r207,%r208,%r209}, [%rd187];
+	// end inline asm
+	add.s64 	%rd191, %rd182, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd190, %rd191;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r210,%r211,%r212,%r213}, [%rd190];
+	// end inline asm
+	add.s64 	%rd194, %rd182, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd193, %rd194;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r214,%r215,%r216,%r217}, [%rd193];
+	// end inline asm
+	add.s64 	%rd197, %rd182, 64;
+	// begin inline asm
+	cvta.to.global.u64 %rd196, %rd197;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r218,%r219,%r220,%r221}, [%rd196];
+	// end inline asm
+	add.s64 	%rd200, %rd182, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd199, %rd200;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r222,%r223,%r224,%r225}, [%rd199];
+	// end inline asm
+	add.s64 	%rd203, %rd182, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd202, %rd203;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r226,%r227,%r228,%r229}, [%rd202];
+	// end inline asm
+	add.s64 	%rd206, %rd182, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd205, %rd206;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r230,%r231,%r232,%r233}, [%rd205];
+	// end inline asm
+	add.s64 	%rd209, %rd182, 128;
+	// begin inline asm
+	cvta.to.global.u64 %rd208, %rd209;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r234,%r235,%r236,%r237}, [%rd208];
+	// end inline asm
+	add.s64 	%rd212, %rd182, 144;
+	// begin inline asm
+	cvta.to.global.u64 %rd211, %rd212;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r238,%r239,%r240,%r241}, [%rd211];
+	// end inline asm
+	mov.b32 	%f972, %r205;
+	mov.b32 	%f973, %r206;
+	and.b32  	%r258, %r204, 65535;
+	add.s32 	%r259, %r258, -1;
+	cvt.rn.f32.s32 	%f974, %r259;
+	sub.f32 	%f975, %f960, %f972;
+	mul.f32 	%f976, %f975, %f974;
+	sub.f32 	%f977, %f973, %f972;
+	div.rn.f32 	%f978, %f976, %f977;
+	min.f32 	%f979, %f974, %f978;
+	mov.f32 	%f980, 0f00000000;
+	max.f32 	%f981, %f980, %f979;
+	cvt.rmi.f32.f32 	%f982, %f981;
+	sub.f32 	%f218, %f981, %f982;
+	cvt.rzi.s32.f32 	%r260, %f982;
+	mul.wide.s32 	%rd226, %r260, 64;
+	add.s64 	%rd215, %rd191, %rd226;
+	// begin inline asm
+	cvta.to.global.u64 %rd214, %rd215;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r242,%r243,%r244,%r245}, [%rd214];
+	// end inline asm
+	mov.b32 	%f1806, %r242;
+	mov.b32 	%f1807, %r243;
+	mov.b32 	%f1808, %r244;
+	add.s64 	%rd218, %rd215, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd217, %rd218;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r246,%r247,%r248,%r249}, [%rd217];
+	// end inline asm
+	mov.b32 	%f1809, %r246;
+	mov.b32 	%f1810, %r247;
+	mov.b32 	%f1811, %r249;
+	add.s64 	%rd221, %rd215, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd220, %rd221;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r250,%r251,%r252,%r253}, [%rd220];
+	// end inline asm
+	mov.b32 	%f1812, %r251;
+	mov.b32 	%f1813, %r252;
+	mov.b32 	%f1814, %r253;
+	add.s64 	%rd224, %rd215, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd223, %rd224;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r254,%r255,%r256,%r257}, [%rd223];
+	// end inline asm
+	mov.b32 	%f1815, %r254;
+	setp.leu.f32 	%p18, %f218, 0f00000000;
+	@%p18 bra 	$L__BB6_35;
+
+	mov.f32 	%f983, 0f3F800000;
+	sub.f32 	%f984, %f983, %f218;
+	add.s64 	%rd228, %rd215, 64;
+	// begin inline asm
 	cvta.to.global.u64 %rd227, %rd228;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r243,%r244,%r245,%r246}, [%rd227];
-	// inline asm
-	sub.f32 	%f209, %f944, %f945;
-	mov.b32 	 %f1761, %r244;
-	mov.b32 	 %f1762, %r245;
-	mov.b32 	 %f1763, %r246;
-	add.s64 	%rd231, %rd222, 48;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r261,%r262,%r263,%r264}, [%rd227];
+	// end inline asm
+	mov.b32 	%f985, %r261;
+	mov.b32 	%f986, %r262;
+	mov.b32 	%f987, %r263;
+	mul.f32 	%f988, %f218, %f985;
+	mul.f32 	%f989, %f218, %f986;
+	mul.f32 	%f990, %f218, %f987;
+	fma.rn.f32 	%f1806, %f984, %f1806, %f988;
+	fma.rn.f32 	%f1807, %f984, %f1807, %f989;
+	fma.rn.f32 	%f1808, %f984, %f1808, %f990;
+	add.s64 	%rd231, %rd215, 80;
+	// begin inline asm
 	cvta.to.global.u64 %rd230, %rd231;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r247,%r248,%r249,%r250}, [%rd230];
-	// inline asm
-	mov.b32 	 %f1764, %r247;
-	setp.leu.f32	%p16, %f209, 0f00000000;
-	@%p16 bra 	BB6_34;
-
-	shl.b64 	%rd246, %rd17, 6;
-	add.s64 	%rd247, %rd246, %rd189;
-	add.s64 	%rd235, %rd247, 96;
-	// inline asm
-	cvta.to.global.u64 %rd234, %rd235;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r254,%r255,%r256,%r257}, [%rd234];
-	// inline asm
-	mov.b32 	 %f946, %r254;
-	mov.b32 	 %f947, %r255;
-	mov.b32 	 %f948, %r256;
-	add.s64 	%rd238, %rd247, 112;
-	// inline asm
-	cvta.to.global.u64 %rd237, %rd238;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r258,%r259,%r260,%r261}, [%rd237];
-	// inline asm
-	mov.b32 	 %f949, %r258;
-	mov.b32 	 %f950, %r259;
-	mov.b32 	 %f951, %r261;
-	add.s64 	%rd241, %rd247, 128;
-	// inline asm
-	cvta.to.global.u64 %rd240, %rd241;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r262,%r263,%r264,%r265}, [%rd240];
-	// inline asm
-	mov.b32 	 %f952, %r263;
-	mov.b32 	 %f953, %r264;
-	mov.b32 	 %f954, %r265;
-	add.s64 	%rd244, %rd247, 144;
-	// inline asm
-	cvta.to.global.u64 %rd243, %rd244;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r266,%r267,%r268,%r269}, [%rd243];
-	// inline asm
-	mov.f32 	%f955, 0f3F800000;
-	sub.f32 	%f956, %f955, %f209;
-	mul.f32 	%f957, %f209, %f946;
-	mul.f32 	%f958, %f209, %f947;
-	mul.f32 	%f959, %f209, %f948;
-	fma.rn.f32 	%f1755, %f956, %f1755, %f957;
-	fma.rn.f32 	%f1756, %f956, %f1756, %f958;
-	fma.rn.f32 	%f1757, %f956, %f1757, %f959;
-	mul.f32 	%f960, %f209, %f949;
-	mul.f32 	%f961, %f209, %f950;
-	mul.f32 	%f962, %f209, %f951;
-	fma.rn.f32 	%f1758, %f956, %f1758, %f960;
-	fma.rn.f32 	%f1759, %f956, %f1759, %f961;
-	fma.rn.f32 	%f1760, %f956, %f1760, %f962;
-	mul.f32 	%f963, %f209, %f952;
-	mul.f32 	%f964, %f209, %f953;
-	mul.f32 	%f965, %f209, %f954;
-	fma.rn.f32 	%f966, %f956, %f1761, %f963;
-	fma.rn.f32 	%f967, %f956, %f1762, %f964;
-	fma.rn.f32 	%f968, %f956, %f1763, %f965;
-	mov.b32 	 %f969, %r266;
-	mul.f32 	%f970, %f209, %f969;
-	fma.rn.f32 	%f971, %f956, %f1764, %f970;
-	mul.f32 	%f972, %f967, %f967;
-	fma.rn.f32 	%f973, %f966, %f966, %f972;
-	fma.rn.f32 	%f974, %f968, %f968, %f973;
-	fma.rn.f32 	%f975, %f971, %f971, %f974;
-	sqrt.rn.f32 	%f976, %f975;
-	rcp.rn.f32 	%f977, %f976;
-	mul.f32 	%f1761, %f966, %f977;
-	mul.f32 	%f1762, %f967, %f977;
-	mul.f32 	%f1763, %f968, %f977;
-	mul.f32 	%f1764, %f971, %f977;
-
-BB6_34:
-	mul.f32 	%f978, %f1762, %f1762;
-	fma.rn.f32 	%f979, %f1761, %f1761, %f978;
-	fma.rn.f32 	%f980, %f1763, %f1763, %f979;
-	fma.rn.f32 	%f981, %f1764, %f1764, %f980;
-	rcp.rn.f32 	%f982, %f981;
-	mul.f32 	%f983, %f1761, %f982;
-	mul.f32 	%f984, %f1762, %f982;
-	mul.f32 	%f985, %f1763, %f982;
-	mul.f32 	%f986, %f1764, %f982;
-	mul.f32 	%f987, %f1761, %f983;
-	mul.f32 	%f988, %f1762, %f984;
-	mul.f32 	%f989, %f1763, %f985;
-	mul.f32 	%f990, %f1761, %f984;
-	mul.f32 	%f991, %f1763, %f986;
-	mul.f32 	%f992, %f1761, %f985;
-	mul.f32 	%f993, %f1762, %f986;
-	mul.f32 	%f994, %f1762, %f985;
-	mul.f32 	%f995, %f1761, %f986;
-	sub.f32 	%f996, %f987, %f988;
-	sub.f32 	%f997, %f996, %f989;
-	fma.rn.f32 	%f998, %f1764, %f986, %f997;
-	sub.f32 	%f999, %f990, %f991;
-	add.f32 	%f1000, %f999, %f999;
-	add.f32 	%f1001, %f992, %f993;
-	add.f32 	%f1002, %f1001, %f1001;
-	add.f32 	%f1003, %f990, %f991;
-	add.f32 	%f1004, %f1003, %f1003;
-	sub.f32 	%f1005, %f988, %f987;
-	sub.f32 	%f1006, %f1005, %f989;
-	fma.rn.f32 	%f1007, %f1764, %f986, %f1006;
-	sub.f32 	%f1008, %f994, %f995;
-	add.f32 	%f1009, %f1008, %f1008;
-	sub.f32 	%f1010, %f992, %f993;
-	add.f32 	%f1011, %f1010, %f1010;
-	add.f32 	%f1012, %f994, %f995;
-	add.f32 	%f1013, %f1012, %f1012;
-	neg.f32 	%f1014, %f987;
-	sub.f32 	%f1015, %f1014, %f988;
-	add.f32 	%f1016, %f989, %f1015;
-	fma.rn.f32 	%f1017, %f1764, %f986, %f1016;
-	mul.f32 	%f1018, %f1757, %f998;
-	fma.rn.f32 	%f1019, %f1759, %f1000, %f1018;
-	fma.rn.f32 	%f1773, %f1760, %f1002, %f1019;
-	mul.f32 	%f1020, %f1759, %f1007;
-	fma.rn.f32 	%f1021, %f1757, %f1004, %f1020;
-	fma.rn.f32 	%f1770, %f1760, %f1009, %f1021;
-	mul.f32 	%f1022, %f1759, %f1013;
-	fma.rn.f32 	%f1023, %f1757, %f1011, %f1022;
-	fma.rn.f32 	%f1767, %f1760, %f1017, %f1023;
-	mul.f32 	%f1024, %f1756, %f998;
-	fma.rn.f32 	%f1772, %f1758, %f1000, %f1024;
-	mul.f32 	%f1025, %f1758, %f1007;
-	fma.rn.f32 	%f1769, %f1756, %f1004, %f1025;
-	mul.f32 	%f1026, %f1758, %f1013;
-	fma.rn.f32 	%f1766, %f1756, %f1011, %f1026;
-	mul.f32 	%f1771, %f1755, %f998;
-	mul.f32 	%f1768, %f1755, %f1004;
-	mul.f32 	%f1765, %f1755, %f1011;
-
-BB6_37:
-	mul.f32 	%f1058, %f1766, %f1770;
-	mul.f32 	%f1059, %f1767, %f1769;
-	sub.f32 	%f1060, %f1059, %f1058;
-	mul.f32 	%f1061, %f1771, %f1060;
-	mul.f32 	%f1062, %f1765, %f1770;
-	mul.f32 	%f1063, %f1767, %f1768;
-	sub.f32 	%f1064, %f1063, %f1062;
-	mul.f32 	%f1065, %f1064, %f1772;
-	sub.f32 	%f1066, %f1061, %f1065;
-	mul.f32 	%f1067, %f1765, %f1769;
-	mul.f32 	%f1068, %f1766, %f1768;
-	sub.f32 	%f1069, %f1068, %f1067;
-	fma.rn.f32 	%f1070, %f1069, %f1773, %f1066;
-	rcp.rn.f32 	%f1071, %f1070;
-	mul.f32 	%f1780, %f1060, %f1071;
-	mul.f32 	%f1072, %f1767, %f1772;
-	mul.f32 	%f1073, %f1766, %f1773;
-	sub.f32 	%f1074, %f1073, %f1072;
-	mul.f32 	%f1781, %f1071, %f1074;
-	mul.f32 	%f1075, %f1769, %f1773;
-	mul.f32 	%f1076, %f1770, %f1772;
-	sub.f32 	%f1077, %f1076, %f1075;
-	mul.f32 	%f1782, %f1071, %f1077;
-	sub.f32 	%f1078, %f1062, %f1063;
-	mul.f32 	%f1777, %f1078, %f1071;
-	mul.f32 	%f1079, %f1765, %f1773;
-	mul.f32 	%f1080, %f1767, %f1771;
-	sub.f32 	%f1081, %f1080, %f1079;
-	mul.f32 	%f1778, %f1071, %f1081;
-	mul.f32 	%f1082, %f1770, %f1771;
-	mul.f32 	%f1083, %f1768, %f1773;
-	sub.f32 	%f1084, %f1083, %f1082;
-	mul.f32 	%f1779, %f1071, %f1084;
-	mul.f32 	%f1774, %f1069, %f1071;
-	mul.f32 	%f1085, %f1766, %f1771;
-	mul.f32 	%f1086, %f1765, %f1772;
-	sub.f32 	%f1087, %f1086, %f1085;
-	mul.f32 	%f1775, %f1087, %f1071;
-	mul.f32 	%f1088, %f1768, %f1772;
-	mul.f32 	%f1089, %f1769, %f1771;
-	sub.f32 	%f1090, %f1089, %f1088;
-	mul.f32 	%f1776, %f1090, %f1071;
-	bra.uni 	BB6_38;
-
-BB6_27:
-	setp.ne.s32	%p14, %r181, 1;
-	mov.f32 	%f1775, %f1774;
-	mov.f32 	%f1777, %f1774;
-	mov.f32 	%f1778, %f1776;
-	mov.f32 	%f1779, %f1774;
-	mov.f32 	%f1780, %f1776;
-	mov.f32 	%f1781, %f1774;
-	mov.f32 	%f1782, %f1774;
-	@%p14 bra 	BB6_38;
-
-	// inline asm
-	call (%rd176), _optix_get_static_transform_from_handle, (%rd174);
-	// inline asm
-	add.s64 	%rd667, %rd176, 64;
-
-BB6_30:
-	// inline asm
-	cvta.to.global.u64 %rd180, %rd667;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r183,%r184,%r185,%r186}, [%rd180];
-	// inline asm
-	mov.b32 	 %f1780, %r183;
-	mov.b32 	 %f1781, %r184;
-	mov.b32 	 %f1782, %r185;
-	add.s64 	%rd184, %rd667, 16;
-	// inline asm
-	cvta.to.global.u64 %rd183, %rd184;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r187,%r188,%r189,%r190}, [%rd183];
-	// inline asm
-	mov.b32 	 %f1777, %r187;
-	mov.b32 	 %f1778, %r188;
-	mov.b32 	 %f1779, %r189;
-	add.s64 	%rd187, %rd667, 32;
-	// inline asm
-	cvta.to.global.u64 %rd186, %rd187;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r191,%r192,%r193,%r194}, [%rd186];
-	// inline asm
-	mov.b32 	 %f1774, %r191;
-	mov.b32 	 %f1775, %r192;
-	mov.b32 	 %f1776, %r193;
-
-BB6_38:
-	setp.eq.s32	%p18, %r635, 0;
-	@%p18 bra 	BB6_39;
-	bra.uni 	BB6_40;
-
-BB6_39:
-	mov.f32 	%f1754, %f1774;
-	mov.f32 	%f1753, %f1775;
-	mov.f32 	%f1752, %f1776;
-	mov.f32 	%f1751, %f1777;
-	mov.f32 	%f1750, %f1778;
-	mov.f32 	%f1749, %f1779;
-	mov.f32 	%f1748, %f1780;
-	mov.f32 	%f1747, %f1781;
-	mov.f32 	%f1746, %f1782;
-	bra.uni 	BB6_41;
-
-BB6_40:
-	mul.f32 	%f1091, %f1751, %f1781;
-	fma.rn.f32 	%f1092, %f1748, %f1780, %f1091;
-	fma.rn.f32 	%f289, %f1754, %f1782, %f1092;
-	mul.f32 	%f1093, %f1750, %f1781;
-	fma.rn.f32 	%f1094, %f1747, %f1780, %f1093;
-	fma.rn.f32 	%f290, %f1753, %f1782, %f1094;
-	mul.f32 	%f1095, %f1749, %f1781;
-	fma.rn.f32 	%f1096, %f1746, %f1780, %f1095;
-	fma.rn.f32 	%f291, %f1752, %f1782, %f1096;
-	mul.f32 	%f1097, %f1751, %f1778;
-	fma.rn.f32 	%f1098, %f1748, %f1777, %f1097;
-	fma.rn.f32 	%f292, %f1754, %f1779, %f1098;
-	mul.f32 	%f1099, %f1750, %f1778;
-	fma.rn.f32 	%f1100, %f1747, %f1777, %f1099;
-	fma.rn.f32 	%f293, %f1753, %f1779, %f1100;
-	mul.f32 	%f1101, %f1749, %f1778;
-	fma.rn.f32 	%f1102, %f1746, %f1777, %f1101;
-	fma.rn.f32 	%f294, %f1752, %f1779, %f1102;
-	mul.f32 	%f1103, %f1751, %f1775;
-	fma.rn.f32 	%f1104, %f1748, %f1774, %f1103;
-	fma.rn.f32 	%f1754, %f1754, %f1776, %f1104;
-	mul.f32 	%f1105, %f1750, %f1775;
-	fma.rn.f32 	%f1106, %f1747, %f1774, %f1105;
-	fma.rn.f32 	%f1753, %f1753, %f1776, %f1106;
-	mul.f32 	%f1107, %f1749, %f1775;
-	fma.rn.f32 	%f1108, %f1746, %f1774, %f1107;
-	fma.rn.f32 	%f1752, %f1752, %f1776, %f1108;
-	mov.f32 	%f1751, %f292;
-	mov.f32 	%f1750, %f293;
-	mov.f32 	%f1749, %f294;
-	mov.f32 	%f1748, %f289;
-	mov.f32 	%f1747, %f290;
-	mov.f32 	%f1746, %f291;
-
-BB6_41:
-	add.s32 	%r635, %r635, 1;
-	setp.lt.u32	%p19, %r635, %r28;
-	@%p19 bra 	BB6_25;
-
-	mul.f32 	%f1109, %f922, %f1747;
-	fma.rn.f32 	%f1110, %f921, %f1748, %f1109;
-	fma.rn.f32 	%f1792, %f1794, %f1746, %f1110;
-	mul.f32 	%f1111, %f922, %f1750;
-	fma.rn.f32 	%f1112, %f921, %f1751, %f1111;
-	fma.rn.f32 	%f1793, %f1794, %f1749, %f1112;
-	mul.f32 	%f1113, %f922, %f1753;
-	fma.rn.f32 	%f1114, %f921, %f1754, %f1113;
-	fma.rn.f32 	%f1794, %f1794, %f1752, %f1114;
-	bra.uni 	BB6_43;
-
-BB6_24:
-	mov.f32 	%f1792, %f921;
-	mov.f32 	%f1793, %f922;
-
-BB6_43:
-	ld.v4.f32 	{%f1117, %f1118, %f1119, %f1120}, [%rd3+80];
-	ld.v4.f32 	{%f1124, %f1125, %f1126, %f1127}, [%rd3+32];
-	fma.rn.f32 	%f1129, %f1745, %f1124, %f1117;
-	fma.rn.f32 	%f1131, %f1745, %f1125, %f1118;
-	fma.rn.f32 	%f1133, %f1745, %f1126, %f1119;
-	ld.v4.f32 	{%f1134, %f1135, %f1136, %f1137}, [%rd3+48];
-	fma.rn.f32 	%f1139, %f1744, %f1134, %f1129;
-	fma.rn.f32 	%f1141, %f1744, %f1135, %f1131;
-	fma.rn.f32 	%f1143, %f1744, %f1136, %f1133;
-	ld.v4.f32 	{%f1144, %f1145, %f1146, %f1147}, [%rd3+64];
-	fma.rn.f32 	%f1149, %f1743, %f1144, %f1139;
-	fma.rn.f32 	%f1151, %f1743, %f1145, %f1141;
-	fma.rn.f32 	%f1153, %f1743, %f1146, %f1143;
-	mul.f32 	%f1154, %f1792, %f1124;
-	mul.f32 	%f1155, %f1792, %f1125;
-	mul.f32 	%f1156, %f1792, %f1126;
-	fma.rn.f32 	%f1157, %f1793, %f1134, %f1154;
-	fma.rn.f32 	%f1158, %f1793, %f1135, %f1155;
-	fma.rn.f32 	%f1159, %f1793, %f1136, %f1156;
-	fma.rn.f32 	%f1160, %f1794, %f1144, %f1157;
-	fma.rn.f32 	%f1161, %f1794, %f1145, %f1158;
-	fma.rn.f32 	%f1162, %f1794, %f1146, %f1159;
-	rcp.rn.f32 	%f1163, %f1162;
-	mul.f32 	%f1164, %f1153, %f1163;
-	neg.f32 	%f313, %f1164;
-	fma.rn.f32 	%f314, %f313, %f1160, %f1149;
-	fma.rn.f32 	%f315, %f313, %f1161, %f1151;
-	ld.const.u64 	%rd21, [params+80];
-	setp.eq.s64	%p20, %rd21, 0;
-	cvt.u64.u32	%rd22, %r1;
-	@%p20 bra 	BB6_48;
-
-	ld.u64 	%rd295, [%rd52];
-	ld.const.u64 	%rd296, [params+328];
-	cvta.to.global.u64 	%rd297, %rd296;
-	shl.b64 	%rd298, %rd22, 3;
-	add.s64 	%rd299, %rd297, %rd298;
-	st.global.u64 	[%rd299], %rd295;
-	ld.const.u64 	%rd300, [params+336];
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r265,%r266,%r267,%r268}, [%rd230];
+	// end inline asm
+	mov.b32 	%f991, %r265;
+	mov.b32 	%f992, %r266;
+	mov.b32 	%f993, %r268;
+	mul.f32 	%f994, %f218, %f991;
+	mul.f32 	%f995, %f218, %f992;
+	mul.f32 	%f996, %f218, %f993;
+	fma.rn.f32 	%f1809, %f984, %f1809, %f994;
+	fma.rn.f32 	%f1810, %f984, %f1810, %f995;
+	fma.rn.f32 	%f1811, %f984, %f1811, %f996;
+	add.s64 	%rd234, %rd215, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd233, %rd234;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r269,%r270,%r271,%r272}, [%rd233];
+	// end inline asm
+	mov.b32 	%f997, %r270;
+	mov.b32 	%f998, %r271;
+	mov.b32 	%f999, %r272;
+	mul.f32 	%f1000, %f218, %f997;
+	mul.f32 	%f1001, %f218, %f998;
+	mul.f32 	%f1002, %f218, %f999;
+	fma.rn.f32 	%f1003, %f984, %f1812, %f1000;
+	fma.rn.f32 	%f1004, %f984, %f1813, %f1001;
+	fma.rn.f32 	%f1005, %f984, %f1814, %f1002;
+	add.s64 	%rd237, %rd215, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd236, %rd237;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r273,%r274,%r275,%r276}, [%rd236];
+	// end inline asm
+	mov.b32 	%f1006, %r273;
+	mul.f32 	%f1007, %f218, %f1006;
+	fma.rn.f32 	%f1008, %f984, %f1815, %f1007;
+	mul.f32 	%f1009, %f1004, %f1004;
+	fma.rn.f32 	%f1010, %f1003, %f1003, %f1009;
+	fma.rn.f32 	%f1011, %f1005, %f1005, %f1010;
+	fma.rn.f32 	%f1012, %f1008, %f1008, %f1011;
+	sqrt.rn.f32 	%f1013, %f1012;
+	rcp.rn.f32 	%f1014, %f1013;
+	mul.f32 	%f1812, %f1003, %f1014;
+	mul.f32 	%f1813, %f1004, %f1014;
+	mul.f32 	%f1814, %f1005, %f1014;
+	mul.f32 	%f1815, %f1014, %f1008;
+
+$L__BB6_35:
+	mul.f32 	%f1015, %f1813, %f1813;
+	fma.rn.f32 	%f1016, %f1812, %f1812, %f1015;
+	fma.rn.f32 	%f1017, %f1814, %f1814, %f1016;
+	fma.rn.f32 	%f1018, %f1815, %f1815, %f1017;
+	rcp.rn.f32 	%f1019, %f1018;
+	mul.f32 	%f1020, %f1812, %f1019;
+	mul.f32 	%f1021, %f1813, %f1019;
+	mul.f32 	%f1022, %f1814, %f1019;
+	mul.f32 	%f1023, %f1815, %f1019;
+	mul.f32 	%f1024, %f1812, %f1020;
+	mul.f32 	%f1025, %f1813, %f1021;
+	mul.f32 	%f1026, %f1814, %f1022;
+	mul.f32 	%f1027, %f1812, %f1021;
+	mul.f32 	%f1028, %f1814, %f1023;
+	mul.f32 	%f1029, %f1812, %f1022;
+	mul.f32 	%f1030, %f1813, %f1023;
+	mul.f32 	%f1031, %f1813, %f1022;
+	mul.f32 	%f1032, %f1812, %f1023;
+	sub.f32 	%f1033, %f1024, %f1025;
+	sub.f32 	%f1034, %f1033, %f1026;
+	fma.rn.f32 	%f1035, %f1815, %f1023, %f1034;
+	sub.f32 	%f1036, %f1027, %f1028;
+	add.f32 	%f1037, %f1036, %f1036;
+	add.f32 	%f1038, %f1029, %f1030;
+	add.f32 	%f1039, %f1038, %f1038;
+	add.f32 	%f1040, %f1027, %f1028;
+	add.f32 	%f1041, %f1040, %f1040;
+	sub.f32 	%f1042, %f1025, %f1024;
+	sub.f32 	%f1043, %f1042, %f1026;
+	fma.rn.f32 	%f1044, %f1815, %f1023, %f1043;
+	sub.f32 	%f1045, %f1031, %f1032;
+	add.f32 	%f1046, %f1045, %f1045;
+	sub.f32 	%f1047, %f1029, %f1030;
+	add.f32 	%f1048, %f1047, %f1047;
+	add.f32 	%f1049, %f1031, %f1032;
+	add.f32 	%f1050, %f1049, %f1049;
+	neg.f32 	%f1051, %f1024;
+	sub.f32 	%f1052, %f1051, %f1025;
+	add.f32 	%f1053, %f1026, %f1052;
+	fma.rn.f32 	%f1054, %f1815, %f1023, %f1053;
+	mul.f32 	%f1055, %f1808, %f1035;
+	fma.rn.f32 	%f1056, %f1810, %f1037, %f1055;
+	fma.rn.f32 	%f1824, %f1811, %f1039, %f1056;
+	mul.f32 	%f1057, %f1810, %f1044;
+	fma.rn.f32 	%f1058, %f1808, %f1041, %f1057;
+	fma.rn.f32 	%f1821, %f1811, %f1046, %f1058;
+	mul.f32 	%f1059, %f1810, %f1050;
+	fma.rn.f32 	%f1060, %f1808, %f1048, %f1059;
+	fma.rn.f32 	%f1818, %f1811, %f1054, %f1060;
+	mul.f32 	%f1061, %f1807, %f1035;
+	fma.rn.f32 	%f1823, %f1809, %f1037, %f1061;
+	mul.f32 	%f1062, %f1809, %f1044;
+	fma.rn.f32 	%f1820, %f1807, %f1041, %f1062;
+	mul.f32 	%f1063, %f1809, %f1050;
+	fma.rn.f32 	%f1817, %f1807, %f1048, %f1063;
+	mul.f32 	%f1822, %f1806, %f1035;
+	mul.f32 	%f1819, %f1806, %f1041;
+	mul.f32 	%f1816, %f1806, %f1048;
+
+$L__BB6_38:
+	mul.f32 	%f1095, %f1817, %f1821;
+	mul.f32 	%f1096, %f1818, %f1820;
+	sub.f32 	%f1097, %f1096, %f1095;
+	mul.f32 	%f1098, %f1822, %f1097;
+	mul.f32 	%f1099, %f1816, %f1821;
+	mul.f32 	%f1100, %f1818, %f1819;
+	sub.f32 	%f1101, %f1100, %f1099;
+	mul.f32 	%f1102, %f1101, %f1823;
+	sub.f32 	%f1103, %f1098, %f1102;
+	mul.f32 	%f1104, %f1816, %f1820;
+	mul.f32 	%f1105, %f1817, %f1819;
+	sub.f32 	%f1106, %f1105, %f1104;
+	fma.rn.f32 	%f1107, %f1106, %f1824, %f1103;
+	rcp.rn.f32 	%f1108, %f1107;
+	mul.f32 	%f1831, %f1097, %f1108;
+	mul.f32 	%f1109, %f1818, %f1823;
+	mul.f32 	%f1110, %f1817, %f1824;
+	sub.f32 	%f1111, %f1110, %f1109;
+	mul.f32 	%f1832, %f1111, %f1108;
+	mul.f32 	%f1112, %f1820, %f1824;
+	mul.f32 	%f1113, %f1821, %f1823;
+	sub.f32 	%f1114, %f1113, %f1112;
+	mul.f32 	%f1833, %f1114, %f1108;
+	sub.f32 	%f1115, %f1099, %f1100;
+	mul.f32 	%f1828, %f1115, %f1108;
+	mul.f32 	%f1116, %f1816, %f1824;
+	mul.f32 	%f1117, %f1818, %f1822;
+	sub.f32 	%f1118, %f1117, %f1116;
+	mul.f32 	%f1829, %f1118, %f1108;
+	mul.f32 	%f1119, %f1821, %f1822;
+	mul.f32 	%f1120, %f1819, %f1824;
+	sub.f32 	%f1121, %f1120, %f1119;
+	mul.f32 	%f1830, %f1121, %f1108;
+	mul.f32 	%f1825, %f1106, %f1108;
+	mul.f32 	%f1122, %f1817, %f1822;
+	mul.f32 	%f1123, %f1816, %f1823;
+	sub.f32 	%f1124, %f1123, %f1122;
+	mul.f32 	%f1826, %f1124, %f1108;
+	mul.f32 	%f1125, %f1819, %f1823;
+	mul.f32 	%f1126, %f1820, %f1822;
+	sub.f32 	%f1127, %f1126, %f1125;
+	mul.f32 	%f1827, %f1127, %f1108;
+	bra.uni 	$L__BB6_39;
+
+$L__BB6_30:
+	// begin inline asm
+	call (%rd653), _optix_get_instance_inverse_transform_from_handle, (%rd167);
+	// end inline asm
+
+$L__BB6_31:
+	// begin inline asm
+	cvta.to.global.u64 %rd173, %rd653;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r190,%r191,%r192,%r193}, [%rd173];
+	// end inline asm
+	mov.b32 	%f1831, %r190;
+	mov.b32 	%f1832, %r191;
+	mov.b32 	%f1833, %r192;
+	add.s64 	%rd177, %rd653, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd176, %rd177;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r194,%r195,%r196,%r197}, [%rd176];
+	// end inline asm
+	mov.b32 	%f1828, %r194;
+	mov.b32 	%f1829, %r195;
+	mov.b32 	%f1830, %r196;
+	add.s64 	%rd180, %rd653, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd179, %rd180;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r198,%r199,%r200,%r201}, [%rd179];
+	// end inline asm
+	mov.b32 	%f1825, %r198;
+	mov.b32 	%f1826, %r199;
+	mov.b32 	%f1827, %r200;
+
+$L__BB6_39:
+	setp.eq.s32 	%p20, %r646, 0;
+	@%p20 bra 	$L__BB6_41;
+
+	mul.f32 	%f1128, %f1802, %f1832;
+	fma.rn.f32 	%f1129, %f1799, %f1831, %f1128;
+	fma.rn.f32 	%f304, %f1805, %f1833, %f1129;
+	mul.f32 	%f1130, %f1801, %f1832;
+	fma.rn.f32 	%f1131, %f1798, %f1831, %f1130;
+	fma.rn.f32 	%f305, %f1804, %f1833, %f1131;
+	mul.f32 	%f1132, %f1800, %f1832;
+	fma.rn.f32 	%f1133, %f1797, %f1831, %f1132;
+	fma.rn.f32 	%f1833, %f1803, %f1833, %f1133;
+	mul.f32 	%f1134, %f1802, %f1829;
+	fma.rn.f32 	%f1135, %f1799, %f1828, %f1134;
+	fma.rn.f32 	%f307, %f1805, %f1830, %f1135;
+	mul.f32 	%f1136, %f1801, %f1829;
+	fma.rn.f32 	%f1137, %f1798, %f1828, %f1136;
+	fma.rn.f32 	%f308, %f1804, %f1830, %f1137;
+	mul.f32 	%f1138, %f1800, %f1829;
+	fma.rn.f32 	%f1139, %f1797, %f1828, %f1138;
+	fma.rn.f32 	%f1830, %f1803, %f1830, %f1139;
+	mul.f32 	%f1140, %f1802, %f1826;
+	fma.rn.f32 	%f1141, %f1799, %f1825, %f1140;
+	fma.rn.f32 	%f310, %f1805, %f1827, %f1141;
+	mul.f32 	%f1142, %f1801, %f1826;
+	fma.rn.f32 	%f1143, %f1798, %f1825, %f1142;
+	fma.rn.f32 	%f311, %f1804, %f1827, %f1143;
+	mul.f32 	%f1144, %f1800, %f1826;
+	fma.rn.f32 	%f1145, %f1797, %f1825, %f1144;
+	fma.rn.f32 	%f1827, %f1803, %f1827, %f1145;
+	mov.f32 	%f1825, %f310;
+	mov.f32 	%f1826, %f311;
+	mov.f32 	%f1828, %f307;
+	mov.f32 	%f1829, %f308;
+	mov.f32 	%f1831, %f304;
+	mov.f32 	%f1832, %f305;
+
+$L__BB6_41:
+	add.s32 	%r646, %r646, 1;
+	setp.lt.u32 	%p21, %r646, %r185;
+	mov.f32 	%f1797, %f1833;
+	mov.f32 	%f1798, %f1832;
+	mov.f32 	%f1799, %f1831;
+	mov.f32 	%f1800, %f1830;
+	mov.f32 	%f1801, %f1829;
+	mov.f32 	%f1802, %f1828;
+	mov.f32 	%f1803, %f1827;
+	mov.f32 	%f1804, %f1826;
+	mov.f32 	%f1805, %f1825;
+	@%p21 bra 	$L__BB6_26;
+
+$L__BB6_42:
+	mul.f32 	%f1146, %f1853, %f1832;
+	fma.rn.f32 	%f1147, %f1852, %f1831, %f1146;
+	mul.f32 	%f1148, %f1853, %f1829;
+	fma.rn.f32 	%f1149, %f1852, %f1828, %f1148;
+	mul.f32 	%f1150, %f1853, %f1826;
+	fma.rn.f32 	%f1151, %f1852, %f1825, %f1150;
+	fma.rn.f32 	%f1854, %f959, %f1827, %f1151;
+	fma.rn.f32 	%f1853, %f959, %f1830, %f1149;
+	fma.rn.f32 	%f1852, %f959, %f1833, %f1147;
+	bra.uni 	$L__BB6_44;
+
+$L__BB6_43:
+	mov.f32 	%f1854, %f959;
+
+$L__BB6_44:
+	add.s64 	%rd18, %rd3, 80;
+	ld.v4.f32 	{%f1155, %f1156, %f1157, %f1158}, [%rd3+80];
+	ld.f32 	%f1162, [%rd3+32];
+	fma.rn.f32 	%f1163, %f1794, %f1162, %f1155;
+	ld.f32 	%f1164, [%rd3+36];
+	fma.rn.f32 	%f1165, %f1794, %f1164, %f1156;
+	ld.f32 	%f1166, [%rd3+40];
+	fma.rn.f32 	%f1167, %f1794, %f1166, %f1157;
+	ld.f32 	%f1168, [%rd3+48];
+	fma.rn.f32 	%f1169, %f1795, %f1168, %f1163;
+	ld.f32 	%f1170, [%rd3+52];
+	fma.rn.f32 	%f1171, %f1795, %f1170, %f1165;
+	ld.f32 	%f1172, [%rd3+56];
+	fma.rn.f32 	%f1173, %f1795, %f1172, %f1167;
+	ld.f32 	%f1174, [%rd3+64];
+	fma.rn.f32 	%f1175, %f1796, %f1174, %f1169;
+	ld.f32 	%f1176, [%rd3+68];
+	fma.rn.f32 	%f1177, %f1796, %f1176, %f1171;
+	ld.f32 	%f1178, [%rd3+72];
+	fma.rn.f32 	%f1179, %f1796, %f1178, %f1173;
+	ld.v4.f32 	{%f1180, %f1181, %f1182, %f1183}, [%rd3+32];
+	mul.f32 	%f1187, %f1852, %f1180;
+	mul.f32 	%f1188, %f1852, %f1181;
+	mul.f32 	%f1189, %f1852, %f1182;
+	fma.rn.f32 	%f1190, %f1853, %f1168, %f1187;
+	fma.rn.f32 	%f1191, %f1853, %f1170, %f1188;
+	fma.rn.f32 	%f1192, %f1853, %f1172, %f1189;
+	fma.rn.f32 	%f1193, %f1854, %f1174, %f1190;
+	fma.rn.f32 	%f1194, %f1854, %f1176, %f1191;
+	fma.rn.f32 	%f1195, %f1854, %f1178, %f1192;
+	rcp.rn.f32 	%f1196, %f1195;
+	mul.f32 	%f1197, %f1179, %f1196;
+	neg.f32 	%f346, %f1197;
+	fma.rn.f32 	%f347, %f346, %f1193, %f1175;
+	fma.rn.f32 	%f348, %f346, %f1194, %f1177;
+	ld.const.u64 	%rd19, [params+80];
+	setp.eq.s64 	%p22, %rd19, 0;
+	cvt.u64.u32 	%rd20, %r1;
+	mov.f32 	%f1989, 0f00000000;
+	mov.f32 	%f1990, 0f00000000;
+	mov.f32 	%f1991, 0f00000000;
+	@%p22 bra 	$L__BB6_49;
+
+	ld.u64 	%rd286, [%rd47];
+	ld.const.u64 	%rd287, [params+328];
+	cvta.to.global.u64 	%rd288, %rd287;
+	shl.b64 	%rd289, %rd20, 3;
+	add.s64 	%rd290, %rd288, %rd289;
+	st.global.u64 	[%rd290], %rd286;
+	ld.const.u64 	%rd291, [params+336];
+	cvta.to.global.u64 	%rd292, %rd291;
+	shl.b64 	%rd293, %rd20, 2;
+	add.s64 	%rd294, %rd292, %rd293;
+	mov.u32 	%r336, 0;
+	st.global.u32 	[%rd294], %r336;
+	ld.const.u64 	%rd295, [params+344];
+	cvta.to.global.u64 	%rd296, %rd295;
+	add.s64 	%rd21, %rd296, %rd293;
+	ld.global.u32 	%r10, [%rd21];
+	setp.eq.s32 	%p23, %r10, 0;
+	@%p23 bra 	$L__BB6_48;
+
+	// begin inline asm
+	call (%r337), _optix_read_instance_id, ();
+	// end inline asm
+	setp.ge.u32 	%p24, %r337, %r10;
+	@%p24 bra 	$L__BB6_48;
+
+	st.global.u32 	[%rd21], %r337;
+
+$L__BB6_48:
+	cvta.to.global.u64 	%rd297, %rd19;
+	add.s64 	%rd299, %rd297, %rd293;
+	st.global.f32 	[%rd299], %f347;
+	ld.const.u64 	%rd300, [params+88];
 	cvta.to.global.u64 	%rd301, %rd300;
-	shl.b64 	%rd302, %rd22, 2;
-	add.s64 	%rd303, %rd301, %rd302;
-	mov.u32 	%r329, 0;
-	st.global.u32 	[%rd303], %r329;
-	ld.const.u64 	%rd304, [params+344];
-	cvta.to.global.u64 	%rd305, %rd304;
-	add.s64 	%rd24, %rd305, %rd302;
-	ld.global.u32 	%r9, [%rd24];
-	setp.eq.s32	%p21, %r9, 0;
-	@%p21 bra 	BB6_47;
-
-	// inline asm
-	call (%r330), _optix_read_instance_id, ();
-	// inline asm
-	setp.ge.u32	%p22, %r330, %r9;
-	@%p22 bra 	BB6_47;
-
-	st.global.u32 	[%rd24], %r330;
-
-BB6_47:
-	cvta.to.global.u64 	%rd306, %rd21;
-	add.s64 	%rd308, %rd306, %rd302;
-	st.global.f32 	[%rd308], %f314;
-	ld.const.u64 	%rd309, [params+88];
-	cvta.to.global.u64 	%rd310, %rd309;
-	add.s64 	%rd311, %rd310, %rd302;
-	st.global.f32 	[%rd311], %f315;
-	ld.const.u64 	%rd312, [params+72];
-	cvta.to.global.u64 	%rd313, %rd312;
-	add.s64 	%rd314, %rd313, %rd302;
-	st.global.f32 	[%rd314], %f313;
-	bra.uni 	BB6_108;
-
-BB6_48:
-	fma.rn.f32 	%f1944, %f313, %f1792, %f1745;
-	fma.rn.f32 	%f1945, %f313, %f1793, %f1744;
-	fma.rn.f32 	%f1946, %f313, %f1794, %f1743;
-	fma.rn.f32 	%f319, %f314, 0f3F000000, 0f3F000000;
-	fma.rn.f32 	%f320, %f315, 0f3F000000, 0f3F000000;
-	ld.v4.f32 	{%f1941, %f1942, %f1943, %f1171}, [%rd3+160];
-	ld.v4.f32 	{%f1935, %f1936, %f1937, %f1175}, [%rd3+176];
-	ld.v4.f32 	{%f1932, %f1933, %f1934, %f1179}, [%rd3+192];
-	ld.u64 	%rd25, [%rd52];
-	ld.const.u64 	%rd315, [params+344];
-	cvta.to.global.u64 	%rd316, %rd315;
-	shl.b64 	%rd317, %rd22, 2;
-	add.s64 	%rd26, %rd316, %rd317;
-	ld.global.u32 	%r11, [%rd26];
-	setp.eq.s32	%p23, %r11, 0;
-	mov.f32 	%f1929, 0f00000000;
-	@%p23 bra 	BB6_49;
-
-	// inline asm
-	call (%r331), _optix_read_instance_id, ();
-	// inline asm
-	setp.ge.u32	%p24, %r331, %r11;
-	@%p24 bra 	BB6_49;
-
-	mov.f32 	%f1860, 0f00000000;
-	mov.f32 	%f1861, 0f3F800000;
-	mov.f32 	%f1798, %f1861;
-	mov.f32 	%f1797, %f1860;
-	mov.f32 	%f1796, %f1860;
-	mov.f32 	%f1795, %f1860;
-	mov.f32 	%f1802, %f1860;
-	mov.f32 	%f1801, %f1861;
-	mov.f32 	%f1800, %f1860;
-	mov.f32 	%f1799, %f1860;
-	mov.f32 	%f1806, %f1860;
-	mov.f32 	%f1805, %f1860;
-	mov.f32 	%f1804, %f1861;
-	mov.f32 	%f1803, %f1860;
-	@%p2 bra 	BB6_69;
-
-	add.s32 	%r636, %r28, -1;
-	setp.lt.s32	%p26, %r636, 0;
-	@%p26 bra 	BB6_69;
-
-BB6_53:
+	add.s64 	%rd302, %rd301, %rd293;
+	st.global.f32 	[%rd302], %f348;
+	ld.const.u64 	%rd303, [params+72];
+	cvta.to.global.u64 	%rd304, %rd303;
+	add.s64 	%rd305, %rd304, %rd293;
+	st.global.f32 	[%rd305], %f346;
+	bra.uni 	$L__BB6_109;
+
+$L__BB6_49:
+	fma.rn.f32 	%f2004, %f346, %f1852, %f1794;
+	fma.rn.f32 	%f2005, %f346, %f1853, %f1795;
+	fma.rn.f32 	%f2006, %f346, %f1854, %f1796;
+	fma.rn.f32 	%f352, %f347, 0f3F000000, 0f3F000000;
+	fma.rn.f32 	%f353, %f348, 0f3F000000, 0f3F000000;
+	ld.v4.f32 	{%f2001, %f2002, %f2003, %f1204}, [%rd18+80];
+	ld.v4.f32 	{%f1995, %f1996, %f1997, %f1208}, [%rd18+96];
+	ld.v4.f32 	{%f1992, %f1993, %f1994, %f1212}, [%rd18+112];
+	ld.u64 	%rd22, [%rd47];
+	ld.const.u64 	%rd306, [params+344];
+	cvta.to.global.u64 	%rd307, %rd306;
+	shl.b64 	%rd308, %rd20, 2;
+	add.s64 	%rd23, %rd307, %rd308;
+	ld.global.u32 	%r12, [%rd23];
+	setp.eq.s32 	%p25, %r12, 0;
+	mov.f32 	%f1998, %f2001;
+	mov.f32 	%f1999, %f2002;
+	mov.f32 	%f2000, %f2003;
+	@%p25 bra 	$L__BB6_97;
+
+	// begin inline asm
+	call (%r338), _optix_read_instance_id, ();
+	// end inline asm
+	setp.ge.u32 	%p26, %r338, %r12;
+	mov.f32 	%f1998, %f2001;
+	mov.f32 	%f1999, %f2002;
+	mov.f32 	%f2000, %f2003;
+	@%p26 bra 	$L__BB6_97;
+
+	// begin inline asm
+	call (%r339), _optix_get_transform_list_size, ();
+	// end inline asm
+	setp.eq.s32 	%p27, %r339, 0;
+	mov.f32 	%f1954, 0f00000000;
+	mov.f32 	%f1953, 0f3F800000;
+	mov.f32 	%f1891, %f1953;
+	mov.f32 	%f1892, %f1954;
+	mov.f32 	%f1893, %f1954;
+	mov.f32 	%f1894, %f1954;
+	mov.f32 	%f1887, %f1954;
+	mov.f32 	%f1888, %f1953;
+	mov.f32 	%f1889, %f1954;
+	mov.f32 	%f1890, %f1954;
+	mov.f32 	%f1883, %f1954;
+	mov.f32 	%f1884, %f1954;
+	mov.f32 	%f1885, %f1953;
+	mov.f32 	%f1886, %f1954;
+	@%p27 bra 	$L__BB6_69;
+
+	// begin inline asm
+	call (%r340), _optix_get_transform_list_size, ();
+	// end inline asm
+	// begin inline asm
+	call (%f1228), _optix_get_ray_time, ();
+	// end inline asm
+	setp.lt.s32 	%p28, %r340, 1;
+	@%p28 bra 	$L__BB6_69;
+
+	add.s32 	%r647, %r340, 1;
+	mov.u32 	%r648, 1;
+
+$L__BB6_54:
 	.pragma "nounroll";
-	// inline asm
-	call (%rd318), _optix_get_transform_list_handle, (%r636);
-	// inline asm
-	// inline asm
-	call (%r333), _optix_get_transform_type_from_handle, (%rd318);
-	// inline asm
-	and.b32  	%r334, %r333, -2;
-	setp.eq.s32	%p27, %r334, 2;
-	@%p27 bra 	BB6_59;
-	bra.uni 	BB6_54;
-
-BB6_59:
-	setp.eq.s32	%p30, %r333, 2;
-	@%p30 bra 	BB6_63;
-	bra.uni 	BB6_60;
-
-BB6_63:
-	// inline asm
-	call (%rd392), _optix_get_matrix_motion_transform_from_handle, (%rd318);
-	// inline asm
-	// inline asm
-	cvta.to.global.u64 %rd394, %rd392;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r422,%r423,%r424,%r425}, [%rd394];
-	// inline asm
-	mov.b32	{%rs12, %rs13}, %r424;
-	add.s64 	%rd398, %rd392, 16;
-	// inline asm
-	cvta.to.global.u64 %rd397, %rd398;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r426,%r427,%r428,%r429}, [%rd397];
-	// inline asm
-	add.s64 	%rd401, %rd392, 32;
-	// inline asm
-	cvta.to.global.u64 %rd400, %rd401;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r430,%r431,%r432,%r433}, [%rd400];
-	// inline asm
-	add.s64 	%rd404, %rd392, 48;
-	// inline asm
-	cvta.to.global.u64 %rd403, %rd404;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r434,%r435,%r436,%r437}, [%rd403];
-	// inline asm
-	add.s64 	%rd407, %rd392, 64;
-	// inline asm
-	cvta.to.global.u64 %rd406, %rd407;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r438,%r439,%r440,%r441}, [%rd406];
-	// inline asm
-	add.s64 	%rd410, %rd392, 80;
-	// inline asm
-	cvta.to.global.u64 %rd409, %rd410;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r442,%r443,%r444,%r445}, [%rd409];
-	// inline asm
-	add.s64 	%rd413, %rd392, 96;
-	// inline asm
-	cvta.to.global.u64 %rd412, %rd413;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r446,%r447,%r448,%r449}, [%rd412];
-	// inline asm
-	add.s64 	%rd416, %rd392, 112;
-	// inline asm
-	cvta.to.global.u64 %rd415, %rd416;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r450,%r451,%r452,%r453}, [%rd415];
-	// inline asm
-	mov.b32 	 %f1321, %r425;
-	mov.b32 	 %f1322, %r426;
-	cvt.u32.u16	%r466, %rs12;
-	add.s32 	%r467, %r466, -1;
-	cvt.rn.f32.s32	%f1323, %r467;
-	sub.f32 	%f1324, %f924, %f1321;
-	mul.f32 	%f1325, %f1324, %f1323;
-	sub.f32 	%f1326, %f1322, %f1321;
-	div.rn.f32 	%f1327, %f1325, %f1326;
-	min.f32 	%f1328, %f1323, %f1327;
-	mov.f32 	%f1329, 0f00000000;
-	max.f32 	%f1330, %f1329, %f1328;
-	cvt.rmi.f32.f32	%f1331, %f1330;
-	cvt.rzi.s32.f32	%r468, %f1331;
-	cvt.s64.s32	%rd34, %r468;
-	mul.wide.s32 	%rd427, %r468, 48;
-	add.s64 	%rd419, %rd401, %rd427;
-	// inline asm
-	cvta.to.global.u64 %rd418, %rd419;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r454,%r455,%r456,%r457}, [%rd418];
-	// inline asm
-	mov.b32 	 %f1831, %r454;
-	mov.b32 	 %f1832, %r455;
-	mov.b32 	 %f1833, %r456;
-	mov.b32 	 %f1834, %r457;
-	add.s64 	%rd422, %rd419, 16;
-	// inline asm
-	cvta.to.global.u64 %rd421, %rd422;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r458,%r459,%r460,%r461}, [%rd421];
-	// inline asm
-	mov.b32 	 %f1827, %r458;
-	mov.b32 	 %f1828, %r459;
-	mov.b32 	 %f1829, %r460;
-	mov.b32 	 %f1830, %r461;
-	add.s64 	%rd425, %rd419, 32;
-	// inline asm
-	cvta.to.global.u64 %rd424, %rd425;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r462,%r463,%r464,%r465}, [%rd424];
-	// inline asm
-	sub.f32 	%f423, %f1330, %f1331;
-	mov.b32 	 %f1823, %r462;
-	mov.b32 	 %f1824, %r463;
-	mov.b32 	 %f1825, %r464;
-	mov.b32 	 %f1826, %r465;
-	setp.leu.f32	%p32, %f423, 0f00000000;
-	@%p32 bra 	BB6_65;
-
-	mul.lo.s64 	%rd437, %rd34, 48;
-	add.s64 	%rd438, %rd392, %rd437;
-	add.s64 	%rd429, %rd438, 80;
-	// inline asm
-	cvta.to.global.u64 %rd428, %rd429;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r469,%r470,%r471,%r472}, [%rd428];
-	// inline asm
-	mov.b32 	 %f1332, %r469;
-	mov.b32 	 %f1333, %r470;
-	mov.b32 	 %f1334, %r471;
-	mov.b32 	 %f1335, %r472;
-	add.s64 	%rd432, %rd438, 96;
-	// inline asm
-	cvta.to.global.u64 %rd431, %rd432;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r473,%r474,%r475,%r476}, [%rd431];
-	// inline asm
-	mov.b32 	 %f1336, %r473;
-	mov.b32 	 %f1337, %r474;
-	mov.b32 	 %f1338, %r475;
-	mov.b32 	 %f1339, %r476;
-	add.s64 	%rd435, %rd438, 112;
-	// inline asm
-	cvta.to.global.u64 %rd434, %rd435;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r477,%r478,%r479,%r480}, [%rd434];
-	// inline asm
-	mov.f32 	%f1340, 0f3F800000;
-	sub.f32 	%f1341, %f1340, %f423;
-	mul.f32 	%f1342, %f423, %f1332;
-	mul.f32 	%f1343, %f423, %f1333;
-	mul.f32 	%f1344, %f423, %f1334;
-	mul.f32 	%f1345, %f423, %f1335;
-	fma.rn.f32 	%f1831, %f1341, %f1831, %f1342;
-	fma.rn.f32 	%f1832, %f1341, %f1832, %f1343;
-	fma.rn.f32 	%f1833, %f1341, %f1833, %f1344;
-	fma.rn.f32 	%f1834, %f1341, %f1834, %f1345;
-	mul.f32 	%f1346, %f423, %f1336;
-	mul.f32 	%f1347, %f423, %f1337;
-	mul.f32 	%f1348, %f423, %f1338;
-	mul.f32 	%f1349, %f423, %f1339;
-	fma.rn.f32 	%f1827, %f1341, %f1827, %f1346;
-	fma.rn.f32 	%f1828, %f1341, %f1828, %f1347;
-	fma.rn.f32 	%f1829, %f1341, %f1829, %f1348;
-	fma.rn.f32 	%f1830, %f1341, %f1830, %f1349;
-	mov.b32 	 %f1350, %r477;
-	mov.b32 	 %f1351, %r478;
-	mov.b32 	 %f1352, %r479;
-	mov.b32 	 %f1353, %r480;
-	mul.f32 	%f1354, %f423, %f1350;
-	mul.f32 	%f1355, %f423, %f1351;
-	mul.f32 	%f1356, %f423, %f1352;
-	mul.f32 	%f1357, %f423, %f1353;
-	fma.rn.f32 	%f1823, %f1341, %f1823, %f1354;
-	fma.rn.f32 	%f1824, %f1341, %f1824, %f1355;
-	fma.rn.f32 	%f1825, %f1341, %f1825, %f1356;
-	fma.rn.f32 	%f1826, %f1341, %f1826, %f1357;
-	bra.uni 	BB6_65;
-
-BB6_54:
-	mov.f32 	%f1823, 0f00000000;
-	mov.f32 	%f1825, 0f3F800000;
-	setp.eq.s32	%p28, %r333, 4;
-	@%p28 bra 	BB6_57;
-	bra.uni 	BB6_55;
-
-BB6_57:
-	// inline asm
-	call (%rd668), _optix_get_instance_transform_from_handle, (%rd318);
-	// inline asm
-	bra.uni 	BB6_58;
-
-BB6_60:
-	// inline asm
-	call (%rd333), _optix_get_srt_motion_transform_from_handle, (%rd318);
-	// inline asm
-	// inline asm
-	cvta.to.global.u64 %rd335, %rd333;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r347,%r348,%r349,%r350}, [%rd335];
-	// inline asm
-	mov.b32	{%rs10, %rs11}, %r349;
-	add.s64 	%rd339, %rd333, 16;
-	// inline asm
+	add.s32 	%r342, %r647, -2;
+	// begin inline asm
+	call (%rd309), _optix_get_transform_list_handle, (%r342);
+	// end inline asm
+	// begin inline asm
+	call (%r343), _optix_get_transform_type_from_handle, (%rd309);
+	// end inline asm
+	or.b32  	%r344, %r343, 1;
+	setp.eq.s32 	%p29, %r344, 3;
+	@%p29 bra 	$L__BB6_60;
+	bra.uni 	$L__BB6_55;
+
+$L__BB6_60:
+	setp.eq.s32 	%p32, %r343, 2;
+	@%p32 bra 	$L__BB6_64;
+	bra.uni 	$L__BB6_61;
+
+$L__BB6_64:
+	// begin inline asm
+	call (%rd381), _optix_get_matrix_motion_transform_from_handle, (%rd309);
+	// end inline asm
+	// begin inline asm
+	cvta.to.global.u64 %rd383, %rd381;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r432,%r433,%r434,%r435}, [%rd383];
+	// end inline asm
+	add.s64 	%rd387, %rd381, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd386, %rd387;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r436,%r437,%r438,%r439}, [%rd386];
+	// end inline asm
+	add.s64 	%rd390, %rd381, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd389, %rd390;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r440,%r441,%r442,%r443}, [%rd389];
+	// end inline asm
+	add.s64 	%rd393, %rd381, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd392, %rd393;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r444,%r445,%r446,%r447}, [%rd392];
+	// end inline asm
+	add.s64 	%rd396, %rd381, 64;
+	// begin inline asm
+	cvta.to.global.u64 %rd395, %rd396;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r448,%r449,%r450,%r451}, [%rd395];
+	// end inline asm
+	add.s64 	%rd399, %rd381, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd398, %rd399;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r452,%r453,%r454,%r455}, [%rd398];
+	// end inline asm
+	add.s64 	%rd402, %rd381, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd401, %rd402;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r456,%r457,%r458,%r459}, [%rd401];
+	// end inline asm
+	add.s64 	%rd405, %rd381, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd404, %rd405;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r460,%r461,%r462,%r463}, [%rd404];
+	// end inline asm
+	mov.b32 	%f1356, %r435;
+	mov.b32 	%f1357, %r436;
+	and.b32  	%r476, %r434, 65535;
+	add.s32 	%r477, %r476, -1;
+	cvt.rn.f32.s32 	%f1358, %r477;
+	sub.f32 	%f1359, %f1228, %f1356;
+	mul.f32 	%f1360, %f1359, %f1358;
+	sub.f32 	%f1361, %f1357, %f1356;
+	div.rn.f32 	%f1362, %f1360, %f1361;
+	min.f32 	%f1363, %f1358, %f1362;
+	mov.f32 	%f1364, 0f00000000;
+	max.f32 	%f1365, %f1364, %f1363;
+	cvt.rmi.f32.f32 	%f1366, %f1365;
+	sub.f32 	%f449, %f1365, %f1366;
+	cvt.rzi.s32.f32 	%r478, %f1366;
+	cvt.s64.s32 	%rd30, %r478;
+	mul.wide.s32 	%rd416, %r478, 48;
+	add.s64 	%rd408, %rd390, %rd416;
+	// begin inline asm
+	cvta.to.global.u64 %rd407, %rd408;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r464,%r465,%r466,%r467}, [%rd407];
+	// end inline asm
+	mov.b32 	%f1891, %r464;
+	mov.b32 	%f1892, %r465;
+	mov.b32 	%f1893, %r466;
+	mov.b32 	%f1894, %r467;
+	add.s64 	%rd411, %rd408, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd410, %rd411;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r468,%r469,%r470,%r471}, [%rd410];
+	// end inline asm
+	mov.b32 	%f1887, %r468;
+	mov.b32 	%f1888, %r469;
+	mov.b32 	%f1889, %r470;
+	mov.b32 	%f1890, %r471;
+	add.s64 	%rd414, %rd408, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd413, %rd414;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r472,%r473,%r474,%r475}, [%rd413];
+	// end inline asm
+	mov.b32 	%f1883, %r472;
+	mov.b32 	%f1884, %r473;
+	mov.b32 	%f1885, %r474;
+	mov.b32 	%f1886, %r475;
+	setp.leu.f32 	%p34, %f449, 0f00000000;
+	@%p34 bra 	$L__BB6_66;
+
+	mov.f32 	%f1367, 0f3F800000;
+	sub.f32 	%f1368, %f1367, %f449;
+	mul.lo.s64 	%rd426, %rd30, 48;
+	add.s64 	%rd427, %rd381, %rd426;
+	add.s64 	%rd418, %rd427, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd417, %rd418;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r479,%r480,%r481,%r482}, [%rd417];
+	// end inline asm
+	mov.b32 	%f1369, %r479;
+	mov.b32 	%f1370, %r480;
+	mov.b32 	%f1371, %r481;
+	mov.b32 	%f1372, %r482;
+	mul.f32 	%f1373, %f449, %f1369;
+	mul.f32 	%f1374, %f449, %f1370;
+	mul.f32 	%f1375, %f449, %f1371;
+	mul.f32 	%f1376, %f449, %f1372;
+	fma.rn.f32 	%f1891, %f1368, %f1891, %f1373;
+	fma.rn.f32 	%f1892, %f1368, %f1892, %f1374;
+	fma.rn.f32 	%f1893, %f1368, %f1893, %f1375;
+	fma.rn.f32 	%f1894, %f1368, %f1894, %f1376;
+	add.s64 	%rd421, %rd427, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd420, %rd421;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r483,%r484,%r485,%r486}, [%rd420];
+	// end inline asm
+	mov.b32 	%f1377, %r483;
+	mov.b32 	%f1378, %r484;
+	mov.b32 	%f1379, %r485;
+	mov.b32 	%f1380, %r486;
+	mul.f32 	%f1381, %f449, %f1377;
+	mul.f32 	%f1382, %f449, %f1378;
+	mul.f32 	%f1383, %f449, %f1379;
+	mul.f32 	%f1384, %f449, %f1380;
+	fma.rn.f32 	%f1887, %f1368, %f1887, %f1381;
+	fma.rn.f32 	%f1888, %f1368, %f1888, %f1382;
+	fma.rn.f32 	%f1889, %f1368, %f1889, %f1383;
+	fma.rn.f32 	%f1890, %f1368, %f1890, %f1384;
+	add.s64 	%rd424, %rd427, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd423, %rd424;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r487,%r488,%r489,%r490}, [%rd423];
+	// end inline asm
+	mov.b32 	%f1385, %r487;
+	mov.b32 	%f1386, %r488;
+	mov.b32 	%f1387, %r489;
+	mov.b32 	%f1388, %r490;
+	mul.f32 	%f1389, %f449, %f1385;
+	mul.f32 	%f1390, %f449, %f1386;
+	mul.f32 	%f1391, %f449, %f1387;
+	mul.f32 	%f1392, %f449, %f1388;
+	fma.rn.f32 	%f1883, %f1368, %f1883, %f1389;
+	fma.rn.f32 	%f1884, %f1368, %f1884, %f1390;
+	fma.rn.f32 	%f1885, %f1368, %f1885, %f1391;
+	fma.rn.f32 	%f1886, %f1368, %f1886, %f1392;
+	bra.uni 	$L__BB6_66;
+
+$L__BB6_55:
+	mov.f32 	%f1883, 0f00000000;
+	mov.f32 	%f1885, 0f3F800000;
+	setp.eq.s32 	%p30, %r343, 4;
+	@%p30 bra 	$L__BB6_58;
+
+	setp.ne.s32 	%p31, %r343, 1;
+	mov.f32 	%f1884, %f1883;
+	mov.f32 	%f1886, %f1883;
+	mov.f32 	%f1887, %f1883;
+	mov.f32 	%f1888, %f1885;
+	mov.f32 	%f1889, %f1883;
+	mov.f32 	%f1890, %f1883;
+	mov.f32 	%f1891, %f1885;
+	mov.f32 	%f1892, %f1883;
+	mov.f32 	%f1893, %f1883;
+	mov.f32 	%f1894, %f1883;
+	@%p31 bra 	$L__BB6_66;
+
+	// begin inline asm
+	call (%rd311), _optix_get_static_transform_from_handle, (%rd309);
+	// end inline asm
+	add.s64 	%rd654, %rd311, 16;
+	bra.uni 	$L__BB6_59;
+
+$L__BB6_61:
+	// begin inline asm
+	call (%rd324), _optix_get_srt_motion_transform_from_handle, (%rd309);
+	// end inline asm
+	// begin inline asm
+	cvta.to.global.u64 %rd326, %rd324;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r357,%r358,%r359,%r360}, [%rd326];
+	// end inline asm
+	add.s64 	%rd330, %rd324, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd329, %rd330;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r361,%r362,%r363,%r364}, [%rd329];
+	// end inline asm
+	add.s64 	%rd333, %rd324, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd332, %rd333;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r365,%r366,%r367,%r368}, [%rd332];
+	// end inline asm
+	add.s64 	%rd336, %rd324, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd335, %rd336;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r369,%r370,%r371,%r372}, [%rd335];
+	// end inline asm
+	add.s64 	%rd339, %rd324, 64;
+	// begin inline asm
 	cvta.to.global.u64 %rd338, %rd339;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r351,%r352,%r353,%r354}, [%rd338];
-	// inline asm
-	add.s64 	%rd342, %rd333, 32;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r373,%r374,%r375,%r376}, [%rd338];
+	// end inline asm
+	add.s64 	%rd342, %rd324, 80;
+	// begin inline asm
 	cvta.to.global.u64 %rd341, %rd342;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r355,%r356,%r357,%r358}, [%rd341];
-	// inline asm
-	add.s64 	%rd345, %rd333, 48;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r377,%r378,%r379,%r380}, [%rd341];
+	// end inline asm
+	add.s64 	%rd345, %rd324, 96;
+	// begin inline asm
 	cvta.to.global.u64 %rd344, %rd345;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r359,%r360,%r361,%r362}, [%rd344];
-	// inline asm
-	add.s64 	%rd348, %rd333, 64;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r381,%r382,%r383,%r384}, [%rd344];
+	// end inline asm
+	add.s64 	%rd348, %rd324, 112;
+	// begin inline asm
 	cvta.to.global.u64 %rd347, %rd348;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r363,%r364,%r365,%r366}, [%rd347];
-	// inline asm
-	add.s64 	%rd351, %rd333, 80;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r385,%r386,%r387,%r388}, [%rd347];
+	// end inline asm
+	add.s64 	%rd351, %rd324, 128;
+	// begin inline asm
 	cvta.to.global.u64 %rd350, %rd351;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r367,%r368,%r369,%r370}, [%rd350];
-	// inline asm
-	add.s64 	%rd354, %rd333, 96;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r389,%r390,%r391,%r392}, [%rd350];
+	// end inline asm
+	add.s64 	%rd354, %rd324, 144;
+	// begin inline asm
 	cvta.to.global.u64 %rd353, %rd354;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r371,%r372,%r373,%r374}, [%rd353];
-	// inline asm
-	add.s64 	%rd357, %rd333, 112;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r393,%r394,%r395,%r396}, [%rd353];
+	// end inline asm
+	mov.b32 	%f1243, %r360;
+	mov.b32 	%f1244, %r361;
+	and.b32  	%r413, %r359, 65535;
+	add.s32 	%r414, %r413, -1;
+	cvt.rn.f32.s32 	%f1245, %r414;
+	sub.f32 	%f1246, %f1228, %f1243;
+	mul.f32 	%f1247, %f1246, %f1245;
+	sub.f32 	%f1248, %f1244, %f1243;
+	div.rn.f32 	%f1249, %f1247, %f1248;
+	min.f32 	%f1250, %f1245, %f1249;
+	mov.f32 	%f1251, 0f00000000;
+	max.f32 	%f1252, %f1251, %f1250;
+	cvt.rmi.f32.f32 	%f1253, %f1252;
+	sub.f32 	%f388, %f1252, %f1253;
+	cvt.rzi.s32.f32 	%r415, %f1253;
+	mul.wide.s32 	%rd368, %r415, 64;
+	add.s64 	%rd357, %rd333, %rd368;
+	// begin inline asm
 	cvta.to.global.u64 %rd356, %rd357;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r375,%r376,%r377,%r378}, [%rd356];
-	// inline asm
-	add.s64 	%rd360, %rd333, 128;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r397,%r398,%r399,%r400}, [%rd356];
+	// end inline asm
+	mov.b32 	%f1867, %r397;
+	mov.b32 	%f1868, %r398;
+	mov.b32 	%f1869, %r399;
+	mov.b32 	%f1870, %r400;
+	add.s64 	%rd360, %rd357, 16;
+	// begin inline asm
 	cvta.to.global.u64 %rd359, %rd360;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r379,%r380,%r381,%r382}, [%rd359];
-	// inline asm
-	add.s64 	%rd363, %rd333, 144;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r401,%r402,%r403,%r404}, [%rd359];
+	// end inline asm
+	mov.b32 	%f1871, %r401;
+	mov.b32 	%f1872, %r402;
+	mov.b32 	%f1873, %r403;
+	mov.b32 	%f1874, %r404;
+	add.s64 	%rd363, %rd357, 32;
+	// begin inline asm
 	cvta.to.global.u64 %rd362, %rd363;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r383,%r384,%r385,%r386}, [%rd362];
-	// inline asm
-	mov.b32 	 %f1208, %r350;
-	mov.b32 	 %f1209, %r351;
-	cvt.u32.u16	%r403, %rs10;
-	add.s32 	%r404, %r403, -1;
-	cvt.rn.f32.s32	%f1210, %r404;
-	sub.f32 	%f1211, %f924, %f1208;
-	mul.f32 	%f1212, %f1211, %f1210;
-	sub.f32 	%f1213, %f1209, %f1208;
-	div.rn.f32 	%f1214, %f1212, %f1213;
-	min.f32 	%f1215, %f1210, %f1214;
-	mov.f32 	%f1216, 0f00000000;
-	max.f32 	%f1217, %f1216, %f1215;
-	cvt.rmi.f32.f32	%f1218, %f1217;
-	cvt.rzi.s32.f32	%r405, %f1218;
-	cvt.s64.s32	%rd32, %r405;
-	mul.wide.s32 	%rd377, %r405, 64;
-	add.s64 	%rd366, %rd342, %rd377;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r405,%r406,%r407,%r408}, [%rd362];
+	// end inline asm
+	mov.b32 	%f1875, %r405;
+	mov.b32 	%f1876, %r406;
+	mov.b32 	%f1877, %r407;
+	mov.b32 	%f1878, %r408;
+	add.s64 	%rd366, %rd357, 48;
+	// begin inline asm
 	cvta.to.global.u64 %rd365, %rd366;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r387,%r388,%r389,%r390}, [%rd365];
-	// inline asm
-	mov.b32 	 %f1807, %r387;
-	mov.b32 	 %f1808, %r388;
-	mov.b32 	 %f1809, %r389;
-	mov.b32 	 %f1810, %r390;
-	add.s64 	%rd369, %rd366, 16;
-	// inline asm
-	cvta.to.global.u64 %rd368, %rd369;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r391,%r392,%r393,%r394}, [%rd368];
-	// inline asm
-	mov.b32 	 %f1811, %r391;
-	mov.b32 	 %f1812, %r392;
-	mov.b32 	 %f1813, %r393;
-	mov.b32 	 %f1814, %r394;
-	add.s64 	%rd372, %rd366, 32;
-	// inline asm
-	cvta.to.global.u64 %rd371, %rd372;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r395,%r396,%r397,%r398}, [%rd371];
-	// inline asm
-	sub.f32 	%f362, %f1217, %f1218;
-	mov.b32 	 %f1815, %r395;
-	mov.b32 	 %f1816, %r396;
-	mov.b32 	 %f1817, %r397;
-	mov.b32 	 %f1818, %r398;
-	add.s64 	%rd375, %rd366, 48;
-	// inline asm
-	cvta.to.global.u64 %rd374, %rd375;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r399,%r400,%r401,%r402}, [%rd374];
-	// inline asm
-	mov.b32 	 %f1819, %r399;
-	mov.b32 	 %f1820, %r400;
-	mov.b32 	 %f1821, %r401;
-	mov.b32 	 %f1822, %r402;
-	setp.leu.f32	%p31, %f362, 0f00000000;
-	@%p31 bra 	BB6_62;
-
-	shl.b64 	%rd390, %rd32, 6;
-	add.s64 	%rd391, %rd390, %rd333;
-	add.s64 	%rd379, %rd391, 96;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r409,%r410,%r411,%r412}, [%rd365];
+	// end inline asm
+	mov.b32 	%f1879, %r409;
+	mov.b32 	%f1880, %r410;
+	mov.b32 	%f1881, %r411;
+	mov.b32 	%f1882, %r412;
+	setp.leu.f32 	%p33, %f388, 0f00000000;
+	@%p33 bra 	$L__BB6_63;
+
+	mov.f32 	%f1254, 0f3F800000;
+	sub.f32 	%f1255, %f1254, %f388;
+	add.s64 	%rd370, %rd357, 64;
+	// begin inline asm
+	cvta.to.global.u64 %rd369, %rd370;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r416,%r417,%r418,%r419}, [%rd369];
+	// end inline asm
+	mov.b32 	%f1256, %r416;
+	mov.b32 	%f1257, %r417;
+	mov.b32 	%f1258, %r418;
+	mov.b32 	%f1259, %r419;
+	mul.f32 	%f1260, %f388, %f1256;
+	mul.f32 	%f1261, %f388, %f1257;
+	mul.f32 	%f1262, %f388, %f1258;
+	mul.f32 	%f1263, %f388, %f1259;
+	fma.rn.f32 	%f1867, %f1255, %f1867, %f1260;
+	fma.rn.f32 	%f1868, %f1255, %f1868, %f1261;
+	fma.rn.f32 	%f1869, %f1255, %f1869, %f1262;
+	fma.rn.f32 	%f1870, %f1255, %f1870, %f1263;
+	add.s64 	%rd373, %rd357, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd372, %rd373;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r420,%r421,%r422,%r423}, [%rd372];
+	// end inline asm
+	mov.b32 	%f1264, %r420;
+	mov.b32 	%f1265, %r421;
+	mov.b32 	%f1266, %r422;
+	mov.b32 	%f1267, %r423;
+	mul.f32 	%f1268, %f388, %f1264;
+	mul.f32 	%f1269, %f388, %f1265;
+	mul.f32 	%f1270, %f388, %f1266;
+	mul.f32 	%f1271, %f388, %f1267;
+	fma.rn.f32 	%f1871, %f1255, %f1871, %f1268;
+	fma.rn.f32 	%f1872, %f1255, %f1872, %f1269;
+	fma.rn.f32 	%f1873, %f1255, %f1873, %f1270;
+	fma.rn.f32 	%f1874, %f1255, %f1874, %f1271;
+	add.s64 	%rd376, %rd357, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd375, %rd376;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r424,%r425,%r426,%r427}, [%rd375];
+	// end inline asm
+	mov.b32 	%f1272, %r424;
+	mov.b32 	%f1273, %r425;
+	mov.b32 	%f1274, %r426;
+	mov.b32 	%f1275, %r427;
+	mul.f32 	%f1276, %f388, %f1272;
+	mul.f32 	%f1277, %f388, %f1273;
+	mul.f32 	%f1278, %f388, %f1274;
+	mul.f32 	%f1279, %f388, %f1275;
+	fma.rn.f32 	%f1875, %f1255, %f1875, %f1276;
+	fma.rn.f32 	%f1280, %f1255, %f1876, %f1277;
+	fma.rn.f32 	%f1281, %f1255, %f1877, %f1278;
+	fma.rn.f32 	%f1282, %f1255, %f1878, %f1279;
+	add.s64 	%rd379, %rd357, 112;
+	// begin inline asm
 	cvta.to.global.u64 %rd378, %rd379;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r406,%r407,%r408,%r409}, [%rd378];
-	// inline asm
-	mov.b32 	 %f1219, %r406;
-	mov.b32 	 %f1220, %r407;
-	mov.b32 	 %f1221, %r408;
-	mov.b32 	 %f1222, %r409;
-	add.s64 	%rd382, %rd391, 112;
-	// inline asm
-	cvta.to.global.u64 %rd381, %rd382;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r410,%r411,%r412,%r413}, [%rd381];
-	// inline asm
-	mov.b32 	 %f1223, %r410;
-	mov.b32 	 %f1224, %r411;
-	mov.b32 	 %f1225, %r412;
-	mov.b32 	 %f1226, %r413;
-	add.s64 	%rd385, %rd391, 128;
-	// inline asm
-	cvta.to.global.u64 %rd384, %rd385;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r414,%r415,%r416,%r417}, [%rd384];
-	// inline asm
-	mov.b32 	 %f1227, %r414;
-	mov.b32 	 %f1228, %r415;
-	mov.b32 	 %f1229, %r416;
-	mov.b32 	 %f1230, %r417;
-	add.s64 	%rd388, %rd391, 144;
-	// inline asm
-	cvta.to.global.u64 %rd387, %rd388;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r418,%r419,%r420,%r421}, [%rd387];
-	// inline asm
-	mov.f32 	%f1231, 0f3F800000;
-	sub.f32 	%f1232, %f1231, %f362;
-	mul.f32 	%f1233, %f362, %f1219;
-	mul.f32 	%f1234, %f362, %f1220;
-	mul.f32 	%f1235, %f362, %f1221;
-	mul.f32 	%f1236, %f362, %f1222;
-	fma.rn.f32 	%f1807, %f1232, %f1807, %f1233;
-	fma.rn.f32 	%f1808, %f1232, %f1808, %f1234;
-	fma.rn.f32 	%f1809, %f1232, %f1809, %f1235;
-	fma.rn.f32 	%f1810, %f1232, %f1810, %f1236;
-	mul.f32 	%f1237, %f362, %f1223;
-	mul.f32 	%f1238, %f362, %f1224;
-	mul.f32 	%f1239, %f362, %f1225;
-	mul.f32 	%f1240, %f362, %f1226;
-	fma.rn.f32 	%f1811, %f1232, %f1811, %f1237;
-	fma.rn.f32 	%f1812, %f1232, %f1812, %f1238;
-	fma.rn.f32 	%f1813, %f1232, %f1813, %f1239;
-	fma.rn.f32 	%f1814, %f1232, %f1814, %f1240;
-	mul.f32 	%f1241, %f362, %f1227;
-	mul.f32 	%f1242, %f362, %f1228;
-	mul.f32 	%f1243, %f362, %f1229;
-	mul.f32 	%f1244, %f362, %f1230;
-	fma.rn.f32 	%f1815, %f1232, %f1815, %f1241;
-	fma.rn.f32 	%f1245, %f1232, %f1816, %f1242;
-	fma.rn.f32 	%f1246, %f1232, %f1817, %f1243;
-	fma.rn.f32 	%f1247, %f1232, %f1818, %f1244;
-	mov.b32 	 %f1248, %r418;
-	mov.b32 	 %f1249, %r419;
-	mov.b32 	 %f1250, %r420;
-	mov.b32 	 %f1251, %r421;
-	mul.f32 	%f1252, %f362, %f1248;
-	mul.f32 	%f1253, %f362, %f1249;
-	mul.f32 	%f1254, %f362, %f1250;
-	mul.f32 	%f1255, %f362, %f1251;
-	fma.rn.f32 	%f1256, %f1232, %f1819, %f1252;
-	fma.rn.f32 	%f1820, %f1232, %f1820, %f1253;
-	fma.rn.f32 	%f1821, %f1232, %f1821, %f1254;
-	fma.rn.f32 	%f1822, %f1232, %f1822, %f1255;
-	mul.f32 	%f1257, %f1246, %f1246;
-	fma.rn.f32 	%f1258, %f1245, %f1245, %f1257;
-	fma.rn.f32 	%f1259, %f1247, %f1247, %f1258;
-	fma.rn.f32 	%f1260, %f1256, %f1256, %f1259;
-	sqrt.rn.f32 	%f1261, %f1260;
-	rcp.rn.f32 	%f1262, %f1261;
-	mul.f32 	%f1816, %f1245, %f1262;
-	mul.f32 	%f1817, %f1246, %f1262;
-	mul.f32 	%f1818, %f1247, %f1262;
-	mul.f32 	%f1819, %f1256, %f1262;
-
-BB6_62:
-	mul.f32 	%f1263, %f1817, %f1817;
-	fma.rn.f32 	%f1264, %f1816, %f1816, %f1263;
-	fma.rn.f32 	%f1265, %f1818, %f1818, %f1264;
-	fma.rn.f32 	%f1266, %f1819, %f1819, %f1265;
-	rcp.rn.f32 	%f1267, %f1266;
-	mul.f32 	%f1268, %f1816, %f1267;
-	mul.f32 	%f1269, %f1817, %f1267;
-	mul.f32 	%f1270, %f1818, %f1267;
-	mul.f32 	%f1271, %f1819, %f1267;
-	mul.f32 	%f1272, %f1816, %f1268;
-	mul.f32 	%f1273, %f1817, %f1269;
-	mul.f32 	%f1274, %f1818, %f1270;
-	mul.f32 	%f1275, %f1816, %f1269;
-	mul.f32 	%f1276, %f1818, %f1271;
-	mul.f32 	%f1277, %f1816, %f1270;
-	mul.f32 	%f1278, %f1817, %f1271;
-	mul.f32 	%f1279, %f1817, %f1270;
-	mul.f32 	%f1280, %f1816, %f1271;
-	sub.f32 	%f1281, %f1272, %f1273;
-	sub.f32 	%f1282, %f1281, %f1274;
-	fma.rn.f32 	%f1283, %f1819, %f1271, %f1282;
-	sub.f32 	%f1284, %f1275, %f1276;
-	add.f32 	%f1285, %f1284, %f1284;
-	add.f32 	%f1286, %f1277, %f1278;
-	add.f32 	%f1287, %f1286, %f1286;
-	add.f32 	%f1288, %f1275, %f1276;
-	add.f32 	%f1289, %f1288, %f1288;
-	sub.f32 	%f1290, %f1273, %f1272;
-	sub.f32 	%f1291, %f1290, %f1274;
-	fma.rn.f32 	%f1292, %f1819, %f1271, %f1291;
-	sub.f32 	%f1293, %f1279, %f1280;
-	add.f32 	%f1294, %f1293, %f1293;
-	sub.f32 	%f1295, %f1277, %f1278;
-	add.f32 	%f1296, %f1295, %f1295;
-	add.f32 	%f1297, %f1279, %f1280;
-	add.f32 	%f1298, %f1297, %f1297;
-	neg.f32 	%f1299, %f1272;
-	sub.f32 	%f1300, %f1299, %f1273;
-	add.f32 	%f1301, %f1274, %f1300;
-	fma.rn.f32 	%f1302, %f1819, %f1271, %f1301;
-	mul.f32 	%f1303, %f1810, %f1283;
-	fma.rn.f32 	%f1304, %f1813, %f1285, %f1303;
-	fma.rn.f32 	%f1305, %f1815, %f1287, %f1304;
-	sub.f32 	%f1834, %f1820, %f1305;
-	mul.f32 	%f1306, %f1813, %f1292;
-	fma.rn.f32 	%f1307, %f1810, %f1289, %f1306;
-	fma.rn.f32 	%f1308, %f1815, %f1294, %f1307;
-	sub.f32 	%f1830, %f1821, %f1308;
-	mul.f32 	%f1309, %f1813, %f1298;
-	fma.rn.f32 	%f1310, %f1810, %f1296, %f1309;
-	fma.rn.f32 	%f1311, %f1815, %f1302, %f1310;
-	sub.f32 	%f1826, %f1822, %f1311;
-	mul.f32 	%f1312, %f1809, %f1283;
-	fma.rn.f32 	%f1313, %f1812, %f1285, %f1312;
-	fma.rn.f32 	%f1833, %f1814, %f1287, %f1313;
-	mul.f32 	%f1314, %f1812, %f1292;
-	fma.rn.f32 	%f1315, %f1809, %f1289, %f1314;
-	fma.rn.f32 	%f1829, %f1814, %f1294, %f1315;
-	mul.f32 	%f1316, %f1812, %f1298;
-	fma.rn.f32 	%f1317, %f1809, %f1296, %f1316;
-	fma.rn.f32 	%f1825, %f1814, %f1302, %f1317;
-	mul.f32 	%f1318, %f1808, %f1283;
-	fma.rn.f32 	%f1832, %f1811, %f1285, %f1318;
-	mul.f32 	%f1319, %f1811, %f1292;
-	fma.rn.f32 	%f1828, %f1808, %f1289, %f1319;
-	mul.f32 	%f1320, %f1811, %f1298;
-	fma.rn.f32 	%f1824, %f1808, %f1296, %f1320;
-	mul.f32 	%f1831, %f1807, %f1283;
-	mul.f32 	%f1827, %f1807, %f1289;
-	mul.f32 	%f1823, %f1807, %f1296;
-	bra.uni 	BB6_65;
-
-BB6_55:
-	setp.ne.s32	%p29, %r333, 1;
-	mov.f32 	%f1824, %f1823;
-	mov.f32 	%f1826, %f1823;
-	mov.f32 	%f1827, %f1823;
-	mov.f32 	%f1828, %f1825;
-	mov.f32 	%f1829, %f1823;
-	mov.f32 	%f1830, %f1823;
-	mov.f32 	%f1831, %f1825;
-	mov.f32 	%f1832, %f1823;
-	mov.f32 	%f1833, %f1823;
-	mov.f32 	%f1834, %f1823;
-	@%p29 bra 	BB6_65;
-
-	// inline asm
-	call (%rd320), _optix_get_static_transform_from_handle, (%rd318);
-	// inline asm
-	add.s64 	%rd668, %rd320, 16;
-
-BB6_58:
-	// inline asm
-	cvta.to.global.u64 %rd324, %rd668;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r335,%r336,%r337,%r338}, [%rd324];
-	// inline asm
-	mov.b32 	 %f1831, %r335;
-	mov.b32 	 %f1832, %r336;
-	mov.b32 	 %f1833, %r337;
-	mov.b32 	 %f1834, %r338;
-	add.s64 	%rd328, %rd668, 16;
-	// inline asm
-	cvta.to.global.u64 %rd327, %rd328;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r339,%r340,%r341,%r342}, [%rd327];
-	// inline asm
-	mov.b32 	 %f1827, %r339;
-	mov.b32 	 %f1828, %r340;
-	mov.b32 	 %f1829, %r341;
-	mov.b32 	 %f1830, %r342;
-	add.s64 	%rd331, %rd668, 32;
-	// inline asm
-	cvta.to.global.u64 %rd330, %rd331;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r343,%r344,%r345,%r346}, [%rd330];
-	// inline asm
-	mov.b32 	 %f1823, %r343;
-	mov.b32 	 %f1824, %r344;
-	mov.b32 	 %f1825, %r345;
-	mov.b32 	 %f1826, %r346;
-
-BB6_65:
-	add.s32 	%r16, %r636, 1;
-	setp.eq.s32	%p33, %r16, %r28;
-	@%p33 bra 	BB6_66;
-	bra.uni 	BB6_67;
-
-BB6_66:
-	mov.f32 	%f1806, %f1823;
-	mov.f32 	%f1805, %f1824;
-	mov.f32 	%f1804, %f1825;
-	mov.f32 	%f1803, %f1826;
-	mov.f32 	%f1802, %f1827;
-	mov.f32 	%f1801, %f1828;
-	mov.f32 	%f1800, %f1829;
-	mov.f32 	%f1799, %f1830;
-	mov.f32 	%f1798, %f1831;
-	mov.f32 	%f1797, %f1832;
-	mov.f32 	%f1796, %f1833;
-	mov.f32 	%f1795, %f1834;
-	bra.uni 	BB6_68;
-
-BB6_67:
-	mul.f32 	%f1358, %f1802, %f1832;
-	fma.rn.f32 	%f1359, %f1798, %f1831, %f1358;
-	fma.rn.f32 	%f452, %f1806, %f1833, %f1359;
-	mul.f32 	%f1360, %f1801, %f1832;
-	fma.rn.f32 	%f1361, %f1797, %f1831, %f1360;
-	fma.rn.f32 	%f453, %f1805, %f1833, %f1361;
-	mul.f32 	%f1362, %f1800, %f1832;
-	fma.rn.f32 	%f1363, %f1796, %f1831, %f1362;
-	fma.rn.f32 	%f454, %f1804, %f1833, %f1363;
-	mul.f32 	%f1364, %f1799, %f1832;
-	fma.rn.f32 	%f1365, %f1795, %f1831, %f1364;
-	fma.rn.f32 	%f1366, %f1803, %f1833, %f1365;
-	add.f32 	%f455, %f1834, %f1366;
-	mul.f32 	%f1367, %f1802, %f1828;
-	fma.rn.f32 	%f1368, %f1798, %f1827, %f1367;
-	fma.rn.f32 	%f456, %f1806, %f1829, %f1368;
-	mul.f32 	%f1369, %f1801, %f1828;
-	fma.rn.f32 	%f1370, %f1797, %f1827, %f1369;
-	fma.rn.f32 	%f457, %f1805, %f1829, %f1370;
-	mul.f32 	%f1371, %f1800, %f1828;
-	fma.rn.f32 	%f1372, %f1796, %f1827, %f1371;
-	fma.rn.f32 	%f458, %f1804, %f1829, %f1372;
-	mul.f32 	%f1373, %f1799, %f1828;
-	fma.rn.f32 	%f1374, %f1795, %f1827, %f1373;
-	fma.rn.f32 	%f1375, %f1803, %f1829, %f1374;
-	add.f32 	%f459, %f1830, %f1375;
-	mul.f32 	%f1376, %f1802, %f1824;
-	fma.rn.f32 	%f1377, %f1798, %f1823, %f1376;
-	fma.rn.f32 	%f1806, %f1806, %f1825, %f1377;
-	mul.f32 	%f1378, %f1801, %f1824;
-	fma.rn.f32 	%f1379, %f1797, %f1823, %f1378;
-	fma.rn.f32 	%f1805, %f1805, %f1825, %f1379;
-	mul.f32 	%f1380, %f1800, %f1824;
-	fma.rn.f32 	%f1381, %f1796, %f1823, %f1380;
-	fma.rn.f32 	%f1804, %f1804, %f1825, %f1381;
-	mul.f32 	%f1382, %f1799, %f1824;
-	fma.rn.f32 	%f1383, %f1795, %f1823, %f1382;
-	fma.rn.f32 	%f1384, %f1803, %f1825, %f1383;
-	add.f32 	%f1803, %f1826, %f1384;
-	mov.f32 	%f1802, %f456;
-	mov.f32 	%f1801, %f457;
-	mov.f32 	%f1800, %f458;
-	mov.f32 	%f1799, %f459;
-	mov.f32 	%f1798, %f452;
-	mov.f32 	%f1797, %f453;
-	mov.f32 	%f1796, %f454;
-	mov.f32 	%f1795, %f455;
-
-BB6_68:
-	add.s32 	%r636, %r16, -2;
-	setp.gt.s32	%p34, %r636, -1;
-	@%p34 bra 	BB6_53;
-
-BB6_69:
-	mov.u32 	%r637, 0;
-	mov.f32 	%f1859, %f1860;
-	mov.f32 	%f1864, %f1860;
-	mov.f32 	%f1863, %f1861;
-	mov.f32 	%f1862, %f1860;
-	mov.f32 	%f1867, %f1860;
-	mov.f32 	%f1866, %f1860;
-	mov.f32 	%f1865, %f1861;
-	@%p2 bra 	BB6_87;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r428,%r429,%r430,%r431}, [%rd378];
+	// end inline asm
+	mov.b32 	%f1283, %r428;
+	mov.b32 	%f1284, %r429;
+	mov.b32 	%f1285, %r430;
+	mov.b32 	%f1286, %r431;
+	mul.f32 	%f1287, %f388, %f1283;
+	mul.f32 	%f1288, %f388, %f1284;
+	mul.f32 	%f1289, %f388, %f1285;
+	mul.f32 	%f1290, %f388, %f1286;
+	fma.rn.f32 	%f1291, %f1255, %f1879, %f1287;
+	fma.rn.f32 	%f1880, %f1255, %f1880, %f1288;
+	fma.rn.f32 	%f1881, %f1255, %f1881, %f1289;
+	fma.rn.f32 	%f1882, %f1255, %f1882, %f1290;
+	mul.f32 	%f1292, %f1281, %f1281;
+	fma.rn.f32 	%f1293, %f1280, %f1280, %f1292;
+	fma.rn.f32 	%f1294, %f1282, %f1282, %f1293;
+	fma.rn.f32 	%f1295, %f1291, %f1291, %f1294;
+	sqrt.rn.f32 	%f1296, %f1295;
+	rcp.rn.f32 	%f1297, %f1296;
+	mul.f32 	%f1876, %f1280, %f1297;
+	mul.f32 	%f1877, %f1281, %f1297;
+	mul.f32 	%f1878, %f1282, %f1297;
+	mul.f32 	%f1879, %f1297, %f1291;
+
+$L__BB6_63:
+	mul.f32 	%f1298, %f1877, %f1877;
+	fma.rn.f32 	%f1299, %f1876, %f1876, %f1298;
+	fma.rn.f32 	%f1300, %f1878, %f1878, %f1299;
+	fma.rn.f32 	%f1301, %f1879, %f1879, %f1300;
+	rcp.rn.f32 	%f1302, %f1301;
+	mul.f32 	%f1303, %f1876, %f1302;
+	mul.f32 	%f1304, %f1877, %f1302;
+	mul.f32 	%f1305, %f1878, %f1302;
+	mul.f32 	%f1306, %f1879, %f1302;
+	mul.f32 	%f1307, %f1876, %f1303;
+	mul.f32 	%f1308, %f1877, %f1304;
+	mul.f32 	%f1309, %f1878, %f1305;
+	mul.f32 	%f1310, %f1876, %f1304;
+	mul.f32 	%f1311, %f1878, %f1306;
+	mul.f32 	%f1312, %f1876, %f1305;
+	mul.f32 	%f1313, %f1877, %f1306;
+	mul.f32 	%f1314, %f1877, %f1305;
+	mul.f32 	%f1315, %f1876, %f1306;
+	sub.f32 	%f1316, %f1307, %f1308;
+	sub.f32 	%f1317, %f1316, %f1309;
+	fma.rn.f32 	%f1318, %f1879, %f1306, %f1317;
+	sub.f32 	%f1319, %f1310, %f1311;
+	add.f32 	%f1320, %f1319, %f1319;
+	add.f32 	%f1321, %f1312, %f1313;
+	add.f32 	%f1322, %f1321, %f1321;
+	add.f32 	%f1323, %f1310, %f1311;
+	add.f32 	%f1324, %f1323, %f1323;
+	sub.f32 	%f1325, %f1308, %f1307;
+	sub.f32 	%f1326, %f1325, %f1309;
+	fma.rn.f32 	%f1327, %f1879, %f1306, %f1326;
+	sub.f32 	%f1328, %f1314, %f1315;
+	add.f32 	%f1329, %f1328, %f1328;
+	sub.f32 	%f1330, %f1312, %f1313;
+	add.f32 	%f1331, %f1330, %f1330;
+	add.f32 	%f1332, %f1314, %f1315;
+	add.f32 	%f1333, %f1332, %f1332;
+	neg.f32 	%f1334, %f1307;
+	sub.f32 	%f1335, %f1334, %f1308;
+	add.f32 	%f1336, %f1309, %f1335;
+	fma.rn.f32 	%f1337, %f1879, %f1306, %f1336;
+	mul.f32 	%f1338, %f1870, %f1318;
+	fma.rn.f32 	%f1339, %f1873, %f1320, %f1338;
+	fma.rn.f32 	%f1340, %f1875, %f1322, %f1339;
+	sub.f32 	%f1894, %f1880, %f1340;
+	mul.f32 	%f1341, %f1873, %f1327;
+	fma.rn.f32 	%f1342, %f1870, %f1324, %f1341;
+	fma.rn.f32 	%f1343, %f1875, %f1329, %f1342;
+	sub.f32 	%f1890, %f1881, %f1343;
+	mul.f32 	%f1344, %f1873, %f1333;
+	fma.rn.f32 	%f1345, %f1870, %f1331, %f1344;
+	fma.rn.f32 	%f1346, %f1875, %f1337, %f1345;
+	sub.f32 	%f1886, %f1882, %f1346;
+	mul.f32 	%f1347, %f1869, %f1318;
+	fma.rn.f32 	%f1348, %f1872, %f1320, %f1347;
+	fma.rn.f32 	%f1893, %f1874, %f1322, %f1348;
+	mul.f32 	%f1349, %f1872, %f1327;
+	fma.rn.f32 	%f1350, %f1869, %f1324, %f1349;
+	fma.rn.f32 	%f1889, %f1874, %f1329, %f1350;
+	mul.f32 	%f1351, %f1872, %f1333;
+	fma.rn.f32 	%f1352, %f1869, %f1331, %f1351;
+	fma.rn.f32 	%f1885, %f1874, %f1337, %f1352;
+	mul.f32 	%f1353, %f1868, %f1318;
+	fma.rn.f32 	%f1892, %f1871, %f1320, %f1353;
+	mul.f32 	%f1354, %f1871, %f1327;
+	fma.rn.f32 	%f1888, %f1868, %f1324, %f1354;
+	mul.f32 	%f1355, %f1871, %f1333;
+	fma.rn.f32 	%f1884, %f1868, %f1331, %f1355;
+	mul.f32 	%f1891, %f1867, %f1318;
+	mul.f32 	%f1887, %f1867, %f1324;
+	mul.f32 	%f1883, %f1867, %f1331;
+	bra.uni 	$L__BB6_66;
+
+$L__BB6_58:
+	// begin inline asm
+	call (%rd654), _optix_get_instance_transform_from_handle, (%rd309);
+	// end inline asm
+
+$L__BB6_59:
+	// begin inline asm
+	cvta.to.global.u64 %rd315, %rd654;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r345,%r346,%r347,%r348}, [%rd315];
+	// end inline asm
+	mov.b32 	%f1891, %r345;
+	mov.b32 	%f1892, %r346;
+	mov.b32 	%f1893, %r347;
+	mov.b32 	%f1894, %r348;
+	add.s64 	%rd319, %rd654, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd318, %rd319;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r349,%r350,%r351,%r352}, [%rd318];
+	// end inline asm
+	mov.b32 	%f1887, %r349;
+	mov.b32 	%f1888, %r350;
+	mov.b32 	%f1889, %r351;
+	mov.b32 	%f1890, %r352;
+	add.s64 	%rd322, %rd654, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd321, %rd322;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r353,%r354,%r355,%r356}, [%rd321];
+	// end inline asm
+	mov.b32 	%f1883, %r353;
+	mov.b32 	%f1884, %r354;
+	mov.b32 	%f1885, %r355;
+	mov.b32 	%f1886, %r356;
+
+$L__BB6_66:
+	setp.eq.s32 	%p35, %r648, 1;
+	@%p35 bra 	$L__BB6_68;
+
+	mul.f32 	%f1393, %f1862, %f1892;
+	fma.rn.f32 	%f1394, %f1858, %f1891, %f1393;
+	fma.rn.f32 	%f486, %f1866, %f1893, %f1394;
+	mul.f32 	%f1395, %f1861, %f1892;
+	fma.rn.f32 	%f1396, %f1857, %f1891, %f1395;
+	fma.rn.f32 	%f487, %f1865, %f1893, %f1396;
+	mul.f32 	%f1397, %f1860, %f1892;
+	fma.rn.f32 	%f1398, %f1856, %f1891, %f1397;
+	fma.rn.f32 	%f488, %f1864, %f1893, %f1398;
+	mul.f32 	%f1399, %f1859, %f1892;
+	fma.rn.f32 	%f1400, %f1855, %f1891, %f1399;
+	fma.rn.f32 	%f1401, %f1863, %f1893, %f1400;
+	add.f32 	%f1894, %f1894, %f1401;
+	mul.f32 	%f1402, %f1862, %f1888;
+	fma.rn.f32 	%f1403, %f1858, %f1887, %f1402;
+	fma.rn.f32 	%f490, %f1866, %f1889, %f1403;
+	mul.f32 	%f1404, %f1861, %f1888;
+	fma.rn.f32 	%f1405, %f1857, %f1887, %f1404;
+	fma.rn.f32 	%f491, %f1865, %f1889, %f1405;
+	mul.f32 	%f1406, %f1860, %f1888;
+	fma.rn.f32 	%f1407, %f1856, %f1887, %f1406;
+	fma.rn.f32 	%f492, %f1864, %f1889, %f1407;
+	mul.f32 	%f1408, %f1859, %f1888;
+	fma.rn.f32 	%f1409, %f1855, %f1887, %f1408;
+	fma.rn.f32 	%f1410, %f1863, %f1889, %f1409;
+	add.f32 	%f1890, %f1890, %f1410;
+	mul.f32 	%f1411, %f1862, %f1884;
+	fma.rn.f32 	%f1412, %f1858, %f1883, %f1411;
+	fma.rn.f32 	%f494, %f1866, %f1885, %f1412;
+	mul.f32 	%f1413, %f1861, %f1884;
+	fma.rn.f32 	%f1414, %f1857, %f1883, %f1413;
+	fma.rn.f32 	%f495, %f1865, %f1885, %f1414;
+	mul.f32 	%f1415, %f1860, %f1884;
+	fma.rn.f32 	%f1416, %f1856, %f1883, %f1415;
+	fma.rn.f32 	%f496, %f1864, %f1885, %f1416;
+	mul.f32 	%f1417, %f1859, %f1884;
+	fma.rn.f32 	%f1418, %f1855, %f1883, %f1417;
+	fma.rn.f32 	%f1419, %f1863, %f1885, %f1418;
+	add.f32 	%f1886, %f1886, %f1419;
+	mov.f32 	%f1883, %f494;
+	mov.f32 	%f1884, %f495;
+	mov.f32 	%f1885, %f496;
+	mov.f32 	%f1887, %f490;
+	mov.f32 	%f1888, %f491;
+	mov.f32 	%f1889, %f492;
+	mov.f32 	%f1891, %f486;
+	mov.f32 	%f1892, %f487;
+	mov.f32 	%f1893, %f488;
+
+$L__BB6_68:
+	add.s32 	%r648, %r648, -1;
+	add.s32 	%r647, %r647, -1;
+	setp.gt.s32 	%p36, %r647, 1;
+	mov.f32 	%f1855, %f1894;
+	mov.f32 	%f1856, %f1893;
+	mov.f32 	%f1857, %f1892;
+	mov.f32 	%f1858, %f1891;
+	mov.f32 	%f1859, %f1890;
+	mov.f32 	%f1860, %f1889;
+	mov.f32 	%f1861, %f1888;
+	mov.f32 	%f1862, %f1887;
+	mov.f32 	%f1863, %f1886;
+	mov.f32 	%f1864, %f1885;
+	mov.f32 	%f1865, %f1884;
+	mov.f32 	%f1866, %f1883;
+	@%p36 bra 	$L__BB6_54;
+
+$L__BB6_69:
+	// begin inline asm
+	call (%r491), _optix_get_transform_list_size, ();
+	// end inline asm
+	setp.eq.s32 	%p37, %r491, 0;
+	mov.f32 	%f1955, %f1954;
+	mov.f32 	%f1950, %f1954;
+	mov.f32 	%f1951, %f1953;
+	mov.f32 	%f1952, %f1954;
+	mov.f32 	%f1947, %f1954;
+	mov.f32 	%f1948, %f1954;
+	mov.f32 	%f1949, %f1953;
+	@%p37 bra 	$L__BB6_88;
+
+	// begin inline asm
+	call (%r492), _optix_get_transform_list_size, ();
+	// end inline asm
+	// begin inline asm
+	call (%f1429), _optix_get_ray_time, ();
+	// end inline asm
+	setp.eq.s32 	%p38, %r492, 0;
+	@%p38 bra 	$L__BB6_88;
+
+	mov.u32 	%r649, 0;
 
-BB6_70:
+$L__BB6_72:
 	.pragma "nounroll";
-	// inline asm
-	call (%rd439), _optix_get_transform_list_handle, (%r637);
-	// inline asm
-	// inline asm
-	call (%r483), _optix_get_transform_type_from_handle, (%rd439);
-	// inline asm
-	and.b32  	%r484, %r483, -2;
-	setp.eq.s32	%p36, %r484, 2;
-	@%p36 bra 	BB6_76;
-	bra.uni 	BB6_71;
-
-BB6_76:
-	setp.eq.s32	%p39, %r483, 2;
-	@%p39 bra 	BB6_80;
-	bra.uni 	BB6_77;
-
-BB6_80:
-	// inline asm
-	call (%rd513), _optix_get_matrix_motion_transform_from_handle, (%rd439);
-	// inline asm
-	// inline asm
-	cvta.to.global.u64 %rd515, %rd513;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r572,%r573,%r574,%r575}, [%rd515];
-	// inline asm
-	mov.b32	{%rs16, %rs17}, %r574;
-	add.s64 	%rd519, %rd513, 16;
-	// inline asm
-	cvta.to.global.u64 %rd518, %rd519;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r576,%r577,%r578,%r579}, [%rd518];
-	// inline asm
-	add.s64 	%rd522, %rd513, 32;
-	// inline asm
-	cvta.to.global.u64 %rd521, %rd522;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r580,%r581,%r582,%r583}, [%rd521];
-	// inline asm
-	add.s64 	%rd525, %rd513, 48;
-	// inline asm
-	cvta.to.global.u64 %rd524, %rd525;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r584,%r585,%r586,%r587}, [%rd524];
-	// inline asm
-	add.s64 	%rd528, %rd513, 64;
-	// inline asm
-	cvta.to.global.u64 %rd527, %rd528;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r588,%r589,%r590,%r591}, [%rd527];
-	// inline asm
-	add.s64 	%rd531, %rd513, 80;
-	// inline asm
-	cvta.to.global.u64 %rd530, %rd531;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r592,%r593,%r594,%r595}, [%rd530];
-	// inline asm
-	add.s64 	%rd534, %rd513, 96;
-	// inline asm
-	cvta.to.global.u64 %rd533, %rd534;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r596,%r597,%r598,%r599}, [%rd533];
-	// inline asm
-	add.s64 	%rd537, %rd513, 112;
-	// inline asm
+	// begin inline asm
+	call (%rd428), _optix_get_transform_list_handle, (%r649);
+	// end inline asm
+	// begin inline asm
+	call (%r495), _optix_get_transform_type_from_handle, (%rd428);
+	// end inline asm
+	or.b32  	%r496, %r495, 1;
+	setp.eq.s32 	%p39, %r496, 3;
+	@%p39 bra 	$L__BB6_78;
+	bra.uni 	$L__BB6_73;
+
+$L__BB6_78:
+	setp.eq.s32 	%p42, %r495, 2;
+	@%p42 bra 	$L__BB6_82;
+	bra.uni 	$L__BB6_79;
+
+$L__BB6_82:
+	// begin inline asm
+	call (%rd500), _optix_get_matrix_motion_transform_from_handle, (%rd428);
+	// end inline asm
+	// begin inline asm
+	cvta.to.global.u64 %rd502, %rd500;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r584,%r585,%r586,%r587}, [%rd502];
+	// end inline asm
+	add.s64 	%rd506, %rd500, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd505, %rd506;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r588,%r589,%r590,%r591}, [%rd505];
+	// end inline asm
+	add.s64 	%rd509, %rd500, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd508, %rd509;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r592,%r593,%r594,%r595}, [%rd508];
+	// end inline asm
+	add.s64 	%rd512, %rd500, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd511, %rd512;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r596,%r597,%r598,%r599}, [%rd511];
+	// end inline asm
+	add.s64 	%rd515, %rd500, 64;
+	// begin inline asm
+	cvta.to.global.u64 %rd514, %rd515;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r600,%r601,%r602,%r603}, [%rd514];
+	// end inline asm
+	add.s64 	%rd518, %rd500, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd517, %rd518;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r604,%r605,%r606,%r607}, [%rd517];
+	// end inline asm
+	add.s64 	%rd521, %rd500, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd520, %rd521;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r608,%r609,%r610,%r611}, [%rd520];
+	// end inline asm
+	add.s64 	%rd524, %rd500, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd523, %rd524;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r612,%r613,%r614,%r615}, [%rd523];
+	// end inline asm
+	mov.b32 	%f1533, %r587;
+	mov.b32 	%f1534, %r588;
+	and.b32  	%r628, %r586, 65535;
+	add.s32 	%r629, %r628, -1;
+	cvt.rn.f32.s32 	%f1535, %r629;
+	sub.f32 	%f1536, %f1429, %f1533;
+	mul.f32 	%f1537, %f1536, %f1535;
+	sub.f32 	%f1538, %f1534, %f1533;
+	div.rn.f32 	%f1539, %f1537, %f1538;
+	min.f32 	%f1540, %f1535, %f1539;
+	mov.f32 	%f1541, 0f00000000;
+	max.f32 	%f1542, %f1541, %f1540;
+	cvt.rmi.f32.f32 	%f1543, %f1542;
+	sub.f32 	%f581, %f1542, %f1543;
+	cvt.rzi.s32.f32 	%r630, %f1543;
+	cvt.s64.s32 	%rd37, %r630;
+	mul.wide.s32 	%rd535, %r630, 48;
+	add.s64 	%rd527, %rd509, %rd535;
+	// begin inline asm
+	cvta.to.global.u64 %rd526, %rd527;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r616,%r617,%r618,%r619}, [%rd526];
+	// end inline asm
+	mov.b32 	%f1944, %r616;
+	mov.b32 	%f1945, %r617;
+	mov.b32 	%f1946, %r618;
+	add.s64 	%rd530, %rd527, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd529, %rd530;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r620,%r621,%r622,%r623}, [%rd529];
+	// end inline asm
+	mov.b32 	%f1941, %r620;
+	mov.b32 	%f1942, %r621;
+	mov.b32 	%f1943, %r622;
+	add.s64 	%rd533, %rd527, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd532, %rd533;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r624,%r625,%r626,%r627}, [%rd532];
+	// end inline asm
+	mov.b32 	%f1938, %r624;
+	mov.b32 	%f1939, %r625;
+	mov.b32 	%f1940, %r626;
+	setp.leu.f32 	%p44, %f581, 0f00000000;
+	@%p44 bra 	$L__BB6_84;
+
+	mov.f32 	%f1544, 0f3F800000;
+	sub.f32 	%f1545, %f1544, %f581;
+	mul.lo.s64 	%rd545, %rd37, 48;
+	add.s64 	%rd546, %rd500, %rd545;
+	add.s64 	%rd537, %rd546, 80;
+	// begin inline asm
 	cvta.to.global.u64 %rd536, %rd537;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r600,%r601,%r602,%r603}, [%rd536];
-	// inline asm
-	mov.b32 	 %f1496, %r575;
-	mov.b32 	 %f1497, %r576;
-	cvt.u32.u16	%r616, %rs16;
-	add.s32 	%r617, %r616, -1;
-	cvt.rn.f32.s32	%f1498, %r617;
-	sub.f32 	%f1499, %f924, %f1496;
-	mul.f32 	%f1500, %f1499, %f1498;
-	sub.f32 	%f1501, %f1497, %f1496;
-	div.rn.f32 	%f1502, %f1500, %f1501;
-	min.f32 	%f1503, %f1498, %f1502;
-	mov.f32 	%f1504, 0f00000000;
-	max.f32 	%f1505, %f1504, %f1503;
-	cvt.rmi.f32.f32	%f1506, %f1505;
-	cvt.rzi.s32.f32	%r618, %f1506;
-	cvt.s64.s32	%rd42, %r618;
-	mul.wide.s32 	%rd548, %r618, 48;
-	add.s64 	%rd540, %rd522, %rd548;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r631,%r632,%r633,%r634}, [%rd536];
+	// end inline asm
+	mov.b32 	%f1546, %r631;
+	mov.b32 	%f1547, %r632;
+	mov.b32 	%f1548, %r633;
+	mul.f32 	%f1549, %f581, %f1546;
+	mul.f32 	%f1550, %f581, %f1547;
+	mul.f32 	%f1551, %f581, %f1548;
+	fma.rn.f32 	%f1944, %f1545, %f1944, %f1549;
+	fma.rn.f32 	%f1945, %f1545, %f1945, %f1550;
+	fma.rn.f32 	%f1946, %f1545, %f1946, %f1551;
+	add.s64 	%rd540, %rd546, 96;
+	// begin inline asm
 	cvta.to.global.u64 %rd539, %rd540;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r604,%r605,%r606,%r607}, [%rd539];
-	// inline asm
-	mov.b32 	 %f1884, %r604;
-	mov.b32 	 %f1885, %r605;
-	mov.b32 	 %f1886, %r606;
-	add.s64 	%rd543, %rd540, 16;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r635,%r636,%r637,%r638}, [%rd539];
+	// end inline asm
+	mov.b32 	%f1552, %r635;
+	mov.b32 	%f1553, %r636;
+	mov.b32 	%f1554, %r637;
+	mul.f32 	%f1555, %f581, %f1552;
+	mul.f32 	%f1556, %f581, %f1553;
+	mul.f32 	%f1557, %f581, %f1554;
+	fma.rn.f32 	%f1941, %f1545, %f1941, %f1555;
+	fma.rn.f32 	%f1942, %f1545, %f1942, %f1556;
+	fma.rn.f32 	%f1943, %f1545, %f1943, %f1557;
+	add.s64 	%rd543, %rd546, 112;
+	// begin inline asm
 	cvta.to.global.u64 %rd542, %rd543;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r608,%r609,%r610,%r611}, [%rd542];
-	// inline asm
-	mov.b32 	 %f1881, %r608;
-	mov.b32 	 %f1882, %r609;
-	mov.b32 	 %f1883, %r610;
-	add.s64 	%rd546, %rd540, 32;
-	// inline asm
-	cvta.to.global.u64 %rd545, %rd546;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r612,%r613,%r614,%r615}, [%rd545];
-	// inline asm
-	sub.f32 	%f552, %f1505, %f1506;
-	mov.b32 	 %f1878, %r612;
-	mov.b32 	 %f1879, %r613;
-	mov.b32 	 %f1880, %r614;
-	setp.leu.f32	%p41, %f552, 0f00000000;
-	@%p41 bra 	BB6_82;
-
-	mul.lo.s64 	%rd558, %rd42, 48;
-	add.s64 	%rd559, %rd513, %rd558;
-	add.s64 	%rd550, %rd559, 80;
-	// inline asm
-	cvta.to.global.u64 %rd549, %rd550;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r619,%r620,%r621,%r622}, [%rd549];
-	// inline asm
-	mov.b32 	 %f1507, %r619;
-	mov.b32 	 %f1508, %r620;
-	mov.b32 	 %f1509, %r621;
-	add.s64 	%rd553, %rd559, 96;
-	// inline asm
-	cvta.to.global.u64 %rd552, %rd553;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r623,%r624,%r625,%r626}, [%rd552];
-	// inline asm
-	mov.b32 	 %f1510, %r623;
-	mov.b32 	 %f1511, %r624;
-	mov.b32 	 %f1512, %r625;
-	add.s64 	%rd556, %rd559, 112;
-	// inline asm
-	cvta.to.global.u64 %rd555, %rd556;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r627,%r628,%r629,%r630}, [%rd555];
-	// inline asm
-	mov.f32 	%f1513, 0f3F800000;
-	sub.f32 	%f1514, %f1513, %f552;
-	mul.f32 	%f1515, %f552, %f1507;
-	mul.f32 	%f1516, %f552, %f1508;
-	mul.f32 	%f1517, %f552, %f1509;
-	fma.rn.f32 	%f1884, %f1514, %f1884, %f1515;
-	fma.rn.f32 	%f1885, %f1514, %f1885, %f1516;
-	fma.rn.f32 	%f1886, %f1514, %f1886, %f1517;
-	mul.f32 	%f1518, %f552, %f1510;
-	mul.f32 	%f1519, %f552, %f1511;
-	mul.f32 	%f1520, %f552, %f1512;
-	fma.rn.f32 	%f1881, %f1514, %f1881, %f1518;
-	fma.rn.f32 	%f1882, %f1514, %f1882, %f1519;
-	fma.rn.f32 	%f1883, %f1514, %f1883, %f1520;
-	mov.b32 	 %f1521, %r627;
-	mov.b32 	 %f1522, %r628;
-	mov.b32 	 %f1523, %r629;
-	mul.f32 	%f1524, %f552, %f1521;
-	mul.f32 	%f1525, %f552, %f1522;
-	mul.f32 	%f1526, %f552, %f1523;
-	fma.rn.f32 	%f1878, %f1514, %f1878, %f1524;
-	fma.rn.f32 	%f1879, %f1514, %f1879, %f1525;
-	fma.rn.f32 	%f1880, %f1514, %f1880, %f1526;
-	bra.uni 	BB6_82;
-
-BB6_71:
-	mov.f32 	%f1887, 0f00000000;
-	mov.f32 	%f1889, 0f3F800000;
-	setp.eq.s32	%p37, %r483, 4;
-	@%p37 bra 	BB6_74;
-	bra.uni 	BB6_72;
-
-BB6_74:
-	// inline asm
-	call (%rd669), _optix_get_instance_inverse_transform_from_handle, (%rd439);
-	// inline asm
-	bra.uni 	BB6_75;
-
-BB6_77:
-	// inline asm
-	call (%rd454), _optix_get_srt_motion_transform_from_handle, (%rd439);
-	// inline asm
-	// inline asm
-	cvta.to.global.u64 %rd456, %rd454;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r497,%r498,%r499,%r500}, [%rd456];
-	// inline asm
-	mov.b32	{%rs14, %rs15}, %r499;
-	add.s64 	%rd460, %rd454, 16;
-	// inline asm
-	cvta.to.global.u64 %rd459, %rd460;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r501,%r502,%r503,%r504}, [%rd459];
-	// inline asm
-	add.s64 	%rd463, %rd454, 32;
-	// inline asm
-	cvta.to.global.u64 %rd462, %rd463;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r505,%r506,%r507,%r508}, [%rd462];
-	// inline asm
-	add.s64 	%rd466, %rd454, 48;
-	// inline asm
-	cvta.to.global.u64 %rd465, %rd466;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r509,%r510,%r511,%r512}, [%rd465];
-	// inline asm
-	add.s64 	%rd469, %rd454, 64;
-	// inline asm
-	cvta.to.global.u64 %rd468, %rd469;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r513,%r514,%r515,%r516}, [%rd468];
-	// inline asm
-	add.s64 	%rd472, %rd454, 80;
-	// inline asm
-	cvta.to.global.u64 %rd471, %rd472;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r517,%r518,%r519,%r520}, [%rd471];
-	// inline asm
-	add.s64 	%rd475, %rd454, 96;
-	// inline asm
-	cvta.to.global.u64 %rd474, %rd475;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r521,%r522,%r523,%r524}, [%rd474];
-	// inline asm
-	add.s64 	%rd478, %rd454, 112;
-	// inline asm
-	cvta.to.global.u64 %rd477, %rd478;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r525,%r526,%r527,%r528}, [%rd477];
-	// inline asm
-	add.s64 	%rd481, %rd454, 128;
-	// inline asm
-	cvta.to.global.u64 %rd480, %rd481;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r529,%r530,%r531,%r532}, [%rd480];
-	// inline asm
-	add.s64 	%rd484, %rd454, 144;
-	// inline asm
-	cvta.to.global.u64 %rd483, %rd484;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r533,%r534,%r535,%r536}, [%rd483];
-	// inline asm
-	mov.b32 	 %f1404, %r500;
-	mov.b32 	 %f1405, %r501;
-	cvt.u32.u16	%r553, %rs14;
-	add.s32 	%r554, %r553, -1;
-	cvt.rn.f32.s32	%f1406, %r554;
-	sub.f32 	%f1407, %f924, %f1404;
-	mul.f32 	%f1408, %f1407, %f1406;
-	sub.f32 	%f1409, %f1405, %f1404;
-	div.rn.f32 	%f1410, %f1408, %f1409;
-	min.f32 	%f1411, %f1406, %f1410;
-	mov.f32 	%f1412, 0f00000000;
-	max.f32 	%f1413, %f1412, %f1411;
-	cvt.rmi.f32.f32	%f1414, %f1413;
-	cvt.rzi.s32.f32	%r555, %f1414;
-	cvt.s64.s32	%rd40, %r555;
-	mul.wide.s32 	%rd498, %r555, 64;
-	add.s64 	%rd487, %rd463, %rd498;
-	// inline asm
-	cvta.to.global.u64 %rd486, %rd487;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r537,%r538,%r539,%r540}, [%rd486];
-	// inline asm
-	mov.b32 	 %f1868, %r537;
-	mov.b32 	 %f1869, %r538;
-	mov.b32 	 %f1870, %r539;
-	add.s64 	%rd490, %rd487, 16;
-	// inline asm
-	cvta.to.global.u64 %rd489, %rd490;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r541,%r542,%r543,%r544}, [%rd489];
-	// inline asm
-	mov.b32 	 %f1871, %r541;
-	mov.b32 	 %f1872, %r542;
-	mov.b32 	 %f1873, %r544;
-	add.s64 	%rd493, %rd487, 32;
-	// inline asm
-	cvta.to.global.u64 %rd492, %rd493;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r545,%r546,%r547,%r548}, [%rd492];
-	// inline asm
-	sub.f32 	%f512, %f1413, %f1414;
-	mov.b32 	 %f1874, %r546;
-	mov.b32 	 %f1875, %r547;
-	mov.b32 	 %f1876, %r548;
-	add.s64 	%rd496, %rd487, 48;
-	// inline asm
-	cvta.to.global.u64 %rd495, %rd496;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r549,%r550,%r551,%r552}, [%rd495];
-	// inline asm
-	mov.b32 	 %f1877, %r549;
-	setp.leu.f32	%p40, %f512, 0f00000000;
-	@%p40 bra 	BB6_79;
-
-	shl.b64 	%rd511, %rd40, 6;
-	add.s64 	%rd512, %rd511, %rd454;
-	add.s64 	%rd500, %rd512, 96;
-	// inline asm
-	cvta.to.global.u64 %rd499, %rd500;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r556,%r557,%r558,%r559}, [%rd499];
-	// inline asm
-	mov.b32 	 %f1415, %r556;
-	mov.b32 	 %f1416, %r557;
-	mov.b32 	 %f1417, %r558;
-	add.s64 	%rd503, %rd512, 112;
-	// inline asm
-	cvta.to.global.u64 %rd502, %rd503;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r560,%r561,%r562,%r563}, [%rd502];
-	// inline asm
-	mov.b32 	 %f1418, %r560;
-	mov.b32 	 %f1419, %r561;
-	mov.b32 	 %f1420, %r563;
-	add.s64 	%rd506, %rd512, 128;
-	// inline asm
-	cvta.to.global.u64 %rd505, %rd506;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r564,%r565,%r566,%r567}, [%rd505];
-	// inline asm
-	mov.b32 	 %f1421, %r565;
-	mov.b32 	 %f1422, %r566;
-	mov.b32 	 %f1423, %r567;
-	add.s64 	%rd509, %rd512, 144;
-	// inline asm
-	cvta.to.global.u64 %rd508, %rd509;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r568,%r569,%r570,%r571}, [%rd508];
-	// inline asm
-	mov.f32 	%f1424, 0f3F800000;
-	sub.f32 	%f1425, %f1424, %f512;
-	mul.f32 	%f1426, %f512, %f1415;
-	mul.f32 	%f1427, %f512, %f1416;
-	mul.f32 	%f1428, %f512, %f1417;
-	fma.rn.f32 	%f1868, %f1425, %f1868, %f1426;
-	fma.rn.f32 	%f1869, %f1425, %f1869, %f1427;
-	fma.rn.f32 	%f1870, %f1425, %f1870, %f1428;
-	mul.f32 	%f1429, %f512, %f1418;
-	mul.f32 	%f1430, %f512, %f1419;
-	mul.f32 	%f1431, %f512, %f1420;
-	fma.rn.f32 	%f1871, %f1425, %f1871, %f1429;
-	fma.rn.f32 	%f1872, %f1425, %f1872, %f1430;
-	fma.rn.f32 	%f1873, %f1425, %f1873, %f1431;
-	mul.f32 	%f1432, %f512, %f1421;
-	mul.f32 	%f1433, %f512, %f1422;
-	mul.f32 	%f1434, %f512, %f1423;
-	fma.rn.f32 	%f1435, %f1425, %f1874, %f1432;
-	fma.rn.f32 	%f1436, %f1425, %f1875, %f1433;
-	fma.rn.f32 	%f1437, %f1425, %f1876, %f1434;
-	mov.b32 	 %f1438, %r568;
-	mul.f32 	%f1439, %f512, %f1438;
-	fma.rn.f32 	%f1440, %f1425, %f1877, %f1439;
-	mul.f32 	%f1441, %f1436, %f1436;
-	fma.rn.f32 	%f1442, %f1435, %f1435, %f1441;
-	fma.rn.f32 	%f1443, %f1437, %f1437, %f1442;
-	fma.rn.f32 	%f1444, %f1440, %f1440, %f1443;
-	sqrt.rn.f32 	%f1445, %f1444;
-	rcp.rn.f32 	%f1446, %f1445;
-	mul.f32 	%f1874, %f1435, %f1446;
-	mul.f32 	%f1875, %f1436, %f1446;
-	mul.f32 	%f1876, %f1437, %f1446;
-	mul.f32 	%f1877, %f1440, %f1446;
-
-BB6_79:
-	mul.f32 	%f1447, %f1875, %f1875;
-	fma.rn.f32 	%f1448, %f1874, %f1874, %f1447;
-	fma.rn.f32 	%f1449, %f1876, %f1876, %f1448;
-	fma.rn.f32 	%f1450, %f1877, %f1877, %f1449;
-	rcp.rn.f32 	%f1451, %f1450;
-	mul.f32 	%f1452, %f1874, %f1451;
-	mul.f32 	%f1453, %f1875, %f1451;
-	mul.f32 	%f1454, %f1876, %f1451;
-	mul.f32 	%f1455, %f1877, %f1451;
-	mul.f32 	%f1456, %f1874, %f1452;
-	mul.f32 	%f1457, %f1875, %f1453;
-	mul.f32 	%f1458, %f1876, %f1454;
-	mul.f32 	%f1459, %f1874, %f1453;
-	mul.f32 	%f1460, %f1876, %f1455;
-	mul.f32 	%f1461, %f1874, %f1454;
-	mul.f32 	%f1462, %f1875, %f1455;
-	mul.f32 	%f1463, %f1875, %f1454;
-	mul.f32 	%f1464, %f1874, %f1455;
-	sub.f32 	%f1465, %f1456, %f1457;
-	sub.f32 	%f1466, %f1465, %f1458;
-	fma.rn.f32 	%f1467, %f1877, %f1455, %f1466;
-	sub.f32 	%f1468, %f1459, %f1460;
-	add.f32 	%f1469, %f1468, %f1468;
-	add.f32 	%f1470, %f1461, %f1462;
-	add.f32 	%f1471, %f1470, %f1470;
-	add.f32 	%f1472, %f1459, %f1460;
-	add.f32 	%f1473, %f1472, %f1472;
-	sub.f32 	%f1474, %f1457, %f1456;
-	sub.f32 	%f1475, %f1474, %f1458;
-	fma.rn.f32 	%f1476, %f1877, %f1455, %f1475;
-	sub.f32 	%f1477, %f1463, %f1464;
-	add.f32 	%f1478, %f1477, %f1477;
-	sub.f32 	%f1479, %f1461, %f1462;
-	add.f32 	%f1480, %f1479, %f1479;
-	add.f32 	%f1481, %f1463, %f1464;
-	add.f32 	%f1482, %f1481, %f1481;
-	neg.f32 	%f1483, %f1456;
-	sub.f32 	%f1484, %f1483, %f1457;
-	add.f32 	%f1485, %f1458, %f1484;
-	fma.rn.f32 	%f1486, %f1877, %f1455, %f1485;
-	mul.f32 	%f1487, %f1870, %f1467;
-	fma.rn.f32 	%f1488, %f1872, %f1469, %f1487;
-	fma.rn.f32 	%f1886, %f1873, %f1471, %f1488;
-	mul.f32 	%f1489, %f1872, %f1476;
-	fma.rn.f32 	%f1490, %f1870, %f1473, %f1489;
-	fma.rn.f32 	%f1883, %f1873, %f1478, %f1490;
-	mul.f32 	%f1491, %f1872, %f1482;
-	fma.rn.f32 	%f1492, %f1870, %f1480, %f1491;
-	fma.rn.f32 	%f1880, %f1873, %f1486, %f1492;
-	mul.f32 	%f1493, %f1869, %f1467;
-	fma.rn.f32 	%f1885, %f1871, %f1469, %f1493;
-	mul.f32 	%f1494, %f1871, %f1476;
-	fma.rn.f32 	%f1882, %f1869, %f1473, %f1494;
-	mul.f32 	%f1495, %f1871, %f1482;
-	fma.rn.f32 	%f1879, %f1869, %f1480, %f1495;
-	mul.f32 	%f1884, %f1868, %f1467;
-	mul.f32 	%f1881, %f1868, %f1473;
-	mul.f32 	%f1878, %f1868, %f1480;
-
-BB6_82:
-	mul.f32 	%f1527, %f1879, %f1883;
-	mul.f32 	%f1528, %f1880, %f1882;
-	sub.f32 	%f1529, %f1528, %f1527;
-	mul.f32 	%f1530, %f1884, %f1529;
-	mul.f32 	%f1531, %f1878, %f1883;
-	mul.f32 	%f1532, %f1880, %f1881;
-	sub.f32 	%f1533, %f1532, %f1531;
-	mul.f32 	%f1534, %f1533, %f1885;
-	sub.f32 	%f1535, %f1530, %f1534;
-	mul.f32 	%f1536, %f1878, %f1882;
-	mul.f32 	%f1537, %f1879, %f1881;
-	sub.f32 	%f1538, %f1537, %f1536;
-	fma.rn.f32 	%f1539, %f1538, %f1886, %f1535;
-	rcp.rn.f32 	%f1540, %f1539;
-	mul.f32 	%f1893, %f1529, %f1540;
-	mul.f32 	%f1541, %f1880, %f1885;
-	mul.f32 	%f1542, %f1879, %f1886;
-	sub.f32 	%f1543, %f1542, %f1541;
-	mul.f32 	%f1894, %f1540, %f1543;
-	mul.f32 	%f1544, %f1882, %f1886;
-	mul.f32 	%f1545, %f1883, %f1885;
-	sub.f32 	%f1546, %f1545, %f1544;
-	mul.f32 	%f1895, %f1540, %f1546;
-	sub.f32 	%f1547, %f1531, %f1532;
-	mul.f32 	%f1890, %f1547, %f1540;
-	mul.f32 	%f1548, %f1878, %f1886;
-	mul.f32 	%f1549, %f1880, %f1884;
-	sub.f32 	%f1550, %f1549, %f1548;
-	mul.f32 	%f1891, %f1540, %f1550;
-	mul.f32 	%f1551, %f1883, %f1884;
-	mul.f32 	%f1552, %f1881, %f1886;
-	sub.f32 	%f1553, %f1552, %f1551;
-	mul.f32 	%f1892, %f1540, %f1553;
-	mul.f32 	%f1887, %f1538, %f1540;
-	mul.f32 	%f1554, %f1879, %f1884;
-	mul.f32 	%f1555, %f1878, %f1885;
-	sub.f32 	%f1556, %f1555, %f1554;
-	mul.f32 	%f1888, %f1556, %f1540;
-	mul.f32 	%f1557, %f1881, %f1885;
-	mul.f32 	%f1558, %f1882, %f1884;
-	sub.f32 	%f1559, %f1558, %f1557;
-	mul.f32 	%f1889, %f1559, %f1540;
-	bra.uni 	BB6_83;
-
-BB6_72:
-	setp.ne.s32	%p38, %r483, 1;
-	mov.f32 	%f1888, %f1887;
-	mov.f32 	%f1890, %f1887;
-	mov.f32 	%f1891, %f1889;
-	mov.f32 	%f1892, %f1887;
-	mov.f32 	%f1893, %f1889;
-	mov.f32 	%f1894, %f1887;
-	mov.f32 	%f1895, %f1887;
-	@%p38 bra 	BB6_83;
-
-	// inline asm
-	call (%rd441), _optix_get_static_transform_from_handle, (%rd439);
-	// inline asm
-	add.s64 	%rd669, %rd441, 64;
-
-BB6_75:
-	// inline asm
-	cvta.to.global.u64 %rd445, %rd669;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r485,%r486,%r487,%r488}, [%rd445];
-	// inline asm
-	mov.b32 	 %f1893, %r485;
-	mov.b32 	 %f1894, %r486;
-	mov.b32 	 %f1895, %r487;
-	add.s64 	%rd449, %rd669, 16;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r639,%r640,%r641,%r642}, [%rd542];
+	// end inline asm
+	mov.b32 	%f1558, %r639;
+	mov.b32 	%f1559, %r640;
+	mov.b32 	%f1560, %r641;
+	mul.f32 	%f1561, %f581, %f1558;
+	mul.f32 	%f1562, %f581, %f1559;
+	mul.f32 	%f1563, %f581, %f1560;
+	fma.rn.f32 	%f1938, %f1545, %f1938, %f1561;
+	fma.rn.f32 	%f1939, %f1545, %f1939, %f1562;
+	fma.rn.f32 	%f1940, %f1545, %f1940, %f1563;
+	bra.uni 	$L__BB6_84;
+
+$L__BB6_73:
+	mov.f32 	%f1947, 0f00000000;
+	mov.f32 	%f1949, 0f3F800000;
+	setp.eq.s32 	%p40, %r495, 4;
+	@%p40 bra 	$L__BB6_76;
+
+	setp.ne.s32 	%p41, %r495, 1;
+	mov.f32 	%f1948, %f1947;
+	mov.f32 	%f1950, %f1947;
+	mov.f32 	%f1951, %f1949;
+	mov.f32 	%f1952, %f1947;
+	mov.f32 	%f1953, %f1949;
+	mov.f32 	%f1954, %f1947;
+	mov.f32 	%f1955, %f1947;
+	@%p41 bra 	$L__BB6_85;
+
+	// begin inline asm
+	call (%rd430), _optix_get_static_transform_from_handle, (%rd428);
+	// end inline asm
+	add.s64 	%rd655, %rd430, 64;
+	bra.uni 	$L__BB6_77;
+
+$L__BB6_79:
+	// begin inline asm
+	call (%rd443), _optix_get_srt_motion_transform_from_handle, (%rd428);
+	// end inline asm
+	// begin inline asm
+	cvta.to.global.u64 %rd445, %rd443;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r509,%r510,%r511,%r512}, [%rd445];
+	// end inline asm
+	add.s64 	%rd449, %rd443, 16;
+	// begin inline asm
 	cvta.to.global.u64 %rd448, %rd449;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r489,%r490,%r491,%r492}, [%rd448];
-	// inline asm
-	mov.b32 	 %f1890, %r489;
-	mov.b32 	 %f1891, %r490;
-	mov.b32 	 %f1892, %r491;
-	add.s64 	%rd452, %rd669, 32;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r513,%r514,%r515,%r516}, [%rd448];
+	// end inline asm
+	add.s64 	%rd452, %rd443, 32;
+	// begin inline asm
 	cvta.to.global.u64 %rd451, %rd452;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r493,%r494,%r495,%r496}, [%rd451];
-	// inline asm
-	mov.b32 	 %f1887, %r493;
-	mov.b32 	 %f1888, %r494;
-	mov.b32 	 %f1889, %r495;
-
-BB6_83:
-	setp.eq.s32	%p42, %r637, 0;
-	@%p42 bra 	BB6_84;
-	bra.uni 	BB6_85;
-
-BB6_84:
-	mov.f32 	%f1867, %f1887;
-	mov.f32 	%f1866, %f1888;
-	mov.f32 	%f1865, %f1889;
-	mov.f32 	%f1864, %f1890;
-	mov.f32 	%f1863, %f1891;
-	mov.f32 	%f1862, %f1892;
-	mov.f32 	%f1861, %f1893;
-	mov.f32 	%f1860, %f1894;
-	mov.f32 	%f1859, %f1895;
-	bra.uni 	BB6_86;
-
-BB6_85:
-	mul.f32 	%f1560, %f1864, %f1894;
-	fma.rn.f32 	%f1561, %f1861, %f1893, %f1560;
-	fma.rn.f32 	%f592, %f1867, %f1895, %f1561;
-	mul.f32 	%f1562, %f1863, %f1894;
-	fma.rn.f32 	%f1563, %f1860, %f1893, %f1562;
-	fma.rn.f32 	%f593, %f1866, %f1895, %f1563;
-	mul.f32 	%f1564, %f1862, %f1894;
-	fma.rn.f32 	%f1565, %f1859, %f1893, %f1564;
-	fma.rn.f32 	%f594, %f1865, %f1895, %f1565;
-	mul.f32 	%f1566, %f1864, %f1891;
-	fma.rn.f32 	%f1567, %f1861, %f1890, %f1566;
-	fma.rn.f32 	%f595, %f1867, %f1892, %f1567;
-	mul.f32 	%f1568, %f1863, %f1891;
-	fma.rn.f32 	%f1569, %f1860, %f1890, %f1568;
-	fma.rn.f32 	%f596, %f1866, %f1892, %f1569;
-	mul.f32 	%f1570, %f1862, %f1891;
-	fma.rn.f32 	%f1571, %f1859, %f1890, %f1570;
-	fma.rn.f32 	%f597, %f1865, %f1892, %f1571;
-	mul.f32 	%f1572, %f1864, %f1888;
-	fma.rn.f32 	%f1573, %f1861, %f1887, %f1572;
-	fma.rn.f32 	%f1867, %f1867, %f1889, %f1573;
-	mul.f32 	%f1574, %f1863, %f1888;
-	fma.rn.f32 	%f1575, %f1860, %f1887, %f1574;
-	fma.rn.f32 	%f1866, %f1866, %f1889, %f1575;
-	mul.f32 	%f1576, %f1862, %f1888;
-	fma.rn.f32 	%f1577, %f1859, %f1887, %f1576;
-	fma.rn.f32 	%f1865, %f1865, %f1889, %f1577;
-	mov.f32 	%f1864, %f595;
-	mov.f32 	%f1863, %f596;
-	mov.f32 	%f1862, %f597;
-	mov.f32 	%f1861, %f592;
-	mov.f32 	%f1860, %f593;
-	mov.f32 	%f1859, %f594;
-
-BB6_86:
-	add.s32 	%r637, %r637, 1;
-	setp.lt.u32	%p43, %r637, %r28;
-	@%p43 bra 	BB6_70;
-
-BB6_87:
-	fma.rn.f32 	%f1578, %f1944, %f1798, %f1795;
-	fma.rn.f32 	%f1579, %f1945, %f1797, %f1578;
-	fma.rn.f32 	%f1580, %f1944, %f1802, %f1799;
-	fma.rn.f32 	%f1581, %f1945, %f1801, %f1580;
-	fma.rn.f32 	%f1582, %f1944, %f1806, %f1803;
-	fma.rn.f32 	%f1583, %f1945, %f1805, %f1582;
-	fma.rn.f32 	%f1944, %f1946, %f1796, %f1579;
-	fma.rn.f32 	%f1945, %f1946, %f1800, %f1581;
-	fma.rn.f32 	%f1946, %f1946, %f1804, %f1583;
-	ld.const.u64 	%rd560, [params+112];
-	setp.eq.s64	%p44, %rd560, 0;
-	mov.f32 	%f1938, %f1941;
-	mov.f32 	%f1939, %f1942;
-	mov.f32 	%f1940, %f1943;
-	@%p44 bra 	BB6_89;
-
-	mul.f32 	%f1584, %f1941, %f1861;
-	fma.rn.f32 	%f1585, %f1942, %f1864, %f1584;
-	mul.f32 	%f1586, %f1941, %f1860;
-	fma.rn.f32 	%f1587, %f1942, %f1863, %f1586;
-	mul.f32 	%f1588, %f1941, %f1859;
-	fma.rn.f32 	%f1589, %f1942, %f1862, %f1588;
-	fma.rn.f32 	%f1590, %f1943, %f1867, %f1585;
-	fma.rn.f32 	%f1591, %f1943, %f1866, %f1587;
-	fma.rn.f32 	%f1592, %f1943, %f1865, %f1589;
-	mul.f32 	%f1593, %f1590, %f1590;
-	fma.rn.f32 	%f1594, %f1591, %f1591, %f1593;
-	fma.rn.f32 	%f1595, %f1592, %f1592, %f1594;
-	sqrt.rn.f32 	%f1596, %f1595;
-	div.rn.f32 	%f1938, %f1590, %f1596;
-	div.rn.f32 	%f1939, %f1591, %f1596;
-	div.rn.f32 	%f1940, %f1592, %f1596;
-
-BB6_89:
-	ld.const.u64 	%rd561, [params+136];
-	setp.eq.s64	%p45, %rd561, 0;
-	@%p45 bra 	BB6_91;
-
-	mul.f32 	%f1597, %f1941, %f1861;
-	fma.rn.f32 	%f1598, %f1942, %f1864, %f1597;
-	mul.f32 	%f1599, %f1941, %f1860;
-	fma.rn.f32 	%f1600, %f1942, %f1863, %f1599;
-	mul.f32 	%f1601, %f1941, %f1859;
-	fma.rn.f32 	%f1602, %f1942, %f1862, %f1601;
-	fma.rn.f32 	%f1603, %f1943, %f1867, %f1598;
-	fma.rn.f32 	%f1604, %f1943, %f1866, %f1600;
-	fma.rn.f32 	%f1605, %f1943, %f1865, %f1602;
-	mul.f32 	%f1606, %f1603, %f1603;
-	fma.rn.f32 	%f1607, %f1604, %f1604, %f1606;
-	fma.rn.f32 	%f1608, %f1605, %f1605, %f1607;
-	sqrt.rn.f32 	%f1609, %f1608;
-	div.rn.f32 	%f1941, %f1603, %f1609;
-	div.rn.f32 	%f1942, %f1604, %f1609;
-	div.rn.f32 	%f1943, %f1605, %f1609;
-
-BB6_91:
-	ld.const.u64 	%rd562, [params+184];
-	setp.eq.s64	%p46, %rd562, 0;
-	@%p46 bra 	BB6_93;
-
-	mul.f32 	%f1610, %f1935, %f1798;
-	fma.rn.f32 	%f1611, %f1936, %f1797, %f1610;
-	mul.f32 	%f1612, %f1935, %f1802;
-	fma.rn.f32 	%f1613, %f1936, %f1801, %f1612;
-	mul.f32 	%f1614, %f1935, %f1806;
-	fma.rn.f32 	%f1615, %f1936, %f1805, %f1614;
-	fma.rn.f32 	%f1935, %f1937, %f1796, %f1611;
-	fma.rn.f32 	%f1936, %f1937, %f1800, %f1613;
-	fma.rn.f32 	%f1937, %f1937, %f1804, %f1615;
-	mul.f32 	%f1616, %f1932, %f1798;
-	fma.rn.f32 	%f1617, %f1933, %f1797, %f1616;
-	mul.f32 	%f1618, %f1932, %f1802;
-	fma.rn.f32 	%f1619, %f1933, %f1801, %f1618;
-	mul.f32 	%f1620, %f1932, %f1806;
-	fma.rn.f32 	%f1621, %f1933, %f1805, %f1620;
-	fma.rn.f32 	%f1932, %f1934, %f1796, %f1617;
-	fma.rn.f32 	%f1933, %f1934, %f1800, %f1619;
-	fma.rn.f32 	%f1934, %f1934, %f1804, %f1621;
-
-BB6_93:
-	ld.const.u64 	%rd563, [params+280];
-	ld.const.u64 	%rd564, [params+232];
-	mov.f32 	%f1929, 0f00000000;
-	or.b64  	%rd565, %rd563, %rd564;
-	setp.eq.s64	%p47, %rd565, 0;
-	mov.f32 	%f1930, %f1929;
-	mov.f32 	%f1931, %f1929;
-	@%p47 bra 	BB6_95;
-
-	mul.f32 	%f1625, %f1941, %f1798;
-	fma.rn.f32 	%f1626, %f1942, %f1802, %f1625;
-	mul.f32 	%f1627, %f1941, %f1797;
-	fma.rn.f32 	%f1628, %f1942, %f1801, %f1627;
-	mul.f32 	%f1629, %f1941, %f1796;
-	fma.rn.f32 	%f1630, %f1942, %f1800, %f1629;
-	fma.rn.f32 	%f1631, %f1943, %f1806, %f1626;
-	fma.rn.f32 	%f1632, %f1943, %f1805, %f1628;
-	fma.rn.f32 	%f1633, %f1943, %f1804, %f1630;
-	mul.f32 	%f1634, %f1631, %f1631;
-	fma.rn.f32 	%f1635, %f1632, %f1632, %f1634;
-	fma.rn.f32 	%f1636, %f1633, %f1633, %f1635;
-	sqrt.rn.f32 	%f1637, %f1636;
-	div.rn.f32 	%f1638, %f1631, %f1637;
-	div.rn.f32 	%f1639, %f1632, %f1637;
-	div.rn.f32 	%f1640, %f1633, %f1637;
-	mul.f32 	%f1641, %f1638, %f1861;
-	mul.f32 	%f1642, %f1638, %f1860;
-	mul.f32 	%f1643, %f1638, %f1859;
-	fma.rn.f32 	%f1644, %f1639, %f1864, %f1641;
-	fma.rn.f32 	%f1645, %f1639, %f1863, %f1642;
-	fma.rn.f32 	%f1646, %f1639, %f1862, %f1643;
-	fma.rn.f32 	%f1647, %f1640, %f1867, %f1644;
-	fma.rn.f32 	%f1648, %f1640, %f1866, %f1645;
-	fma.rn.f32 	%f1649, %f1640, %f1865, %f1646;
-	mul.f32 	%f1650, %f1647, %f1647;
-	fma.rn.f32 	%f1651, %f1648, %f1648, %f1650;
-	fma.rn.f32 	%f1652, %f1649, %f1649, %f1651;
-	sqrt.rn.f32 	%f1653, %f1652;
-	rcp.rn.f32 	%f1654, %f1653;
-	mul.f32 	%f1655, %f1654, %f1647;
-	mul.f32 	%f1656, %f1654, %f1648;
-	mul.f32 	%f1657, %f1654, %f1649;
-	mul.f32 	%f1658, %f1861, 0f00000000;
-	mov.f32 	%f1659, 0f00000000;
-	fma.rn.f32 	%f1660, %f1659, %f1864, %f1658;
-	mul.f32 	%f1661, %f1860, 0f00000000;
-	fma.rn.f32 	%f1662, %f1659, %f1863, %f1661;
-	mul.f32 	%f1663, %f1859, 0f00000000;
-	fma.rn.f32 	%f1664, %f1659, %f1862, %f1663;
-	fma.rn.f32 	%f1665, %f1659, %f1867, %f1660;
-	fma.rn.f32 	%f1666, %f1659, %f1866, %f1662;
-	fma.rn.f32 	%f1667, %f1659, %f1865, %f1664;
-	mul.f32 	%f1668, %f1665, %f1654;
-	mul.f32 	%f1669, %f1666, %f1654;
-	mul.f32 	%f1670, %f1667, %f1654;
-	mul.f32 	%f1671, %f1655, %f1668;
-	fma.rn.f32 	%f1672, %f1656, %f1669, %f1671;
-	fma.rn.f32 	%f1673, %f1657, %f1670, %f1672;
-	mul.f32 	%f1674, %f1655, %f1673;
-	mul.f32 	%f1675, %f1656, %f1673;
-	mul.f32 	%f1676, %f1657, %f1673;
-	sub.f32 	%f1929, %f1668, %f1674;
-	sub.f32 	%f1930, %f1669, %f1675;
-	sub.f32 	%f1931, %f1670, %f1676;
-
-BB6_95:
-	st.global.u32 	[%rd26], %r331;
-	bra.uni 	BB6_96;
-
-BB6_49:
-	mov.f32 	%f1930, %f1929;
-	mov.f32 	%f1931, %f1929;
-	mov.f32 	%f1938, %f1941;
-	mov.f32 	%f1939, %f1942;
-	mov.f32 	%f1940, %f1943;
-
-BB6_96:
-	ld.const.u64 	%rd566, [params+328];
-	cvta.to.global.u64 	%rd567, %rd566;
-	shl.b64 	%rd568, %rd22, 3;
-	add.s64 	%rd569, %rd567, %rd568;
-	st.global.u64 	[%rd569], %rd25;
-	ld.const.u64 	%rd570, [params+336];
-	cvta.to.global.u64 	%rd571, %rd570;
-	add.s64 	%rd573, %rd571, %rd317;
-	mov.u32 	%r631, 0;
-	st.global.u32 	[%rd573], %r631;
-	ld.const.u64 	%rd574, [params+160];
-	cvta.to.global.u64 	%rd575, %rd574;
-	add.s64 	%rd576, %rd575, %rd317;
-	st.global.f32 	[%rd576], %f1944;
-	ld.const.u64 	%rd577, [params+168];
-	cvta.to.global.u64 	%rd578, %rd577;
-	add.s64 	%rd579, %rd578, %rd317;
-	st.global.f32 	[%rd579], %f1945;
-	ld.const.u64 	%rd580, [params+176];
-	cvta.to.global.u64 	%rd581, %rd580;
-	add.s64 	%rd582, %rd581, %rd317;
-	st.global.f32 	[%rd582], %f1946;
-	ld.const.u64 	%rd583, [params+72];
-	cvta.to.global.u64 	%rd584, %rd583;
-	add.s64 	%rd585, %rd584, %rd317;
-	st.global.f32 	[%rd585], %f313;
-	ld.const.u64 	%rd43, [params+96];
-	setp.eq.s64	%p48, %rd43, 0;
-	@%p48 bra 	BB6_98;
-
-	cvta.to.global.u64 	%rd586, %rd43;
-	add.s64 	%rd588, %rd586, %rd317;
-	st.global.f32 	[%rd588], %f319;
-	ld.const.u64 	%rd589, [params+104];
-	cvta.to.global.u64 	%rd590, %rd589;
-	add.s64 	%rd591, %rd590, %rd317;
-	st.global.f32 	[%rd591], %f320;
-
-BB6_98:
-	ld.const.u64 	%rd44, [params+112];
-	setp.eq.s64	%p49, %rd44, 0;
-	@%p49 bra 	BB6_100;
-
-	cvta.to.global.u64 	%rd592, %rd44;
-	add.s64 	%rd594, %rd592, %rd317;
-	st.global.f32 	[%rd594], %f1938;
-	ld.const.u64 	%rd595, [params+120];
-	cvta.to.global.u64 	%rd596, %rd595;
-	add.s64 	%rd597, %rd596, %rd317;
-	st.global.f32 	[%rd597], %f1939;
-	ld.const.u64 	%rd598, [params+128];
-	cvta.to.global.u64 	%rd599, %rd598;
-	add.s64 	%rd600, %rd599, %rd317;
-	st.global.f32 	[%rd600], %f1940;
-
-BB6_100:
-	ld.const.u64 	%rd45, [params+136];
-	setp.eq.s64	%p50, %rd45, 0;
-	@%p50 bra 	BB6_102;
-
-	cvta.to.global.u64 	%rd601, %rd45;
-	add.s64 	%rd603, %rd601, %rd317;
-	st.global.f32 	[%rd603], %f1941;
-	ld.const.u64 	%rd604, [params+144];
-	cvta.to.global.u64 	%rd605, %rd604;
-	add.s64 	%rd606, %rd605, %rd317;
-	st.global.f32 	[%rd606], %f1942;
-	ld.const.u64 	%rd607, [params+152];
-	cvta.to.global.u64 	%rd608, %rd607;
-	add.s64 	%rd609, %rd608, %rd317;
-	st.global.f32 	[%rd609], %f1943;
-
-BB6_102:
-	ld.const.u64 	%rd46, [params+184];
-	setp.eq.s64	%p51, %rd46, 0;
-	@%p51 bra 	BB6_104;
-
-	cvta.to.global.u64 	%rd610, %rd46;
-	add.s64 	%rd612, %rd610, %rd317;
-	st.global.f32 	[%rd612], %f1935;
-	ld.const.u64 	%rd613, [params+192];
-	cvta.to.global.u64 	%rd614, %rd613;
-	add.s64 	%rd615, %rd614, %rd317;
-	st.global.f32 	[%rd615], %f1936;
-	ld.const.u64 	%rd616, [params+200];
-	cvta.to.global.u64 	%rd617, %rd616;
-	add.s64 	%rd618, %rd617, %rd317;
-	st.global.f32 	[%rd618], %f1937;
-	ld.const.u64 	%rd619, [params+208];
-	cvta.to.global.u64 	%rd620, %rd619;
-	add.s64 	%rd621, %rd620, %rd317;
-	st.global.f32 	[%rd621], %f1932;
-	ld.const.u64 	%rd622, [params+216];
-	cvta.to.global.u64 	%rd623, %rd622;
-	add.s64 	%rd624, %rd623, %rd317;
-	st.global.f32 	[%rd624], %f1933;
-	ld.const.u64 	%rd625, [params+224];
-	cvta.to.global.u64 	%rd626, %rd625;
-	add.s64 	%rd627, %rd626, %rd317;
-	st.global.f32 	[%rd627], %f1934;
-
-BB6_104:
-	ld.const.u64 	%rd47, [params+232];
-	setp.eq.s64	%p52, %rd47, 0;
-	@%p52 bra 	BB6_106;
-
-	cvta.to.global.u64 	%rd628, %rd47;
-	add.s64 	%rd630, %rd628, %rd317;
-	st.global.f32 	[%rd630], %f1929;
-	ld.const.u64 	%rd631, [params+240];
-	cvta.to.global.u64 	%rd632, %rd631;
-	add.s64 	%rd633, %rd632, %rd317;
-	st.global.f32 	[%rd633], %f1930;
-	ld.const.u64 	%rd634, [params+248];
-	cvta.to.global.u64 	%rd635, %rd634;
-	add.s64 	%rd636, %rd635, %rd317;
-	st.global.f32 	[%rd636], %f1931;
-	ld.const.u64 	%rd637, [params+256];
-	cvta.to.global.u64 	%rd638, %rd637;
-	add.s64 	%rd639, %rd638, %rd317;
-	st.global.f32 	[%rd639], %f1929;
-	ld.const.u64 	%rd640, [params+264];
-	cvta.to.global.u64 	%rd641, %rd640;
-	add.s64 	%rd642, %rd641, %rd317;
-	st.global.f32 	[%rd642], %f1930;
-	ld.const.u64 	%rd643, [params+272];
-	cvta.to.global.u64 	%rd644, %rd643;
-	add.s64 	%rd645, %rd644, %rd317;
-	st.global.f32 	[%rd645], %f1931;
-
-BB6_106:
-	ld.const.u64 	%rd48, [params+280];
-	setp.eq.s64	%p53, %rd48, 0;
-	@%p53 bra 	BB6_108;
-
-	cvta.to.global.u64 	%rd646, %rd48;
-	add.s64 	%rd648, %rd646, %rd317;
-	st.global.f32 	[%rd648], %f1929;
-	ld.const.u64 	%rd649, [params+288];
-	cvta.to.global.u64 	%rd650, %rd649;
-	add.s64 	%rd651, %rd650, %rd317;
-	st.global.f32 	[%rd651], %f1930;
-	ld.const.u64 	%rd652, [params+296];
-	cvta.to.global.u64 	%rd653, %rd652;
-	add.s64 	%rd654, %rd653, %rd317;
-	st.global.f32 	[%rd654], %f1931;
-	ld.const.u64 	%rd655, [params+304];
-	cvta.to.global.u64 	%rd656, %rd655;
-	add.s64 	%rd657, %rd656, %rd317;
-	st.global.f32 	[%rd657], %f1929;
-	ld.const.u64 	%rd658, [params+312];
-	cvta.to.global.u64 	%rd659, %rd658;
-	add.s64 	%rd660, %rd659, %rd317;
-	st.global.f32 	[%rd660], %f1930;
-	ld.const.u64 	%rd661, [params+320];
-	cvta.to.global.u64 	%rd662, %rd661;
-	add.s64 	%rd663, %rd662, %rd317;
-	st.global.f32 	[%rd663], %f1931;
-
-BB6_108:
-	ret;
-}
-
-	// .globl	__intersection__sphere
-.visible .entry __intersection__sphere(
-
-)
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r517,%r518,%r519,%r520}, [%rd451];
+	// end inline asm
+	add.s64 	%rd455, %rd443, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd454, %rd455;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r521,%r522,%r523,%r524}, [%rd454];
+	// end inline asm
+	add.s64 	%rd458, %rd443, 64;
+	// begin inline asm
+	cvta.to.global.u64 %rd457, %rd458;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r525,%r526,%r527,%r528}, [%rd457];
+	// end inline asm
+	add.s64 	%rd461, %rd443, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd460, %rd461;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r529,%r530,%r531,%r532}, [%rd460];
+	// end inline asm
+	add.s64 	%rd464, %rd443, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd463, %rd464;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r533,%r534,%r535,%r536}, [%rd463];
+	// end inline asm
+	add.s64 	%rd467, %rd443, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd466, %rd467;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r537,%r538,%r539,%r540}, [%rd466];
+	// end inline asm
+	add.s64 	%rd470, %rd443, 128;
+	// begin inline asm
+	cvta.to.global.u64 %rd469, %rd470;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r541,%r542,%r543,%r544}, [%rd469];
+	// end inline asm
+	add.s64 	%rd473, %rd443, 144;
+	// begin inline asm
+	cvta.to.global.u64 %rd472, %rd473;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r545,%r546,%r547,%r548}, [%rd472];
+	// end inline asm
+	mov.b32 	%f1441, %r512;
+	mov.b32 	%f1442, %r513;
+	and.b32  	%r565, %r511, 65535;
+	add.s32 	%r566, %r565, -1;
+	cvt.rn.f32.s32 	%f1443, %r566;
+	sub.f32 	%f1444, %f1429, %f1441;
+	mul.f32 	%f1445, %f1444, %f1443;
+	sub.f32 	%f1446, %f1442, %f1441;
+	div.rn.f32 	%f1447, %f1445, %f1446;
+	min.f32 	%f1448, %f1443, %f1447;
+	mov.f32 	%f1449, 0f00000000;
+	max.f32 	%f1450, %f1449, %f1448;
+	cvt.rmi.f32.f32 	%f1451, %f1450;
+	sub.f32 	%f541, %f1450, %f1451;
+	cvt.rzi.s32.f32 	%r567, %f1451;
+	mul.wide.s32 	%rd487, %r567, 64;
+	add.s64 	%rd476, %rd452, %rd487;
+	// begin inline asm
+	cvta.to.global.u64 %rd475, %rd476;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r549,%r550,%r551,%r552}, [%rd475];
+	// end inline asm
+	mov.b32 	%f1928, %r549;
+	mov.b32 	%f1929, %r550;
+	mov.b32 	%f1930, %r551;
+	add.s64 	%rd479, %rd476, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd478, %rd479;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r553,%r554,%r555,%r556}, [%rd478];
+	// end inline asm
+	mov.b32 	%f1931, %r553;
+	mov.b32 	%f1932, %r554;
+	mov.b32 	%f1933, %r556;
+	add.s64 	%rd482, %rd476, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd481, %rd482;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r557,%r558,%r559,%r560}, [%rd481];
+	// end inline asm
+	mov.b32 	%f1934, %r558;
+	mov.b32 	%f1935, %r559;
+	mov.b32 	%f1936, %r560;
+	add.s64 	%rd485, %rd476, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd484, %rd485;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r561,%r562,%r563,%r564}, [%rd484];
+	// end inline asm
+	mov.b32 	%f1937, %r561;
+	setp.leu.f32 	%p43, %f541, 0f00000000;
+	@%p43 bra 	$L__BB6_81;
+
+	mov.f32 	%f1452, 0f3F800000;
+	sub.f32 	%f1453, %f1452, %f541;
+	add.s64 	%rd489, %rd476, 64;
+	// begin inline asm
+	cvta.to.global.u64 %rd488, %rd489;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r568,%r569,%r570,%r571}, [%rd488];
+	// end inline asm
+	mov.b32 	%f1454, %r568;
+	mov.b32 	%f1455, %r569;
+	mov.b32 	%f1456, %r570;
+	mul.f32 	%f1457, %f541, %f1454;
+	mul.f32 	%f1458, %f541, %f1455;
+	mul.f32 	%f1459, %f541, %f1456;
+	fma.rn.f32 	%f1928, %f1453, %f1928, %f1457;
+	fma.rn.f32 	%f1929, %f1453, %f1929, %f1458;
+	fma.rn.f32 	%f1930, %f1453, %f1930, %f1459;
+	add.s64 	%rd492, %rd476, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd491, %rd492;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r572,%r573,%r574,%r575}, [%rd491];
+	// end inline asm
+	mov.b32 	%f1460, %r572;
+	mov.b32 	%f1461, %r573;
+	mov.b32 	%f1462, %r575;
+	mul.f32 	%f1463, %f541, %f1460;
+	mul.f32 	%f1464, %f541, %f1461;
+	mul.f32 	%f1465, %f541, %f1462;
+	fma.rn.f32 	%f1931, %f1453, %f1931, %f1463;
+	fma.rn.f32 	%f1932, %f1453, %f1932, %f1464;
+	fma.rn.f32 	%f1933, %f1453, %f1933, %f1465;
+	add.s64 	%rd495, %rd476, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd494, %rd495;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r576,%r577,%r578,%r579}, [%rd494];
+	// end inline asm
+	mov.b32 	%f1466, %r577;
+	mov.b32 	%f1467, %r578;
+	mov.b32 	%f1468, %r579;
+	mul.f32 	%f1469, %f541, %f1466;
+	mul.f32 	%f1470, %f541, %f1467;
+	mul.f32 	%f1471, %f541, %f1468;
+	fma.rn.f32 	%f1472, %f1453, %f1934, %f1469;
+	fma.rn.f32 	%f1473, %f1453, %f1935, %f1470;
+	fma.rn.f32 	%f1474, %f1453, %f1936, %f1471;
+	add.s64 	%rd498, %rd476, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd497, %rd498;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r580,%r581,%r582,%r583}, [%rd497];
+	// end inline asm
+	mov.b32 	%f1475, %r580;
+	mul.f32 	%f1476, %f541, %f1475;
+	fma.rn.f32 	%f1477, %f1453, %f1937, %f1476;
+	mul.f32 	%f1478, %f1473, %f1473;
+	fma.rn.f32 	%f1479, %f1472, %f1472, %f1478;
+	fma.rn.f32 	%f1480, %f1474, %f1474, %f1479;
+	fma.rn.f32 	%f1481, %f1477, %f1477, %f1480;
+	sqrt.rn.f32 	%f1482, %f1481;
+	rcp.rn.f32 	%f1483, %f1482;
+	mul.f32 	%f1934, %f1472, %f1483;
+	mul.f32 	%f1935, %f1473, %f1483;
+	mul.f32 	%f1936, %f1474, %f1483;
+	mul.f32 	%f1937, %f1483, %f1477;
+
+$L__BB6_81:
+	mul.f32 	%f1484, %f1935, %f1935;
+	fma.rn.f32 	%f1485, %f1934, %f1934, %f1484;
+	fma.rn.f32 	%f1486, %f1936, %f1936, %f1485;
+	fma.rn.f32 	%f1487, %f1937, %f1937, %f1486;
+	rcp.rn.f32 	%f1488, %f1487;
+	mul.f32 	%f1489, %f1934, %f1488;
+	mul.f32 	%f1490, %f1935, %f1488;
+	mul.f32 	%f1491, %f1936, %f1488;
+	mul.f32 	%f1492, %f1937, %f1488;
+	mul.f32 	%f1493, %f1934, %f1489;
+	mul.f32 	%f1494, %f1935, %f1490;
+	mul.f32 	%f1495, %f1936, %f1491;
+	mul.f32 	%f1496, %f1934, %f1490;
+	mul.f32 	%f1497, %f1936, %f1492;
+	mul.f32 	%f1498, %f1934, %f1491;
+	mul.f32 	%f1499, %f1935, %f1492;
+	mul.f32 	%f1500, %f1935, %f1491;
+	mul.f32 	%f1501, %f1934, %f1492;
+	sub.f32 	%f1502, %f1493, %f1494;
+	sub.f32 	%f1503, %f1502, %f1495;
+	fma.rn.f32 	%f1504, %f1937, %f1492, %f1503;
+	sub.f32 	%f1505, %f1496, %f1497;
+	add.f32 	%f1506, %f1505, %f1505;
+	add.f32 	%f1507, %f1498, %f1499;
+	add.f32 	%f1508, %f1507, %f1507;
+	add.f32 	%f1509, %f1496, %f1497;
+	add.f32 	%f1510, %f1509, %f1509;
+	sub.f32 	%f1511, %f1494, %f1493;
+	sub.f32 	%f1512, %f1511, %f1495;
+	fma.rn.f32 	%f1513, %f1937, %f1492, %f1512;
+	sub.f32 	%f1514, %f1500, %f1501;
+	add.f32 	%f1515, %f1514, %f1514;
+	sub.f32 	%f1516, %f1498, %f1499;
+	add.f32 	%f1517, %f1516, %f1516;
+	add.f32 	%f1518, %f1500, %f1501;
+	add.f32 	%f1519, %f1518, %f1518;
+	neg.f32 	%f1520, %f1493;
+	sub.f32 	%f1521, %f1520, %f1494;
+	add.f32 	%f1522, %f1495, %f1521;
+	fma.rn.f32 	%f1523, %f1937, %f1492, %f1522;
+	mul.f32 	%f1524, %f1930, %f1504;
+	fma.rn.f32 	%f1525, %f1932, %f1506, %f1524;
+	fma.rn.f32 	%f1946, %f1933, %f1508, %f1525;
+	mul.f32 	%f1526, %f1932, %f1513;
+	fma.rn.f32 	%f1527, %f1930, %f1510, %f1526;
+	fma.rn.f32 	%f1943, %f1933, %f1515, %f1527;
+	mul.f32 	%f1528, %f1932, %f1519;
+	fma.rn.f32 	%f1529, %f1930, %f1517, %f1528;
+	fma.rn.f32 	%f1940, %f1933, %f1523, %f1529;
+	mul.f32 	%f1530, %f1929, %f1504;
+	fma.rn.f32 	%f1945, %f1931, %f1506, %f1530;
+	mul.f32 	%f1531, %f1931, %f1513;
+	fma.rn.f32 	%f1942, %f1929, %f1510, %f1531;
+	mul.f32 	%f1532, %f1931, %f1519;
+	fma.rn.f32 	%f1939, %f1929, %f1517, %f1532;
+	mul.f32 	%f1944, %f1928, %f1504;
+	mul.f32 	%f1941, %f1928, %f1510;
+	mul.f32 	%f1938, %f1928, %f1517;
+
+$L__BB6_84:
+	mul.f32 	%f1564, %f1939, %f1943;
+	mul.f32 	%f1565, %f1940, %f1942;
+	sub.f32 	%f1566, %f1565, %f1564;
+	mul.f32 	%f1567, %f1944, %f1566;
+	mul.f32 	%f1568, %f1938, %f1943;
+	mul.f32 	%f1569, %f1940, %f1941;
+	sub.f32 	%f1570, %f1569, %f1568;
+	mul.f32 	%f1571, %f1570, %f1945;
+	sub.f32 	%f1572, %f1567, %f1571;
+	mul.f32 	%f1573, %f1938, %f1942;
+	mul.f32 	%f1574, %f1939, %f1941;
+	sub.f32 	%f1575, %f1574, %f1573;
+	fma.rn.f32 	%f1576, %f1575, %f1946, %f1572;
+	rcp.rn.f32 	%f1577, %f1576;
+	mul.f32 	%f1953, %f1566, %f1577;
+	mul.f32 	%f1578, %f1940, %f1945;
+	mul.f32 	%f1579, %f1939, %f1946;
+	sub.f32 	%f1580, %f1579, %f1578;
+	mul.f32 	%f1954, %f1580, %f1577;
+	mul.f32 	%f1581, %f1942, %f1946;
+	mul.f32 	%f1582, %f1943, %f1945;
+	sub.f32 	%f1583, %f1582, %f1581;
+	mul.f32 	%f1955, %f1583, %f1577;
+	sub.f32 	%f1584, %f1568, %f1569;
+	mul.f32 	%f1950, %f1584, %f1577;
+	mul.f32 	%f1585, %f1938, %f1946;
+	mul.f32 	%f1586, %f1940, %f1944;
+	sub.f32 	%f1587, %f1586, %f1585;
+	mul.f32 	%f1951, %f1587, %f1577;
+	mul.f32 	%f1588, %f1943, %f1944;
+	mul.f32 	%f1589, %f1941, %f1946;
+	sub.f32 	%f1590, %f1589, %f1588;
+	mul.f32 	%f1952, %f1590, %f1577;
+	mul.f32 	%f1947, %f1575, %f1577;
+	mul.f32 	%f1591, %f1939, %f1944;
+	mul.f32 	%f1592, %f1938, %f1945;
+	sub.f32 	%f1593, %f1592, %f1591;
+	mul.f32 	%f1948, %f1593, %f1577;
+	mul.f32 	%f1594, %f1941, %f1945;
+	mul.f32 	%f1595, %f1942, %f1944;
+	sub.f32 	%f1596, %f1595, %f1594;
+	mul.f32 	%f1949, %f1596, %f1577;
+	bra.uni 	$L__BB6_85;
+
+$L__BB6_76:
+	// begin inline asm
+	call (%rd655), _optix_get_instance_inverse_transform_from_handle, (%rd428);
+	// end inline asm
+
+$L__BB6_77:
+	// begin inline asm
+	cvta.to.global.u64 %rd434, %rd655;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r497,%r498,%r499,%r500}, [%rd434];
+	// end inline asm
+	mov.b32 	%f1953, %r497;
+	mov.b32 	%f1954, %r498;
+	mov.b32 	%f1955, %r499;
+	add.s64 	%rd438, %rd655, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd437, %rd438;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r501,%r502,%r503,%r504}, [%rd437];
+	// end inline asm
+	mov.b32 	%f1950, %r501;
+	mov.b32 	%f1951, %r502;
+	mov.b32 	%f1952, %r503;
+	add.s64 	%rd441, %rd655, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd440, %rd441;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r505,%r506,%r507,%r508}, [%rd440];
+	// end inline asm
+	mov.b32 	%f1947, %r505;
+	mov.b32 	%f1948, %r506;
+	mov.b32 	%f1949, %r507;
+
+$L__BB6_85:
+	setp.eq.s32 	%p45, %r649, 0;
+	@%p45 bra 	$L__BB6_87;
+
+	mul.f32 	%f1597, %f1924, %f1954;
+	fma.rn.f32 	%f1598, %f1921, %f1953, %f1597;
+	fma.rn.f32 	%f627, %f1927, %f1955, %f1598;
+	mul.f32 	%f1599, %f1923, %f1954;
+	fma.rn.f32 	%f1600, %f1920, %f1953, %f1599;
+	fma.rn.f32 	%f628, %f1926, %f1955, %f1600;
+	mul.f32 	%f1601, %f1922, %f1954;
+	fma.rn.f32 	%f1602, %f1919, %f1953, %f1601;
+	fma.rn.f32 	%f1955, %f1925, %f1955, %f1602;
+	mul.f32 	%f1603, %f1924, %f1951;
+	fma.rn.f32 	%f1604, %f1921, %f1950, %f1603;
+	fma.rn.f32 	%f630, %f1927, %f1952, %f1604;
+	mul.f32 	%f1605, %f1923, %f1951;
+	fma.rn.f32 	%f1606, %f1920, %f1950, %f1605;
+	fma.rn.f32 	%f631, %f1926, %f1952, %f1606;
+	mul.f32 	%f1607, %f1922, %f1951;
+	fma.rn.f32 	%f1608, %f1919, %f1950, %f1607;
+	fma.rn.f32 	%f1952, %f1925, %f1952, %f1608;
+	mul.f32 	%f1609, %f1924, %f1948;
+	fma.rn.f32 	%f1610, %f1921, %f1947, %f1609;
+	fma.rn.f32 	%f633, %f1927, %f1949, %f1610;
+	mul.f32 	%f1611, %f1923, %f1948;
+	fma.rn.f32 	%f1612, %f1920, %f1947, %f1611;
+	fma.rn.f32 	%f634, %f1926, %f1949, %f1612;
+	mul.f32 	%f1613, %f1922, %f1948;
+	fma.rn.f32 	%f1614, %f1919, %f1947, %f1613;
+	fma.rn.f32 	%f1949, %f1925, %f1949, %f1614;
+	mov.f32 	%f1947, %f633;
+	mov.f32 	%f1948, %f634;
+	mov.f32 	%f1950, %f630;
+	mov.f32 	%f1951, %f631;
+	mov.f32 	%f1953, %f627;
+	mov.f32 	%f1954, %f628;
+
+$L__BB6_87:
+	add.s32 	%r649, %r649, 1;
+	setp.lt.u32 	%p46, %r649, %r492;
+	mov.f32 	%f1919, %f1955;
+	mov.f32 	%f1920, %f1954;
+	mov.f32 	%f1921, %f1953;
+	mov.f32 	%f1922, %f1952;
+	mov.f32 	%f1923, %f1951;
+	mov.f32 	%f1924, %f1950;
+	mov.f32 	%f1925, %f1949;
+	mov.f32 	%f1926, %f1948;
+	mov.f32 	%f1927, %f1947;
+	@%p46 bra 	$L__BB6_72;
+
+$L__BB6_88:
+	fma.rn.f32 	%f1615, %f2004, %f1891, %f1894;
+	fma.rn.f32 	%f1616, %f2005, %f1892, %f1615;
+	fma.rn.f32 	%f1617, %f2004, %f1887, %f1890;
+	fma.rn.f32 	%f1618, %f2005, %f1888, %f1617;
+	fma.rn.f32 	%f1619, %f2004, %f1883, %f1886;
+	fma.rn.f32 	%f1620, %f2005, %f1884, %f1619;
+	fma.rn.f32 	%f2004, %f2006, %f1893, %f1616;
+	fma.rn.f32 	%f2005, %f2006, %f1889, %f1618;
+	fma.rn.f32 	%f2006, %f2006, %f1885, %f1620;
+	ld.const.u64 	%rd547, [params+112];
+	setp.eq.s64 	%p47, %rd547, 0;
+	mov.f32 	%f1998, %f2001;
+	mov.f32 	%f1999, %f2002;
+	mov.f32 	%f2000, %f2003;
+	@%p47 bra 	$L__BB6_90;
+
+	mul.f32 	%f1621, %f2001, %f1953;
+	fma.rn.f32 	%f1622, %f2002, %f1950, %f1621;
+	mul.f32 	%f1623, %f2001, %f1954;
+	fma.rn.f32 	%f1624, %f2002, %f1951, %f1623;
+	mul.f32 	%f1625, %f2001, %f1955;
+	fma.rn.f32 	%f1626, %f2002, %f1952, %f1625;
+	fma.rn.f32 	%f1627, %f2003, %f1947, %f1622;
+	fma.rn.f32 	%f1628, %f2003, %f1948, %f1624;
+	fma.rn.f32 	%f1629, %f2003, %f1949, %f1626;
+	mul.f32 	%f1630, %f1627, %f1627;
+	fma.rn.f32 	%f1631, %f1628, %f1628, %f1630;
+	fma.rn.f32 	%f1632, %f1629, %f1629, %f1631;
+	sqrt.rn.f32 	%f1633, %f1632;
+	div.rn.f32 	%f1998, %f1627, %f1633;
+	div.rn.f32 	%f1999, %f1628, %f1633;
+	div.rn.f32 	%f2000, %f1629, %f1633;
+
+$L__BB6_90:
+	ld.const.u64 	%rd548, [params+136];
+	setp.eq.s64 	%p48, %rd548, 0;
+	@%p48 bra 	$L__BB6_92;
+
+	mul.f32 	%f1634, %f2001, %f1953;
+	fma.rn.f32 	%f1635, %f2002, %f1950, %f1634;
+	mul.f32 	%f1636, %f2001, %f1954;
+	fma.rn.f32 	%f1637, %f2002, %f1951, %f1636;
+	mul.f32 	%f1638, %f2001, %f1955;
+	fma.rn.f32 	%f1639, %f2002, %f1952, %f1638;
+	fma.rn.f32 	%f1640, %f2003, %f1947, %f1635;
+	fma.rn.f32 	%f1641, %f2003, %f1948, %f1637;
+	fma.rn.f32 	%f1642, %f2003, %f1949, %f1639;
+	mul.f32 	%f1643, %f1640, %f1640;
+	fma.rn.f32 	%f1644, %f1641, %f1641, %f1643;
+	fma.rn.f32 	%f1645, %f1642, %f1642, %f1644;
+	sqrt.rn.f32 	%f1646, %f1645;
+	div.rn.f32 	%f2001, %f1640, %f1646;
+	div.rn.f32 	%f2002, %f1641, %f1646;
+	div.rn.f32 	%f2003, %f1642, %f1646;
+
+$L__BB6_92:
+	ld.const.u64 	%rd549, [params+184];
+	setp.eq.s64 	%p49, %rd549, 0;
+	@%p49 bra 	$L__BB6_94;
+
+	mul.f32 	%f1647, %f1995, %f1891;
+	fma.rn.f32 	%f1648, %f1996, %f1892, %f1647;
+	mul.f32 	%f1649, %f1995, %f1887;
+	fma.rn.f32 	%f1650, %f1996, %f1888, %f1649;
+	mul.f32 	%f1651, %f1995, %f1883;
+	fma.rn.f32 	%f1652, %f1996, %f1884, %f1651;
+	fma.rn.f32 	%f1995, %f1997, %f1893, %f1648;
+	fma.rn.f32 	%f1996, %f1997, %f1889, %f1650;
+	fma.rn.f32 	%f1997, %f1997, %f1885, %f1652;
+	mul.f32 	%f1653, %f1992, %f1891;
+	fma.rn.f32 	%f1654, %f1993, %f1892, %f1653;
+	mul.f32 	%f1655, %f1992, %f1887;
+	fma.rn.f32 	%f1656, %f1993, %f1888, %f1655;
+	mul.f32 	%f1657, %f1992, %f1883;
+	fma.rn.f32 	%f1658, %f1993, %f1884, %f1657;
+	fma.rn.f32 	%f1992, %f1994, %f1893, %f1654;
+	fma.rn.f32 	%f1993, %f1994, %f1889, %f1656;
+	fma.rn.f32 	%f1994, %f1994, %f1885, %f1658;
+
+$L__BB6_94:
+	ld.const.u64 	%rd550, [params+232];
+	ld.const.u64 	%rd551, [params+280];
+	or.b64  	%rd552, %rd550, %rd551;
+	setp.eq.s64 	%p50, %rd552, 0;
+	mov.f32 	%f1989, 0f00000000;
+	mov.f32 	%f1990, %f1989;
+	mov.f32 	%f1991, %f1989;
+	@%p50 bra 	$L__BB6_96;
+
+	mul.f32 	%f1662, %f2001, %f1891;
+	fma.rn.f32 	%f1663, %f2002, %f1887, %f1662;
+	mul.f32 	%f1664, %f2001, %f1892;
+	fma.rn.f32 	%f1665, %f2002, %f1888, %f1664;
+	mul.f32 	%f1666, %f2001, %f1893;
+	fma.rn.f32 	%f1667, %f2002, %f1889, %f1666;
+	fma.rn.f32 	%f1668, %f2003, %f1883, %f1663;
+	fma.rn.f32 	%f1669, %f2003, %f1884, %f1665;
+	fma.rn.f32 	%f1670, %f2003, %f1885, %f1667;
+	mul.f32 	%f1671, %f1668, %f1668;
+	fma.rn.f32 	%f1672, %f1669, %f1669, %f1671;
+	fma.rn.f32 	%f1673, %f1670, %f1670, %f1672;
+	sqrt.rn.f32 	%f1674, %f1673;
+	div.rn.f32 	%f1675, %f1668, %f1674;
+	div.rn.f32 	%f1676, %f1669, %f1674;
+	div.rn.f32 	%f1677, %f1670, %f1674;
+	mul.f32 	%f1678, %f1675, %f1953;
+	mul.f32 	%f1679, %f1675, %f1954;
+	mul.f32 	%f1680, %f1675, %f1955;
+	fma.rn.f32 	%f1681, %f1676, %f1950, %f1678;
+	fma.rn.f32 	%f1682, %f1676, %f1951, %f1679;
+	fma.rn.f32 	%f1683, %f1676, %f1952, %f1680;
+	fma.rn.f32 	%f1684, %f1677, %f1947, %f1681;
+	fma.rn.f32 	%f1685, %f1677, %f1948, %f1682;
+	fma.rn.f32 	%f1686, %f1677, %f1949, %f1683;
+	mul.f32 	%f1687, %f1684, %f1684;
+	fma.rn.f32 	%f1688, %f1685, %f1685, %f1687;
+	fma.rn.f32 	%f1689, %f1686, %f1686, %f1688;
+	sqrt.rn.f32 	%f1690, %f1689;
+	rcp.rn.f32 	%f1691, %f1690;
+	mul.f32 	%f1692, %f1691, %f1684;
+	mul.f32 	%f1693, %f1691, %f1685;
+	mul.f32 	%f1694, %f1691, %f1686;
+	mul.f32 	%f1695, %f1953, 0f00000000;
+	mov.f32 	%f1696, 0f00000000;
+	fma.rn.f32 	%f1697, %f1696, %f1950, %f1695;
+	mul.f32 	%f1698, %f1954, 0f00000000;
+	fma.rn.f32 	%f1699, %f1696, %f1951, %f1698;
+	mul.f32 	%f1700, %f1955, 0f00000000;
+	fma.rn.f32 	%f1701, %f1696, %f1952, %f1700;
+	fma.rn.f32 	%f1702, %f1696, %f1947, %f1697;
+	fma.rn.f32 	%f1703, %f1696, %f1948, %f1699;
+	fma.rn.f32 	%f1704, %f1696, %f1949, %f1701;
+	mul.f32 	%f1705, %f1702, %f1691;
+	mul.f32 	%f1706, %f1703, %f1691;
+	mul.f32 	%f1707, %f1704, %f1691;
+	mul.f32 	%f1708, %f1692, %f1705;
+	fma.rn.f32 	%f1709, %f1693, %f1706, %f1708;
+	fma.rn.f32 	%f1710, %f1694, %f1707, %f1709;
+	mul.f32 	%f1711, %f1692, %f1710;
+	mul.f32 	%f1712, %f1693, %f1710;
+	mul.f32 	%f1713, %f1694, %f1710;
+	sub.f32 	%f1989, %f1705, %f1711;
+	sub.f32 	%f1990, %f1706, %f1712;
+	sub.f32 	%f1991, %f1707, %f1713;
+
+$L__BB6_96:
+	st.global.u32 	[%rd23], %r338;
+
+$L__BB6_97:
+	ld.const.u64 	%rd553, [params+328];
+	cvta.to.global.u64 	%rd554, %rd553;
+	shl.b64 	%rd555, %rd20, 3;
+	add.s64 	%rd556, %rd554, %rd555;
+	st.global.u64 	[%rd556], %rd22;
+	ld.const.u64 	%rd557, [params+336];
+	cvta.to.global.u64 	%rd558, %rd557;
+	add.s64 	%rd560, %rd558, %rd308;
+	mov.u32 	%r643, 0;
+	st.global.u32 	[%rd560], %r643;
+	ld.const.u64 	%rd561, [params+160];
+	cvta.to.global.u64 	%rd562, %rd561;
+	add.s64 	%rd563, %rd562, %rd308;
+	st.global.f32 	[%rd563], %f2004;
+	ld.const.u64 	%rd564, [params+168];
+	cvta.to.global.u64 	%rd565, %rd564;
+	add.s64 	%rd566, %rd565, %rd308;
+	st.global.f32 	[%rd566], %f2005;
+	ld.const.u64 	%rd567, [params+176];
+	cvta.to.global.u64 	%rd568, %rd567;
+	add.s64 	%rd569, %rd568, %rd308;
+	st.global.f32 	[%rd569], %f2006;
+	ld.const.u64 	%rd570, [params+72];
+	cvta.to.global.u64 	%rd571, %rd570;
+	add.s64 	%rd572, %rd571, %rd308;
+	st.global.f32 	[%rd572], %f346;
+	ld.const.u64 	%rd38, [params+96];
+	setp.eq.s64 	%p51, %rd38, 0;
+	@%p51 bra 	$L__BB6_99;
+
+	cvta.to.global.u64 	%rd573, %rd38;
+	add.s64 	%rd575, %rd573, %rd308;
+	st.global.f32 	[%rd575], %f352;
+	ld.const.u64 	%rd576, [params+104];
+	cvta.to.global.u64 	%rd577, %rd576;
+	add.s64 	%rd578, %rd577, %rd308;
+	st.global.f32 	[%rd578], %f353;
+
+$L__BB6_99:
+	ld.const.u64 	%rd39, [params+112];
+	setp.eq.s64 	%p52, %rd39, 0;
+	@%p52 bra 	$L__BB6_101;
+
+	cvta.to.global.u64 	%rd579, %rd39;
+	add.s64 	%rd581, %rd579, %rd308;
+	st.global.f32 	[%rd581], %f1998;
+	ld.const.u64 	%rd582, [params+120];
+	cvta.to.global.u64 	%rd583, %rd582;
+	add.s64 	%rd584, %rd583, %rd308;
+	st.global.f32 	[%rd584], %f1999;
+	ld.const.u64 	%rd585, [params+128];
+	cvta.to.global.u64 	%rd586, %rd585;
+	add.s64 	%rd587, %rd586, %rd308;
+	st.global.f32 	[%rd587], %f2000;
+
+$L__BB6_101:
+	ld.const.u64 	%rd40, [params+136];
+	setp.eq.s64 	%p53, %rd40, 0;
+	@%p53 bra 	$L__BB6_103;
+
+	cvta.to.global.u64 	%rd588, %rd40;
+	add.s64 	%rd590, %rd588, %rd308;
+	st.global.f32 	[%rd590], %f2001;
+	ld.const.u64 	%rd591, [params+144];
+	cvta.to.global.u64 	%rd592, %rd591;
+	add.s64 	%rd593, %rd592, %rd308;
+	st.global.f32 	[%rd593], %f2002;
+	ld.const.u64 	%rd594, [params+152];
+	cvta.to.global.u64 	%rd595, %rd594;
+	add.s64 	%rd596, %rd595, %rd308;
+	st.global.f32 	[%rd596], %f2003;
+
+$L__BB6_103:
+	ld.const.u64 	%rd41, [params+184];
+	setp.eq.s64 	%p54, %rd41, 0;
+	@%p54 bra 	$L__BB6_105;
+
+	cvta.to.global.u64 	%rd597, %rd41;
+	add.s64 	%rd599, %rd597, %rd308;
+	st.global.f32 	[%rd599], %f1995;
+	ld.const.u64 	%rd600, [params+192];
+	cvta.to.global.u64 	%rd601, %rd600;
+	add.s64 	%rd602, %rd601, %rd308;
+	st.global.f32 	[%rd602], %f1996;
+	ld.const.u64 	%rd603, [params+200];
+	cvta.to.global.u64 	%rd604, %rd603;
+	add.s64 	%rd605, %rd604, %rd308;
+	st.global.f32 	[%rd605], %f1997;
+	ld.const.u64 	%rd606, [params+208];
+	cvta.to.global.u64 	%rd607, %rd606;
+	add.s64 	%rd608, %rd607, %rd308;
+	st.global.f32 	[%rd608], %f1992;
+	ld.const.u64 	%rd609, [params+216];
+	cvta.to.global.u64 	%rd610, %rd609;
+	add.s64 	%rd611, %rd610, %rd308;
+	st.global.f32 	[%rd611], %f1993;
+	ld.const.u64 	%rd612, [params+224];
+	cvta.to.global.u64 	%rd613, %rd612;
+	add.s64 	%rd614, %rd613, %rd308;
+	st.global.f32 	[%rd614], %f1994;
+
+$L__BB6_105:
+	ld.const.u64 	%rd42, [params+232];
+	setp.eq.s64 	%p55, %rd42, 0;
+	@%p55 bra 	$L__BB6_107;
+
+	cvta.to.global.u64 	%rd615, %rd42;
+	add.s64 	%rd617, %rd615, %rd308;
+	st.global.f32 	[%rd617], %f1989;
+	ld.const.u64 	%rd618, [params+240];
+	cvta.to.global.u64 	%rd619, %rd618;
+	add.s64 	%rd620, %rd619, %rd308;
+	st.global.f32 	[%rd620], %f1990;
+	ld.const.u64 	%rd621, [params+248];
+	cvta.to.global.u64 	%rd622, %rd621;
+	add.s64 	%rd623, %rd622, %rd308;
+	st.global.f32 	[%rd623], %f1991;
+	ld.const.u64 	%rd624, [params+256];
+	cvta.to.global.u64 	%rd625, %rd624;
+	add.s64 	%rd626, %rd625, %rd308;
+	st.global.f32 	[%rd626], %f1989;
+	ld.const.u64 	%rd627, [params+264];
+	cvta.to.global.u64 	%rd628, %rd627;
+	add.s64 	%rd629, %rd628, %rd308;
+	st.global.f32 	[%rd629], %f1990;
+	ld.const.u64 	%rd630, [params+272];
+	cvta.to.global.u64 	%rd631, %rd630;
+	add.s64 	%rd632, %rd631, %rd308;
+	st.global.f32 	[%rd632], %f1991;
+
+$L__BB6_107:
+	ld.const.u64 	%rd43, [params+280];
+	setp.eq.s64 	%p56, %rd43, 0;
+	@%p56 bra 	$L__BB6_109;
+
+	cvta.to.global.u64 	%rd633, %rd43;
+	add.s64 	%rd635, %rd633, %rd308;
+	st.global.f32 	[%rd635], %f1989;
+	ld.const.u64 	%rd636, [params+288];
+	cvta.to.global.u64 	%rd637, %rd636;
+	add.s64 	%rd638, %rd637, %rd308;
+	st.global.f32 	[%rd638], %f1990;
+	ld.const.u64 	%rd639, [params+296];
+	cvta.to.global.u64 	%rd640, %rd639;
+	add.s64 	%rd641, %rd640, %rd308;
+	st.global.f32 	[%rd641], %f1991;
+	ld.const.u64 	%rd642, [params+304];
+	cvta.to.global.u64 	%rd643, %rd642;
+	add.s64 	%rd644, %rd643, %rd308;
+	st.global.f32 	[%rd644], %f1989;
+	ld.const.u64 	%rd645, [params+312];
+	cvta.to.global.u64 	%rd646, %rd645;
+	add.s64 	%rd647, %rd646, %rd308;
+	st.global.f32 	[%rd647], %f1990;
+	ld.const.u64 	%rd648, [params+320];
+	cvta.to.global.u64 	%rd649, %rd648;
+	add.s64 	%rd650, %rd649, %rd308;
+	st.global.f32 	[%rd650], %f1991;
+
+$L__BB6_109:
+	ret;
+
+}
+	// .globl	__intersection__sphere
+.visible .entry __intersection__sphere()
 {
-	.reg .pred 	%p<39>;
-	.reg .b16 	%rs<14>;
-	.reg .f32 	%f<921>;
-	.reg .b32 	%r<320>;
-	.reg .b64 	%rd<265>;
-
-
-	// inline asm
-	call (%rd18), _optix_get_sbt_data_ptr_64, ();
-	// inline asm
-	ld.u64 	%rd1, [%rd18+8];
-	// inline asm
-	call (%f325), _optix_get_world_ray_origin_x, ();
-	// inline asm
-	// inline asm
-	call (%f326), _optix_get_world_ray_origin_y, ();
-	// inline asm
-	// inline asm
-	call (%f867), _optix_get_world_ray_origin_z, ();
-	// inline asm
-	// inline asm
-	call (%r8), _optix_get_transform_list_size, ();
-	// inline asm
-	setp.eq.s32	%p3, %r8, 0;
-	@%p3 bra 	BB7_1;
-
-	mov.u32 	%r318, 0;
-	// inline asm
-	call (%f328), _optix_get_ray_time, ();
-	// inline asm
-
-BB7_3:
+	.reg .pred 	%p<43>;
+	.reg .f32 	%f<971>;
+	.reg .b32 	%r<323>;
+	.reg .b64 	%rd<258>;
+
+
+	// begin inline asm
+	call (%rd16), _optix_get_sbt_data_ptr_64, ();
+	// end inline asm
+	ld.u64 	%rd1, [%rd16+8];
+	// begin inline asm
+	call (%f908), _optix_get_world_ray_origin_x, ();
+	// end inline asm
+	// begin inline asm
+	call (%f909), _optix_get_world_ray_origin_y, ();
+	// end inline asm
+	// begin inline asm
+	call (%f910), _optix_get_world_ray_origin_z, ();
+	// end inline asm
+	// begin inline asm
+	call (%r9), _optix_get_transform_list_size, ();
+	// end inline asm
+	setp.eq.s32 	%p2, %r9, 0;
+	@%p2 bra 	$L__BB7_21;
+
+	// begin inline asm
+	call (%r10), _optix_get_transform_list_size, ();
+	// end inline asm
+	// begin inline asm
+	call (%f355), _optix_get_ray_time, ();
+	// end inline asm
+	setp.eq.s32 	%p3, %r10, 0;
+	@%p3 bra 	$L__BB7_19;
+
+	mov.u32 	%r321, 0;
+
+$L__BB7_3:
 	.pragma "nounroll";
-	// inline asm
-	call (%rd19), _optix_get_transform_list_handle, (%r318);
-	// inline asm
-	// inline asm
-	call (%r11), _optix_get_transform_type_from_handle, (%rd19);
-	// inline asm
-	and.b32  	%r12, %r11, -2;
-	setp.eq.s32	%p4, %r12, 2;
-	@%p4 bra 	BB7_9;
-	bra.uni 	BB7_4;
-
-BB7_9:
-	setp.eq.s32	%p7, %r11, 2;
-	@%p7 bra 	BB7_13;
-	bra.uni 	BB7_10;
-
-BB7_13:
-	// inline asm
-	call (%rd93), _optix_get_matrix_motion_transform_from_handle, (%rd19);
-	// inline asm
-	// inline asm
-	cvta.to.global.u64 %rd95, %rd93;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r100,%r101,%r102,%r103}, [%rd95];
-	// inline asm
-	mov.b32	{%rs4, %rs5}, %r102;
-	add.s64 	%rd99, %rd93, 16;
-	// inline asm
-	cvta.to.global.u64 %rd98, %rd99;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r104,%r105,%r106,%r107}, [%rd98];
-	// inline asm
-	add.s64 	%rd102, %rd93, 32;
-	// inline asm
-	cvta.to.global.u64 %rd101, %rd102;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r108,%r109,%r110,%r111}, [%rd101];
-	// inline asm
-	add.s64 	%rd105, %rd93, 48;
-	// inline asm
-	cvta.to.global.u64 %rd104, %rd105;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r112,%r113,%r114,%r115}, [%rd104];
-	// inline asm
-	add.s64 	%rd108, %rd93, 64;
-	// inline asm
-	cvta.to.global.u64 %rd107, %rd108;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r116,%r117,%r118,%r119}, [%rd107];
-	// inline asm
-	add.s64 	%rd111, %rd93, 80;
-	// inline asm
-	cvta.to.global.u64 %rd110, %rd111;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r120,%r121,%r122,%r123}, [%rd110];
-	// inline asm
-	add.s64 	%rd114, %rd93, 96;
-	// inline asm
-	cvta.to.global.u64 %rd113, %rd114;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r124,%r125,%r126,%r127}, [%rd113];
-	// inline asm
-	add.s64 	%rd117, %rd93, 112;
-	// inline asm
-	cvta.to.global.u64 %rd116, %rd117;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r128,%r129,%r130,%r131}, [%rd116];
-	// inline asm
-	mov.b32 	 %f455, %r103;
-	mov.b32 	 %f456, %r104;
-	cvt.u32.u16	%r144, %rs4;
-	add.s32 	%r145, %r144, -1;
-	cvt.rn.f32.s32	%f457, %r145;
-	sub.f32 	%f458, %f328, %f455;
-	mul.f32 	%f459, %f458, %f457;
-	sub.f32 	%f460, %f456, %f455;
-	div.rn.f32 	%f461, %f459, %f460;
-	min.f32 	%f462, %f457, %f461;
-	mov.f32 	%f463, 0f00000000;
-	max.f32 	%f464, %f463, %f462;
-	cvt.rmi.f32.f32	%f465, %f464;
-	cvt.rzi.s32.f32	%r146, %f465;
-	mul.wide.s32 	%rd128, %r146, 48;
-	add.s64 	%rd120, %rd102, %rd128;
-	// inline asm
-	cvta.to.global.u64 %rd119, %rd120;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r132,%r133,%r134,%r135}, [%rd119];
-	// inline asm
-	mov.b32 	 %f839, %r132;
-	mov.b32 	 %f840, %r133;
-	mov.b32 	 %f841, %r134;
-	mov.b32 	 %f842, %r135;
-	add.s64 	%rd123, %rd120, 16;
-	// inline asm
-	cvta.to.global.u64 %rd122, %rd123;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r136,%r137,%r138,%r139}, [%rd122];
-	// inline asm
-	mov.b32 	 %f835, %r136;
-	mov.b32 	 %f836, %r137;
-	mov.b32 	 %f837, %r138;
-	mov.b32 	 %f838, %r139;
-	add.s64 	%rd126, %rd120, 32;
-	// inline asm
+	// begin inline asm
+	call (%rd17), _optix_get_transform_list_handle, (%r321);
+	// end inline asm
+	// begin inline asm
+	call (%r13), _optix_get_transform_type_from_handle, (%rd17);
+	// end inline asm
+	or.b32  	%r14, %r13, 1;
+	setp.eq.s32 	%p4, %r14, 3;
+	@%p4 bra 	$L__BB7_9;
+	bra.uni 	$L__BB7_4;
+
+$L__BB7_9:
+	setp.eq.s32 	%p7, %r13, 2;
+	@%p7 bra 	$L__BB7_13;
+	bra.uni 	$L__BB7_10;
+
+$L__BB7_13:
+	// begin inline asm
+	call (%rd89), _optix_get_matrix_motion_transform_from_handle, (%rd17);
+	// end inline asm
+	// begin inline asm
+	cvta.to.global.u64 %rd91, %rd89;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r102,%r103,%r104,%r105}, [%rd91];
+	// end inline asm
+	add.s64 	%rd95, %rd89, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd94, %rd95;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r106,%r107,%r108,%r109}, [%rd94];
+	// end inline asm
+	add.s64 	%rd98, %rd89, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd97, %rd98;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r110,%r111,%r112,%r113}, [%rd97];
+	// end inline asm
+	add.s64 	%rd101, %rd89, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd100, %rd101;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r114,%r115,%r116,%r117}, [%rd100];
+	// end inline asm
+	add.s64 	%rd104, %rd89, 64;
+	// begin inline asm
+	cvta.to.global.u64 %rd103, %rd104;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r118,%r119,%r120,%r121}, [%rd103];
+	// end inline asm
+	add.s64 	%rd107, %rd89, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd106, %rd107;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r122,%r123,%r124,%r125}, [%rd106];
+	// end inline asm
+	add.s64 	%rd110, %rd89, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd109, %rd110;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r126,%r127,%r128,%r129}, [%rd109];
+	// end inline asm
+	add.s64 	%rd113, %rd89, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd112, %rd113;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r130,%r131,%r132,%r133}, [%rd112];
+	// end inline asm
+	mov.b32 	%f483, %r105;
+	mov.b32 	%f484, %r106;
+	and.b32  	%r146, %r104, 65535;
+	add.s32 	%r147, %r146, -1;
+	cvt.rn.f32.s32 	%f485, %r147;
+	sub.f32 	%f486, %f355, %f483;
+	mul.f32 	%f487, %f486, %f485;
+	sub.f32 	%f488, %f484, %f483;
+	div.rn.f32 	%f489, %f487, %f488;
+	min.f32 	%f490, %f485, %f489;
+	mov.f32 	%f491, 0f00000000;
+	max.f32 	%f492, %f491, %f490;
+	cvt.rmi.f32.f32 	%f493, %f492;
+	sub.f32 	%f90, %f492, %f493;
+	cvt.rzi.s32.f32 	%r148, %f493;
+	mul.wide.s32 	%rd124, %r148, 48;
+	add.s64 	%rd116, %rd98, %rd124;
+	// begin inline asm
+	cvta.to.global.u64 %rd115, %rd116;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r134,%r135,%r136,%r137}, [%rd115];
+	// end inline asm
+	mov.b32 	%f863, %r134;
+	mov.b32 	%f862, %r135;
+	mov.b32 	%f861, %r136;
+	mov.b32 	%f860, %r137;
+	add.s64 	%rd119, %rd116, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd118, %rd119;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r138,%r139,%r140,%r141}, [%rd118];
+	// end inline asm
+	mov.b32 	%f867, %r138;
+	mov.b32 	%f866, %r139;
+	mov.b32 	%f865, %r140;
+	mov.b32 	%f864, %r141;
+	add.s64 	%rd122, %rd116, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd121, %rd122;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r142,%r143,%r144,%r145}, [%rd121];
+	// end inline asm
+	mov.b32 	%f871, %r142;
+	mov.b32 	%f870, %r143;
+	mov.b32 	%f869, %r144;
+	mov.b32 	%f868, %r145;
+	setp.leu.f32 	%p9, %f90, 0f00000000;
+	@%p9 bra 	$L__BB7_15;
+
+	cvt.rmi.f32.f32 	%f831, %f492;
+	cvt.rzi.s32.f32 	%r320, %f831;
+	cvt.s64.s32 	%rd255, %r320;
+	mov.f32 	%f494, 0f3F800000;
+	sub.f32 	%f495, %f494, %f90;
+	mul.lo.s64 	%rd134, %rd255, 48;
+	add.s64 	%rd135, %rd89, %rd134;
+	add.s64 	%rd126, %rd135, 80;
+	// begin inline asm
 	cvta.to.global.u64 %rd125, %rd126;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r140,%r141,%r142,%r143}, [%rd125];
-	// inline asm
-	sub.f32 	%f98, %f464, %f465;
-	mov.b32 	 %f831, %r140;
-	mov.b32 	 %f832, %r141;
-	mov.b32 	 %f833, %r142;
-	mov.b32 	 %f834, %r143;
-	setp.leu.f32	%p9, %f98, 0f00000000;
-	@%p9 bra 	BB7_15;
-
-	cvt.rmi.f32.f32	%f802, %f464;
-	cvt.rzi.s32.f32	%r317, %f802;
-	cvt.s64.s32	%rd262, %r317;
-	mul.lo.s64 	%rd138, %rd262, 48;
-	add.s64 	%rd139, %rd93, %rd138;
-	add.s64 	%rd130, %rd139, 80;
-	// inline asm
-	cvta.to.global.u64 %rd129, %rd130;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r147,%r148,%r149,%r150}, [%rd129];
-	// inline asm
-	mov.b32 	 %f466, %r147;
-	mov.b32 	 %f467, %r148;
-	mov.b32 	 %f468, %r149;
-	mov.b32 	 %f469, %r150;
-	add.s64 	%rd133, %rd139, 96;
-	// inline asm
-	cvta.to.global.u64 %rd132, %rd133;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r151,%r152,%r153,%r154}, [%rd132];
-	// inline asm
-	mov.b32 	 %f470, %r151;
-	mov.b32 	 %f471, %r152;
-	mov.b32 	 %f472, %r153;
-	mov.b32 	 %f473, %r154;
-	add.s64 	%rd136, %rd139, 112;
-	// inline asm
-	cvta.to.global.u64 %rd135, %rd136;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r155,%r156,%r157,%r158}, [%rd135];
-	// inline asm
-	mov.f32 	%f474, 0f3F800000;
-	sub.f32 	%f475, %f474, %f98;
-	mul.f32 	%f476, %f98, %f466;
-	mul.f32 	%f477, %f98, %f467;
-	mul.f32 	%f478, %f98, %f468;
-	mul.f32 	%f479, %f98, %f469;
-	fma.rn.f32 	%f839, %f475, %f839, %f476;
-	fma.rn.f32 	%f840, %f475, %f840, %f477;
-	fma.rn.f32 	%f841, %f475, %f841, %f478;
-	fma.rn.f32 	%f842, %f475, %f842, %f479;
-	mul.f32 	%f480, %f98, %f470;
-	mul.f32 	%f481, %f98, %f471;
-	mul.f32 	%f482, %f98, %f472;
-	mul.f32 	%f483, %f98, %f473;
-	fma.rn.f32 	%f835, %f475, %f835, %f480;
-	fma.rn.f32 	%f836, %f475, %f836, %f481;
-	fma.rn.f32 	%f837, %f475, %f837, %f482;
-	fma.rn.f32 	%f838, %f475, %f838, %f483;
-	mov.b32 	 %f484, %r155;
-	mov.b32 	 %f485, %r156;
-	mov.b32 	 %f486, %r157;
-	mov.b32 	 %f487, %r158;
-	mul.f32 	%f488, %f98, %f484;
-	mul.f32 	%f489, %f98, %f485;
-	mul.f32 	%f490, %f98, %f486;
-	mul.f32 	%f491, %f98, %f487;
-	fma.rn.f32 	%f831, %f475, %f831, %f488;
-	fma.rn.f32 	%f832, %f475, %f832, %f489;
-	fma.rn.f32 	%f833, %f475, %f833, %f490;
-	fma.rn.f32 	%f834, %f475, %f834, %f491;
-	bra.uni 	BB7_15;
-
-BB7_4:
-	mov.f32 	%f843, 0f00000000;
-	mov.f32 	%f845, 0f3F800000;
-	setp.eq.s32	%p5, %r11, 4;
-	@%p5 bra 	BB7_7;
-	bra.uni 	BB7_5;
-
-BB7_7:
-	// inline asm
-	call (%rd263), _optix_get_instance_inverse_transform_from_handle, (%rd19);
-	// inline asm
-	bra.uni 	BB7_8;
-
-BB7_10:
-	// inline asm
-	call (%rd34), _optix_get_srt_motion_transform_from_handle, (%rd19);
-	// inline asm
-	// inline asm
-	cvta.to.global.u64 %rd36, %rd34;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r25,%r26,%r27,%r28}, [%rd36];
-	// inline asm
-	mov.b32	{%rs2, %rs3}, %r27;
-	add.s64 	%rd40, %rd34, 16;
-	// inline asm
-	cvta.to.global.u64 %rd39, %rd40;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r29,%r30,%r31,%r32}, [%rd39];
-	// inline asm
-	add.s64 	%rd43, %rd34, 32;
-	// inline asm
-	cvta.to.global.u64 %rd42, %rd43;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r33,%r34,%r35,%r36}, [%rd42];
-	// inline asm
-	add.s64 	%rd46, %rd34, 48;
-	// inline asm
-	cvta.to.global.u64 %rd45, %rd46;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r37,%r38,%r39,%r40}, [%rd45];
-	// inline asm
-	add.s64 	%rd49, %rd34, 64;
-	// inline asm
-	cvta.to.global.u64 %rd48, %rd49;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r41,%r42,%r43,%r44}, [%rd48];
-	// inline asm
-	add.s64 	%rd52, %rd34, 80;
-	// inline asm
-	cvta.to.global.u64 %rd51, %rd52;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r45,%r46,%r47,%r48}, [%rd51];
-	// inline asm
-	add.s64 	%rd55, %rd34, 96;
-	// inline asm
-	cvta.to.global.u64 %rd54, %rd55;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r49,%r50,%r51,%r52}, [%rd54];
-	// inline asm
-	add.s64 	%rd58, %rd34, 112;
-	// inline asm
-	cvta.to.global.u64 %rd57, %rd58;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r53,%r54,%r55,%r56}, [%rd57];
-	// inline asm
-	add.s64 	%rd61, %rd34, 128;
-	// inline asm
-	cvta.to.global.u64 %rd60, %rd61;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r57,%r58,%r59,%r60}, [%rd60];
-	// inline asm
-	add.s64 	%rd64, %rd34, 144;
-	// inline asm
-	cvta.to.global.u64 %rd63, %rd64;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r61,%r62,%r63,%r64}, [%rd63];
-	// inline asm
-	mov.b32 	 %f342, %r28;
-	mov.b32 	 %f343, %r29;
-	cvt.u32.u16	%r81, %rs2;
-	add.s32 	%r82, %r81, -1;
-	cvt.rn.f32.s32	%f344, %r82;
-	sub.f32 	%f345, %f328, %f342;
-	mul.f32 	%f346, %f345, %f344;
-	sub.f32 	%f347, %f343, %f342;
-	div.rn.f32 	%f348, %f346, %f347;
-	min.f32 	%f349, %f344, %f348;
-	mov.f32 	%f350, 0f00000000;
-	max.f32 	%f351, %f350, %f349;
-	cvt.rmi.f32.f32	%f352, %f351;
-	cvt.rzi.s32.f32	%r83, %f352;
-	mul.wide.s32 	%rd78, %r83, 64;
-	add.s64 	%rd67, %rd43, %rd78;
-	// inline asm
-	cvta.to.global.u64 %rd66, %rd67;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r65,%r66,%r67,%r68}, [%rd66];
-	// inline asm
-	mov.b32 	 %f815, %r65;
-	mov.b32 	 %f816, %r66;
-	mov.b32 	 %f817, %r67;
-	mov.b32 	 %f818, %r68;
-	add.s64 	%rd70, %rd67, 16;
-	// inline asm
-	cvta.to.global.u64 %rd69, %rd70;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r69,%r70,%r71,%r72}, [%rd69];
-	// inline asm
-	mov.b32 	 %f819, %r69;
-	mov.b32 	 %f820, %r70;
-	mov.b32 	 %f821, %r71;
-	mov.b32 	 %f822, %r72;
-	add.s64 	%rd73, %rd67, 32;
-	// inline asm
-	cvta.to.global.u64 %rd72, %rd73;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r73,%r74,%r75,%r76}, [%rd72];
-	// inline asm
-	sub.f32 	%f37, %f351, %f352;
-	mov.b32 	 %f823, %r73;
-	mov.b32 	 %f824, %r74;
-	mov.b32 	 %f825, %r75;
-	mov.b32 	 %f826, %r76;
-	add.s64 	%rd76, %rd67, 48;
-	// inline asm
-	cvta.to.global.u64 %rd75, %rd76;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r77,%r78,%r79,%r80}, [%rd75];
-	// inline asm
-	mov.b32 	 %f827, %r77;
-	mov.b32 	 %f828, %r78;
-	mov.b32 	 %f829, %r79;
-	mov.b32 	 %f830, %r80;
-	setp.leu.f32	%p8, %f37, 0f00000000;
-	@%p8 bra 	BB7_12;
-
-	cvt.rmi.f32.f32	%f801, %f351;
-	cvt.rzi.s32.f32	%r316, %f801;
-	cvt.s64.s32	%rd261, %r316;
-	shl.b64 	%rd91, %rd261, 6;
-	add.s64 	%rd92, %rd91, %rd34;
-	add.s64 	%rd80, %rd92, 96;
-	// inline asm
-	cvta.to.global.u64 %rd79, %rd80;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r84,%r85,%r86,%r87}, [%rd79];
-	// inline asm
-	mov.b32 	 %f353, %r84;
-	mov.b32 	 %f354, %r85;
-	mov.b32 	 %f355, %r86;
-	mov.b32 	 %f356, %r87;
-	add.s64 	%rd83, %rd92, 112;
-	// inline asm
-	cvta.to.global.u64 %rd82, %rd83;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r88,%r89,%r90,%r91}, [%rd82];
-	// inline asm
-	mov.b32 	 %f357, %r88;
-	mov.b32 	 %f358, %r89;
-	mov.b32 	 %f359, %r90;
-	mov.b32 	 %f360, %r91;
-	add.s64 	%rd86, %rd92, 128;
-	// inline asm
-	cvta.to.global.u64 %rd85, %rd86;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r92,%r93,%r94,%r95}, [%rd85];
-	// inline asm
-	mov.b32 	 %f361, %r92;
-	mov.b32 	 %f362, %r93;
-	mov.b32 	 %f363, %r94;
-	mov.b32 	 %f364, %r95;
-	add.s64 	%rd89, %rd92, 144;
-	// inline asm
-	cvta.to.global.u64 %rd88, %rd89;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r96,%r97,%r98,%r99}, [%rd88];
-	// inline asm
-	mov.f32 	%f365, 0f3F800000;
-	sub.f32 	%f366, %f365, %f37;
-	mul.f32 	%f367, %f37, %f353;
-	mul.f32 	%f368, %f37, %f354;
-	mul.f32 	%f369, %f37, %f355;
-	mul.f32 	%f370, %f37, %f356;
-	fma.rn.f32 	%f815, %f366, %f815, %f367;
-	fma.rn.f32 	%f816, %f366, %f816, %f368;
-	fma.rn.f32 	%f817, %f366, %f817, %f369;
-	fma.rn.f32 	%f818, %f366, %f818, %f370;
-	mul.f32 	%f371, %f37, %f357;
-	mul.f32 	%f372, %f37, %f358;
-	mul.f32 	%f373, %f37, %f359;
-	mul.f32 	%f374, %f37, %f360;
-	fma.rn.f32 	%f819, %f366, %f819, %f371;
-	fma.rn.f32 	%f820, %f366, %f820, %f372;
-	fma.rn.f32 	%f821, %f366, %f821, %f373;
-	fma.rn.f32 	%f822, %f366, %f822, %f374;
-	mul.f32 	%f375, %f37, %f361;
-	mul.f32 	%f376, %f37, %f362;
-	mul.f32 	%f377, %f37, %f363;
-	mul.f32 	%f378, %f37, %f364;
-	fma.rn.f32 	%f823, %f366, %f823, %f375;
-	fma.rn.f32 	%f379, %f366, %f824, %f376;
-	fma.rn.f32 	%f380, %f366, %f825, %f377;
-	fma.rn.f32 	%f381, %f366, %f826, %f378;
-	mov.b32 	 %f382, %r96;
-	mov.b32 	 %f383, %r97;
-	mov.b32 	 %f384, %r98;
-	mov.b32 	 %f385, %r99;
-	mul.f32 	%f386, %f37, %f382;
-	mul.f32 	%f387, %f37, %f383;
-	mul.f32 	%f388, %f37, %f384;
-	mul.f32 	%f389, %f37, %f385;
-	fma.rn.f32 	%f390, %f366, %f827, %f386;
-	fma.rn.f32 	%f828, %f366, %f828, %f387;
-	fma.rn.f32 	%f829, %f366, %f829, %f388;
-	fma.rn.f32 	%f830, %f366, %f830, %f389;
-	mul.f32 	%f391, %f380, %f380;
-	fma.rn.f32 	%f392, %f379, %f379, %f391;
-	fma.rn.f32 	%f393, %f381, %f381, %f392;
-	fma.rn.f32 	%f394, %f390, %f390, %f393;
-	sqrt.rn.f32 	%f395, %f394;
-	rcp.rn.f32 	%f396, %f395;
-	mul.f32 	%f824, %f379, %f396;
-	mul.f32 	%f825, %f380, %f396;
-	mul.f32 	%f826, %f381, %f396;
-	mul.f32 	%f827, %f390, %f396;
-
-BB7_12:
-	mul.f32 	%f397, %f825, %f825;
-	fma.rn.f32 	%f398, %f824, %f824, %f397;
-	fma.rn.f32 	%f399, %f826, %f826, %f398;
-	fma.rn.f32 	%f400, %f827, %f827, %f399;
-	rcp.rn.f32 	%f401, %f400;
-	mul.f32 	%f402, %f824, %f401;
-	mul.f32 	%f403, %f825, %f401;
-	mul.f32 	%f404, %f826, %f401;
-	mul.f32 	%f405, %f827, %f401;
-	mul.f32 	%f406, %f824, %f402;
-	mul.f32 	%f407, %f825, %f403;
-	mul.f32 	%f408, %f826, %f404;
-	mul.f32 	%f409, %f824, %f403;
-	mul.f32 	%f410, %f826, %f405;
-	mul.f32 	%f411, %f824, %f404;
-	mul.f32 	%f412, %f825, %f405;
-	mul.f32 	%f413, %f825, %f404;
-	mul.f32 	%f414, %f824, %f405;
-	sub.f32 	%f415, %f406, %f407;
-	sub.f32 	%f416, %f415, %f408;
-	fma.rn.f32 	%f417, %f827, %f405, %f416;
-	sub.f32 	%f418, %f409, %f410;
-	add.f32 	%f419, %f418, %f418;
-	add.f32 	%f420, %f411, %f412;
-	add.f32 	%f421, %f420, %f420;
-	add.f32 	%f422, %f409, %f410;
-	add.f32 	%f423, %f422, %f422;
-	sub.f32 	%f424, %f407, %f406;
-	sub.f32 	%f425, %f424, %f408;
-	fma.rn.f32 	%f426, %f827, %f405, %f425;
-	sub.f32 	%f427, %f413, %f414;
-	add.f32 	%f428, %f427, %f427;
-	sub.f32 	%f429, %f411, %f412;
-	add.f32 	%f430, %f429, %f429;
-	add.f32 	%f431, %f413, %f414;
-	add.f32 	%f432, %f431, %f431;
-	neg.f32 	%f433, %f406;
-	sub.f32 	%f434, %f433, %f407;
-	add.f32 	%f435, %f408, %f434;
-	fma.rn.f32 	%f436, %f827, %f405, %f435;
-	mul.f32 	%f437, %f818, %f417;
-	fma.rn.f32 	%f438, %f821, %f419, %f437;
-	fma.rn.f32 	%f439, %f823, %f421, %f438;
-	sub.f32 	%f842, %f828, %f439;
-	mul.f32 	%f440, %f821, %f426;
-	fma.rn.f32 	%f441, %f818, %f423, %f440;
-	fma.rn.f32 	%f442, %f823, %f428, %f441;
-	sub.f32 	%f838, %f829, %f442;
-	mul.f32 	%f443, %f821, %f432;
-	fma.rn.f32 	%f444, %f818, %f430, %f443;
-	fma.rn.f32 	%f445, %f823, %f436, %f444;
-	sub.f32 	%f834, %f830, %f445;
-	mul.f32 	%f446, %f817, %f417;
-	fma.rn.f32 	%f447, %f820, %f419, %f446;
-	fma.rn.f32 	%f841, %f822, %f421, %f447;
-	mul.f32 	%f448, %f820, %f426;
-	fma.rn.f32 	%f449, %f817, %f423, %f448;
-	fma.rn.f32 	%f837, %f822, %f428, %f449;
-	mul.f32 	%f450, %f820, %f432;
-	fma.rn.f32 	%f451, %f817, %f430, %f450;
-	fma.rn.f32 	%f833, %f822, %f436, %f451;
-	mul.f32 	%f452, %f816, %f417;
-	fma.rn.f32 	%f840, %f819, %f419, %f452;
-	mul.f32 	%f453, %f819, %f426;
-	fma.rn.f32 	%f836, %f816, %f423, %f453;
-	mul.f32 	%f454, %f819, %f432;
-	fma.rn.f32 	%f832, %f816, %f430, %f454;
-	mul.f32 	%f839, %f815, %f417;
-	mul.f32 	%f835, %f815, %f423;
-	mul.f32 	%f831, %f815, %f430;
-
-BB7_15:
-	mul.f32 	%f492, %f832, %f837;
-	mul.f32 	%f493, %f833, %f836;
-	sub.f32 	%f494, %f493, %f492;
-	mul.f32 	%f495, %f839, %f494;
-	mul.f32 	%f496, %f831, %f837;
-	mul.f32 	%f497, %f833, %f835;
-	sub.f32 	%f498, %f497, %f496;
-	mul.f32 	%f499, %f498, %f840;
-	sub.f32 	%f500, %f495, %f499;
-	mul.f32 	%f501, %f831, %f836;
-	mul.f32 	%f502, %f832, %f835;
-	sub.f32 	%f503, %f502, %f501;
-	fma.rn.f32 	%f504, %f503, %f841, %f500;
-	rcp.rn.f32 	%f505, %f504;
-	mul.f32 	%f851, %f494, %f505;
-	mul.f32 	%f506, %f833, %f840;
-	mul.f32 	%f507, %f832, %f841;
-	sub.f32 	%f508, %f507, %f506;
-	mul.f32 	%f852, %f505, %f508;
-	mul.f32 	%f509, %f836, %f841;
-	mul.f32 	%f510, %f837, %f840;
-	sub.f32 	%f511, %f510, %f509;
-	mul.f32 	%f853, %f505, %f511;
-	sub.f32 	%f512, %f496, %f497;
-	mul.f32 	%f847, %f512, %f505;
-	mul.f32 	%f513, %f831, %f841;
-	mul.f32 	%f514, %f833, %f839;
-	sub.f32 	%f515, %f514, %f513;
-	mul.f32 	%f848, %f505, %f515;
-	mul.f32 	%f516, %f837, %f839;
-	mul.f32 	%f517, %f835, %f841;
-	sub.f32 	%f518, %f517, %f516;
-	mul.f32 	%f849, %f505, %f518;
-	mul.f32 	%f843, %f503, %f505;
-	mul.f32 	%f519, %f832, %f839;
-	mul.f32 	%f520, %f831, %f840;
-	sub.f32 	%f521, %f520, %f519;
-	mul.f32 	%f844, %f521, %f505;
-	mul.f32 	%f522, %f835, %f840;
-	mul.f32 	%f523, %f836, %f839;
-	sub.f32 	%f524, %f523, %f522;
-	mul.f32 	%f845, %f524, %f505;
-	mul.f32 	%f525, %f842, %f851;
-	neg.f32 	%f526, %f525;
-	mul.f32 	%f527, %f838, %f852;
-	sub.f32 	%f528, %f526, %f527;
-	mul.f32 	%f529, %f834, %f853;
-	sub.f32 	%f854, %f528, %f529;
-	mul.f32 	%f530, %f842, %f847;
-	neg.f32 	%f531, %f530;
-	mul.f32 	%f532, %f838, %f848;
-	sub.f32 	%f533, %f531, %f532;
-	mul.f32 	%f534, %f834, %f849;
-	sub.f32 	%f850, %f533, %f534;
-	mul.f32 	%f535, %f842, %f843;
-	neg.f32 	%f536, %f535;
-	mul.f32 	%f537, %f838, %f844;
-	sub.f32 	%f538, %f536, %f537;
-	mul.f32 	%f539, %f834, %f845;
-	sub.f32 	%f846, %f538, %f539;
-	bra.uni 	BB7_16;
-
-BB7_5:
-	setp.ne.s32	%p6, %r11, 1;
-	mov.f32 	%f844, %f843;
-	mov.f32 	%f846, %f843;
-	mov.f32 	%f847, %f843;
-	mov.f32 	%f848, %f845;
-	mov.f32 	%f849, %f843;
-	mov.f32 	%f850, %f843;
-	mov.f32 	%f851, %f845;
-	mov.f32 	%f852, %f843;
-	mov.f32 	%f853, %f843;
-	mov.f32 	%f854, %f843;
-	@%p6 bra 	BB7_16;
-
-	// inline asm
-	call (%rd21), _optix_get_static_transform_from_handle, (%rd19);
-	// inline asm
-	add.s64 	%rd263, %rd21, 64;
-
-BB7_8:
-	// inline asm
-	cvta.to.global.u64 %rd25, %rd263;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r13,%r14,%r15,%r16}, [%rd25];
-	// inline asm
-	mov.b32 	 %f851, %r13;
-	mov.b32 	 %f852, %r14;
-	mov.b32 	 %f853, %r15;
-	mov.b32 	 %f854, %r16;
-	add.s64 	%rd29, %rd263, 16;
-	// inline asm
-	cvta.to.global.u64 %rd28, %rd29;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r17,%r18,%r19,%r20}, [%rd28];
-	// inline asm
-	mov.b32 	 %f847, %r17;
-	mov.b32 	 %f848, %r18;
-	mov.b32 	 %f849, %r19;
-	mov.b32 	 %f850, %r20;
-	add.s64 	%rd32, %rd263, 32;
-	// inline asm
-	cvta.to.global.u64 %rd31, %rd32;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r21,%r22,%r23,%r24}, [%rd31];
-	// inline asm
-	mov.b32 	 %f843, %r21;
-	mov.b32 	 %f844, %r22;
-	mov.b32 	 %f845, %r23;
-	mov.b32 	 %f846, %r24;
-
-BB7_16:
-	setp.eq.s32	%p10, %r318, 0;
-	@%p10 bra 	BB7_17;
-	bra.uni 	BB7_18;
-
-BB7_17:
-	mov.f32 	%f814, %f854;
-	mov.f32 	%f813, %f853;
-	mov.f32 	%f812, %f852;
-	mov.f32 	%f811, %f851;
-	mov.f32 	%f810, %f850;
-	mov.f32 	%f809, %f849;
-	mov.f32 	%f808, %f848;
-	mov.f32 	%f807, %f847;
-	mov.f32 	%f806, %f846;
-	mov.f32 	%f805, %f845;
-	mov.f32 	%f804, %f844;
-	mov.f32 	%f803, %f843;
-	bra.uni 	BB7_19;
-
-BB7_18:
-	mul.f32 	%f540, %f807, %f852;
-	fma.rn.f32 	%f541, %f811, %f851, %f540;
-	fma.rn.f32 	%f151, %f803, %f853, %f541;
-	mul.f32 	%f542, %f808, %f852;
-	fma.rn.f32 	%f543, %f812, %f851, %f542;
-	fma.rn.f32 	%f152, %f804, %f853, %f543;
-	mul.f32 	%f544, %f809, %f852;
-	fma.rn.f32 	%f545, %f813, %f851, %f544;
-	fma.rn.f32 	%f153, %f805, %f853, %f545;
-	mul.f32 	%f546, %f810, %f852;
-	fma.rn.f32 	%f547, %f814, %f851, %f546;
-	fma.rn.f32 	%f548, %f806, %f853, %f547;
-	add.f32 	%f154, %f854, %f548;
-	mul.f32 	%f549, %f807, %f848;
-	fma.rn.f32 	%f550, %f811, %f847, %f549;
-	fma.rn.f32 	%f155, %f803, %f849, %f550;
-	mul.f32 	%f551, %f808, %f848;
-	fma.rn.f32 	%f552, %f812, %f847, %f551;
-	fma.rn.f32 	%f156, %f804, %f849, %f552;
-	mul.f32 	%f553, %f809, %f848;
-	fma.rn.f32 	%f554, %f813, %f847, %f553;
-	fma.rn.f32 	%f157, %f805, %f849, %f554;
-	mul.f32 	%f555, %f810, %f848;
-	fma.rn.f32 	%f556, %f814, %f847, %f555;
-	fma.rn.f32 	%f557, %f806, %f849, %f556;
-	add.f32 	%f158, %f850, %f557;
-	mul.f32 	%f558, %f807, %f844;
-	fma.rn.f32 	%f559, %f811, %f843, %f558;
-	fma.rn.f32 	%f803, %f803, %f845, %f559;
-	mul.f32 	%f560, %f808, %f844;
-	fma.rn.f32 	%f561, %f812, %f843, %f560;
-	fma.rn.f32 	%f804, %f804, %f845, %f561;
-	mul.f32 	%f562, %f809, %f844;
-	fma.rn.f32 	%f563, %f813, %f843, %f562;
-	fma.rn.f32 	%f805, %f805, %f845, %f563;
-	mul.f32 	%f564, %f810, %f844;
-	fma.rn.f32 	%f565, %f814, %f843, %f564;
-	fma.rn.f32 	%f566, %f806, %f845, %f565;
-	add.f32 	%f806, %f846, %f566;
-	mov.f32 	%f814, %f154;
-	mov.f32 	%f813, %f153;
-	mov.f32 	%f812, %f152;
-	mov.f32 	%f811, %f151;
-	mov.f32 	%f810, %f158;
-	mov.f32 	%f809, %f157;
-	mov.f32 	%f808, %f156;
-	mov.f32 	%f807, %f155;
-
-BB7_19:
-	add.s32 	%r318, %r318, 1;
-	setp.lt.u32	%p11, %r318, %r8;
-	@%p11 bra 	BB7_3;
-
-	mul.f32 	%f567, %f325, %f811;
-	fma.rn.f32 	%f568, %f326, %f812, %f567;
-	fma.rn.f32 	%f569, %f867, %f813, %f568;
-	add.f32 	%f869, %f814, %f569;
-	mul.f32 	%f570, %f325, %f807;
-	fma.rn.f32 	%f571, %f326, %f808, %f570;
-	fma.rn.f32 	%f572, %f867, %f809, %f571;
-	add.f32 	%f868, %f810, %f572;
-	mul.f32 	%f573, %f325, %f803;
-	fma.rn.f32 	%f574, %f326, %f804, %f573;
-	fma.rn.f32 	%f575, %f867, %f805, %f574;
-	add.f32 	%f867, %f806, %f575;
-	bra.uni 	BB7_21;
-
-BB7_1:
-	mov.f32 	%f868, %f326;
-	mov.f32 	%f869, %f325;
-
-BB7_21:
-	setp.eq.s32	%p37, %r8, 0;
-	// inline asm
-	call (%f576), _optix_get_world_ray_direction_x, ();
-	// inline asm
-	// inline asm
-	call (%f577), _optix_get_world_ray_direction_y, ();
-	// inline asm
-	// inline asm
-	call (%f918), _optix_get_world_ray_direction_z, ();
-	// inline asm
-	// inline asm
-	call (%f579), _optix_get_ray_time, ();
-	// inline asm
-	mov.u32 	%r319, 0;
-	@%p37 bra 	BB7_22;
-
-BB7_23:
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r149,%r150,%r151,%r152}, [%rd125];
+	// end inline asm
+	mov.b32 	%f496, %r149;
+	mov.b32 	%f497, %r150;
+	mov.b32 	%f498, %r151;
+	mov.b32 	%f499, %r152;
+	mul.f32 	%f500, %f90, %f496;
+	mul.f32 	%f501, %f90, %f497;
+	mul.f32 	%f502, %f90, %f498;
+	mul.f32 	%f503, %f90, %f499;
+	fma.rn.f32 	%f863, %f495, %f863, %f500;
+	fma.rn.f32 	%f862, %f495, %f862, %f501;
+	fma.rn.f32 	%f861, %f495, %f861, %f502;
+	fma.rn.f32 	%f860, %f495, %f860, %f503;
+	add.s64 	%rd129, %rd135, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd128, %rd129;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r153,%r154,%r155,%r156}, [%rd128];
+	// end inline asm
+	mov.b32 	%f504, %r153;
+	mov.b32 	%f505, %r154;
+	mov.b32 	%f506, %r155;
+	mov.b32 	%f507, %r156;
+	mul.f32 	%f508, %f90, %f504;
+	mul.f32 	%f509, %f90, %f505;
+	mul.f32 	%f510, %f90, %f506;
+	mul.f32 	%f511, %f90, %f507;
+	fma.rn.f32 	%f867, %f495, %f867, %f508;
+	fma.rn.f32 	%f866, %f495, %f866, %f509;
+	fma.rn.f32 	%f865, %f495, %f865, %f510;
+	fma.rn.f32 	%f864, %f495, %f864, %f511;
+	add.s64 	%rd132, %rd135, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd131, %rd132;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r157,%r158,%r159,%r160}, [%rd131];
+	// end inline asm
+	mov.b32 	%f512, %r157;
+	mov.b32 	%f513, %r158;
+	mov.b32 	%f514, %r159;
+	mov.b32 	%f515, %r160;
+	mul.f32 	%f516, %f90, %f512;
+	mul.f32 	%f517, %f90, %f513;
+	mul.f32 	%f518, %f90, %f514;
+	mul.f32 	%f519, %f90, %f515;
+	fma.rn.f32 	%f871, %f495, %f871, %f516;
+	fma.rn.f32 	%f870, %f495, %f870, %f517;
+	fma.rn.f32 	%f869, %f495, %f869, %f518;
+	fma.rn.f32 	%f868, %f495, %f868, %f519;
+	bra.uni 	$L__BB7_15;
+
+$L__BB7_4:
+	mov.f32 	%f872, 0f00000000;
+	mov.f32 	%f875, 0f3F800000;
+	setp.eq.s32 	%p5, %r13, 4;
+	@%p5 bra 	$L__BB7_7;
+
+	setp.ne.s32 	%p6, %r13, 1;
+	mov.f32 	%f873, %f872;
+	mov.f32 	%f874, %f872;
+	mov.f32 	%f876, %f872;
+	mov.f32 	%f877, %f872;
+	mov.f32 	%f878, %f875;
+	mov.f32 	%f879, %f872;
+	mov.f32 	%f880, %f872;
+	mov.f32 	%f881, %f875;
+	mov.f32 	%f882, %f872;
+	mov.f32 	%f883, %f872;
+	@%p6 bra 	$L__BB7_16;
+
+	// begin inline asm
+	call (%rd19), _optix_get_static_transform_from_handle, (%rd17);
+	// end inline asm
+	add.s64 	%rd256, %rd19, 64;
+	bra.uni 	$L__BB7_8;
+
+$L__BB7_10:
+	// begin inline asm
+	call (%rd32), _optix_get_srt_motion_transform_from_handle, (%rd17);
+	// end inline asm
+	// begin inline asm
+	cvta.to.global.u64 %rd34, %rd32;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r27,%r28,%r29,%r30}, [%rd34];
+	// end inline asm
+	add.s64 	%rd38, %rd32, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd37, %rd38;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r31,%r32,%r33,%r34}, [%rd37];
+	// end inline asm
+	add.s64 	%rd41, %rd32, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd40, %rd41;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r35,%r36,%r37,%r38}, [%rd40];
+	// end inline asm
+	add.s64 	%rd44, %rd32, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd43, %rd44;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r39,%r40,%r41,%r42}, [%rd43];
+	// end inline asm
+	add.s64 	%rd47, %rd32, 64;
+	// begin inline asm
+	cvta.to.global.u64 %rd46, %rd47;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r43,%r44,%r45,%r46}, [%rd46];
+	// end inline asm
+	add.s64 	%rd50, %rd32, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd49, %rd50;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r47,%r48,%r49,%r50}, [%rd49];
+	// end inline asm
+	add.s64 	%rd53, %rd32, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd52, %rd53;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r51,%r52,%r53,%r54}, [%rd52];
+	// end inline asm
+	add.s64 	%rd56, %rd32, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd55, %rd56;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r55,%r56,%r57,%r58}, [%rd55];
+	// end inline asm
+	add.s64 	%rd59, %rd32, 128;
+	// begin inline asm
+	cvta.to.global.u64 %rd58, %rd59;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r59,%r60,%r61,%r62}, [%rd58];
+	// end inline asm
+	add.s64 	%rd62, %rd32, 144;
+	// begin inline asm
+	cvta.to.global.u64 %rd61, %rd62;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r63,%r64,%r65,%r66}, [%rd61];
+	// end inline asm
+	mov.b32 	%f370, %r30;
+	mov.b32 	%f371, %r31;
+	and.b32  	%r83, %r29, 65535;
+	add.s32 	%r84, %r83, -1;
+	cvt.rn.f32.s32 	%f372, %r84;
+	sub.f32 	%f373, %f355, %f370;
+	mul.f32 	%f374, %f373, %f372;
+	sub.f32 	%f375, %f371, %f370;
+	div.rn.f32 	%f376, %f374, %f375;
+	min.f32 	%f377, %f372, %f376;
+	mov.f32 	%f378, 0f00000000;
+	max.f32 	%f379, %f378, %f377;
+	cvt.rmi.f32.f32 	%f380, %f379;
+	sub.f32 	%f29, %f379, %f380;
+	cvt.rzi.s32.f32 	%r85, %f380;
+	mul.wide.s32 	%rd76, %r85, 64;
+	add.s64 	%rd65, %rd41, %rd76;
+	// begin inline asm
+	cvta.to.global.u64 %rd64, %rd65;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r67,%r68,%r69,%r70}, [%rd64];
+	// end inline asm
+	mov.b32 	%f844, %r67;
+	mov.b32 	%f845, %r68;
+	mov.b32 	%f846, %r69;
+	mov.b32 	%f847, %r70;
+	add.s64 	%rd68, %rd65, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd67, %rd68;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r71,%r72,%r73,%r74}, [%rd67];
+	// end inline asm
+	mov.b32 	%f848, %r71;
+	mov.b32 	%f849, %r72;
+	mov.b32 	%f850, %r73;
+	mov.b32 	%f851, %r74;
+	add.s64 	%rd71, %rd65, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd70, %rd71;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r75,%r76,%r77,%r78}, [%rd70];
+	// end inline asm
+	mov.b32 	%f852, %r75;
+	mov.b32 	%f853, %r76;
+	mov.b32 	%f854, %r77;
+	mov.b32 	%f855, %r78;
+	add.s64 	%rd74, %rd65, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd73, %rd74;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r79,%r80,%r81,%r82}, [%rd73];
+	// end inline asm
+	mov.b32 	%f856, %r79;
+	mov.b32 	%f857, %r80;
+	mov.b32 	%f858, %r81;
+	mov.b32 	%f859, %r82;
+	setp.leu.f32 	%p8, %f29, 0f00000000;
+	@%p8 bra 	$L__BB7_12;
+
+	mov.f32 	%f381, 0f3F800000;
+	sub.f32 	%f382, %f381, %f29;
+	add.s64 	%rd78, %rd65, 64;
+	// begin inline asm
+	cvta.to.global.u64 %rd77, %rd78;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r86,%r87,%r88,%r89}, [%rd77];
+	// end inline asm
+	mov.b32 	%f383, %r86;
+	mov.b32 	%f384, %r87;
+	mov.b32 	%f385, %r88;
+	mov.b32 	%f386, %r89;
+	mul.f32 	%f387, %f29, %f383;
+	mul.f32 	%f388, %f29, %f384;
+	mul.f32 	%f389, %f29, %f385;
+	mul.f32 	%f390, %f29, %f386;
+	fma.rn.f32 	%f844, %f382, %f844, %f387;
+	fma.rn.f32 	%f845, %f382, %f845, %f388;
+	fma.rn.f32 	%f846, %f382, %f846, %f389;
+	fma.rn.f32 	%f847, %f382, %f847, %f390;
+	add.s64 	%rd81, %rd65, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd80, %rd81;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r90,%r91,%r92,%r93}, [%rd80];
+	// end inline asm
+	mov.b32 	%f391, %r90;
+	mov.b32 	%f392, %r91;
+	mov.b32 	%f393, %r92;
+	mov.b32 	%f394, %r93;
+	mul.f32 	%f395, %f29, %f391;
+	mul.f32 	%f396, %f29, %f392;
+	mul.f32 	%f397, %f29, %f393;
+	mul.f32 	%f398, %f29, %f394;
+	fma.rn.f32 	%f848, %f382, %f848, %f395;
+	fma.rn.f32 	%f849, %f382, %f849, %f396;
+	fma.rn.f32 	%f850, %f382, %f850, %f397;
+	fma.rn.f32 	%f851, %f382, %f851, %f398;
+	add.s64 	%rd84, %rd65, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd83, %rd84;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r94,%r95,%r96,%r97}, [%rd83];
+	// end inline asm
+	mov.b32 	%f399, %r94;
+	mov.b32 	%f400, %r95;
+	mov.b32 	%f401, %r96;
+	mov.b32 	%f402, %r97;
+	mul.f32 	%f403, %f29, %f399;
+	mul.f32 	%f404, %f29, %f400;
+	mul.f32 	%f405, %f29, %f401;
+	mul.f32 	%f406, %f29, %f402;
+	fma.rn.f32 	%f852, %f382, %f852, %f403;
+	fma.rn.f32 	%f407, %f382, %f853, %f404;
+	fma.rn.f32 	%f408, %f382, %f854, %f405;
+	fma.rn.f32 	%f409, %f382, %f855, %f406;
+	add.s64 	%rd87, %rd65, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd86, %rd87;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r98,%r99,%r100,%r101}, [%rd86];
+	// end inline asm
+	mov.b32 	%f410, %r98;
+	mov.b32 	%f411, %r99;
+	mov.b32 	%f412, %r100;
+	mov.b32 	%f413, %r101;
+	mul.f32 	%f414, %f29, %f410;
+	mul.f32 	%f415, %f29, %f411;
+	mul.f32 	%f416, %f29, %f412;
+	mul.f32 	%f417, %f29, %f413;
+	fma.rn.f32 	%f418, %f382, %f856, %f414;
+	fma.rn.f32 	%f857, %f382, %f857, %f415;
+	fma.rn.f32 	%f858, %f382, %f858, %f416;
+	fma.rn.f32 	%f859, %f382, %f859, %f417;
+	mul.f32 	%f419, %f408, %f408;
+	fma.rn.f32 	%f420, %f407, %f407, %f419;
+	fma.rn.f32 	%f421, %f409, %f409, %f420;
+	fma.rn.f32 	%f422, %f418, %f418, %f421;
+	sqrt.rn.f32 	%f423, %f422;
+	rcp.rn.f32 	%f424, %f423;
+	mul.f32 	%f853, %f407, %f424;
+	mul.f32 	%f854, %f408, %f424;
+	mul.f32 	%f855, %f409, %f424;
+	mul.f32 	%f856, %f424, %f418;
+
+$L__BB7_12:
+	mul.f32 	%f425, %f854, %f854;
+	fma.rn.f32 	%f426, %f853, %f853, %f425;
+	fma.rn.f32 	%f427, %f855, %f855, %f426;
+	fma.rn.f32 	%f428, %f856, %f856, %f427;
+	rcp.rn.f32 	%f429, %f428;
+	mul.f32 	%f430, %f853, %f429;
+	mul.f32 	%f431, %f854, %f429;
+	mul.f32 	%f432, %f855, %f429;
+	mul.f32 	%f433, %f856, %f429;
+	mul.f32 	%f434, %f853, %f430;
+	mul.f32 	%f435, %f854, %f431;
+	mul.f32 	%f436, %f855, %f432;
+	mul.f32 	%f437, %f853, %f431;
+	mul.f32 	%f438, %f855, %f433;
+	mul.f32 	%f439, %f853, %f432;
+	mul.f32 	%f440, %f854, %f433;
+	mul.f32 	%f441, %f854, %f432;
+	mul.f32 	%f442, %f853, %f433;
+	sub.f32 	%f443, %f434, %f435;
+	sub.f32 	%f444, %f443, %f436;
+	fma.rn.f32 	%f445, %f856, %f433, %f444;
+	sub.f32 	%f446, %f437, %f438;
+	add.f32 	%f447, %f446, %f446;
+	add.f32 	%f448, %f439, %f440;
+	add.f32 	%f449, %f448, %f448;
+	add.f32 	%f450, %f437, %f438;
+	add.f32 	%f451, %f450, %f450;
+	sub.f32 	%f452, %f435, %f434;
+	sub.f32 	%f453, %f452, %f436;
+	fma.rn.f32 	%f454, %f856, %f433, %f453;
+	sub.f32 	%f455, %f441, %f442;
+	add.f32 	%f456, %f455, %f455;
+	sub.f32 	%f457, %f439, %f440;
+	add.f32 	%f458, %f457, %f457;
+	add.f32 	%f459, %f441, %f442;
+	add.f32 	%f460, %f459, %f459;
+	neg.f32 	%f461, %f434;
+	sub.f32 	%f462, %f461, %f435;
+	add.f32 	%f463, %f436, %f462;
+	fma.rn.f32 	%f464, %f856, %f433, %f463;
+	mul.f32 	%f465, %f847, %f445;
+	fma.rn.f32 	%f466, %f850, %f447, %f465;
+	fma.rn.f32 	%f467, %f852, %f449, %f466;
+	sub.f32 	%f860, %f857, %f467;
+	mul.f32 	%f468, %f850, %f454;
+	fma.rn.f32 	%f469, %f847, %f451, %f468;
+	fma.rn.f32 	%f470, %f852, %f456, %f469;
+	sub.f32 	%f864, %f858, %f470;
+	mul.f32 	%f471, %f850, %f460;
+	fma.rn.f32 	%f472, %f847, %f458, %f471;
+	fma.rn.f32 	%f473, %f852, %f464, %f472;
+	sub.f32 	%f868, %f859, %f473;
+	mul.f32 	%f474, %f846, %f445;
+	fma.rn.f32 	%f475, %f849, %f447, %f474;
+	fma.rn.f32 	%f861, %f851, %f449, %f475;
+	mul.f32 	%f476, %f849, %f454;
+	fma.rn.f32 	%f477, %f846, %f451, %f476;
+	fma.rn.f32 	%f865, %f851, %f456, %f477;
+	mul.f32 	%f478, %f849, %f460;
+	fma.rn.f32 	%f479, %f846, %f458, %f478;
+	fma.rn.f32 	%f869, %f851, %f464, %f479;
+	mul.f32 	%f480, %f845, %f445;
+	fma.rn.f32 	%f862, %f848, %f447, %f480;
+	mul.f32 	%f481, %f848, %f454;
+	fma.rn.f32 	%f866, %f845, %f451, %f481;
+	mul.f32 	%f482, %f848, %f460;
+	fma.rn.f32 	%f870, %f845, %f458, %f482;
+	mul.f32 	%f863, %f844, %f445;
+	mul.f32 	%f867, %f844, %f451;
+	mul.f32 	%f871, %f844, %f458;
+
+$L__BB7_15:
+	mul.f32 	%f520, %f865, %f870;
+	mul.f32 	%f521, %f866, %f869;
+	sub.f32 	%f522, %f521, %f520;
+	mul.f32 	%f523, %f863, %f522;
+	mul.f32 	%f524, %f865, %f871;
+	mul.f32 	%f525, %f867, %f869;
+	sub.f32 	%f526, %f525, %f524;
+	mul.f32 	%f527, %f862, %f526;
+	sub.f32 	%f528, %f523, %f527;
+	mul.f32 	%f529, %f866, %f871;
+	mul.f32 	%f530, %f867, %f870;
+	sub.f32 	%f531, %f530, %f529;
+	fma.rn.f32 	%f532, %f861, %f531, %f528;
+	rcp.rn.f32 	%f533, %f532;
+	mul.f32 	%f875, %f522, %f533;
+	mul.f32 	%f534, %f862, %f869;
+	mul.f32 	%f535, %f861, %f870;
+	sub.f32 	%f536, %f535, %f534;
+	mul.f32 	%f874, %f536, %f533;
+	mul.f32 	%f537, %f861, %f866;
+	mul.f32 	%f538, %f862, %f865;
+	sub.f32 	%f539, %f538, %f537;
+	mul.f32 	%f873, %f539, %f533;
+	sub.f32 	%f540, %f524, %f525;
+	mul.f32 	%f879, %f540, %f533;
+	mul.f32 	%f541, %f861, %f871;
+	mul.f32 	%f542, %f863, %f869;
+	sub.f32 	%f543, %f542, %f541;
+	mul.f32 	%f878, %f543, %f533;
+	mul.f32 	%f544, %f863, %f865;
+	mul.f32 	%f545, %f861, %f867;
+	sub.f32 	%f546, %f545, %f544;
+	mul.f32 	%f877, %f546, %f533;
+	mul.f32 	%f883, %f531, %f533;
+	mul.f32 	%f547, %f863, %f870;
+	mul.f32 	%f548, %f862, %f871;
+	sub.f32 	%f549, %f548, %f547;
+	mul.f32 	%f882, %f549, %f533;
+	mul.f32 	%f550, %f862, %f867;
+	mul.f32 	%f551, %f863, %f866;
+	sub.f32 	%f552, %f551, %f550;
+	mul.f32 	%f881, %f552, %f533;
+	mul.f32 	%f553, %f860, %f875;
+	neg.f32 	%f554, %f553;
+	mul.f32 	%f555, %f864, %f874;
+	sub.f32 	%f556, %f554, %f555;
+	mul.f32 	%f557, %f868, %f873;
+	sub.f32 	%f872, %f556, %f557;
+	mul.f32 	%f558, %f860, %f879;
+	neg.f32 	%f559, %f558;
+	mul.f32 	%f560, %f864, %f878;
+	sub.f32 	%f561, %f559, %f560;
+	mul.f32 	%f562, %f868, %f877;
+	sub.f32 	%f876, %f561, %f562;
+	mul.f32 	%f563, %f860, %f883;
+	neg.f32 	%f564, %f563;
+	mul.f32 	%f565, %f864, %f882;
+	sub.f32 	%f566, %f564, %f565;
+	mul.f32 	%f567, %f868, %f881;
+	sub.f32 	%f880, %f566, %f567;
+	bra.uni 	$L__BB7_16;
+
+$L__BB7_7:
+	// begin inline asm
+	call (%rd256), _optix_get_instance_inverse_transform_from_handle, (%rd17);
+	// end inline asm
+
+$L__BB7_8:
+	// begin inline asm
+	cvta.to.global.u64 %rd23, %rd256;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r15,%r16,%r17,%r18}, [%rd23];
+	// end inline asm
+	mov.b32 	%f875, %r15;
+	mov.b32 	%f874, %r16;
+	mov.b32 	%f873, %r17;
+	mov.b32 	%f872, %r18;
+	add.s64 	%rd27, %rd256, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd26, %rd27;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r19,%r20,%r21,%r22}, [%rd26];
+	// end inline asm
+	mov.b32 	%f879, %r19;
+	mov.b32 	%f878, %r20;
+	mov.b32 	%f877, %r21;
+	mov.b32 	%f876, %r22;
+	add.s64 	%rd30, %rd256, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd29, %rd30;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r23,%r24,%r25,%r26}, [%rd29];
+	// end inline asm
+	mov.b32 	%f883, %r23;
+	mov.b32 	%f882, %r24;
+	mov.b32 	%f881, %r25;
+	mov.b32 	%f880, %r26;
+
+$L__BB7_16:
+	setp.eq.s32 	%p10, %r321, 0;
+	@%p10 bra 	$L__BB7_18;
+
+	mul.f32 	%f568, %f840, %f875;
+	fma.rn.f32 	%f569, %f836, %f874, %f568;
+	fma.rn.f32 	%f151, %f832, %f873, %f569;
+	mul.f32 	%f570, %f841, %f875;
+	fma.rn.f32 	%f571, %f837, %f874, %f570;
+	fma.rn.f32 	%f152, %f833, %f873, %f571;
+	mul.f32 	%f572, %f842, %f875;
+	fma.rn.f32 	%f573, %f838, %f874, %f572;
+	fma.rn.f32 	%f153, %f834, %f873, %f573;
+	mul.f32 	%f574, %f843, %f875;
+	fma.rn.f32 	%f575, %f839, %f874, %f574;
+	fma.rn.f32 	%f576, %f835, %f873, %f575;
+	add.f32 	%f872, %f872, %f576;
+	mul.f32 	%f577, %f840, %f879;
+	fma.rn.f32 	%f578, %f836, %f878, %f577;
+	fma.rn.f32 	%f155, %f832, %f877, %f578;
+	mul.f32 	%f579, %f841, %f879;
+	fma.rn.f32 	%f580, %f837, %f878, %f579;
+	fma.rn.f32 	%f156, %f833, %f877, %f580;
+	mul.f32 	%f581, %f842, %f879;
+	fma.rn.f32 	%f582, %f838, %f878, %f581;
+	fma.rn.f32 	%f157, %f834, %f877, %f582;
+	mul.f32 	%f583, %f843, %f879;
+	fma.rn.f32 	%f584, %f839, %f878, %f583;
+	fma.rn.f32 	%f585, %f835, %f877, %f584;
+	add.f32 	%f876, %f876, %f585;
+	mul.f32 	%f586, %f840, %f883;
+	fma.rn.f32 	%f587, %f836, %f882, %f586;
+	fma.rn.f32 	%f159, %f832, %f881, %f587;
+	mul.f32 	%f588, %f841, %f883;
+	fma.rn.f32 	%f589, %f837, %f882, %f588;
+	fma.rn.f32 	%f160, %f833, %f881, %f589;
+	mul.f32 	%f590, %f842, %f883;
+	fma.rn.f32 	%f591, %f838, %f882, %f590;
+	fma.rn.f32 	%f161, %f834, %f881, %f591;
+	mul.f32 	%f592, %f843, %f883;
+	fma.rn.f32 	%f593, %f839, %f882, %f592;
+	fma.rn.f32 	%f594, %f835, %f881, %f593;
+	add.f32 	%f880, %f880, %f594;
+	mov.f32 	%f873, %f153;
+	mov.f32 	%f874, %f152;
+	mov.f32 	%f875, %f151;
+	mov.f32 	%f877, %f157;
+	mov.f32 	%f878, %f156;
+	mov.f32 	%f879, %f155;
+	mov.f32 	%f881, %f161;
+	mov.f32 	%f882, %f160;
+	mov.f32 	%f883, %f159;
+
+$L__BB7_18:
+	add.s32 	%r321, %r321, 1;
+	setp.lt.u32 	%p11, %r321, %r10;
+	mov.f32 	%f832, %f883;
+	mov.f32 	%f833, %f882;
+	mov.f32 	%f834, %f881;
+	mov.f32 	%f835, %f880;
+	mov.f32 	%f836, %f879;
+	mov.f32 	%f837, %f878;
+	mov.f32 	%f838, %f877;
+	mov.f32 	%f839, %f876;
+	mov.f32 	%f840, %f875;
+	mov.f32 	%f841, %f874;
+	mov.f32 	%f842, %f873;
+	mov.f32 	%f843, %f872;
+	@%p11 bra 	$L__BB7_3;
+
+$L__BB7_19:
+	mul.f32 	%f595, %f908, %f875;
+	fma.rn.f32 	%f596, %f909, %f874, %f595;
+	fma.rn.f32 	%f597, %f910, %f873, %f596;
+	mul.f32 	%f598, %f908, %f879;
+	fma.rn.f32 	%f599, %f909, %f878, %f598;
+	fma.rn.f32 	%f600, %f910, %f877, %f599;
+	mul.f32 	%f601, %f908, %f883;
+	fma.rn.f32 	%f602, %f909, %f882, %f601;
+	fma.rn.f32 	%f603, %f910, %f881, %f602;
+	add.f32 	%f910, %f880, %f603;
+	add.f32 	%f909, %f876, %f600;
+	add.f32 	%f908, %f872, %f597;
+
+$L__BB7_21:
+	// begin inline asm
+	call (%f966), _optix_get_world_ray_direction_x, ();
+	// end inline asm
+	// begin inline asm
+	call (%f967), _optix_get_world_ray_direction_y, ();
+	// end inline asm
+	// begin inline asm
+	call (%f606), _optix_get_world_ray_direction_z, ();
+	// end inline asm
+	// begin inline asm
+	call (%r161), _optix_get_transform_list_size, ();
+	// end inline asm
+	setp.eq.s32 	%p12, %r161, 0;
+	@%p12 bra 	$L__BB7_41;
+
+	// begin inline asm
+	call (%r162), _optix_get_transform_list_size, ();
+	// end inline asm
+	// begin inline asm
+	call (%f607), _optix_get_ray_time, ();
+	// end inline asm
+	setp.eq.s32 	%p13, %r162, 0;
+	@%p13 bra 	$L__BB7_40;
+
+	mov.u32 	%r322, 0;
+
+$L__BB7_24:
 	.pragma "nounroll";
-	// inline asm
-	call (%rd140), _optix_get_transform_list_handle, (%r319);
-	// inline asm
-	// inline asm
-	call (%r161), _optix_get_transform_type_from_handle, (%rd140);
-	// inline asm
-	and.b32  	%r162, %r161, -2;
-	setp.eq.s32	%p13, %r162, 2;
-	@%p13 bra 	BB7_29;
-	bra.uni 	BB7_24;
-
-BB7_29:
-	setp.eq.s32	%p16, %r161, 2;
-	@%p16 bra 	BB7_33;
-	bra.uni 	BB7_30;
-
-BB7_33:
-	// inline asm
-	call (%rd214), _optix_get_matrix_motion_transform_from_handle, (%rd140);
-	// inline asm
-	// inline asm
-	cvta.to.global.u64 %rd216, %rd214;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r250,%r251,%r252,%r253}, [%rd216];
-	// inline asm
-	mov.b32	{%rs8, %rs9}, %r252;
-	add.s64 	%rd220, %rd214, 16;
-	// inline asm
+	// begin inline asm
+	call (%rd136), _optix_get_transform_list_handle, (%r322);
+	// end inline asm
+	// begin inline asm
+	call (%r165), _optix_get_transform_type_from_handle, (%rd136);
+	// end inline asm
+	or.b32  	%r166, %r165, 1;
+	setp.eq.s32 	%p14, %r166, 3;
+	@%p14 bra 	$L__BB7_30;
+	bra.uni 	$L__BB7_25;
+
+$L__BB7_30:
+	setp.eq.s32 	%p17, %r165, 2;
+	@%p17 bra 	$L__BB7_34;
+	bra.uni 	$L__BB7_31;
+
+$L__BB7_34:
+	// begin inline asm
+	call (%rd208), _optix_get_matrix_motion_transform_from_handle, (%rd136);
+	// end inline asm
+	// begin inline asm
+	cvta.to.global.u64 %rd210, %rd208;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r254,%r255,%r256,%r257}, [%rd210];
+	// end inline asm
+	add.s64 	%rd214, %rd208, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd213, %rd214;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r258,%r259,%r260,%r261}, [%rd213];
+	// end inline asm
+	add.s64 	%rd217, %rd208, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd216, %rd217;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r262,%r263,%r264,%r265}, [%rd216];
+	// end inline asm
+	add.s64 	%rd220, %rd208, 48;
+	// begin inline asm
 	cvta.to.global.u64 %rd219, %rd220;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r254,%r255,%r256,%r257}, [%rd219];
-	// inline asm
-	add.s64 	%rd223, %rd214, 32;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r266,%r267,%r268,%r269}, [%rd219];
+	// end inline asm
+	add.s64 	%rd223, %rd208, 64;
+	// begin inline asm
 	cvta.to.global.u64 %rd222, %rd223;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r258,%r259,%r260,%r261}, [%rd222];
-	// inline asm
-	add.s64 	%rd226, %rd214, 48;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r270,%r271,%r272,%r273}, [%rd222];
+	// end inline asm
+	add.s64 	%rd226, %rd208, 80;
+	// begin inline asm
 	cvta.to.global.u64 %rd225, %rd226;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r262,%r263,%r264,%r265}, [%rd225];
-	// inline asm
-	add.s64 	%rd229, %rd214, 64;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r274,%r275,%r276,%r277}, [%rd225];
+	// end inline asm
+	add.s64 	%rd229, %rd208, 96;
+	// begin inline asm
 	cvta.to.global.u64 %rd228, %rd229;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r266,%r267,%r268,%r269}, [%rd228];
-	// inline asm
-	add.s64 	%rd232, %rd214, 80;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r278,%r279,%r280,%r281}, [%rd228];
+	// end inline asm
+	add.s64 	%rd232, %rd208, 112;
+	// begin inline asm
 	cvta.to.global.u64 %rd231, %rd232;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r270,%r271,%r272,%r273}, [%rd231];
-	// inline asm
-	add.s64 	%rd235, %rd214, 96;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r282,%r283,%r284,%r285}, [%rd231];
+	// end inline asm
+	mov.b32 	%f711, %r257;
+	mov.b32 	%f712, %r258;
+	and.b32  	%r298, %r256, 65535;
+	add.s32 	%r299, %r298, -1;
+	cvt.rn.f32.s32 	%f713, %r299;
+	sub.f32 	%f714, %f607, %f711;
+	mul.f32 	%f715, %f714, %f713;
+	sub.f32 	%f716, %f712, %f711;
+	div.rn.f32 	%f717, %f715, %f716;
+	min.f32 	%f718, %f713, %f717;
+	mov.f32 	%f719, 0f00000000;
+	max.f32 	%f720, %f719, %f718;
+	cvt.rmi.f32.f32 	%f721, %f720;
+	sub.f32 	%f258, %f720, %f721;
+	cvt.rzi.s32.f32 	%r300, %f721;
+	cvt.s64.s32 	%rd15, %r300;
+	mul.wide.s32 	%rd243, %r300, 48;
+	add.s64 	%rd235, %rd217, %rd243;
+	// begin inline asm
 	cvta.to.global.u64 %rd234, %rd235;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r274,%r275,%r276,%r277}, [%rd234];
-	// inline asm
-	add.s64 	%rd238, %rd214, 112;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r286,%r287,%r288,%r289}, [%rd234];
+	// end inline asm
+	mov.b32 	%f936, %r286;
+	mov.b32 	%f937, %r287;
+	mov.b32 	%f938, %r288;
+	add.s64 	%rd238, %rd235, 16;
+	// begin inline asm
 	cvta.to.global.u64 %rd237, %rd238;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r278,%r279,%r280,%r281}, [%rd237];
-	// inline asm
-	mov.b32 	 %f682, %r253;
-	mov.b32 	 %f683, %r254;
-	cvt.u32.u16	%r294, %rs8;
-	add.s32 	%r295, %r294, -1;
-	cvt.rn.f32.s32	%f684, %r295;
-	sub.f32 	%f685, %f579, %f682;
-	mul.f32 	%f686, %f685, %f684;
-	sub.f32 	%f687, %f683, %f682;
-	div.rn.f32 	%f688, %f686, %f687;
-	min.f32 	%f689, %f684, %f688;
-	mov.f32 	%f690, 0f00000000;
-	max.f32 	%f691, %f690, %f689;
-	cvt.rmi.f32.f32	%f692, %f691;
-	cvt.rzi.s32.f32	%r296, %f692;
-	cvt.s64.s32	%rd17, %r296;
-	mul.wide.s32 	%rd249, %r296, 48;
-	add.s64 	%rd241, %rd223, %rd249;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r290,%r291,%r292,%r293}, [%rd237];
+	// end inline asm
+	mov.b32 	%f933, %r290;
+	mov.b32 	%f934, %r291;
+	mov.b32 	%f935, %r292;
+	add.s64 	%rd241, %rd235, 32;
+	// begin inline asm
 	cvta.to.global.u64 %rd240, %rd241;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r282,%r283,%r284,%r285}, [%rd240];
-	// inline asm
-	mov.b32 	 %f895, %r282;
-	mov.b32 	 %f896, %r283;
-	mov.b32 	 %f897, %r284;
-	add.s64 	%rd244, %rd241, 16;
-	// inline asm
-	cvta.to.global.u64 %rd243, %rd244;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r286,%r287,%r288,%r289}, [%rd243];
-	// inline asm
-	mov.b32 	 %f892, %r286;
-	mov.b32 	 %f893, %r287;
-	mov.b32 	 %f894, %r288;
-	add.s64 	%rd247, %rd241, 32;
-	// inline asm
-	cvta.to.global.u64 %rd246, %rd247;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r290,%r291,%r292,%r293}, [%rd246];
-	// inline asm
-	sub.f32 	%f249, %f691, %f692;
-	mov.b32 	 %f889, %r290;
-	mov.b32 	 %f890, %r291;
-	mov.b32 	 %f891, %r292;
-	setp.leu.f32	%p18, %f249, 0f00000000;
-	@%p18 bra 	BB7_35;
-
-	mul.lo.s64 	%rd259, %rd17, 48;
-	add.s64 	%rd260, %rd214, %rd259;
-	add.s64 	%rd251, %rd260, 80;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r294,%r295,%r296,%r297}, [%rd240];
+	// end inline asm
+	mov.b32 	%f930, %r294;
+	mov.b32 	%f931, %r295;
+	mov.b32 	%f932, %r296;
+	setp.leu.f32 	%p19, %f258, 0f00000000;
+	@%p19 bra 	$L__BB7_36;
+
+	mov.f32 	%f722, 0f3F800000;
+	sub.f32 	%f723, %f722, %f258;
+	mul.lo.s64 	%rd253, %rd15, 48;
+	add.s64 	%rd254, %rd208, %rd253;
+	add.s64 	%rd245, %rd254, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd244, %rd245;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r301,%r302,%r303,%r304}, [%rd244];
+	// end inline asm
+	mov.b32 	%f724, %r301;
+	mov.b32 	%f725, %r302;
+	mov.b32 	%f726, %r303;
+	mul.f32 	%f727, %f258, %f724;
+	mul.f32 	%f728, %f258, %f725;
+	mul.f32 	%f729, %f258, %f726;
+	fma.rn.f32 	%f936, %f723, %f936, %f727;
+	fma.rn.f32 	%f937, %f723, %f937, %f728;
+	fma.rn.f32 	%f938, %f723, %f938, %f729;
+	add.s64 	%rd248, %rd254, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd247, %rd248;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r305,%r306,%r307,%r308}, [%rd247];
+	// end inline asm
+	mov.b32 	%f730, %r305;
+	mov.b32 	%f731, %r306;
+	mov.b32 	%f732, %r307;
+	mul.f32 	%f733, %f258, %f730;
+	mul.f32 	%f734, %f258, %f731;
+	mul.f32 	%f735, %f258, %f732;
+	fma.rn.f32 	%f933, %f723, %f933, %f733;
+	fma.rn.f32 	%f934, %f723, %f934, %f734;
+	fma.rn.f32 	%f935, %f723, %f935, %f735;
+	add.s64 	%rd251, %rd254, 112;
+	// begin inline asm
 	cvta.to.global.u64 %rd250, %rd251;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r297,%r298,%r299,%r300}, [%rd250];
-	// inline asm
-	mov.b32 	 %f693, %r297;
-	mov.b32 	 %f694, %r298;
-	mov.b32 	 %f695, %r299;
-	add.s64 	%rd254, %rd260, 96;
-	// inline asm
-	cvta.to.global.u64 %rd253, %rd254;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r301,%r302,%r303,%r304}, [%rd253];
-	// inline asm
-	mov.b32 	 %f696, %r301;
-	mov.b32 	 %f697, %r302;
-	mov.b32 	 %f698, %r303;
-	add.s64 	%rd257, %rd260, 112;
-	// inline asm
-	cvta.to.global.u64 %rd256, %rd257;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r305,%r306,%r307,%r308}, [%rd256];
-	// inline asm
-	mov.f32 	%f699, 0f3F800000;
-	sub.f32 	%f700, %f699, %f249;
-	mul.f32 	%f701, %f249, %f693;
-	mul.f32 	%f702, %f249, %f694;
-	mul.f32 	%f703, %f249, %f695;
-	fma.rn.f32 	%f895, %f700, %f895, %f701;
-	fma.rn.f32 	%f896, %f700, %f896, %f702;
-	fma.rn.f32 	%f897, %f700, %f897, %f703;
-	mul.f32 	%f704, %f249, %f696;
-	mul.f32 	%f705, %f249, %f697;
-	mul.f32 	%f706, %f249, %f698;
-	fma.rn.f32 	%f892, %f700, %f892, %f704;
-	fma.rn.f32 	%f893, %f700, %f893, %f705;
-	fma.rn.f32 	%f894, %f700, %f894, %f706;
-	mov.b32 	 %f707, %r305;
-	mov.b32 	 %f708, %r306;
-	mov.b32 	 %f709, %r307;
-	mul.f32 	%f710, %f249, %f707;
-	mul.f32 	%f711, %f249, %f708;
-	mul.f32 	%f712, %f249, %f709;
-	fma.rn.f32 	%f889, %f700, %f889, %f710;
-	fma.rn.f32 	%f890, %f700, %f890, %f711;
-	fma.rn.f32 	%f891, %f700, %f891, %f712;
-	bra.uni 	BB7_35;
-
-BB7_24:
-	mov.f32 	%f898, 0f00000000;
-	mov.f32 	%f900, 0f3F800000;
-	setp.eq.s32	%p14, %r161, 4;
-	@%p14 bra 	BB7_27;
-	bra.uni 	BB7_25;
-
-BB7_27:
-	// inline asm
-	call (%rd264), _optix_get_instance_inverse_transform_from_handle, (%rd140);
-	// inline asm
-	bra.uni 	BB7_28;
-
-BB7_30:
-	// inline asm
-	call (%rd155), _optix_get_srt_motion_transform_from_handle, (%rd140);
-	// inline asm
-	// inline asm
-	cvta.to.global.u64 %rd157, %rd155;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r175,%r176,%r177,%r178}, [%rd157];
-	// inline asm
-	mov.b32	{%rs6, %rs7}, %r177;
-	add.s64 	%rd161, %rd155, 16;
-	// inline asm
-	cvta.to.global.u64 %rd160, %rd161;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r179,%r180,%r181,%r182}, [%rd160];
-	// inline asm
-	add.s64 	%rd164, %rd155, 32;
-	// inline asm
-	cvta.to.global.u64 %rd163, %rd164;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r183,%r184,%r185,%r186}, [%rd163];
-	// inline asm
-	add.s64 	%rd167, %rd155, 48;
-	// inline asm
-	cvta.to.global.u64 %rd166, %rd167;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r187,%r188,%r189,%r190}, [%rd166];
-	// inline asm
-	add.s64 	%rd170, %rd155, 64;
-	// inline asm
-	cvta.to.global.u64 %rd169, %rd170;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r191,%r192,%r193,%r194}, [%rd169];
-	// inline asm
-	add.s64 	%rd173, %rd155, 80;
-	// inline asm
-	cvta.to.global.u64 %rd172, %rd173;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r195,%r196,%r197,%r198}, [%rd172];
-	// inline asm
-	add.s64 	%rd176, %rd155, 96;
-	// inline asm
-	cvta.to.global.u64 %rd175, %rd176;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r199,%r200,%r201,%r202}, [%rd175];
-	// inline asm
-	add.s64 	%rd179, %rd155, 112;
-	// inline asm
-	cvta.to.global.u64 %rd178, %rd179;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r203,%r204,%r205,%r206}, [%rd178];
-	// inline asm
-	add.s64 	%rd182, %rd155, 128;
-	// inline asm
-	cvta.to.global.u64 %rd181, %rd182;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r207,%r208,%r209,%r210}, [%rd181];
-	// inline asm
-	add.s64 	%rd185, %rd155, 144;
-	// inline asm
-	cvta.to.global.u64 %rd184, %rd185;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r211,%r212,%r213,%r214}, [%rd184];
-	// inline asm
-	mov.b32 	 %f590, %r178;
-	mov.b32 	 %f591, %r179;
-	cvt.u32.u16	%r231, %rs6;
-	add.s32 	%r232, %r231, -1;
-	cvt.rn.f32.s32	%f592, %r232;
-	sub.f32 	%f593, %f579, %f590;
-	mul.f32 	%f594, %f593, %f592;
-	sub.f32 	%f595, %f591, %f590;
-	div.rn.f32 	%f596, %f594, %f595;
-	min.f32 	%f597, %f592, %f596;
-	mov.f32 	%f598, 0f00000000;
-	max.f32 	%f599, %f598, %f597;
-	cvt.rmi.f32.f32	%f600, %f599;
-	cvt.rzi.s32.f32	%r233, %f600;
-	cvt.s64.s32	%rd15, %r233;
-	mul.wide.s32 	%rd199, %r233, 64;
-	add.s64 	%rd188, %rd164, %rd199;
-	// inline asm
-	cvta.to.global.u64 %rd187, %rd188;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r215,%r216,%r217,%r218}, [%rd187];
-	// inline asm
-	mov.b32 	 %f879, %r215;
-	mov.b32 	 %f880, %r216;
-	mov.b32 	 %f881, %r217;
-	add.s64 	%rd191, %rd188, 16;
-	// inline asm
-	cvta.to.global.u64 %rd190, %rd191;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r219,%r220,%r221,%r222}, [%rd190];
-	// inline asm
-	mov.b32 	 %f882, %r219;
-	mov.b32 	 %f883, %r220;
-	mov.b32 	 %f884, %r222;
-	add.s64 	%rd194, %rd188, 32;
-	// inline asm
-	cvta.to.global.u64 %rd193, %rd194;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r223,%r224,%r225,%r226}, [%rd193];
-	// inline asm
-	sub.f32 	%f209, %f599, %f600;
-	mov.b32 	 %f885, %r224;
-	mov.b32 	 %f886, %r225;
-	mov.b32 	 %f887, %r226;
-	add.s64 	%rd197, %rd188, 48;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r309,%r310,%r311,%r312}, [%rd250];
+	// end inline asm
+	mov.b32 	%f736, %r309;
+	mov.b32 	%f737, %r310;
+	mov.b32 	%f738, %r311;
+	mul.f32 	%f739, %f258, %f736;
+	mul.f32 	%f740, %f258, %f737;
+	mul.f32 	%f741, %f258, %f738;
+	fma.rn.f32 	%f930, %f723, %f930, %f739;
+	fma.rn.f32 	%f931, %f723, %f931, %f740;
+	fma.rn.f32 	%f932, %f723, %f932, %f741;
+	bra.uni 	$L__BB7_36;
+
+$L__BB7_25:
+	mov.f32 	%f939, 0f00000000;
+	mov.f32 	%f941, 0f3F800000;
+	setp.eq.s32 	%p15, %r165, 4;
+	@%p15 bra 	$L__BB7_28;
+
+	setp.ne.s32 	%p16, %r165, 1;
+	mov.f32 	%f940, %f939;
+	mov.f32 	%f942, %f939;
+	mov.f32 	%f943, %f941;
+	mov.f32 	%f944, %f939;
+	mov.f32 	%f945, %f941;
+	mov.f32 	%f946, %f939;
+	mov.f32 	%f947, %f939;
+	@%p16 bra 	$L__BB7_37;
+
+	// begin inline asm
+	call (%rd138), _optix_get_static_transform_from_handle, (%rd136);
+	// end inline asm
+	add.s64 	%rd257, %rd138, 64;
+	bra.uni 	$L__BB7_29;
+
+$L__BB7_31:
+	// begin inline asm
+	call (%rd151), _optix_get_srt_motion_transform_from_handle, (%rd136);
+	// end inline asm
+	// begin inline asm
+	cvta.to.global.u64 %rd153, %rd151;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r179,%r180,%r181,%r182}, [%rd153];
+	// end inline asm
+	add.s64 	%rd157, %rd151, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd156, %rd157;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r183,%r184,%r185,%r186}, [%rd156];
+	// end inline asm
+	add.s64 	%rd160, %rd151, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd159, %rd160;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r187,%r188,%r189,%r190}, [%rd159];
+	// end inline asm
+	add.s64 	%rd163, %rd151, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd162, %rd163;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r191,%r192,%r193,%r194}, [%rd162];
+	// end inline asm
+	add.s64 	%rd166, %rd151, 64;
+	// begin inline asm
+	cvta.to.global.u64 %rd165, %rd166;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r195,%r196,%r197,%r198}, [%rd165];
+	// end inline asm
+	add.s64 	%rd169, %rd151, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd168, %rd169;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r199,%r200,%r201,%r202}, [%rd168];
+	// end inline asm
+	add.s64 	%rd172, %rd151, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd171, %rd172;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r203,%r204,%r205,%r206}, [%rd171];
+	// end inline asm
+	add.s64 	%rd175, %rd151, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd174, %rd175;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r207,%r208,%r209,%r210}, [%rd174];
+	// end inline asm
+	add.s64 	%rd178, %rd151, 128;
+	// begin inline asm
+	cvta.to.global.u64 %rd177, %rd178;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r211,%r212,%r213,%r214}, [%rd177];
+	// end inline asm
+	add.s64 	%rd181, %rd151, 144;
+	// begin inline asm
+	cvta.to.global.u64 %rd180, %rd181;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r215,%r216,%r217,%r218}, [%rd180];
+	// end inline asm
+	mov.b32 	%f619, %r182;
+	mov.b32 	%f620, %r183;
+	and.b32  	%r235, %r181, 65535;
+	add.s32 	%r236, %r235, -1;
+	cvt.rn.f32.s32 	%f621, %r236;
+	sub.f32 	%f622, %f607, %f619;
+	mul.f32 	%f623, %f622, %f621;
+	sub.f32 	%f624, %f620, %f619;
+	div.rn.f32 	%f625, %f623, %f624;
+	min.f32 	%f626, %f621, %f625;
+	mov.f32 	%f627, 0f00000000;
+	max.f32 	%f628, %f627, %f626;
+	cvt.rmi.f32.f32 	%f629, %f628;
+	sub.f32 	%f218, %f628, %f629;
+	cvt.rzi.s32.f32 	%r237, %f629;
+	mul.wide.s32 	%rd195, %r237, 64;
+	add.s64 	%rd184, %rd160, %rd195;
+	// begin inline asm
+	cvta.to.global.u64 %rd183, %rd184;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r219,%r220,%r221,%r222}, [%rd183];
+	// end inline asm
+	mov.b32 	%f920, %r219;
+	mov.b32 	%f921, %r220;
+	mov.b32 	%f922, %r221;
+	add.s64 	%rd187, %rd184, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd186, %rd187;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r223,%r224,%r225,%r226}, [%rd186];
+	// end inline asm
+	mov.b32 	%f923, %r223;
+	mov.b32 	%f924, %r224;
+	mov.b32 	%f925, %r226;
+	add.s64 	%rd190, %rd184, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd189, %rd190;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r227,%r228,%r229,%r230}, [%rd189];
+	// end inline asm
+	mov.b32 	%f926, %r228;
+	mov.b32 	%f927, %r229;
+	mov.b32 	%f928, %r230;
+	add.s64 	%rd193, %rd184, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd192, %rd193;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r231,%r232,%r233,%r234}, [%rd192];
+	// end inline asm
+	mov.b32 	%f929, %r231;
+	setp.leu.f32 	%p18, %f218, 0f00000000;
+	@%p18 bra 	$L__BB7_33;
+
+	mov.f32 	%f630, 0f3F800000;
+	sub.f32 	%f631, %f630, %f218;
+	add.s64 	%rd197, %rd184, 64;
+	// begin inline asm
 	cvta.to.global.u64 %rd196, %rd197;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r227,%r228,%r229,%r230}, [%rd196];
-	// inline asm
-	mov.b32 	 %f888, %r227;
-	setp.leu.f32	%p17, %f209, 0f00000000;
-	@%p17 bra 	BB7_32;
-
-	shl.b64 	%rd212, %rd15, 6;
-	add.s64 	%rd213, %rd212, %rd155;
-	add.s64 	%rd201, %rd213, 96;
-	// inline asm
-	cvta.to.global.u64 %rd200, %rd201;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r234,%r235,%r236,%r237}, [%rd200];
-	// inline asm
-	mov.b32 	 %f601, %r234;
-	mov.b32 	 %f602, %r235;
-	mov.b32 	 %f603, %r236;
-	add.s64 	%rd204, %rd213, 112;
-	// inline asm
-	cvta.to.global.u64 %rd203, %rd204;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r238,%r239,%r240,%r241}, [%rd203];
-	// inline asm
-	mov.b32 	 %f604, %r238;
-	mov.b32 	 %f605, %r239;
-	mov.b32 	 %f606, %r241;
-	add.s64 	%rd207, %rd213, 128;
-	// inline asm
-	cvta.to.global.u64 %rd206, %rd207;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r242,%r243,%r244,%r245}, [%rd206];
-	// inline asm
-	mov.b32 	 %f607, %r243;
-	mov.b32 	 %f608, %r244;
-	mov.b32 	 %f609, %r245;
-	add.s64 	%rd210, %rd213, 144;
-	// inline asm
-	cvta.to.global.u64 %rd209, %rd210;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r246,%r247,%r248,%r249}, [%rd209];
-	// inline asm
-	mov.f32 	%f610, 0f3F800000;
-	sub.f32 	%f611, %f610, %f209;
-	mul.f32 	%f612, %f209, %f601;
-	mul.f32 	%f613, %f209, %f602;
-	mul.f32 	%f614, %f209, %f603;
-	fma.rn.f32 	%f879, %f611, %f879, %f612;
-	fma.rn.f32 	%f880, %f611, %f880, %f613;
-	fma.rn.f32 	%f881, %f611, %f881, %f614;
-	mul.f32 	%f615, %f209, %f604;
-	mul.f32 	%f616, %f209, %f605;
-	mul.f32 	%f617, %f209, %f606;
-	fma.rn.f32 	%f882, %f611, %f882, %f615;
-	fma.rn.f32 	%f883, %f611, %f883, %f616;
-	fma.rn.f32 	%f884, %f611, %f884, %f617;
-	mul.f32 	%f618, %f209, %f607;
-	mul.f32 	%f619, %f209, %f608;
-	mul.f32 	%f620, %f209, %f609;
-	fma.rn.f32 	%f621, %f611, %f885, %f618;
-	fma.rn.f32 	%f622, %f611, %f886, %f619;
-	fma.rn.f32 	%f623, %f611, %f887, %f620;
-	mov.b32 	 %f624, %r246;
-	mul.f32 	%f625, %f209, %f624;
-	fma.rn.f32 	%f626, %f611, %f888, %f625;
-	mul.f32 	%f627, %f622, %f622;
-	fma.rn.f32 	%f628, %f621, %f621, %f627;
-	fma.rn.f32 	%f629, %f623, %f623, %f628;
-	fma.rn.f32 	%f630, %f626, %f626, %f629;
-	sqrt.rn.f32 	%f631, %f630;
-	rcp.rn.f32 	%f632, %f631;
-	mul.f32 	%f885, %f621, %f632;
-	mul.f32 	%f886, %f622, %f632;
-	mul.f32 	%f887, %f623, %f632;
-	mul.f32 	%f888, %f626, %f632;
-
-BB7_32:
-	mul.f32 	%f633, %f886, %f886;
-	fma.rn.f32 	%f634, %f885, %f885, %f633;
-	fma.rn.f32 	%f635, %f887, %f887, %f634;
-	fma.rn.f32 	%f636, %f888, %f888, %f635;
-	rcp.rn.f32 	%f637, %f636;
-	mul.f32 	%f638, %f885, %f637;
-	mul.f32 	%f639, %f886, %f637;
-	mul.f32 	%f640, %f887, %f637;
-	mul.f32 	%f641, %f888, %f637;
-	mul.f32 	%f642, %f885, %f638;
-	mul.f32 	%f643, %f886, %f639;
-	mul.f32 	%f644, %f887, %f640;
-	mul.f32 	%f645, %f885, %f639;
-	mul.f32 	%f646, %f887, %f641;
-	mul.f32 	%f647, %f885, %f640;
-	mul.f32 	%f648, %f886, %f641;
-	mul.f32 	%f649, %f886, %f640;
-	mul.f32 	%f650, %f885, %f641;
-	sub.f32 	%f651, %f642, %f643;
-	sub.f32 	%f652, %f651, %f644;
-	fma.rn.f32 	%f653, %f888, %f641, %f652;
-	sub.f32 	%f654, %f645, %f646;
-	add.f32 	%f655, %f654, %f654;
-	add.f32 	%f656, %f647, %f648;
-	add.f32 	%f657, %f656, %f656;
-	add.f32 	%f658, %f645, %f646;
-	add.f32 	%f659, %f658, %f658;
-	sub.f32 	%f660, %f643, %f642;
-	sub.f32 	%f661, %f660, %f644;
-	fma.rn.f32 	%f662, %f888, %f641, %f661;
-	sub.f32 	%f663, %f649, %f650;
-	add.f32 	%f664, %f663, %f663;
-	sub.f32 	%f665, %f647, %f648;
-	add.f32 	%f666, %f665, %f665;
-	add.f32 	%f667, %f649, %f650;
-	add.f32 	%f668, %f667, %f667;
-	neg.f32 	%f669, %f642;
-	sub.f32 	%f670, %f669, %f643;
-	add.f32 	%f671, %f644, %f670;
-	fma.rn.f32 	%f672, %f888, %f641, %f671;
-	mul.f32 	%f673, %f881, %f653;
-	fma.rn.f32 	%f674, %f883, %f655, %f673;
-	fma.rn.f32 	%f897, %f884, %f657, %f674;
-	mul.f32 	%f675, %f883, %f662;
-	fma.rn.f32 	%f676, %f881, %f659, %f675;
-	fma.rn.f32 	%f894, %f884, %f664, %f676;
-	mul.f32 	%f677, %f883, %f668;
-	fma.rn.f32 	%f678, %f881, %f666, %f677;
-	fma.rn.f32 	%f891, %f884, %f672, %f678;
-	mul.f32 	%f679, %f880, %f653;
-	fma.rn.f32 	%f896, %f882, %f655, %f679;
-	mul.f32 	%f680, %f882, %f662;
-	fma.rn.f32 	%f893, %f880, %f659, %f680;
-	mul.f32 	%f681, %f882, %f668;
-	fma.rn.f32 	%f890, %f880, %f666, %f681;
-	mul.f32 	%f895, %f879, %f653;
-	mul.f32 	%f892, %f879, %f659;
-	mul.f32 	%f889, %f879, %f666;
-
-BB7_35:
-	mul.f32 	%f713, %f890, %f894;
-	mul.f32 	%f714, %f891, %f893;
-	sub.f32 	%f715, %f714, %f713;
-	mul.f32 	%f716, %f895, %f715;
-	mul.f32 	%f717, %f889, %f894;
-	mul.f32 	%f718, %f891, %f892;
-	sub.f32 	%f719, %f718, %f717;
-	mul.f32 	%f720, %f719, %f896;
-	sub.f32 	%f721, %f716, %f720;
-	mul.f32 	%f722, %f889, %f893;
-	mul.f32 	%f723, %f890, %f892;
-	sub.f32 	%f724, %f723, %f722;
-	fma.rn.f32 	%f725, %f724, %f897, %f721;
-	rcp.rn.f32 	%f726, %f725;
-	mul.f32 	%f904, %f715, %f726;
-	mul.f32 	%f727, %f891, %f896;
-	mul.f32 	%f728, %f890, %f897;
-	sub.f32 	%f729, %f728, %f727;
-	mul.f32 	%f905, %f726, %f729;
-	mul.f32 	%f730, %f893, %f897;
-	mul.f32 	%f731, %f894, %f896;
-	sub.f32 	%f732, %f731, %f730;
-	mul.f32 	%f906, %f726, %f732;
-	sub.f32 	%f733, %f717, %f718;
-	mul.f32 	%f901, %f733, %f726;
-	mul.f32 	%f734, %f889, %f897;
-	mul.f32 	%f735, %f891, %f895;
-	sub.f32 	%f736, %f735, %f734;
-	mul.f32 	%f902, %f726, %f736;
-	mul.f32 	%f737, %f894, %f895;
-	mul.f32 	%f738, %f892, %f897;
-	sub.f32 	%f739, %f738, %f737;
-	mul.f32 	%f903, %f726, %f739;
-	mul.f32 	%f898, %f724, %f726;
-	mul.f32 	%f740, %f890, %f895;
-	mul.f32 	%f741, %f889, %f896;
-	sub.f32 	%f742, %f741, %f740;
-	mul.f32 	%f899, %f742, %f726;
-	mul.f32 	%f743, %f892, %f896;
-	mul.f32 	%f744, %f893, %f895;
-	sub.f32 	%f745, %f744, %f743;
-	mul.f32 	%f900, %f745, %f726;
-	bra.uni 	BB7_36;
-
-BB7_25:
-	setp.ne.s32	%p15, %r161, 1;
-	mov.f32 	%f899, %f898;
-	mov.f32 	%f901, %f898;
-	mov.f32 	%f902, %f900;
-	mov.f32 	%f903, %f898;
-	mov.f32 	%f904, %f900;
-	mov.f32 	%f905, %f898;
-	mov.f32 	%f906, %f898;
-	@%p15 bra 	BB7_36;
-
-	// inline asm
-	call (%rd142), _optix_get_static_transform_from_handle, (%rd140);
-	// inline asm
-	add.s64 	%rd264, %rd142, 64;
-
-BB7_28:
-	// inline asm
-	cvta.to.global.u64 %rd146, %rd264;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r163,%r164,%r165,%r166}, [%rd146];
-	// inline asm
-	mov.b32 	 %f904, %r163;
-	mov.b32 	 %f905, %r164;
-	mov.b32 	 %f906, %r165;
-	add.s64 	%rd150, %rd264, 16;
-	// inline asm
-	cvta.to.global.u64 %rd149, %rd150;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r167,%r168,%r169,%r170}, [%rd149];
-	// inline asm
-	mov.b32 	 %f901, %r167;
-	mov.b32 	 %f902, %r168;
-	mov.b32 	 %f903, %r169;
-	add.s64 	%rd153, %rd264, 32;
-	// inline asm
-	cvta.to.global.u64 %rd152, %rd153;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r171,%r172,%r173,%r174}, [%rd152];
-	// inline asm
-	mov.b32 	 %f898, %r171;
-	mov.b32 	 %f899, %r172;
-	mov.b32 	 %f900, %r173;
-
-BB7_36:
-	setp.eq.s32	%p19, %r319, 0;
-	@%p19 bra 	BB7_37;
-	bra.uni 	BB7_38;
-
-BB7_37:
-	mov.f32 	%f878, %f898;
-	mov.f32 	%f877, %f899;
-	mov.f32 	%f876, %f900;
-	mov.f32 	%f875, %f901;
-	mov.f32 	%f874, %f902;
-	mov.f32 	%f873, %f903;
-	mov.f32 	%f872, %f904;
-	mov.f32 	%f871, %f905;
-	mov.f32 	%f870, %f906;
-	bra.uni 	BB7_39;
-
-BB7_38:
-	mul.f32 	%f746, %f875, %f905;
-	fma.rn.f32 	%f747, %f872, %f904, %f746;
-	fma.rn.f32 	%f289, %f878, %f906, %f747;
-	mul.f32 	%f748, %f874, %f905;
-	fma.rn.f32 	%f749, %f871, %f904, %f748;
-	fma.rn.f32 	%f290, %f877, %f906, %f749;
-	mul.f32 	%f750, %f873, %f905;
-	fma.rn.f32 	%f751, %f870, %f904, %f750;
-	fma.rn.f32 	%f291, %f876, %f906, %f751;
-	mul.f32 	%f752, %f875, %f902;
-	fma.rn.f32 	%f753, %f872, %f901, %f752;
-	fma.rn.f32 	%f292, %f878, %f903, %f753;
-	mul.f32 	%f754, %f874, %f902;
-	fma.rn.f32 	%f755, %f871, %f901, %f754;
-	fma.rn.f32 	%f293, %f877, %f903, %f755;
-	mul.f32 	%f756, %f873, %f902;
-	fma.rn.f32 	%f757, %f870, %f901, %f756;
-	fma.rn.f32 	%f294, %f876, %f903, %f757;
-	mul.f32 	%f758, %f875, %f899;
-	fma.rn.f32 	%f759, %f872, %f898, %f758;
-	fma.rn.f32 	%f878, %f878, %f900, %f759;
-	mul.f32 	%f760, %f874, %f899;
-	fma.rn.f32 	%f761, %f871, %f898, %f760;
-	fma.rn.f32 	%f877, %f877, %f900, %f761;
-	mul.f32 	%f762, %f873, %f899;
-	fma.rn.f32 	%f763, %f870, %f898, %f762;
-	fma.rn.f32 	%f876, %f876, %f900, %f763;
-	mov.f32 	%f875, %f292;
-	mov.f32 	%f874, %f293;
-	mov.f32 	%f873, %f294;
-	mov.f32 	%f872, %f289;
-	mov.f32 	%f871, %f290;
-	mov.f32 	%f870, %f291;
-
-BB7_39:
-	add.s32 	%r319, %r319, 1;
-	setp.lt.u32	%p20, %r319, %r8;
-	@%p20 bra 	BB7_23;
-
-	mul.f32 	%f764, %f577, %f871;
-	fma.rn.f32 	%f765, %f576, %f872, %f764;
-	fma.rn.f32 	%f916, %f918, %f870, %f765;
-	mul.f32 	%f766, %f577, %f874;
-	fma.rn.f32 	%f767, %f576, %f875, %f766;
-	fma.rn.f32 	%f917, %f918, %f873, %f767;
-	mul.f32 	%f768, %f577, %f877;
-	fma.rn.f32 	%f769, %f576, %f878, %f768;
-	fma.rn.f32 	%f918, %f918, %f876, %f769;
-	bra.uni 	BB7_41;
-
-BB7_22:
-	mov.f32 	%f916, %f576;
-	mov.f32 	%f917, %f577;
-
-BB7_41:
-	// inline asm
-	call (%f770), _optix_get_ray_tmin, ();
-	// inline asm
-	// inline asm
-	call (%f771), _optix_get_ray_tmax, ();
-	// inline asm
-	ld.f32 	%f773, [%rd1+288];
-	sub.f32 	%f774, %f869, %f773;
-	ld.f32 	%f775, [%rd1+292];
-	sub.f32 	%f776, %f868, %f775;
-	ld.f32 	%f777, [%rd1+296];
-	sub.f32 	%f778, %f867, %f777;
-	mul.f32 	%f779, %f916, %f916;
-	fma.rn.f32 	%f780, %f917, %f917, %f779;
-	fma.rn.f32 	%f315, %f918, %f918, %f780;
-	mul.f32 	%f781, %f774, %f916;
-	fma.rn.f32 	%f782, %f776, %f917, %f781;
-	fma.rn.f32 	%f783, %f778, %f918, %f782;
-	add.f32 	%f316, %f783, %f783;
-	mul.f32 	%f784, %f774, %f774;
-	fma.rn.f32 	%f785, %f776, %f776, %f784;
-	fma.rn.f32 	%f786, %f778, %f778, %f785;
-	ld.f32 	%f787, [%rd1+304];
-	mul.f32 	%f788, %f787, %f787;
-	sub.f32 	%f317, %f786, %f788;
-	setp.eq.f32	%p21, %f315, 0f00000000;
-	setp.eq.f32	%p22, %f316, 0f00000000;
-	and.pred  	%p23, %p21, %p22;
-	mov.u16 	%rs13, 0;
-	@%p23 bra 	BB7_45;
-
-	neg.f32 	%f789, %f317;
-	div.rn.f32 	%f920, %f789, %f316;
-	mul.f32 	%f790, %f315, 0fC0800000;
-	mul.f32 	%f791, %f790, %f317;
-	fma.rn.f32 	%f319, %f316, %f316, %f791;
-	setp.lt.f32	%p24, %f319, 0f00000000;
-	setp.neu.f32	%p25, %f315, 0f00000000;
-	and.pred  	%p26, %p24, %p25;
-	@%p26 bra 	BB7_43;
-	bra.uni 	BB7_44;
-
-BB7_43:
-	mov.f32 	%f919, %f920;
-	bra.uni 	BB7_45;
-
-BB7_44:
-	mov.b32 	 %r309, %f316;
-	and.b32  	%r310, %r309, -2147483648;
-	sqrt.rn.f32 	%f792, %f319;
-	mov.b32 	 %r311, %f792;
-	and.b32  	%r312, %r311, 2147483647;
-	or.b32  	%r313, %r312, %r310;
-	mov.b32 	 %f793, %r313;
-	add.f32 	%f794, %f316, %f793;
-	mul.f32 	%f795, %f794, 0fBF000000;
-	div.rn.f32 	%f796, %f795, %f315;
-	div.rn.f32 	%f797, %f317, %f795;
-	min.f32 	%f798, %f796, %f797;
-	max.f32 	%f799, %f796, %f797;
-	selp.f32	%f919, %f920, %f798, %p21;
-	selp.f32	%f920, %f920, %f799, %p21;
-	mov.u16 	%rs13, 1;
-
-BB7_45:
-	setp.gtu.f32	%p29, %f919, %f771;
-	mov.pred 	%p38, 0;
-	@%p29 bra 	BB7_47;
-
-	setp.ge.f32	%p38, %f920, %f770;
-
-BB7_47:
-	setp.lt.f32	%p30, %f919, %f770;
-	setp.geu.f32	%p31, %f919, %f770;
-	setp.leu.f32	%p32, %f920, %f771;
-	or.pred  	%p33, %p32, %p31;
-	selp.f32	%f324, %f920, %f919, %p30;
-	setp.ne.s16	%p34, %rs13, 0;
-	and.pred  	%p35, %p34, %p38;
-	and.pred  	%p36, %p35, %p33;
-	@!%p36 bra 	BB7_49;
-	bra.uni 	BB7_48;
-
-BB7_48:
-	mov.u32 	%r315, 254;
-	// inline asm
-	call (%r314), _optix_report_intersection_0, (%f324, %r315);
-	// inline asm
-
-BB7_49:
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r238,%r239,%r240,%r241}, [%rd196];
+	// end inline asm
+	mov.b32 	%f632, %r238;
+	mov.b32 	%f633, %r239;
+	mov.b32 	%f634, %r240;
+	mul.f32 	%f635, %f218, %f632;
+	mul.f32 	%f636, %f218, %f633;
+	mul.f32 	%f637, %f218, %f634;
+	fma.rn.f32 	%f920, %f631, %f920, %f635;
+	fma.rn.f32 	%f921, %f631, %f921, %f636;
+	fma.rn.f32 	%f922, %f631, %f922, %f637;
+	add.s64 	%rd200, %rd184, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd199, %rd200;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r242,%r243,%r244,%r245}, [%rd199];
+	// end inline asm
+	mov.b32 	%f638, %r242;
+	mov.b32 	%f639, %r243;
+	mov.b32 	%f640, %r245;
+	mul.f32 	%f641, %f218, %f638;
+	mul.f32 	%f642, %f218, %f639;
+	mul.f32 	%f643, %f218, %f640;
+	fma.rn.f32 	%f923, %f631, %f923, %f641;
+	fma.rn.f32 	%f924, %f631, %f924, %f642;
+	fma.rn.f32 	%f925, %f631, %f925, %f643;
+	add.s64 	%rd203, %rd184, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd202, %rd203;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r246,%r247,%r248,%r249}, [%rd202];
+	// end inline asm
+	mov.b32 	%f644, %r247;
+	mov.b32 	%f645, %r248;
+	mov.b32 	%f646, %r249;
+	mul.f32 	%f647, %f218, %f644;
+	mul.f32 	%f648, %f218, %f645;
+	mul.f32 	%f649, %f218, %f646;
+	fma.rn.f32 	%f650, %f631, %f926, %f647;
+	fma.rn.f32 	%f651, %f631, %f927, %f648;
+	fma.rn.f32 	%f652, %f631, %f928, %f649;
+	add.s64 	%rd206, %rd184, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd205, %rd206;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r250,%r251,%r252,%r253}, [%rd205];
+	// end inline asm
+	mov.b32 	%f653, %r250;
+	mul.f32 	%f654, %f218, %f653;
+	fma.rn.f32 	%f655, %f631, %f929, %f654;
+	mul.f32 	%f656, %f651, %f651;
+	fma.rn.f32 	%f657, %f650, %f650, %f656;
+	fma.rn.f32 	%f658, %f652, %f652, %f657;
+	fma.rn.f32 	%f659, %f655, %f655, %f658;
+	sqrt.rn.f32 	%f660, %f659;
+	rcp.rn.f32 	%f661, %f660;
+	mul.f32 	%f926, %f650, %f661;
+	mul.f32 	%f927, %f651, %f661;
+	mul.f32 	%f928, %f652, %f661;
+	mul.f32 	%f929, %f661, %f655;
+
+$L__BB7_33:
+	mul.f32 	%f662, %f927, %f927;
+	fma.rn.f32 	%f663, %f926, %f926, %f662;
+	fma.rn.f32 	%f664, %f928, %f928, %f663;
+	fma.rn.f32 	%f665, %f929, %f929, %f664;
+	rcp.rn.f32 	%f666, %f665;
+	mul.f32 	%f667, %f926, %f666;
+	mul.f32 	%f668, %f927, %f666;
+	mul.f32 	%f669, %f928, %f666;
+	mul.f32 	%f670, %f929, %f666;
+	mul.f32 	%f671, %f926, %f667;
+	mul.f32 	%f672, %f927, %f668;
+	mul.f32 	%f673, %f928, %f669;
+	mul.f32 	%f674, %f926, %f668;
+	mul.f32 	%f675, %f928, %f670;
+	mul.f32 	%f676, %f926, %f669;
+	mul.f32 	%f677, %f927, %f670;
+	mul.f32 	%f678, %f927, %f669;
+	mul.f32 	%f679, %f926, %f670;
+	sub.f32 	%f680, %f671, %f672;
+	sub.f32 	%f681, %f680, %f673;
+	fma.rn.f32 	%f682, %f929, %f670, %f681;
+	sub.f32 	%f683, %f674, %f675;
+	add.f32 	%f684, %f683, %f683;
+	add.f32 	%f685, %f676, %f677;
+	add.f32 	%f686, %f685, %f685;
+	add.f32 	%f687, %f674, %f675;
+	add.f32 	%f688, %f687, %f687;
+	sub.f32 	%f689, %f672, %f671;
+	sub.f32 	%f690, %f689, %f673;
+	fma.rn.f32 	%f691, %f929, %f670, %f690;
+	sub.f32 	%f692, %f678, %f679;
+	add.f32 	%f693, %f692, %f692;
+	sub.f32 	%f694, %f676, %f677;
+	add.f32 	%f695, %f694, %f694;
+	add.f32 	%f696, %f678, %f679;
+	add.f32 	%f697, %f696, %f696;
+	neg.f32 	%f698, %f671;
+	sub.f32 	%f699, %f698, %f672;
+	add.f32 	%f700, %f673, %f699;
+	fma.rn.f32 	%f701, %f929, %f670, %f700;
+	mul.f32 	%f702, %f922, %f682;
+	fma.rn.f32 	%f703, %f924, %f684, %f702;
+	fma.rn.f32 	%f938, %f925, %f686, %f703;
+	mul.f32 	%f704, %f924, %f691;
+	fma.rn.f32 	%f705, %f922, %f688, %f704;
+	fma.rn.f32 	%f935, %f925, %f693, %f705;
+	mul.f32 	%f706, %f924, %f697;
+	fma.rn.f32 	%f707, %f922, %f695, %f706;
+	fma.rn.f32 	%f932, %f925, %f701, %f707;
+	mul.f32 	%f708, %f921, %f682;
+	fma.rn.f32 	%f937, %f923, %f684, %f708;
+	mul.f32 	%f709, %f923, %f691;
+	fma.rn.f32 	%f934, %f921, %f688, %f709;
+	mul.f32 	%f710, %f923, %f697;
+	fma.rn.f32 	%f931, %f921, %f695, %f710;
+	mul.f32 	%f936, %f920, %f682;
+	mul.f32 	%f933, %f920, %f688;
+	mul.f32 	%f930, %f920, %f695;
+
+$L__BB7_36:
+	mul.f32 	%f742, %f931, %f935;
+	mul.f32 	%f743, %f932, %f934;
+	sub.f32 	%f744, %f743, %f742;
+	mul.f32 	%f745, %f936, %f744;
+	mul.f32 	%f746, %f930, %f935;
+	mul.f32 	%f747, %f932, %f933;
+	sub.f32 	%f748, %f747, %f746;
+	mul.f32 	%f749, %f748, %f937;
+	sub.f32 	%f750, %f745, %f749;
+	mul.f32 	%f751, %f930, %f934;
+	mul.f32 	%f752, %f931, %f933;
+	sub.f32 	%f753, %f752, %f751;
+	fma.rn.f32 	%f754, %f753, %f938, %f750;
+	rcp.rn.f32 	%f755, %f754;
+	mul.f32 	%f945, %f744, %f755;
+	mul.f32 	%f756, %f932, %f937;
+	mul.f32 	%f757, %f931, %f938;
+	sub.f32 	%f758, %f757, %f756;
+	mul.f32 	%f946, %f758, %f755;
+	mul.f32 	%f759, %f934, %f938;
+	mul.f32 	%f760, %f935, %f937;
+	sub.f32 	%f761, %f760, %f759;
+	mul.f32 	%f947, %f761, %f755;
+	sub.f32 	%f762, %f746, %f747;
+	mul.f32 	%f942, %f762, %f755;
+	mul.f32 	%f763, %f930, %f938;
+	mul.f32 	%f764, %f932, %f936;
+	sub.f32 	%f765, %f764, %f763;
+	mul.f32 	%f943, %f765, %f755;
+	mul.f32 	%f766, %f935, %f936;
+	mul.f32 	%f767, %f933, %f938;
+	sub.f32 	%f768, %f767, %f766;
+	mul.f32 	%f944, %f768, %f755;
+	mul.f32 	%f939, %f753, %f755;
+	mul.f32 	%f769, %f931, %f936;
+	mul.f32 	%f770, %f930, %f937;
+	sub.f32 	%f771, %f770, %f769;
+	mul.f32 	%f940, %f771, %f755;
+	mul.f32 	%f772, %f933, %f937;
+	mul.f32 	%f773, %f934, %f936;
+	sub.f32 	%f774, %f773, %f772;
+	mul.f32 	%f941, %f774, %f755;
+	bra.uni 	$L__BB7_37;
+
+$L__BB7_28:
+	// begin inline asm
+	call (%rd257), _optix_get_instance_inverse_transform_from_handle, (%rd136);
+	// end inline asm
+
+$L__BB7_29:
+	// begin inline asm
+	cvta.to.global.u64 %rd142, %rd257;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r167,%r168,%r169,%r170}, [%rd142];
+	// end inline asm
+	mov.b32 	%f945, %r167;
+	mov.b32 	%f946, %r168;
+	mov.b32 	%f947, %r169;
+	add.s64 	%rd146, %rd257, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd145, %rd146;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r171,%r172,%r173,%r174}, [%rd145];
+	// end inline asm
+	mov.b32 	%f942, %r171;
+	mov.b32 	%f943, %r172;
+	mov.b32 	%f944, %r173;
+	add.s64 	%rd149, %rd257, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd148, %rd149;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r175,%r176,%r177,%r178}, [%rd148];
+	// end inline asm
+	mov.b32 	%f939, %r175;
+	mov.b32 	%f940, %r176;
+	mov.b32 	%f941, %r177;
+
+$L__BB7_37:
+	setp.eq.s32 	%p20, %r322, 0;
+	@%p20 bra 	$L__BB7_39;
+
+	mul.f32 	%f775, %f916, %f946;
+	fma.rn.f32 	%f776, %f913, %f945, %f775;
+	fma.rn.f32 	%f304, %f919, %f947, %f776;
+	mul.f32 	%f777, %f915, %f946;
+	fma.rn.f32 	%f778, %f912, %f945, %f777;
+	fma.rn.f32 	%f305, %f918, %f947, %f778;
+	mul.f32 	%f779, %f914, %f946;
+	fma.rn.f32 	%f780, %f911, %f945, %f779;
+	fma.rn.f32 	%f947, %f917, %f947, %f780;
+	mul.f32 	%f781, %f916, %f943;
+	fma.rn.f32 	%f782, %f913, %f942, %f781;
+	fma.rn.f32 	%f307, %f919, %f944, %f782;
+	mul.f32 	%f783, %f915, %f943;
+	fma.rn.f32 	%f784, %f912, %f942, %f783;
+	fma.rn.f32 	%f308, %f918, %f944, %f784;
+	mul.f32 	%f785, %f914, %f943;
+	fma.rn.f32 	%f786, %f911, %f942, %f785;
+	fma.rn.f32 	%f944, %f917, %f944, %f786;
+	mul.f32 	%f787, %f916, %f940;
+	fma.rn.f32 	%f788, %f913, %f939, %f787;
+	fma.rn.f32 	%f310, %f919, %f941, %f788;
+	mul.f32 	%f789, %f915, %f940;
+	fma.rn.f32 	%f790, %f912, %f939, %f789;
+	fma.rn.f32 	%f311, %f918, %f941, %f790;
+	mul.f32 	%f791, %f914, %f940;
+	fma.rn.f32 	%f792, %f911, %f939, %f791;
+	fma.rn.f32 	%f941, %f917, %f941, %f792;
+	mov.f32 	%f939, %f310;
+	mov.f32 	%f940, %f311;
+	mov.f32 	%f942, %f307;
+	mov.f32 	%f943, %f308;
+	mov.f32 	%f945, %f304;
+	mov.f32 	%f946, %f305;
+
+$L__BB7_39:
+	add.s32 	%r322, %r322, 1;
+	setp.lt.u32 	%p21, %r322, %r162;
+	mov.f32 	%f911, %f947;
+	mov.f32 	%f912, %f946;
+	mov.f32 	%f913, %f945;
+	mov.f32 	%f914, %f944;
+	mov.f32 	%f915, %f943;
+	mov.f32 	%f916, %f942;
+	mov.f32 	%f917, %f941;
+	mov.f32 	%f918, %f940;
+	mov.f32 	%f919, %f939;
+	@%p21 bra 	$L__BB7_24;
+
+$L__BB7_40:
+	mul.f32 	%f793, %f967, %f946;
+	fma.rn.f32 	%f794, %f966, %f945, %f793;
+	mul.f32 	%f795, %f967, %f943;
+	fma.rn.f32 	%f796, %f966, %f942, %f795;
+	mul.f32 	%f797, %f967, %f940;
+	fma.rn.f32 	%f798, %f966, %f939, %f797;
+	fma.rn.f32 	%f968, %f606, %f941, %f798;
+	fma.rn.f32 	%f967, %f606, %f944, %f796;
+	fma.rn.f32 	%f966, %f606, %f947, %f794;
+	bra.uni 	$L__BB7_42;
+
+$L__BB7_41:
+	mov.f32 	%f968, %f606;
+
+$L__BB7_42:
+	// begin inline asm
+	call (%f799), _optix_get_ray_tmin, ();
+	// end inline asm
+	// begin inline asm
+	call (%f800), _optix_get_ray_tmax, ();
+	// end inline asm
+	ld.f32 	%f803, [%rd1+288];
+	sub.f32 	%f804, %f908, %f803;
+	ld.f32 	%f805, [%rd1+292];
+	sub.f32 	%f806, %f909, %f805;
+	ld.f32 	%f807, [%rd1+296];
+	sub.f32 	%f808, %f910, %f807;
+	mul.f32 	%f809, %f966, %f966;
+	fma.rn.f32 	%f810, %f967, %f967, %f809;
+	fma.rn.f32 	%f342, %f968, %f968, %f810;
+	mul.f32 	%f811, %f804, %f966;
+	fma.rn.f32 	%f812, %f806, %f967, %f811;
+	fma.rn.f32 	%f813, %f808, %f968, %f812;
+	add.f32 	%f343, %f813, %f813;
+	mul.f32 	%f814, %f804, %f804;
+	fma.rn.f32 	%f815, %f806, %f806, %f814;
+	fma.rn.f32 	%f816, %f808, %f808, %f815;
+	ld.f32 	%f817, [%rd1+304];
+	mul.f32 	%f818, %f817, %f817;
+	sub.f32 	%f344, %f816, %f818;
+	setp.eq.f32 	%p23, %f342, 0f00000000;
+	setp.eq.f32 	%p24, %f343, 0f00000000;
+	and.pred  	%p25, %p23, %p24;
+	mov.pred 	%p42, 0;
+	@%p25 bra 	$L__BB7_45;
+
+	neg.f32 	%f819, %f344;
+	div.rn.f32 	%f969, %f819, %f343;
+	mul.f32 	%f820, %f342, 0fC0800000;
+	mul.f32 	%f821, %f820, %f344;
+	fma.rn.f32 	%f346, %f343, %f343, %f821;
+	setp.neu.f32 	%p27, %f342, 0f00000000;
+	setp.lt.f32 	%p28, %f346, 0f00000000;
+	and.pred  	%p29, %p28, %p27;
+	mov.f32 	%f970, %f969;
+	@%p29 bra 	$L__BB7_45;
+
+	mov.b32 	%r313, %f343;
+	and.b32  	%r314, %r313, -2147483648;
+	sqrt.rn.f32 	%f822, %f346;
+	mov.b32 	%r315, %f822;
+	and.b32  	%r316, %r315, 2147483647;
+	or.b32  	%r317, %r316, %r314;
+	mov.b32 	%f823, %r317;
+	add.f32 	%f824, %f343, %f823;
+	mul.f32 	%f825, %f824, 0fBF000000;
+	div.rn.f32 	%f826, %f825, %f342;
+	div.rn.f32 	%f827, %f344, %f825;
+	min.f32 	%f828, %f826, %f827;
+	max.f32 	%f829, %f826, %f827;
+	selp.f32 	%f347, %f969, %f828, %p23;
+	selp.f32 	%f970, %f969, %f829, %p23;
+	mov.pred 	%p42, -1;
+	mov.f32 	%f969, %f347;
+
+$L__BB7_45:
+	setp.lt.f32 	%p32, %f969, %f799;
+	selp.f32 	%f351, %f970, %f969, %p32;
+	setp.ge.f32 	%p33, %f970, %f799;
+	setp.le.f32 	%p34, %f969, %f800;
+	and.pred  	%p35, %p34, %p33;
+	and.pred  	%p36, %p42, %p35;
+	setp.leu.f32 	%p37, %f970, %f800;
+	setp.geu.f32 	%p38, %f969, %f799;
+	or.pred  	%p39, %p37, %p38;
+	and.pred  	%p40, %p39, %p36;
+	not.pred 	%p41, %p40;
+	@%p41 bra 	$L__BB7_47;
+
+	mov.u32 	%r319, 254;
+	// begin inline asm
+	call (%r318), _optix_report_intersection_0, (%f351, %r319);
+	// end inline asm
+
+$L__BB7_47:
 	ret;
-}
 
+}
 	// .globl	__closesthit__sphere
-.visible .entry __closesthit__sphere(
-
-)
+.visible .entry __closesthit__sphere()
 {
-	.reg .pred 	%p<71>;
-	.reg .b16 	%rs<19>;
-	.reg .f32 	%f<2104>;
-	.reg .b32 	%r<650>;
-	.reg .b64 	%rd<666>;
-
-
-	// inline asm
-	call (%r23), _optix_get_launch_dimension_x, ();
-	// inline asm
-	// inline asm
-	call (%r24), _optix_get_launch_dimension_y, ();
-	// inline asm
-	// inline asm
-	call (%r26), _optix_get_launch_index_x, ();
-	// inline asm
-	// inline asm
-	call (%r27), _optix_get_launch_index_y, ();
-	// inline asm
-	// inline asm
-	call (%r28), _optix_get_launch_index_z, ();
-	// inline asm
-	mad.lo.s32 	%r29, %r28, %r24, %r27;
-	mad.lo.s32 	%r1, %r29, %r23, %r26;
+	.reg .pred 	%p<75>;
+	.reg .b16 	%rs<3>;
+	.reg .f32 	%f<2144>;
+	.reg .b32 	%r<660>;
+	.reg .b64 	%rd<653>;
+
+
+	// begin inline asm
+	call (%r27), _optix_get_launch_dimension_x, ();
+	// end inline asm
+	// begin inline asm
+	call (%r28), _optix_get_launch_dimension_y, ();
+	// end inline asm
+	// begin inline asm
+	call (%r30), _optix_get_launch_index_x, ();
+	// end inline asm
+	// begin inline asm
+	call (%r31), _optix_get_launch_index_y, ();
+	// end inline asm
+	// begin inline asm
+	call (%r32), _optix_get_launch_index_z, ();
+	// end inline asm
+	mad.lo.s32 	%r33, %r32, %r28, %r31;
+	mad.lo.s32 	%r1, %r33, %r27, %r30;
 	ld.const.u64 	%rd1, [params+352];
-	setp.eq.s64	%p1, %rd1, 0;
-	@%p1 bra 	BB8_2;
+	setp.eq.s64 	%p1, %rd1, 0;
+	@%p1 bra 	$L__BB8_2;
 
-	cvta.to.global.u64 	%rd48, %rd1;
-	cvt.u64.u32	%rd49, %r1;
-	add.s64 	%rd50, %rd48, %rd49;
+	cvta.to.global.u64 	%rd44, %rd1;
+	cvt.u64.u32 	%rd45, %r1;
+	add.s64 	%rd46, %rd44, %rd45;
 	mov.u16 	%rs2, 1;
-	st.global.u8 	[%rd50], %rs2;
-	bra.uni 	BB8_116;
-
-BB8_2:
-	// inline asm
-	call (%rd51), _optix_get_sbt_data_ptr_64, ();
-	// inline asm
-	ld.u64 	%rd3, [%rd51+8];
-	// inline asm
-	call (%f703), _optix_get_world_ray_origin_x, ();
-	// inline asm
-	// inline asm
-	call (%f704), _optix_get_world_ray_origin_y, ();
-	// inline asm
-	// inline asm
-	call (%f1885), _optix_get_world_ray_origin_z, ();
-	// inline asm
-	// inline asm
-	call (%r30), _optix_get_transform_list_size, ();
-	// inline asm
-	setp.eq.s32	%p2, %r30, 0;
-	@%p2 bra 	BB8_3;
-
-	mov.u32 	%r646, 0;
-	// inline asm
-	call (%f706), _optix_get_ray_time, ();
-	// inline asm
-
-BB8_5:
+	st.global.u8 	[%rd46], %rs2;
+	bra.uni 	$L__BB8_117;
+
+$L__BB8_2:
+	// begin inline asm
+	call (%rd47), _optix_get_sbt_data_ptr_64, ();
+	// end inline asm
+	ld.u64 	%rd3, [%rd47+8];
+	// begin inline asm
+	call (%f1916), _optix_get_world_ray_origin_x, ();
+	// end inline asm
+	// begin inline asm
+	call (%f1917), _optix_get_world_ray_origin_y, ();
+	// end inline asm
+	// begin inline asm
+	call (%f1918), _optix_get_world_ray_origin_z, ();
+	// end inline asm
+	// begin inline asm
+	call (%r34), _optix_get_transform_list_size, ();
+	// end inline asm
+	setp.eq.s32 	%p2, %r34, 0;
+	@%p2 bra 	$L__BB8_23;
+
+	// begin inline asm
+	call (%r35), _optix_get_transform_list_size, ();
+	// end inline asm
+	// begin inline asm
+	call (%f734), _optix_get_ray_time, ();
+	// end inline asm
+	setp.eq.s32 	%p3, %r35, 0;
+	@%p3 bra 	$L__BB8_21;
+
+	mov.u32 	%r655, 0;
+
+$L__BB8_5:
 	.pragma "nounroll";
-	// inline asm
-	call (%rd52), _optix_get_transform_list_handle, (%r646);
-	// inline asm
-	// inline asm
-	call (%r33), _optix_get_transform_type_from_handle, (%rd52);
-	// inline asm
-	and.b32  	%r34, %r33, -2;
-	setp.eq.s32	%p3, %r34, 2;
-	@%p3 bra 	BB8_11;
-	bra.uni 	BB8_6;
-
-BB8_11:
-	setp.eq.s32	%p6, %r33, 2;
-	@%p6 bra 	BB8_15;
-	bra.uni 	BB8_12;
-
-BB8_15:
-	// inline asm
-	call (%rd126), _optix_get_matrix_motion_transform_from_handle, (%rd52);
-	// inline asm
-	// inline asm
-	cvta.to.global.u64 %rd128, %rd126;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r122,%r123,%r124,%r125}, [%rd128];
-	// inline asm
-	mov.b32	{%rs5, %rs6}, %r124;
-	add.s64 	%rd132, %rd126, 16;
-	// inline asm
+	// begin inline asm
+	call (%rd48), _optix_get_transform_list_handle, (%r655);
+	// end inline asm
+	// begin inline asm
+	call (%r38), _optix_get_transform_type_from_handle, (%rd48);
+	// end inline asm
+	or.b32  	%r39, %r38, 1;
+	setp.eq.s32 	%p4, %r39, 3;
+	@%p4 bra 	$L__BB8_11;
+	bra.uni 	$L__BB8_6;
+
+$L__BB8_11:
+	setp.eq.s32 	%p7, %r38, 2;
+	@%p7 bra 	$L__BB8_15;
+	bra.uni 	$L__BB8_12;
+
+$L__BB8_15:
+	// begin inline asm
+	call (%rd120), _optix_get_matrix_motion_transform_from_handle, (%rd48);
+	// end inline asm
+	// begin inline asm
+	cvta.to.global.u64 %rd122, %rd120;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r127,%r128,%r129,%r130}, [%rd122];
+	// end inline asm
+	add.s64 	%rd126, %rd120, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd125, %rd126;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r131,%r132,%r133,%r134}, [%rd125];
+	// end inline asm
+	add.s64 	%rd129, %rd120, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd128, %rd129;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r135,%r136,%r137,%r138}, [%rd128];
+	// end inline asm
+	add.s64 	%rd132, %rd120, 48;
+	// begin inline asm
 	cvta.to.global.u64 %rd131, %rd132;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r126,%r127,%r128,%r129}, [%rd131];
-	// inline asm
-	add.s64 	%rd135, %rd126, 32;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r139,%r140,%r141,%r142}, [%rd131];
+	// end inline asm
+	add.s64 	%rd135, %rd120, 64;
+	// begin inline asm
 	cvta.to.global.u64 %rd134, %rd135;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r130,%r131,%r132,%r133}, [%rd134];
-	// inline asm
-	add.s64 	%rd138, %rd126, 48;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r143,%r144,%r145,%r146}, [%rd134];
+	// end inline asm
+	add.s64 	%rd138, %rd120, 80;
+	// begin inline asm
 	cvta.to.global.u64 %rd137, %rd138;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r134,%r135,%r136,%r137}, [%rd137];
-	// inline asm
-	add.s64 	%rd141, %rd126, 64;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r147,%r148,%r149,%r150}, [%rd137];
+	// end inline asm
+	add.s64 	%rd141, %rd120, 96;
+	// begin inline asm
 	cvta.to.global.u64 %rd140, %rd141;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r138,%r139,%r140,%r141}, [%rd140];
-	// inline asm
-	add.s64 	%rd144, %rd126, 80;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r151,%r152,%r153,%r154}, [%rd140];
+	// end inline asm
+	add.s64 	%rd144, %rd120, 112;
+	// begin inline asm
 	cvta.to.global.u64 %rd143, %rd144;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r142,%r143,%r144,%r145}, [%rd143];
-	// inline asm
-	add.s64 	%rd147, %rd126, 96;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r155,%r156,%r157,%r158}, [%rd143];
+	// end inline asm
+	mov.b32 	%f862, %r130;
+	mov.b32 	%f863, %r131;
+	and.b32  	%r171, %r129, 65535;
+	add.s32 	%r172, %r171, -1;
+	cvt.rn.f32.s32 	%f864, %r172;
+	sub.f32 	%f865, %f734, %f862;
+	mul.f32 	%f866, %f865, %f864;
+	sub.f32 	%f867, %f863, %f862;
+	div.rn.f32 	%f868, %f866, %f867;
+	min.f32 	%f869, %f864, %f868;
+	mov.f32 	%f870, 0f00000000;
+	max.f32 	%f871, %f870, %f869;
+	cvt.rmi.f32.f32 	%f872, %f871;
+	sub.f32 	%f90, %f871, %f872;
+	cvt.rzi.s32.f32 	%r173, %f872;
+	mul.wide.s32 	%rd155, %r173, 48;
+	add.s64 	%rd147, %rd129, %rd155;
+	// begin inline asm
 	cvta.to.global.u64 %rd146, %rd147;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r146,%r147,%r148,%r149}, [%rd146];
-	// inline asm
-	add.s64 	%rd150, %rd126, 112;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r159,%r160,%r161,%r162}, [%rd146];
+	// end inline asm
+	mov.b32 	%f1871, %r159;
+	mov.b32 	%f1870, %r160;
+	mov.b32 	%f1869, %r161;
+	mov.b32 	%f1868, %r162;
+	add.s64 	%rd150, %rd147, 16;
+	// begin inline asm
 	cvta.to.global.u64 %rd149, %rd150;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r150,%r151,%r152,%r153}, [%rd149];
-	// inline asm
-	mov.b32 	 %f833, %r125;
-	mov.b32 	 %f834, %r126;
-	cvt.u32.u16	%r166, %rs5;
-	add.s32 	%r167, %r166, -1;
-	cvt.rn.f32.s32	%f835, %r167;
-	sub.f32 	%f836, %f706, %f833;
-	mul.f32 	%f837, %f836, %f835;
-	sub.f32 	%f838, %f834, %f833;
-	div.rn.f32 	%f839, %f837, %f838;
-	min.f32 	%f840, %f835, %f839;
-	mov.f32 	%f841, 0f00000000;
-	max.f32 	%f842, %f841, %f840;
-	cvt.rmi.f32.f32	%f843, %f842;
-	cvt.rzi.s32.f32	%r168, %f843;
-	mul.wide.s32 	%rd161, %r168, 48;
-	add.s64 	%rd153, %rd135, %rd161;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r163,%r164,%r165,%r166}, [%rd149];
+	// end inline asm
+	mov.b32 	%f1875, %r163;
+	mov.b32 	%f1874, %r164;
+	mov.b32 	%f1873, %r165;
+	mov.b32 	%f1872, %r166;
+	add.s64 	%rd153, %rd147, 32;
+	// begin inline asm
 	cvta.to.global.u64 %rd152, %rd153;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r154,%r155,%r156,%r157}, [%rd152];
-	// inline asm
-	mov.b32 	 %f1852, %r154;
-	mov.b32 	 %f1851, %r155;
-	mov.b32 	 %f1850, %r156;
-	mov.b32 	 %f1849, %r157;
-	add.s64 	%rd156, %rd153, 16;
-	// inline asm
-	cvta.to.global.u64 %rd155, %rd156;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r158,%r159,%r160,%r161}, [%rd155];
-	// inline asm
-	mov.b32 	 %f1856, %r158;
-	mov.b32 	 %f1855, %r159;
-	mov.b32 	 %f1854, %r160;
-	mov.b32 	 %f1853, %r161;
-	add.s64 	%rd159, %rd153, 32;
-	// inline asm
-	cvta.to.global.u64 %rd158, %rd159;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r162,%r163,%r164,%r165}, [%rd158];
-	// inline asm
-	sub.f32 	%f98, %f842, %f843;
-	mov.b32 	 %f1860, %r162;
-	mov.b32 	 %f1859, %r163;
-	mov.b32 	 %f1858, %r164;
-	mov.b32 	 %f1857, %r165;
-	setp.leu.f32	%p8, %f98, 0f00000000;
-	@%p8 bra 	BB8_17;
-
-	cvt.rmi.f32.f32	%f1820, %f842;
-	cvt.rzi.s32.f32	%r645, %f1820;
-	cvt.s64.s32	%rd661, %r645;
-	mul.lo.s64 	%rd171, %rd661, 48;
-	add.s64 	%rd172, %rd126, %rd171;
-	add.s64 	%rd163, %rd172, 80;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r167,%r168,%r169,%r170}, [%rd152];
+	// end inline asm
+	mov.b32 	%f1879, %r167;
+	mov.b32 	%f1878, %r168;
+	mov.b32 	%f1877, %r169;
+	mov.b32 	%f1876, %r170;
+	setp.leu.f32 	%p9, %f90, 0f00000000;
+	@%p9 bra 	$L__BB8_17;
+
+	cvt.rmi.f32.f32 	%f1839, %f871;
+	cvt.rzi.s32.f32 	%r654, %f1839;
+	cvt.s64.s32 	%rd648, %r654;
+	mov.f32 	%f873, 0f3F800000;
+	sub.f32 	%f874, %f873, %f90;
+	mul.lo.s64 	%rd165, %rd648, 48;
+	add.s64 	%rd166, %rd120, %rd165;
+	add.s64 	%rd157, %rd166, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd156, %rd157;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r174,%r175,%r176,%r177}, [%rd156];
+	// end inline asm
+	mov.b32 	%f875, %r174;
+	mov.b32 	%f876, %r175;
+	mov.b32 	%f877, %r176;
+	mov.b32 	%f878, %r177;
+	mul.f32 	%f879, %f90, %f875;
+	mul.f32 	%f880, %f90, %f876;
+	mul.f32 	%f881, %f90, %f877;
+	mul.f32 	%f882, %f90, %f878;
+	fma.rn.f32 	%f1871, %f874, %f1871, %f879;
+	fma.rn.f32 	%f1870, %f874, %f1870, %f880;
+	fma.rn.f32 	%f1869, %f874, %f1869, %f881;
+	fma.rn.f32 	%f1868, %f874, %f1868, %f882;
+	add.s64 	%rd160, %rd166, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd159, %rd160;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r178,%r179,%r180,%r181}, [%rd159];
+	// end inline asm
+	mov.b32 	%f883, %r178;
+	mov.b32 	%f884, %r179;
+	mov.b32 	%f885, %r180;
+	mov.b32 	%f886, %r181;
+	mul.f32 	%f887, %f90, %f883;
+	mul.f32 	%f888, %f90, %f884;
+	mul.f32 	%f889, %f90, %f885;
+	mul.f32 	%f890, %f90, %f886;
+	fma.rn.f32 	%f1875, %f874, %f1875, %f887;
+	fma.rn.f32 	%f1874, %f874, %f1874, %f888;
+	fma.rn.f32 	%f1873, %f874, %f1873, %f889;
+	fma.rn.f32 	%f1872, %f874, %f1872, %f890;
+	add.s64 	%rd163, %rd166, 112;
+	// begin inline asm
 	cvta.to.global.u64 %rd162, %rd163;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r169,%r170,%r171,%r172}, [%rd162];
-	// inline asm
-	mov.b32 	 %f844, %r169;
-	mov.b32 	 %f845, %r170;
-	mov.b32 	 %f846, %r171;
-	mov.b32 	 %f847, %r172;
-	add.s64 	%rd166, %rd172, 96;
-	// inline asm
-	cvta.to.global.u64 %rd165, %rd166;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r173,%r174,%r175,%r176}, [%rd165];
-	// inline asm
-	mov.b32 	 %f848, %r173;
-	mov.b32 	 %f849, %r174;
-	mov.b32 	 %f850, %r175;
-	mov.b32 	 %f851, %r176;
-	add.s64 	%rd169, %rd172, 112;
-	// inline asm
-	cvta.to.global.u64 %rd168, %rd169;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r177,%r178,%r179,%r180}, [%rd168];
-	// inline asm
-	mov.f32 	%f852, 0f3F800000;
-	sub.f32 	%f853, %f852, %f98;
-	mul.f32 	%f854, %f98, %f844;
-	mul.f32 	%f855, %f98, %f845;
-	mul.f32 	%f856, %f98, %f846;
-	mul.f32 	%f857, %f98, %f847;
-	fma.rn.f32 	%f1852, %f853, %f1852, %f854;
-	fma.rn.f32 	%f1851, %f853, %f1851, %f855;
-	fma.rn.f32 	%f1850, %f853, %f1850, %f856;
-	fma.rn.f32 	%f1849, %f853, %f1849, %f857;
-	mul.f32 	%f858, %f98, %f848;
-	mul.f32 	%f859, %f98, %f849;
-	mul.f32 	%f860, %f98, %f850;
-	mul.f32 	%f861, %f98, %f851;
-	fma.rn.f32 	%f1856, %f853, %f1856, %f858;
-	fma.rn.f32 	%f1855, %f853, %f1855, %f859;
-	fma.rn.f32 	%f1854, %f853, %f1854, %f860;
-	fma.rn.f32 	%f1853, %f853, %f1853, %f861;
-	mov.b32 	 %f862, %r177;
-	mov.b32 	 %f863, %r178;
-	mov.b32 	 %f864, %r179;
-	mov.b32 	 %f865, %r180;
-	mul.f32 	%f866, %f98, %f862;
-	mul.f32 	%f867, %f98, %f863;
-	mul.f32 	%f868, %f98, %f864;
-	mul.f32 	%f869, %f98, %f865;
-	fma.rn.f32 	%f1860, %f853, %f1860, %f866;
-	fma.rn.f32 	%f1859, %f853, %f1859, %f867;
-	fma.rn.f32 	%f1858, %f853, %f1858, %f868;
-	fma.rn.f32 	%f1857, %f853, %f1857, %f869;
-	bra.uni 	BB8_17;
-
-BB8_6:
-	mov.f32 	%f1861, 0f00000000;
-	mov.f32 	%f1864, 0f3F800000;
-	setp.eq.s32	%p4, %r33, 4;
-	@%p4 bra 	BB8_9;
-	bra.uni 	BB8_7;
-
-BB8_9:
-	// inline asm
-	call (%rd662), _optix_get_instance_inverse_transform_from_handle, (%rd52);
-	// inline asm
-	bra.uni 	BB8_10;
-
-BB8_12:
-	// inline asm
-	call (%rd67), _optix_get_srt_motion_transform_from_handle, (%rd52);
-	// inline asm
-	// inline asm
-	cvta.to.global.u64 %rd69, %rd67;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r47,%r48,%r49,%r50}, [%rd69];
-	// inline asm
-	mov.b32	{%rs3, %rs4}, %r49;
-	add.s64 	%rd73, %rd67, 16;
-	// inline asm
-	cvta.to.global.u64 %rd72, %rd73;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r51,%r52,%r53,%r54}, [%rd72];
-	// inline asm
-	add.s64 	%rd76, %rd67, 32;
-	// inline asm
-	cvta.to.global.u64 %rd75, %rd76;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r55,%r56,%r57,%r58}, [%rd75];
-	// inline asm
-	add.s64 	%rd79, %rd67, 48;
-	// inline asm
-	cvta.to.global.u64 %rd78, %rd79;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r59,%r60,%r61,%r62}, [%rd78];
-	// inline asm
-	add.s64 	%rd82, %rd67, 64;
-	// inline asm
-	cvta.to.global.u64 %rd81, %rd82;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r63,%r64,%r65,%r66}, [%rd81];
-	// inline asm
-	add.s64 	%rd85, %rd67, 80;
-	// inline asm
-	cvta.to.global.u64 %rd84, %rd85;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r67,%r68,%r69,%r70}, [%rd84];
-	// inline asm
-	add.s64 	%rd88, %rd67, 96;
-	// inline asm
-	cvta.to.global.u64 %rd87, %rd88;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r71,%r72,%r73,%r74}, [%rd87];
-	// inline asm
-	add.s64 	%rd91, %rd67, 112;
-	// inline asm
-	cvta.to.global.u64 %rd90, %rd91;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r75,%r76,%r77,%r78}, [%rd90];
-	// inline asm
-	add.s64 	%rd94, %rd67, 128;
-	// inline asm
-	cvta.to.global.u64 %rd93, %rd94;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r79,%r80,%r81,%r82}, [%rd93];
-	// inline asm
-	add.s64 	%rd97, %rd67, 144;
-	// inline asm
-	cvta.to.global.u64 %rd96, %rd97;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r83,%r84,%r85,%r86}, [%rd96];
-	// inline asm
-	mov.b32 	 %f720, %r50;
-	mov.b32 	 %f721, %r51;
-	cvt.u32.u16	%r103, %rs3;
-	add.s32 	%r104, %r103, -1;
-	cvt.rn.f32.s32	%f722, %r104;
-	sub.f32 	%f723, %f706, %f720;
-	mul.f32 	%f724, %f723, %f722;
-	sub.f32 	%f725, %f721, %f720;
-	div.rn.f32 	%f726, %f724, %f725;
-	min.f32 	%f727, %f722, %f726;
-	mov.f32 	%f728, 0f00000000;
-	max.f32 	%f729, %f728, %f727;
-	cvt.rmi.f32.f32	%f730, %f729;
-	cvt.rzi.s32.f32	%r105, %f730;
-	mul.wide.s32 	%rd111, %r105, 64;
-	add.s64 	%rd100, %rd76, %rd111;
-	// inline asm
-	cvta.to.global.u64 %rd99, %rd100;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r87,%r88,%r89,%r90}, [%rd99];
-	// inline asm
-	mov.b32 	 %f1833, %r87;
-	mov.b32 	 %f1834, %r88;
-	mov.b32 	 %f1835, %r89;
-	mov.b32 	 %f1836, %r90;
-	add.s64 	%rd103, %rd100, 16;
-	// inline asm
-	cvta.to.global.u64 %rd102, %rd103;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r91,%r92,%r93,%r94}, [%rd102];
-	// inline asm
-	mov.b32 	 %f1837, %r91;
-	mov.b32 	 %f1838, %r92;
-	mov.b32 	 %f1839, %r93;
-	mov.b32 	 %f1840, %r94;
-	add.s64 	%rd106, %rd100, 32;
-	// inline asm
-	cvta.to.global.u64 %rd105, %rd106;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r95,%r96,%r97,%r98}, [%rd105];
-	// inline asm
-	sub.f32 	%f37, %f729, %f730;
-	mov.b32 	 %f1841, %r95;
-	mov.b32 	 %f1842, %r96;
-	mov.b32 	 %f1843, %r97;
-	mov.b32 	 %f1844, %r98;
-	add.s64 	%rd109, %rd100, 48;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r182,%r183,%r184,%r185}, [%rd162];
+	// end inline asm
+	mov.b32 	%f891, %r182;
+	mov.b32 	%f892, %r183;
+	mov.b32 	%f893, %r184;
+	mov.b32 	%f894, %r185;
+	mul.f32 	%f895, %f90, %f891;
+	mul.f32 	%f896, %f90, %f892;
+	mul.f32 	%f897, %f90, %f893;
+	mul.f32 	%f898, %f90, %f894;
+	fma.rn.f32 	%f1879, %f874, %f1879, %f895;
+	fma.rn.f32 	%f1878, %f874, %f1878, %f896;
+	fma.rn.f32 	%f1877, %f874, %f1877, %f897;
+	fma.rn.f32 	%f1876, %f874, %f1876, %f898;
+	bra.uni 	$L__BB8_17;
+
+$L__BB8_6:
+	mov.f32 	%f1880, 0f00000000;
+	mov.f32 	%f1883, 0f3F800000;
+	setp.eq.s32 	%p5, %r38, 4;
+	@%p5 bra 	$L__BB8_9;
+
+	setp.ne.s32 	%p6, %r38, 1;
+	mov.f32 	%f1881, %f1880;
+	mov.f32 	%f1882, %f1880;
+	mov.f32 	%f1884, %f1880;
+	mov.f32 	%f1885, %f1880;
+	mov.f32 	%f1886, %f1883;
+	mov.f32 	%f1887, %f1880;
+	mov.f32 	%f1888, %f1880;
+	mov.f32 	%f1889, %f1883;
+	mov.f32 	%f1890, %f1880;
+	mov.f32 	%f1891, %f1880;
+	@%p6 bra 	$L__BB8_18;
+
+	// begin inline asm
+	call (%rd50), _optix_get_static_transform_from_handle, (%rd48);
+	// end inline asm
+	add.s64 	%rd649, %rd50, 64;
+	bra.uni 	$L__BB8_10;
+
+$L__BB8_12:
+	// begin inline asm
+	call (%rd63), _optix_get_srt_motion_transform_from_handle, (%rd48);
+	// end inline asm
+	// begin inline asm
+	cvta.to.global.u64 %rd65, %rd63;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r52,%r53,%r54,%r55}, [%rd65];
+	// end inline asm
+	add.s64 	%rd69, %rd63, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd68, %rd69;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r56,%r57,%r58,%r59}, [%rd68];
+	// end inline asm
+	add.s64 	%rd72, %rd63, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd71, %rd72;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r60,%r61,%r62,%r63}, [%rd71];
+	// end inline asm
+	add.s64 	%rd75, %rd63, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd74, %rd75;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r64,%r65,%r66,%r67}, [%rd74];
+	// end inline asm
+	add.s64 	%rd78, %rd63, 64;
+	// begin inline asm
+	cvta.to.global.u64 %rd77, %rd78;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r68,%r69,%r70,%r71}, [%rd77];
+	// end inline asm
+	add.s64 	%rd81, %rd63, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd80, %rd81;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r72,%r73,%r74,%r75}, [%rd80];
+	// end inline asm
+	add.s64 	%rd84, %rd63, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd83, %rd84;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r76,%r77,%r78,%r79}, [%rd83];
+	// end inline asm
+	add.s64 	%rd87, %rd63, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd86, %rd87;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r80,%r81,%r82,%r83}, [%rd86];
+	// end inline asm
+	add.s64 	%rd90, %rd63, 128;
+	// begin inline asm
+	cvta.to.global.u64 %rd89, %rd90;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r84,%r85,%r86,%r87}, [%rd89];
+	// end inline asm
+	add.s64 	%rd93, %rd63, 144;
+	// begin inline asm
+	cvta.to.global.u64 %rd92, %rd93;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r88,%r89,%r90,%r91}, [%rd92];
+	// end inline asm
+	mov.b32 	%f749, %r55;
+	mov.b32 	%f750, %r56;
+	and.b32  	%r108, %r54, 65535;
+	add.s32 	%r109, %r108, -1;
+	cvt.rn.f32.s32 	%f751, %r109;
+	sub.f32 	%f752, %f734, %f749;
+	mul.f32 	%f753, %f752, %f751;
+	sub.f32 	%f754, %f750, %f749;
+	div.rn.f32 	%f755, %f753, %f754;
+	min.f32 	%f756, %f751, %f755;
+	mov.f32 	%f757, 0f00000000;
+	max.f32 	%f758, %f757, %f756;
+	cvt.rmi.f32.f32 	%f759, %f758;
+	sub.f32 	%f29, %f758, %f759;
+	cvt.rzi.s32.f32 	%r110, %f759;
+	mul.wide.s32 	%rd107, %r110, 64;
+	add.s64 	%rd96, %rd72, %rd107;
+	// begin inline asm
+	cvta.to.global.u64 %rd95, %rd96;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r92,%r93,%r94,%r95}, [%rd95];
+	// end inline asm
+	mov.b32 	%f1852, %r92;
+	mov.b32 	%f1853, %r93;
+	mov.b32 	%f1854, %r94;
+	mov.b32 	%f1855, %r95;
+	add.s64 	%rd99, %rd96, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd98, %rd99;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r96,%r97,%r98,%r99}, [%rd98];
+	// end inline asm
+	mov.b32 	%f1856, %r96;
+	mov.b32 	%f1857, %r97;
+	mov.b32 	%f1858, %r98;
+	mov.b32 	%f1859, %r99;
+	add.s64 	%rd102, %rd96, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd101, %rd102;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r100,%r101,%r102,%r103}, [%rd101];
+	// end inline asm
+	mov.b32 	%f1860, %r100;
+	mov.b32 	%f1861, %r101;
+	mov.b32 	%f1862, %r102;
+	mov.b32 	%f1863, %r103;
+	add.s64 	%rd105, %rd96, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd104, %rd105;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r104,%r105,%r106,%r107}, [%rd104];
+	// end inline asm
+	mov.b32 	%f1864, %r104;
+	mov.b32 	%f1865, %r105;
+	mov.b32 	%f1866, %r106;
+	mov.b32 	%f1867, %r107;
+	setp.leu.f32 	%p8, %f29, 0f00000000;
+	@%p8 bra 	$L__BB8_14;
+
+	mov.f32 	%f760, 0f3F800000;
+	sub.f32 	%f761, %f760, %f29;
+	add.s64 	%rd109, %rd96, 64;
+	// begin inline asm
 	cvta.to.global.u64 %rd108, %rd109;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r99,%r100,%r101,%r102}, [%rd108];
-	// inline asm
-	mov.b32 	 %f1845, %r99;
-	mov.b32 	 %f1846, %r100;
-	mov.b32 	 %f1847, %r101;
-	mov.b32 	 %f1848, %r102;
-	setp.leu.f32	%p7, %f37, 0f00000000;
-	@%p7 bra 	BB8_14;
-
-	cvt.rmi.f32.f32	%f1819, %f729;
-	cvt.rzi.s32.f32	%r644, %f1819;
-	cvt.s64.s32	%rd660, %r644;
-	shl.b64 	%rd124, %rd660, 6;
-	add.s64 	%rd125, %rd124, %rd67;
-	add.s64 	%rd113, %rd125, 96;
-	// inline asm
-	cvta.to.global.u64 %rd112, %rd113;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r106,%r107,%r108,%r109}, [%rd112];
-	// inline asm
-	mov.b32 	 %f731, %r106;
-	mov.b32 	 %f732, %r107;
-	mov.b32 	 %f733, %r108;
-	mov.b32 	 %f734, %r109;
-	add.s64 	%rd116, %rd125, 112;
-	// inline asm
-	cvta.to.global.u64 %rd115, %rd116;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r110,%r111,%r112,%r113}, [%rd115];
-	// inline asm
-	mov.b32 	 %f735, %r110;
-	mov.b32 	 %f736, %r111;
-	mov.b32 	 %f737, %r112;
-	mov.b32 	 %f738, %r113;
-	add.s64 	%rd119, %rd125, 128;
-	// inline asm
-	cvta.to.global.u64 %rd118, %rd119;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r114,%r115,%r116,%r117}, [%rd118];
-	// inline asm
-	mov.b32 	 %f739, %r114;
-	mov.b32 	 %f740, %r115;
-	mov.b32 	 %f741, %r116;
-	mov.b32 	 %f742, %r117;
-	add.s64 	%rd122, %rd125, 144;
-	// inline asm
-	cvta.to.global.u64 %rd121, %rd122;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r118,%r119,%r120,%r121}, [%rd121];
-	// inline asm
-	mov.f32 	%f743, 0f3F800000;
-	sub.f32 	%f744, %f743, %f37;
-	mul.f32 	%f745, %f37, %f731;
-	mul.f32 	%f746, %f37, %f732;
-	mul.f32 	%f747, %f37, %f733;
-	mul.f32 	%f748, %f37, %f734;
-	fma.rn.f32 	%f1833, %f744, %f1833, %f745;
-	fma.rn.f32 	%f1834, %f744, %f1834, %f746;
-	fma.rn.f32 	%f1835, %f744, %f1835, %f747;
-	fma.rn.f32 	%f1836, %f744, %f1836, %f748;
-	mul.f32 	%f749, %f37, %f735;
-	mul.f32 	%f750, %f37, %f736;
-	mul.f32 	%f751, %f37, %f737;
-	mul.f32 	%f752, %f37, %f738;
-	fma.rn.f32 	%f1837, %f744, %f1837, %f749;
-	fma.rn.f32 	%f1838, %f744, %f1838, %f750;
-	fma.rn.f32 	%f1839, %f744, %f1839, %f751;
-	fma.rn.f32 	%f1840, %f744, %f1840, %f752;
-	mul.f32 	%f753, %f37, %f739;
-	mul.f32 	%f754, %f37, %f740;
-	mul.f32 	%f755, %f37, %f741;
-	mul.f32 	%f756, %f37, %f742;
-	fma.rn.f32 	%f1841, %f744, %f1841, %f753;
-	fma.rn.f32 	%f757, %f744, %f1842, %f754;
-	fma.rn.f32 	%f758, %f744, %f1843, %f755;
-	fma.rn.f32 	%f759, %f744, %f1844, %f756;
-	mov.b32 	 %f760, %r118;
-	mov.b32 	 %f761, %r119;
-	mov.b32 	 %f762, %r120;
-	mov.b32 	 %f763, %r121;
-	mul.f32 	%f764, %f37, %f760;
-	mul.f32 	%f765, %f37, %f761;
-	mul.f32 	%f766, %f37, %f762;
-	mul.f32 	%f767, %f37, %f763;
-	fma.rn.f32 	%f768, %f744, %f1845, %f764;
-	fma.rn.f32 	%f1846, %f744, %f1846, %f765;
-	fma.rn.f32 	%f1847, %f744, %f1847, %f766;
-	fma.rn.f32 	%f1848, %f744, %f1848, %f767;
-	mul.f32 	%f769, %f758, %f758;
-	fma.rn.f32 	%f770, %f757, %f757, %f769;
-	fma.rn.f32 	%f771, %f759, %f759, %f770;
-	fma.rn.f32 	%f772, %f768, %f768, %f771;
-	sqrt.rn.f32 	%f773, %f772;
-	rcp.rn.f32 	%f774, %f773;
-	mul.f32 	%f1842, %f757, %f774;
-	mul.f32 	%f1843, %f758, %f774;
-	mul.f32 	%f1844, %f759, %f774;
-	mul.f32 	%f1845, %f768, %f774;
-
-BB8_14:
-	mul.f32 	%f775, %f1843, %f1843;
-	fma.rn.f32 	%f776, %f1842, %f1842, %f775;
-	fma.rn.f32 	%f777, %f1844, %f1844, %f776;
-	fma.rn.f32 	%f778, %f1845, %f1845, %f777;
-	rcp.rn.f32 	%f779, %f778;
-	mul.f32 	%f780, %f1842, %f779;
-	mul.f32 	%f781, %f1843, %f779;
-	mul.f32 	%f782, %f1844, %f779;
-	mul.f32 	%f783, %f1845, %f779;
-	mul.f32 	%f784, %f1842, %f780;
-	mul.f32 	%f785, %f1843, %f781;
-	mul.f32 	%f786, %f1844, %f782;
-	mul.f32 	%f787, %f1842, %f781;
-	mul.f32 	%f788, %f1844, %f783;
-	mul.f32 	%f789, %f1842, %f782;
-	mul.f32 	%f790, %f1843, %f783;
-	mul.f32 	%f791, %f1843, %f782;
-	mul.f32 	%f792, %f1842, %f783;
-	sub.f32 	%f793, %f784, %f785;
-	sub.f32 	%f794, %f793, %f786;
-	fma.rn.f32 	%f795, %f1845, %f783, %f794;
-	sub.f32 	%f796, %f787, %f788;
-	add.f32 	%f797, %f796, %f796;
-	add.f32 	%f798, %f789, %f790;
-	add.f32 	%f799, %f798, %f798;
-	add.f32 	%f800, %f787, %f788;
-	add.f32 	%f801, %f800, %f800;
-	sub.f32 	%f802, %f785, %f784;
-	sub.f32 	%f803, %f802, %f786;
-	fma.rn.f32 	%f804, %f1845, %f783, %f803;
-	sub.f32 	%f805, %f791, %f792;
-	add.f32 	%f806, %f805, %f805;
-	sub.f32 	%f807, %f789, %f790;
-	add.f32 	%f808, %f807, %f807;
-	add.f32 	%f809, %f791, %f792;
-	add.f32 	%f810, %f809, %f809;
-	neg.f32 	%f811, %f784;
-	sub.f32 	%f812, %f811, %f785;
-	add.f32 	%f813, %f786, %f812;
-	fma.rn.f32 	%f814, %f1845, %f783, %f813;
-	mul.f32 	%f815, %f1836, %f795;
-	fma.rn.f32 	%f816, %f1839, %f797, %f815;
-	fma.rn.f32 	%f817, %f1841, %f799, %f816;
-	sub.f32 	%f1849, %f1846, %f817;
-	mul.f32 	%f818, %f1839, %f804;
-	fma.rn.f32 	%f819, %f1836, %f801, %f818;
-	fma.rn.f32 	%f820, %f1841, %f806, %f819;
-	sub.f32 	%f1853, %f1847, %f820;
-	mul.f32 	%f821, %f1839, %f810;
-	fma.rn.f32 	%f822, %f1836, %f808, %f821;
-	fma.rn.f32 	%f823, %f1841, %f814, %f822;
-	sub.f32 	%f1857, %f1848, %f823;
-	mul.f32 	%f824, %f1835, %f795;
-	fma.rn.f32 	%f825, %f1838, %f797, %f824;
-	fma.rn.f32 	%f1850, %f1840, %f799, %f825;
-	mul.f32 	%f826, %f1838, %f804;
-	fma.rn.f32 	%f827, %f1835, %f801, %f826;
-	fma.rn.f32 	%f1854, %f1840, %f806, %f827;
-	mul.f32 	%f828, %f1838, %f810;
-	fma.rn.f32 	%f829, %f1835, %f808, %f828;
-	fma.rn.f32 	%f1858, %f1840, %f814, %f829;
-	mul.f32 	%f830, %f1834, %f795;
-	fma.rn.f32 	%f1851, %f1837, %f797, %f830;
-	mul.f32 	%f831, %f1837, %f804;
-	fma.rn.f32 	%f1855, %f1834, %f801, %f831;
-	mul.f32 	%f832, %f1837, %f810;
-	fma.rn.f32 	%f1859, %f1834, %f808, %f832;
-	mul.f32 	%f1852, %f1833, %f795;
-	mul.f32 	%f1856, %f1833, %f801;
-	mul.f32 	%f1860, %f1833, %f808;
-
-BB8_17:
-	mul.f32 	%f870, %f1854, %f1859;
-	mul.f32 	%f871, %f1855, %f1858;
-	sub.f32 	%f872, %f871, %f870;
-	mul.f32 	%f873, %f1852, %f872;
-	mul.f32 	%f874, %f1854, %f1860;
-	mul.f32 	%f875, %f1856, %f1858;
-	sub.f32 	%f876, %f875, %f874;
-	mul.f32 	%f877, %f1851, %f876;
-	sub.f32 	%f878, %f873, %f877;
-	mul.f32 	%f879, %f1855, %f1860;
-	mul.f32 	%f880, %f1856, %f1859;
-	sub.f32 	%f881, %f880, %f879;
-	fma.rn.f32 	%f882, %f1850, %f881, %f878;
-	rcp.rn.f32 	%f883, %f882;
-	mul.f32 	%f1864, %f883, %f872;
-	mul.f32 	%f884, %f1851, %f1858;
-	mul.f32 	%f885, %f1850, %f1859;
-	sub.f32 	%f886, %f885, %f884;
-	mul.f32 	%f1863, %f883, %f886;
-	mul.f32 	%f887, %f1850, %f1855;
-	mul.f32 	%f888, %f1851, %f1854;
-	sub.f32 	%f889, %f888, %f887;
-	mul.f32 	%f1862, %f889, %f883;
-	sub.f32 	%f890, %f874, %f875;
-	mul.f32 	%f1868, %f883, %f890;
-	mul.f32 	%f891, %f1850, %f1860;
-	mul.f32 	%f892, %f1852, %f1858;
-	sub.f32 	%f893, %f892, %f891;
-	mul.f32 	%f1867, %f883, %f893;
-	mul.f32 	%f894, %f1852, %f1854;
-	mul.f32 	%f895, %f1850, %f1856;
-	sub.f32 	%f896, %f895, %f894;
-	mul.f32 	%f1866, %f896, %f883;
-	mul.f32 	%f1872, %f883, %f881;
-	mul.f32 	%f897, %f1852, %f1859;
-	mul.f32 	%f898, %f1851, %f1860;
-	sub.f32 	%f899, %f898, %f897;
-	mul.f32 	%f1871, %f883, %f899;
-	mul.f32 	%f900, %f1851, %f1856;
-	mul.f32 	%f901, %f1852, %f1855;
-	sub.f32 	%f902, %f901, %f900;
-	mul.f32 	%f1870, %f902, %f883;
-	mul.f32 	%f903, %f1849, %f1864;
-	neg.f32 	%f904, %f903;
-	mul.f32 	%f905, %f1853, %f1863;
-	sub.f32 	%f906, %f904, %f905;
-	mul.f32 	%f907, %f1857, %f1862;
-	sub.f32 	%f1861, %f906, %f907;
-	mul.f32 	%f908, %f1849, %f1868;
-	neg.f32 	%f909, %f908;
-	mul.f32 	%f910, %f1853, %f1867;
-	sub.f32 	%f911, %f909, %f910;
-	mul.f32 	%f912, %f1857, %f1866;
-	sub.f32 	%f1865, %f911, %f912;
-	mul.f32 	%f913, %f1849, %f1872;
-	neg.f32 	%f914, %f913;
-	mul.f32 	%f915, %f1853, %f1871;
-	sub.f32 	%f916, %f914, %f915;
-	mul.f32 	%f917, %f1857, %f1870;
-	sub.f32 	%f1869, %f916, %f917;
-	bra.uni 	BB8_18;
-
-BB8_7:
-	setp.ne.s32	%p5, %r33, 1;
-	mov.f32 	%f1862, %f1861;
-	mov.f32 	%f1863, %f1861;
-	mov.f32 	%f1865, %f1861;
-	mov.f32 	%f1866, %f1861;
-	mov.f32 	%f1867, %f1864;
-	mov.f32 	%f1868, %f1861;
-	mov.f32 	%f1869, %f1861;
-	mov.f32 	%f1870, %f1864;
-	mov.f32 	%f1871, %f1861;
-	mov.f32 	%f1872, %f1861;
-	@%p5 bra 	BB8_18;
-
-	// inline asm
-	call (%rd54), _optix_get_static_transform_from_handle, (%rd52);
-	// inline asm
-	add.s64 	%rd662, %rd54, 64;
-
-BB8_10:
-	// inline asm
-	cvta.to.global.u64 %rd58, %rd662;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r35,%r36,%r37,%r38}, [%rd58];
-	// inline asm
-	mov.b32 	 %f1864, %r35;
-	mov.b32 	 %f1863, %r36;
-	mov.b32 	 %f1862, %r37;
-	mov.b32 	 %f1861, %r38;
-	add.s64 	%rd62, %rd662, 16;
-	// inline asm
-	cvta.to.global.u64 %rd61, %rd62;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r39,%r40,%r41,%r42}, [%rd61];
-	// inline asm
-	mov.b32 	 %f1868, %r39;
-	mov.b32 	 %f1867, %r40;
-	mov.b32 	 %f1866, %r41;
-	mov.b32 	 %f1865, %r42;
-	add.s64 	%rd65, %rd662, 32;
-	// inline asm
-	cvta.to.global.u64 %rd64, %rd65;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r43,%r44,%r45,%r46}, [%rd64];
-	// inline asm
-	mov.b32 	 %f1872, %r43;
-	mov.b32 	 %f1871, %r44;
-	mov.b32 	 %f1870, %r45;
-	mov.b32 	 %f1869, %r46;
-
-BB8_18:
-	setp.eq.s32	%p9, %r646, 0;
-	@%p9 bra 	BB8_19;
-	bra.uni 	BB8_20;
-
-BB8_19:
-	mov.f32 	%f1832, %f1861;
-	mov.f32 	%f1831, %f1862;
-	mov.f32 	%f1830, %f1863;
-	mov.f32 	%f1829, %f1864;
-	mov.f32 	%f1828, %f1865;
-	mov.f32 	%f1827, %f1866;
-	mov.f32 	%f1826, %f1867;
-	mov.f32 	%f1825, %f1868;
-	mov.f32 	%f1824, %f1869;
-	mov.f32 	%f1823, %f1870;
-	mov.f32 	%f1822, %f1871;
-	mov.f32 	%f1821, %f1872;
-	bra.uni 	BB8_21;
-
-BB8_20:
-	mul.f32 	%f918, %f1829, %f1864;
-	fma.rn.f32 	%f919, %f1825, %f1863, %f918;
-	fma.rn.f32 	%f151, %f1821, %f1862, %f919;
-	mul.f32 	%f920, %f1830, %f1864;
-	fma.rn.f32 	%f921, %f1826, %f1863, %f920;
-	fma.rn.f32 	%f152, %f1822, %f1862, %f921;
-	mul.f32 	%f922, %f1831, %f1864;
-	fma.rn.f32 	%f923, %f1827, %f1863, %f922;
-	fma.rn.f32 	%f153, %f1823, %f1862, %f923;
-	mul.f32 	%f924, %f1832, %f1864;
-	fma.rn.f32 	%f925, %f1828, %f1863, %f924;
-	fma.rn.f32 	%f926, %f1824, %f1862, %f925;
-	add.f32 	%f154, %f1861, %f926;
-	mul.f32 	%f927, %f1829, %f1868;
-	fma.rn.f32 	%f928, %f1825, %f1867, %f927;
-	fma.rn.f32 	%f155, %f1821, %f1866, %f928;
-	mul.f32 	%f929, %f1830, %f1868;
-	fma.rn.f32 	%f930, %f1826, %f1867, %f929;
-	fma.rn.f32 	%f156, %f1822, %f1866, %f930;
-	mul.f32 	%f931, %f1831, %f1868;
-	fma.rn.f32 	%f932, %f1827, %f1867, %f931;
-	fma.rn.f32 	%f157, %f1823, %f1866, %f932;
-	mul.f32 	%f933, %f1832, %f1868;
-	fma.rn.f32 	%f934, %f1828, %f1867, %f933;
-	fma.rn.f32 	%f935, %f1824, %f1866, %f934;
-	add.f32 	%f158, %f1865, %f935;
-	mul.f32 	%f936, %f1829, %f1872;
-	fma.rn.f32 	%f937, %f1825, %f1871, %f936;
-	fma.rn.f32 	%f1821, %f1821, %f1870, %f937;
-	mul.f32 	%f938, %f1830, %f1872;
-	fma.rn.f32 	%f939, %f1826, %f1871, %f938;
-	fma.rn.f32 	%f1822, %f1822, %f1870, %f939;
-	mul.f32 	%f940, %f1831, %f1872;
-	fma.rn.f32 	%f941, %f1827, %f1871, %f940;
-	fma.rn.f32 	%f1823, %f1823, %f1870, %f941;
-	mul.f32 	%f942, %f1832, %f1872;
-	fma.rn.f32 	%f943, %f1828, %f1871, %f942;
-	fma.rn.f32 	%f944, %f1824, %f1870, %f943;
-	add.f32 	%f1824, %f1869, %f944;
-	mov.f32 	%f1832, %f154;
-	mov.f32 	%f1831, %f153;
-	mov.f32 	%f1830, %f152;
-	mov.f32 	%f1829, %f151;
-	mov.f32 	%f1828, %f158;
-	mov.f32 	%f1827, %f157;
-	mov.f32 	%f1826, %f156;
-	mov.f32 	%f1825, %f155;
-
-BB8_21:
-	add.s32 	%r646, %r646, 1;
-	setp.lt.u32	%p10, %r646, %r30;
-	@%p10 bra 	BB8_5;
-
-	mul.f32 	%f945, %f703, %f1829;
-	fma.rn.f32 	%f946, %f704, %f1830, %f945;
-	fma.rn.f32 	%f947, %f1885, %f1831, %f946;
-	add.f32 	%f1887, %f1832, %f947;
-	mul.f32 	%f948, %f703, %f1825;
-	fma.rn.f32 	%f949, %f704, %f1826, %f948;
-	fma.rn.f32 	%f950, %f1885, %f1827, %f949;
-	add.f32 	%f1886, %f1828, %f950;
-	mul.f32 	%f951, %f703, %f1821;
-	fma.rn.f32 	%f952, %f704, %f1822, %f951;
-	fma.rn.f32 	%f953, %f1885, %f1823, %f952;
-	add.f32 	%f1885, %f1824, %f953;
-	bra.uni 	BB8_23;
-
-BB8_3:
-	mov.f32 	%f1886, %f704;
-	mov.f32 	%f1887, %f703;
-
-BB8_23:
-	// inline asm
-	call (%f954), _optix_get_world_ray_direction_x, ();
-	// inline asm
-	// inline asm
-	call (%f955), _optix_get_world_ray_direction_y, ();
-	// inline asm
-	// inline asm
-	call (%f1936), _optix_get_world_ray_direction_z, ();
-	// inline asm
-	// inline asm
-	call (%f957), _optix_get_ray_time, ();
-	// inline asm
-	mov.u32 	%r647, 0;
-	@%p2 bra 	BB8_24;
-
-BB8_25:
-	.pragma "nounroll";
-	// inline asm
-	call (%rd173), _optix_get_transform_list_handle, (%r647);
-	// inline asm
-	// inline asm
-	call (%r183), _optix_get_transform_type_from_handle, (%rd173);
-	// inline asm
-	and.b32  	%r184, %r183, -2;
-	setp.eq.s32	%p12, %r184, 2;
-	@%p12 bra 	BB8_31;
-	bra.uni 	BB8_26;
-
-BB8_31:
-	setp.eq.s32	%p15, %r183, 2;
-	@%p15 bra 	BB8_35;
-	bra.uni 	BB8_32;
-
-BB8_35:
-	// inline asm
-	call (%rd247), _optix_get_matrix_motion_transform_from_handle, (%rd173);
-	// inline asm
-	// inline asm
-	cvta.to.global.u64 %rd249, %rd247;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r272,%r273,%r274,%r275}, [%rd249];
-	// inline asm
-	mov.b32	{%rs9, %rs10}, %r274;
-	add.s64 	%rd253, %rd247, 16;
-	// inline asm
-	cvta.to.global.u64 %rd252, %rd253;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r276,%r277,%r278,%r279}, [%rd252];
-	// inline asm
-	add.s64 	%rd256, %rd247, 32;
-	// inline asm
-	cvta.to.global.u64 %rd255, %rd256;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r280,%r281,%r282,%r283}, [%rd255];
-	// inline asm
-	add.s64 	%rd259, %rd247, 48;
-	// inline asm
-	cvta.to.global.u64 %rd258, %rd259;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r284,%r285,%r286,%r287}, [%rd258];
-	// inline asm
-	add.s64 	%rd262, %rd247, 64;
-	// inline asm
-	cvta.to.global.u64 %rd261, %rd262;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r288,%r289,%r290,%r291}, [%rd261];
-	// inline asm
-	add.s64 	%rd265, %rd247, 80;
-	// inline asm
-	cvta.to.global.u64 %rd264, %rd265;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r292,%r293,%r294,%r295}, [%rd264];
-	// inline asm
-	add.s64 	%rd268, %rd247, 96;
-	// inline asm
-	cvta.to.global.u64 %rd267, %rd268;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r296,%r297,%r298,%r299}, [%rd267];
-	// inline asm
-	add.s64 	%rd271, %rd247, 112;
-	// inline asm
-	cvta.to.global.u64 %rd270, %rd271;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r300,%r301,%r302,%r303}, [%rd270];
-	// inline asm
-	mov.b32 	 %f1060, %r275;
-	mov.b32 	 %f1061, %r276;
-	cvt.u32.u16	%r316, %rs9;
-	add.s32 	%r317, %r316, -1;
-	cvt.rn.f32.s32	%f1062, %r317;
-	sub.f32 	%f1063, %f957, %f1060;
-	mul.f32 	%f1064, %f1063, %f1062;
-	sub.f32 	%f1065, %f1061, %f1060;
-	div.rn.f32 	%f1066, %f1064, %f1065;
-	min.f32 	%f1067, %f1062, %f1066;
-	mov.f32 	%f1068, 0f00000000;
-	max.f32 	%f1069, %f1068, %f1067;
-	cvt.rmi.f32.f32	%f1070, %f1069;
-	cvt.rzi.s32.f32	%r318, %f1070;
-	cvt.s64.s32	%rd19, %r318;
-	mul.wide.s32 	%rd282, %r318, 48;
-	add.s64 	%rd274, %rd256, %rd282;
-	// inline asm
-	cvta.to.global.u64 %rd273, %rd274;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r304,%r305,%r306,%r307}, [%rd273];
-	// inline asm
-	mov.b32 	 %f1913, %r304;
-	mov.b32 	 %f1914, %r305;
-	mov.b32 	 %f1915, %r306;
-	add.s64 	%rd277, %rd274, 16;
-	// inline asm
-	cvta.to.global.u64 %rd276, %rd277;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r308,%r309,%r310,%r311}, [%rd276];
-	// inline asm
-	mov.b32 	 %f1910, %r308;
-	mov.b32 	 %f1911, %r309;
-	mov.b32 	 %f1912, %r310;
-	add.s64 	%rd280, %rd274, 32;
-	// inline asm
-	cvta.to.global.u64 %rd279, %rd280;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r312,%r313,%r314,%r315}, [%rd279];
-	// inline asm
-	sub.f32 	%f249, %f1069, %f1070;
-	mov.b32 	 %f1907, %r312;
-	mov.b32 	 %f1908, %r313;
-	mov.b32 	 %f1909, %r314;
-	setp.leu.f32	%p17, %f249, 0f00000000;
-	@%p17 bra 	BB8_37;
-
-	mul.lo.s64 	%rd292, %rd19, 48;
-	add.s64 	%rd293, %rd247, %rd292;
-	add.s64 	%rd284, %rd293, 80;
-	// inline asm
-	cvta.to.global.u64 %rd283, %rd284;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r319,%r320,%r321,%r322}, [%rd283];
-	// inline asm
-	mov.b32 	 %f1071, %r319;
-	mov.b32 	 %f1072, %r320;
-	mov.b32 	 %f1073, %r321;
-	add.s64 	%rd287, %rd293, 96;
-	// inline asm
-	cvta.to.global.u64 %rd286, %rd287;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r323,%r324,%r325,%r326}, [%rd286];
-	// inline asm
-	mov.b32 	 %f1074, %r323;
-	mov.b32 	 %f1075, %r324;
-	mov.b32 	 %f1076, %r325;
-	add.s64 	%rd290, %rd293, 112;
-	// inline asm
-	cvta.to.global.u64 %rd289, %rd290;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r327,%r328,%r329,%r330}, [%rd289];
-	// inline asm
-	mov.f32 	%f1077, 0f3F800000;
-	sub.f32 	%f1078, %f1077, %f249;
-	mul.f32 	%f1079, %f249, %f1071;
-	mul.f32 	%f1080, %f249, %f1072;
-	mul.f32 	%f1081, %f249, %f1073;
-	fma.rn.f32 	%f1913, %f1078, %f1913, %f1079;
-	fma.rn.f32 	%f1914, %f1078, %f1914, %f1080;
-	fma.rn.f32 	%f1915, %f1078, %f1915, %f1081;
-	mul.f32 	%f1082, %f249, %f1074;
-	mul.f32 	%f1083, %f249, %f1075;
-	mul.f32 	%f1084, %f249, %f1076;
-	fma.rn.f32 	%f1910, %f1078, %f1910, %f1082;
-	fma.rn.f32 	%f1911, %f1078, %f1911, %f1083;
-	fma.rn.f32 	%f1912, %f1078, %f1912, %f1084;
-	mov.b32 	 %f1085, %r327;
-	mov.b32 	 %f1086, %r328;
-	mov.b32 	 %f1087, %r329;
-	mul.f32 	%f1088, %f249, %f1085;
-	mul.f32 	%f1089, %f249, %f1086;
-	mul.f32 	%f1090, %f249, %f1087;
-	fma.rn.f32 	%f1907, %f1078, %f1907, %f1088;
-	fma.rn.f32 	%f1908, %f1078, %f1908, %f1089;
-	fma.rn.f32 	%f1909, %f1078, %f1909, %f1090;
-	bra.uni 	BB8_37;
-
-BB8_26:
-	mov.f32 	%f1916, 0f00000000;
-	mov.f32 	%f1918, 0f3F800000;
-	setp.eq.s32	%p13, %r183, 4;
-	@%p13 bra 	BB8_29;
-	bra.uni 	BB8_27;
-
-BB8_29:
-	// inline asm
-	call (%rd663), _optix_get_instance_inverse_transform_from_handle, (%rd173);
-	// inline asm
-	bra.uni 	BB8_30;
-
-BB8_32:
-	// inline asm
-	call (%rd188), _optix_get_srt_motion_transform_from_handle, (%rd173);
-	// inline asm
-	// inline asm
-	cvta.to.global.u64 %rd190, %rd188;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r197,%r198,%r199,%r200}, [%rd190];
-	// inline asm
-	mov.b32	{%rs7, %rs8}, %r199;
-	add.s64 	%rd194, %rd188, 16;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r111,%r112,%r113,%r114}, [%rd108];
+	// end inline asm
+	mov.b32 	%f762, %r111;
+	mov.b32 	%f763, %r112;
+	mov.b32 	%f764, %r113;
+	mov.b32 	%f765, %r114;
+	mul.f32 	%f766, %f29, %f762;
+	mul.f32 	%f767, %f29, %f763;
+	mul.f32 	%f768, %f29, %f764;
+	mul.f32 	%f769, %f29, %f765;
+	fma.rn.f32 	%f1852, %f761, %f1852, %f766;
+	fma.rn.f32 	%f1853, %f761, %f1853, %f767;
+	fma.rn.f32 	%f1854, %f761, %f1854, %f768;
+	fma.rn.f32 	%f1855, %f761, %f1855, %f769;
+	add.s64 	%rd112, %rd96, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd111, %rd112;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r115,%r116,%r117,%r118}, [%rd111];
+	// end inline asm
+	mov.b32 	%f770, %r115;
+	mov.b32 	%f771, %r116;
+	mov.b32 	%f772, %r117;
+	mov.b32 	%f773, %r118;
+	mul.f32 	%f774, %f29, %f770;
+	mul.f32 	%f775, %f29, %f771;
+	mul.f32 	%f776, %f29, %f772;
+	mul.f32 	%f777, %f29, %f773;
+	fma.rn.f32 	%f1856, %f761, %f1856, %f774;
+	fma.rn.f32 	%f1857, %f761, %f1857, %f775;
+	fma.rn.f32 	%f1858, %f761, %f1858, %f776;
+	fma.rn.f32 	%f1859, %f761, %f1859, %f777;
+	add.s64 	%rd115, %rd96, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd114, %rd115;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r119,%r120,%r121,%r122}, [%rd114];
+	// end inline asm
+	mov.b32 	%f778, %r119;
+	mov.b32 	%f779, %r120;
+	mov.b32 	%f780, %r121;
+	mov.b32 	%f781, %r122;
+	mul.f32 	%f782, %f29, %f778;
+	mul.f32 	%f783, %f29, %f779;
+	mul.f32 	%f784, %f29, %f780;
+	mul.f32 	%f785, %f29, %f781;
+	fma.rn.f32 	%f1860, %f761, %f1860, %f782;
+	fma.rn.f32 	%f786, %f761, %f1861, %f783;
+	fma.rn.f32 	%f787, %f761, %f1862, %f784;
+	fma.rn.f32 	%f788, %f761, %f1863, %f785;
+	add.s64 	%rd118, %rd96, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd117, %rd118;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r123,%r124,%r125,%r126}, [%rd117];
+	// end inline asm
+	mov.b32 	%f789, %r123;
+	mov.b32 	%f790, %r124;
+	mov.b32 	%f791, %r125;
+	mov.b32 	%f792, %r126;
+	mul.f32 	%f793, %f29, %f789;
+	mul.f32 	%f794, %f29, %f790;
+	mul.f32 	%f795, %f29, %f791;
+	mul.f32 	%f796, %f29, %f792;
+	fma.rn.f32 	%f797, %f761, %f1864, %f793;
+	fma.rn.f32 	%f1865, %f761, %f1865, %f794;
+	fma.rn.f32 	%f1866, %f761, %f1866, %f795;
+	fma.rn.f32 	%f1867, %f761, %f1867, %f796;
+	mul.f32 	%f798, %f787, %f787;
+	fma.rn.f32 	%f799, %f786, %f786, %f798;
+	fma.rn.f32 	%f800, %f788, %f788, %f799;
+	fma.rn.f32 	%f801, %f797, %f797, %f800;
+	sqrt.rn.f32 	%f802, %f801;
+	rcp.rn.f32 	%f803, %f802;
+	mul.f32 	%f1861, %f786, %f803;
+	mul.f32 	%f1862, %f787, %f803;
+	mul.f32 	%f1863, %f788, %f803;
+	mul.f32 	%f1864, %f803, %f797;
+
+$L__BB8_14:
+	mul.f32 	%f804, %f1862, %f1862;
+	fma.rn.f32 	%f805, %f1861, %f1861, %f804;
+	fma.rn.f32 	%f806, %f1863, %f1863, %f805;
+	fma.rn.f32 	%f807, %f1864, %f1864, %f806;
+	rcp.rn.f32 	%f808, %f807;
+	mul.f32 	%f809, %f1861, %f808;
+	mul.f32 	%f810, %f1862, %f808;
+	mul.f32 	%f811, %f1863, %f808;
+	mul.f32 	%f812, %f1864, %f808;
+	mul.f32 	%f813, %f1861, %f809;
+	mul.f32 	%f814, %f1862, %f810;
+	mul.f32 	%f815, %f1863, %f811;
+	mul.f32 	%f816, %f1861, %f810;
+	mul.f32 	%f817, %f1863, %f812;
+	mul.f32 	%f818, %f1861, %f811;
+	mul.f32 	%f819, %f1862, %f812;
+	mul.f32 	%f820, %f1862, %f811;
+	mul.f32 	%f821, %f1861, %f812;
+	sub.f32 	%f822, %f813, %f814;
+	sub.f32 	%f823, %f822, %f815;
+	fma.rn.f32 	%f824, %f1864, %f812, %f823;
+	sub.f32 	%f825, %f816, %f817;
+	add.f32 	%f826, %f825, %f825;
+	add.f32 	%f827, %f818, %f819;
+	add.f32 	%f828, %f827, %f827;
+	add.f32 	%f829, %f816, %f817;
+	add.f32 	%f830, %f829, %f829;
+	sub.f32 	%f831, %f814, %f813;
+	sub.f32 	%f832, %f831, %f815;
+	fma.rn.f32 	%f833, %f1864, %f812, %f832;
+	sub.f32 	%f834, %f820, %f821;
+	add.f32 	%f835, %f834, %f834;
+	sub.f32 	%f836, %f818, %f819;
+	add.f32 	%f837, %f836, %f836;
+	add.f32 	%f838, %f820, %f821;
+	add.f32 	%f839, %f838, %f838;
+	neg.f32 	%f840, %f813;
+	sub.f32 	%f841, %f840, %f814;
+	add.f32 	%f842, %f815, %f841;
+	fma.rn.f32 	%f843, %f1864, %f812, %f842;
+	mul.f32 	%f844, %f1855, %f824;
+	fma.rn.f32 	%f845, %f1858, %f826, %f844;
+	fma.rn.f32 	%f846, %f1860, %f828, %f845;
+	sub.f32 	%f1868, %f1865, %f846;
+	mul.f32 	%f847, %f1858, %f833;
+	fma.rn.f32 	%f848, %f1855, %f830, %f847;
+	fma.rn.f32 	%f849, %f1860, %f835, %f848;
+	sub.f32 	%f1872, %f1866, %f849;
+	mul.f32 	%f850, %f1858, %f839;
+	fma.rn.f32 	%f851, %f1855, %f837, %f850;
+	fma.rn.f32 	%f852, %f1860, %f843, %f851;
+	sub.f32 	%f1876, %f1867, %f852;
+	mul.f32 	%f853, %f1854, %f824;
+	fma.rn.f32 	%f854, %f1857, %f826, %f853;
+	fma.rn.f32 	%f1869, %f1859, %f828, %f854;
+	mul.f32 	%f855, %f1857, %f833;
+	fma.rn.f32 	%f856, %f1854, %f830, %f855;
+	fma.rn.f32 	%f1873, %f1859, %f835, %f856;
+	mul.f32 	%f857, %f1857, %f839;
+	fma.rn.f32 	%f858, %f1854, %f837, %f857;
+	fma.rn.f32 	%f1877, %f1859, %f843, %f858;
+	mul.f32 	%f859, %f1853, %f824;
+	fma.rn.f32 	%f1870, %f1856, %f826, %f859;
+	mul.f32 	%f860, %f1856, %f833;
+	fma.rn.f32 	%f1874, %f1853, %f830, %f860;
+	mul.f32 	%f861, %f1856, %f839;
+	fma.rn.f32 	%f1878, %f1853, %f837, %f861;
+	mul.f32 	%f1871, %f1852, %f824;
+	mul.f32 	%f1875, %f1852, %f830;
+	mul.f32 	%f1879, %f1852, %f837;
+
+$L__BB8_17:
+	mul.f32 	%f899, %f1873, %f1878;
+	mul.f32 	%f900, %f1874, %f1877;
+	sub.f32 	%f901, %f900, %f899;
+	mul.f32 	%f902, %f1871, %f901;
+	mul.f32 	%f903, %f1873, %f1879;
+	mul.f32 	%f904, %f1875, %f1877;
+	sub.f32 	%f905, %f904, %f903;
+	mul.f32 	%f906, %f1870, %f905;
+	sub.f32 	%f907, %f902, %f906;
+	mul.f32 	%f908, %f1874, %f1879;
+	mul.f32 	%f909, %f1875, %f1878;
+	sub.f32 	%f910, %f909, %f908;
+	fma.rn.f32 	%f911, %f1869, %f910, %f907;
+	rcp.rn.f32 	%f912, %f911;
+	mul.f32 	%f1883, %f901, %f912;
+	mul.f32 	%f913, %f1870, %f1877;
+	mul.f32 	%f914, %f1869, %f1878;
+	sub.f32 	%f915, %f914, %f913;
+	mul.f32 	%f1882, %f915, %f912;
+	mul.f32 	%f916, %f1869, %f1874;
+	mul.f32 	%f917, %f1870, %f1873;
+	sub.f32 	%f918, %f917, %f916;
+	mul.f32 	%f1881, %f918, %f912;
+	sub.f32 	%f919, %f903, %f904;
+	mul.f32 	%f1887, %f919, %f912;
+	mul.f32 	%f920, %f1869, %f1879;
+	mul.f32 	%f921, %f1871, %f1877;
+	sub.f32 	%f922, %f921, %f920;
+	mul.f32 	%f1886, %f922, %f912;
+	mul.f32 	%f923, %f1871, %f1873;
+	mul.f32 	%f924, %f1869, %f1875;
+	sub.f32 	%f925, %f924, %f923;
+	mul.f32 	%f1885, %f925, %f912;
+	mul.f32 	%f1891, %f910, %f912;
+	mul.f32 	%f926, %f1871, %f1878;
+	mul.f32 	%f927, %f1870, %f1879;
+	sub.f32 	%f928, %f927, %f926;
+	mul.f32 	%f1890, %f928, %f912;
+	mul.f32 	%f929, %f1870, %f1875;
+	mul.f32 	%f930, %f1871, %f1874;
+	sub.f32 	%f931, %f930, %f929;
+	mul.f32 	%f1889, %f931, %f912;
+	mul.f32 	%f932, %f1868, %f1883;
+	neg.f32 	%f933, %f932;
+	mul.f32 	%f934, %f1872, %f1882;
+	sub.f32 	%f935, %f933, %f934;
+	mul.f32 	%f936, %f1876, %f1881;
+	sub.f32 	%f1880, %f935, %f936;
+	mul.f32 	%f937, %f1868, %f1887;
+	neg.f32 	%f938, %f937;
+	mul.f32 	%f939, %f1872, %f1886;
+	sub.f32 	%f940, %f938, %f939;
+	mul.f32 	%f941, %f1876, %f1885;
+	sub.f32 	%f1884, %f940, %f941;
+	mul.f32 	%f942, %f1868, %f1891;
+	neg.f32 	%f943, %f942;
+	mul.f32 	%f944, %f1872, %f1890;
+	sub.f32 	%f945, %f943, %f944;
+	mul.f32 	%f946, %f1876, %f1889;
+	sub.f32 	%f1888, %f945, %f946;
+	bra.uni 	$L__BB8_18;
+
+$L__BB8_9:
+	// begin inline asm
+	call (%rd649), _optix_get_instance_inverse_transform_from_handle, (%rd48);
+	// end inline asm
+
+$L__BB8_10:
+	// begin inline asm
+	cvta.to.global.u64 %rd54, %rd649;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r40,%r41,%r42,%r43}, [%rd54];
+	// end inline asm
+	mov.b32 	%f1883, %r40;
+	mov.b32 	%f1882, %r41;
+	mov.b32 	%f1881, %r42;
+	mov.b32 	%f1880, %r43;
+	add.s64 	%rd58, %rd649, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd57, %rd58;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r44,%r45,%r46,%r47}, [%rd57];
+	// end inline asm
+	mov.b32 	%f1887, %r44;
+	mov.b32 	%f1886, %r45;
+	mov.b32 	%f1885, %r46;
+	mov.b32 	%f1884, %r47;
+	add.s64 	%rd61, %rd649, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd60, %rd61;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r48,%r49,%r50,%r51}, [%rd60];
+	// end inline asm
+	mov.b32 	%f1891, %r48;
+	mov.b32 	%f1890, %r49;
+	mov.b32 	%f1889, %r50;
+	mov.b32 	%f1888, %r51;
+
+$L__BB8_18:
+	setp.eq.s32 	%p10, %r655, 0;
+	@%p10 bra 	$L__BB8_20;
+
+	mul.f32 	%f947, %f1848, %f1883;
+	fma.rn.f32 	%f948, %f1844, %f1882, %f947;
+	fma.rn.f32 	%f151, %f1840, %f1881, %f948;
+	mul.f32 	%f949, %f1849, %f1883;
+	fma.rn.f32 	%f950, %f1845, %f1882, %f949;
+	fma.rn.f32 	%f152, %f1841, %f1881, %f950;
+	mul.f32 	%f951, %f1850, %f1883;
+	fma.rn.f32 	%f952, %f1846, %f1882, %f951;
+	fma.rn.f32 	%f153, %f1842, %f1881, %f952;
+	mul.f32 	%f953, %f1851, %f1883;
+	fma.rn.f32 	%f954, %f1847, %f1882, %f953;
+	fma.rn.f32 	%f955, %f1843, %f1881, %f954;
+	add.f32 	%f1880, %f1880, %f955;
+	mul.f32 	%f956, %f1848, %f1887;
+	fma.rn.f32 	%f957, %f1844, %f1886, %f956;
+	fma.rn.f32 	%f155, %f1840, %f1885, %f957;
+	mul.f32 	%f958, %f1849, %f1887;
+	fma.rn.f32 	%f959, %f1845, %f1886, %f958;
+	fma.rn.f32 	%f156, %f1841, %f1885, %f959;
+	mul.f32 	%f960, %f1850, %f1887;
+	fma.rn.f32 	%f961, %f1846, %f1886, %f960;
+	fma.rn.f32 	%f157, %f1842, %f1885, %f961;
+	mul.f32 	%f962, %f1851, %f1887;
+	fma.rn.f32 	%f963, %f1847, %f1886, %f962;
+	fma.rn.f32 	%f964, %f1843, %f1885, %f963;
+	add.f32 	%f1884, %f1884, %f964;
+	mul.f32 	%f965, %f1848, %f1891;
+	fma.rn.f32 	%f966, %f1844, %f1890, %f965;
+	fma.rn.f32 	%f159, %f1840, %f1889, %f966;
+	mul.f32 	%f967, %f1849, %f1891;
+	fma.rn.f32 	%f968, %f1845, %f1890, %f967;
+	fma.rn.f32 	%f160, %f1841, %f1889, %f968;
+	mul.f32 	%f969, %f1850, %f1891;
+	fma.rn.f32 	%f970, %f1846, %f1890, %f969;
+	fma.rn.f32 	%f161, %f1842, %f1889, %f970;
+	mul.f32 	%f971, %f1851, %f1891;
+	fma.rn.f32 	%f972, %f1847, %f1890, %f971;
+	fma.rn.f32 	%f973, %f1843, %f1889, %f972;
+	add.f32 	%f1888, %f1888, %f973;
+	mov.f32 	%f1881, %f153;
+	mov.f32 	%f1882, %f152;
+	mov.f32 	%f1883, %f151;
+	mov.f32 	%f1885, %f157;
+	mov.f32 	%f1886, %f156;
+	mov.f32 	%f1887, %f155;
+	mov.f32 	%f1889, %f161;
+	mov.f32 	%f1890, %f160;
+	mov.f32 	%f1891, %f159;
+
+$L__BB8_20:
+	add.s32 	%r655, %r655, 1;
+	setp.lt.u32 	%p11, %r655, %r35;
+	mov.f32 	%f1840, %f1891;
+	mov.f32 	%f1841, %f1890;
+	mov.f32 	%f1842, %f1889;
+	mov.f32 	%f1843, %f1888;
+	mov.f32 	%f1844, %f1887;
+	mov.f32 	%f1845, %f1886;
+	mov.f32 	%f1846, %f1885;
+	mov.f32 	%f1847, %f1884;
+	mov.f32 	%f1848, %f1883;
+	mov.f32 	%f1849, %f1882;
+	mov.f32 	%f1850, %f1881;
+	mov.f32 	%f1851, %f1880;
+	@%p11 bra 	$L__BB8_5;
+
+$L__BB8_21:
+	mul.f32 	%f974, %f1916, %f1883;
+	fma.rn.f32 	%f975, %f1917, %f1882, %f974;
+	fma.rn.f32 	%f976, %f1918, %f1881, %f975;
+	mul.f32 	%f977, %f1916, %f1887;
+	fma.rn.f32 	%f978, %f1917, %f1886, %f977;
+	fma.rn.f32 	%f979, %f1918, %f1885, %f978;
+	mul.f32 	%f980, %f1916, %f1891;
+	fma.rn.f32 	%f981, %f1917, %f1890, %f980;
+	fma.rn.f32 	%f982, %f1918, %f1889, %f981;
+	add.f32 	%f1918, %f1888, %f982;
+	add.f32 	%f1917, %f1884, %f979;
+	add.f32 	%f1916, %f1880, %f976;
+
+$L__BB8_23:
+	// begin inline asm
+	call (%f1974), _optix_get_world_ray_direction_x, ();
+	// end inline asm
+	// begin inline asm
+	call (%f1975), _optix_get_world_ray_direction_y, ();
+	// end inline asm
+	// begin inline asm
+	call (%f985), _optix_get_world_ray_direction_z, ();
+	// end inline asm
+	// begin inline asm
+	call (%r186), _optix_get_transform_list_size, ();
+	// end inline asm
+	setp.eq.s32 	%p12, %r186, 0;
+	@%p12 bra 	$L__BB8_43;
+
+	// begin inline asm
+	call (%r187), _optix_get_transform_list_size, ();
+	// end inline asm
+	// begin inline asm
+	call (%f986), _optix_get_ray_time, ();
+	// end inline asm
+	setp.eq.s32 	%p13, %r187, 0;
+	@%p13 bra 	$L__BB8_42;
+
+	mov.u32 	%r656, 0;
+
+$L__BB8_26:
+	.pragma "nounroll";
+	// begin inline asm
+	call (%rd167), _optix_get_transform_list_handle, (%r656);
+	// end inline asm
+	// begin inline asm
+	call (%r190), _optix_get_transform_type_from_handle, (%rd167);
+	// end inline asm
+	or.b32  	%r191, %r190, 1;
+	setp.eq.s32 	%p14, %r191, 3;
+	@%p14 bra 	$L__BB8_32;
+	bra.uni 	$L__BB8_27;
+
+$L__BB8_32:
+	setp.eq.s32 	%p17, %r190, 2;
+	@%p17 bra 	$L__BB8_36;
+	bra.uni 	$L__BB8_33;
+
+$L__BB8_36:
+	// begin inline asm
+	call (%rd239), _optix_get_matrix_motion_transform_from_handle, (%rd167);
+	// end inline asm
+	// begin inline asm
+	cvta.to.global.u64 %rd241, %rd239;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r279,%r280,%r281,%r282}, [%rd241];
+	// end inline asm
+	add.s64 	%rd245, %rd239, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd244, %rd245;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r283,%r284,%r285,%r286}, [%rd244];
+	// end inline asm
+	add.s64 	%rd248, %rd239, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd247, %rd248;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r287,%r288,%r289,%r290}, [%rd247];
+	// end inline asm
+	add.s64 	%rd251, %rd239, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd250, %rd251;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r291,%r292,%r293,%r294}, [%rd250];
+	// end inline asm
+	add.s64 	%rd254, %rd239, 64;
+	// begin inline asm
+	cvta.to.global.u64 %rd253, %rd254;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r295,%r296,%r297,%r298}, [%rd253];
+	// end inline asm
+	add.s64 	%rd257, %rd239, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd256, %rd257;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r299,%r300,%r301,%r302}, [%rd256];
+	// end inline asm
+	add.s64 	%rd260, %rd239, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd259, %rd260;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r303,%r304,%r305,%r306}, [%rd259];
+	// end inline asm
+	add.s64 	%rd263, %rd239, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd262, %rd263;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r307,%r308,%r309,%r310}, [%rd262];
+	// end inline asm
+	mov.b32 	%f1090, %r282;
+	mov.b32 	%f1091, %r283;
+	and.b32  	%r323, %r281, 65535;
+	add.s32 	%r324, %r323, -1;
+	cvt.rn.f32.s32 	%f1092, %r324;
+	sub.f32 	%f1093, %f986, %f1090;
+	mul.f32 	%f1094, %f1093, %f1092;
+	sub.f32 	%f1095, %f1091, %f1090;
+	div.rn.f32 	%f1096, %f1094, %f1095;
+	min.f32 	%f1097, %f1092, %f1096;
+	mov.f32 	%f1098, 0f00000000;
+	max.f32 	%f1099, %f1098, %f1097;
+	cvt.rmi.f32.f32 	%f1100, %f1099;
+	sub.f32 	%f258, %f1099, %f1100;
+	cvt.rzi.s32.f32 	%r325, %f1100;
+	cvt.s64.s32 	%rd17, %r325;
+	mul.wide.s32 	%rd274, %r325, 48;
+	add.s64 	%rd266, %rd248, %rd274;
+	// begin inline asm
+	cvta.to.global.u64 %rd265, %rd266;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r311,%r312,%r313,%r314}, [%rd265];
+	// end inline asm
+	mov.b32 	%f1944, %r311;
+	mov.b32 	%f1945, %r312;
+	mov.b32 	%f1946, %r313;
+	add.s64 	%rd269, %rd266, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd268, %rd269;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r315,%r316,%r317,%r318}, [%rd268];
+	// end inline asm
+	mov.b32 	%f1941, %r315;
+	mov.b32 	%f1942, %r316;
+	mov.b32 	%f1943, %r317;
+	add.s64 	%rd272, %rd266, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd271, %rd272;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r319,%r320,%r321,%r322}, [%rd271];
+	// end inline asm
+	mov.b32 	%f1938, %r319;
+	mov.b32 	%f1939, %r320;
+	mov.b32 	%f1940, %r321;
+	setp.leu.f32 	%p19, %f258, 0f00000000;
+	@%p19 bra 	$L__BB8_38;
+
+	mov.f32 	%f1101, 0f3F800000;
+	sub.f32 	%f1102, %f1101, %f258;
+	mul.lo.s64 	%rd284, %rd17, 48;
+	add.s64 	%rd285, %rd239, %rd284;
+	add.s64 	%rd276, %rd285, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd275, %rd276;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r326,%r327,%r328,%r329}, [%rd275];
+	// end inline asm
+	mov.b32 	%f1103, %r326;
+	mov.b32 	%f1104, %r327;
+	mov.b32 	%f1105, %r328;
+	mul.f32 	%f1106, %f258, %f1103;
+	mul.f32 	%f1107, %f258, %f1104;
+	mul.f32 	%f1108, %f258, %f1105;
+	fma.rn.f32 	%f1944, %f1102, %f1944, %f1106;
+	fma.rn.f32 	%f1945, %f1102, %f1945, %f1107;
+	fma.rn.f32 	%f1946, %f1102, %f1946, %f1108;
+	add.s64 	%rd279, %rd285, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd278, %rd279;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r330,%r331,%r332,%r333}, [%rd278];
+	// end inline asm
+	mov.b32 	%f1109, %r330;
+	mov.b32 	%f1110, %r331;
+	mov.b32 	%f1111, %r332;
+	mul.f32 	%f1112, %f258, %f1109;
+	mul.f32 	%f1113, %f258, %f1110;
+	mul.f32 	%f1114, %f258, %f1111;
+	fma.rn.f32 	%f1941, %f1102, %f1941, %f1112;
+	fma.rn.f32 	%f1942, %f1102, %f1942, %f1113;
+	fma.rn.f32 	%f1943, %f1102, %f1943, %f1114;
+	add.s64 	%rd282, %rd285, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd281, %rd282;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r334,%r335,%r336,%r337}, [%rd281];
+	// end inline asm
+	mov.b32 	%f1115, %r334;
+	mov.b32 	%f1116, %r335;
+	mov.b32 	%f1117, %r336;
+	mul.f32 	%f1118, %f258, %f1115;
+	mul.f32 	%f1119, %f258, %f1116;
+	mul.f32 	%f1120, %f258, %f1117;
+	fma.rn.f32 	%f1938, %f1102, %f1938, %f1118;
+	fma.rn.f32 	%f1939, %f1102, %f1939, %f1119;
+	fma.rn.f32 	%f1940, %f1102, %f1940, %f1120;
+	bra.uni 	$L__BB8_38;
+
+$L__BB8_27:
+	mov.f32 	%f1947, 0f00000000;
+	mov.f32 	%f1949, 0f3F800000;
+	setp.eq.s32 	%p15, %r190, 4;
+	@%p15 bra 	$L__BB8_30;
+
+	setp.ne.s32 	%p16, %r190, 1;
+	mov.f32 	%f1948, %f1947;
+	mov.f32 	%f1950, %f1947;
+	mov.f32 	%f1951, %f1949;
+	mov.f32 	%f1952, %f1947;
+	mov.f32 	%f1953, %f1949;
+	mov.f32 	%f1954, %f1947;
+	mov.f32 	%f1955, %f1947;
+	@%p16 bra 	$L__BB8_39;
+
+	// begin inline asm
+	call (%rd169), _optix_get_static_transform_from_handle, (%rd167);
+	// end inline asm
+	add.s64 	%rd650, %rd169, 64;
+	bra.uni 	$L__BB8_31;
+
+$L__BB8_33:
+	// begin inline asm
+	call (%rd182), _optix_get_srt_motion_transform_from_handle, (%rd167);
+	// end inline asm
+	// begin inline asm
+	cvta.to.global.u64 %rd184, %rd182;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r204,%r205,%r206,%r207}, [%rd184];
+	// end inline asm
+	add.s64 	%rd188, %rd182, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd187, %rd188;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r208,%r209,%r210,%r211}, [%rd187];
+	// end inline asm
+	add.s64 	%rd191, %rd182, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd190, %rd191;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r212,%r213,%r214,%r215}, [%rd190];
+	// end inline asm
+	add.s64 	%rd194, %rd182, 48;
+	// begin inline asm
 	cvta.to.global.u64 %rd193, %rd194;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r201,%r202,%r203,%r204}, [%rd193];
-	// inline asm
-	add.s64 	%rd197, %rd188, 32;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r216,%r217,%r218,%r219}, [%rd193];
+	// end inline asm
+	add.s64 	%rd197, %rd182, 64;
+	// begin inline asm
 	cvta.to.global.u64 %rd196, %rd197;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r205,%r206,%r207,%r208}, [%rd196];
-	// inline asm
-	add.s64 	%rd200, %rd188, 48;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r220,%r221,%r222,%r223}, [%rd196];
+	// end inline asm
+	add.s64 	%rd200, %rd182, 80;
+	// begin inline asm
 	cvta.to.global.u64 %rd199, %rd200;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r209,%r210,%r211,%r212}, [%rd199];
-	// inline asm
-	add.s64 	%rd203, %rd188, 64;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r224,%r225,%r226,%r227}, [%rd199];
+	// end inline asm
+	add.s64 	%rd203, %rd182, 96;
+	// begin inline asm
 	cvta.to.global.u64 %rd202, %rd203;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r213,%r214,%r215,%r216}, [%rd202];
-	// inline asm
-	add.s64 	%rd206, %rd188, 80;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r228,%r229,%r230,%r231}, [%rd202];
+	// end inline asm
+	add.s64 	%rd206, %rd182, 112;
+	// begin inline asm
 	cvta.to.global.u64 %rd205, %rd206;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r217,%r218,%r219,%r220}, [%rd205];
-	// inline asm
-	add.s64 	%rd209, %rd188, 96;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r232,%r233,%r234,%r235}, [%rd205];
+	// end inline asm
+	add.s64 	%rd209, %rd182, 128;
+	// begin inline asm
 	cvta.to.global.u64 %rd208, %rd209;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r221,%r222,%r223,%r224}, [%rd208];
-	// inline asm
-	add.s64 	%rd212, %rd188, 112;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r236,%r237,%r238,%r239}, [%rd208];
+	// end inline asm
+	add.s64 	%rd212, %rd182, 144;
+	// begin inline asm
 	cvta.to.global.u64 %rd211, %rd212;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r225,%r226,%r227,%r228}, [%rd211];
-	// inline asm
-	add.s64 	%rd215, %rd188, 128;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r240,%r241,%r242,%r243}, [%rd211];
+	// end inline asm
+	mov.b32 	%f998, %r207;
+	mov.b32 	%f999, %r208;
+	and.b32  	%r260, %r206, 65535;
+	add.s32 	%r261, %r260, -1;
+	cvt.rn.f32.s32 	%f1000, %r261;
+	sub.f32 	%f1001, %f986, %f998;
+	mul.f32 	%f1002, %f1001, %f1000;
+	sub.f32 	%f1003, %f999, %f998;
+	div.rn.f32 	%f1004, %f1002, %f1003;
+	min.f32 	%f1005, %f1000, %f1004;
+	mov.f32 	%f1006, 0f00000000;
+	max.f32 	%f1007, %f1006, %f1005;
+	cvt.rmi.f32.f32 	%f1008, %f1007;
+	sub.f32 	%f218, %f1007, %f1008;
+	cvt.rzi.s32.f32 	%r262, %f1008;
+	mul.wide.s32 	%rd226, %r262, 64;
+	add.s64 	%rd215, %rd191, %rd226;
+	// begin inline asm
 	cvta.to.global.u64 %rd214, %rd215;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r229,%r230,%r231,%r232}, [%rd214];
-	// inline asm
-	add.s64 	%rd218, %rd188, 144;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r244,%r245,%r246,%r247}, [%rd214];
+	// end inline asm
+	mov.b32 	%f1928, %r244;
+	mov.b32 	%f1929, %r245;
+	mov.b32 	%f1930, %r246;
+	add.s64 	%rd218, %rd215, 16;
+	// begin inline asm
 	cvta.to.global.u64 %rd217, %rd218;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r233,%r234,%r235,%r236}, [%rd217];
-	// inline asm
-	mov.b32 	 %f968, %r200;
-	mov.b32 	 %f969, %r201;
-	cvt.u32.u16	%r253, %rs7;
-	add.s32 	%r254, %r253, -1;
-	cvt.rn.f32.s32	%f970, %r254;
-	sub.f32 	%f971, %f957, %f968;
-	mul.f32 	%f972, %f971, %f970;
-	sub.f32 	%f973, %f969, %f968;
-	div.rn.f32 	%f974, %f972, %f973;
-	min.f32 	%f975, %f970, %f974;
-	mov.f32 	%f976, 0f00000000;
-	max.f32 	%f977, %f976, %f975;
-	cvt.rmi.f32.f32	%f978, %f977;
-	cvt.rzi.s32.f32	%r255, %f978;
-	cvt.s64.s32	%rd17, %r255;
-	mul.wide.s32 	%rd232, %r255, 64;
-	add.s64 	%rd221, %rd197, %rd232;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r248,%r249,%r250,%r251}, [%rd217];
+	// end inline asm
+	mov.b32 	%f1931, %r248;
+	mov.b32 	%f1932, %r249;
+	mov.b32 	%f1933, %r251;
+	add.s64 	%rd221, %rd215, 32;
+	// begin inline asm
 	cvta.to.global.u64 %rd220, %rd221;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r237,%r238,%r239,%r240}, [%rd220];
-	// inline asm
-	mov.b32 	 %f1897, %r237;
-	mov.b32 	 %f1898, %r238;
-	mov.b32 	 %f1899, %r239;
-	add.s64 	%rd224, %rd221, 16;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r252,%r253,%r254,%r255}, [%rd220];
+	// end inline asm
+	mov.b32 	%f1934, %r253;
+	mov.b32 	%f1935, %r254;
+	mov.b32 	%f1936, %r255;
+	add.s64 	%rd224, %rd215, 48;
+	// begin inline asm
 	cvta.to.global.u64 %rd223, %rd224;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r241,%r242,%r243,%r244}, [%rd223];
-	// inline asm
-	mov.b32 	 %f1900, %r241;
-	mov.b32 	 %f1901, %r242;
-	mov.b32 	 %f1902, %r244;
-	add.s64 	%rd227, %rd221, 32;
-	// inline asm
-	cvta.to.global.u64 %rd226, %rd227;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r245,%r246,%r247,%r248}, [%rd226];
-	// inline asm
-	sub.f32 	%f209, %f977, %f978;
-	mov.b32 	 %f1903, %r246;
-	mov.b32 	 %f1904, %r247;
-	mov.b32 	 %f1905, %r248;
-	add.s64 	%rd230, %rd221, 48;
-	// inline asm
-	cvta.to.global.u64 %rd229, %rd230;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r249,%r250,%r251,%r252}, [%rd229];
-	// inline asm
-	mov.b32 	 %f1906, %r249;
-	setp.leu.f32	%p16, %f209, 0f00000000;
-	@%p16 bra 	BB8_34;
-
-	shl.b64 	%rd245, %rd17, 6;
-	add.s64 	%rd246, %rd245, %rd188;
-	add.s64 	%rd234, %rd246, 96;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r256,%r257,%r258,%r259}, [%rd223];
+	// end inline asm
+	mov.b32 	%f1937, %r256;
+	setp.leu.f32 	%p18, %f218, 0f00000000;
+	@%p18 bra 	$L__BB8_35;
+
+	mov.f32 	%f1009, 0f3F800000;
+	sub.f32 	%f1010, %f1009, %f218;
+	add.s64 	%rd228, %rd215, 64;
+	// begin inline asm
+	cvta.to.global.u64 %rd227, %rd228;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r263,%r264,%r265,%r266}, [%rd227];
+	// end inline asm
+	mov.b32 	%f1011, %r263;
+	mov.b32 	%f1012, %r264;
+	mov.b32 	%f1013, %r265;
+	mul.f32 	%f1014, %f218, %f1011;
+	mul.f32 	%f1015, %f218, %f1012;
+	mul.f32 	%f1016, %f218, %f1013;
+	fma.rn.f32 	%f1928, %f1010, %f1928, %f1014;
+	fma.rn.f32 	%f1929, %f1010, %f1929, %f1015;
+	fma.rn.f32 	%f1930, %f1010, %f1930, %f1016;
+	add.s64 	%rd231, %rd215, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd230, %rd231;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r267,%r268,%r269,%r270}, [%rd230];
+	// end inline asm
+	mov.b32 	%f1017, %r267;
+	mov.b32 	%f1018, %r268;
+	mov.b32 	%f1019, %r270;
+	mul.f32 	%f1020, %f218, %f1017;
+	mul.f32 	%f1021, %f218, %f1018;
+	mul.f32 	%f1022, %f218, %f1019;
+	fma.rn.f32 	%f1931, %f1010, %f1931, %f1020;
+	fma.rn.f32 	%f1932, %f1010, %f1932, %f1021;
+	fma.rn.f32 	%f1933, %f1010, %f1933, %f1022;
+	add.s64 	%rd234, %rd215, 96;
+	// begin inline asm
 	cvta.to.global.u64 %rd233, %rd234;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r256,%r257,%r258,%r259}, [%rd233];
-	// inline asm
-	mov.b32 	 %f979, %r256;
-	mov.b32 	 %f980, %r257;
-	mov.b32 	 %f981, %r258;
-	add.s64 	%rd237, %rd246, 112;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r271,%r272,%r273,%r274}, [%rd233];
+	// end inline asm
+	mov.b32 	%f1023, %r272;
+	mov.b32 	%f1024, %r273;
+	mov.b32 	%f1025, %r274;
+	mul.f32 	%f1026, %f218, %f1023;
+	mul.f32 	%f1027, %f218, %f1024;
+	mul.f32 	%f1028, %f218, %f1025;
+	fma.rn.f32 	%f1029, %f1010, %f1934, %f1026;
+	fma.rn.f32 	%f1030, %f1010, %f1935, %f1027;
+	fma.rn.f32 	%f1031, %f1010, %f1936, %f1028;
+	add.s64 	%rd237, %rd215, 112;
+	// begin inline asm
 	cvta.to.global.u64 %rd236, %rd237;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r260,%r261,%r262,%r263}, [%rd236];
-	// inline asm
-	mov.b32 	 %f982, %r260;
-	mov.b32 	 %f983, %r261;
-	mov.b32 	 %f984, %r263;
-	add.s64 	%rd240, %rd246, 128;
-	// inline asm
-	cvta.to.global.u64 %rd239, %rd240;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r264,%r265,%r266,%r267}, [%rd239];
-	// inline asm
-	mov.b32 	 %f985, %r265;
-	mov.b32 	 %f986, %r266;
-	mov.b32 	 %f987, %r267;
-	add.s64 	%rd243, %rd246, 144;
-	// inline asm
-	cvta.to.global.u64 %rd242, %rd243;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r268,%r269,%r270,%r271}, [%rd242];
-	// inline asm
-	mov.f32 	%f988, 0f3F800000;
-	sub.f32 	%f989, %f988, %f209;
-	mul.f32 	%f990, %f209, %f979;
-	mul.f32 	%f991, %f209, %f980;
-	mul.f32 	%f992, %f209, %f981;
-	fma.rn.f32 	%f1897, %f989, %f1897, %f990;
-	fma.rn.f32 	%f1898, %f989, %f1898, %f991;
-	fma.rn.f32 	%f1899, %f989, %f1899, %f992;
-	mul.f32 	%f993, %f209, %f982;
-	mul.f32 	%f994, %f209, %f983;
-	mul.f32 	%f995, %f209, %f984;
-	fma.rn.f32 	%f1900, %f989, %f1900, %f993;
-	fma.rn.f32 	%f1901, %f989, %f1901, %f994;
-	fma.rn.f32 	%f1902, %f989, %f1902, %f995;
-	mul.f32 	%f996, %f209, %f985;
-	mul.f32 	%f997, %f209, %f986;
-	mul.f32 	%f998, %f209, %f987;
-	fma.rn.f32 	%f999, %f989, %f1903, %f996;
-	fma.rn.f32 	%f1000, %f989, %f1904, %f997;
-	fma.rn.f32 	%f1001, %f989, %f1905, %f998;
-	mov.b32 	 %f1002, %r268;
-	mul.f32 	%f1003, %f209, %f1002;
-	fma.rn.f32 	%f1004, %f989, %f1906, %f1003;
-	mul.f32 	%f1005, %f1000, %f1000;
-	fma.rn.f32 	%f1006, %f999, %f999, %f1005;
-	fma.rn.f32 	%f1007, %f1001, %f1001, %f1006;
-	fma.rn.f32 	%f1008, %f1004, %f1004, %f1007;
-	sqrt.rn.f32 	%f1009, %f1008;
-	rcp.rn.f32 	%f1010, %f1009;
-	mul.f32 	%f1903, %f999, %f1010;
-	mul.f32 	%f1904, %f1000, %f1010;
-	mul.f32 	%f1905, %f1001, %f1010;
-	mul.f32 	%f1906, %f1004, %f1010;
-
-BB8_34:
-	mul.f32 	%f1011, %f1904, %f1904;
-	fma.rn.f32 	%f1012, %f1903, %f1903, %f1011;
-	fma.rn.f32 	%f1013, %f1905, %f1905, %f1012;
-	fma.rn.f32 	%f1014, %f1906, %f1906, %f1013;
-	rcp.rn.f32 	%f1015, %f1014;
-	mul.f32 	%f1016, %f1903, %f1015;
-	mul.f32 	%f1017, %f1904, %f1015;
-	mul.f32 	%f1018, %f1905, %f1015;
-	mul.f32 	%f1019, %f1906, %f1015;
-	mul.f32 	%f1020, %f1903, %f1016;
-	mul.f32 	%f1021, %f1904, %f1017;
-	mul.f32 	%f1022, %f1905, %f1018;
-	mul.f32 	%f1023, %f1903, %f1017;
-	mul.f32 	%f1024, %f1905, %f1019;
-	mul.f32 	%f1025, %f1903, %f1018;
-	mul.f32 	%f1026, %f1904, %f1019;
-	mul.f32 	%f1027, %f1904, %f1018;
-	mul.f32 	%f1028, %f1903, %f1019;
-	sub.f32 	%f1029, %f1020, %f1021;
-	sub.f32 	%f1030, %f1029, %f1022;
-	fma.rn.f32 	%f1031, %f1906, %f1019, %f1030;
-	sub.f32 	%f1032, %f1023, %f1024;
-	add.f32 	%f1033, %f1032, %f1032;
-	add.f32 	%f1034, %f1025, %f1026;
-	add.f32 	%f1035, %f1034, %f1034;
-	add.f32 	%f1036, %f1023, %f1024;
-	add.f32 	%f1037, %f1036, %f1036;
-	sub.f32 	%f1038, %f1021, %f1020;
-	sub.f32 	%f1039, %f1038, %f1022;
-	fma.rn.f32 	%f1040, %f1906, %f1019, %f1039;
-	sub.f32 	%f1041, %f1027, %f1028;
-	add.f32 	%f1042, %f1041, %f1041;
-	sub.f32 	%f1043, %f1025, %f1026;
-	add.f32 	%f1044, %f1043, %f1043;
-	add.f32 	%f1045, %f1027, %f1028;
-	add.f32 	%f1046, %f1045, %f1045;
-	neg.f32 	%f1047, %f1020;
-	sub.f32 	%f1048, %f1047, %f1021;
-	add.f32 	%f1049, %f1022, %f1048;
-	fma.rn.f32 	%f1050, %f1906, %f1019, %f1049;
-	mul.f32 	%f1051, %f1899, %f1031;
-	fma.rn.f32 	%f1052, %f1901, %f1033, %f1051;
-	fma.rn.f32 	%f1915, %f1902, %f1035, %f1052;
-	mul.f32 	%f1053, %f1901, %f1040;
-	fma.rn.f32 	%f1054, %f1899, %f1037, %f1053;
-	fma.rn.f32 	%f1912, %f1902, %f1042, %f1054;
-	mul.f32 	%f1055, %f1901, %f1046;
-	fma.rn.f32 	%f1056, %f1899, %f1044, %f1055;
-	fma.rn.f32 	%f1909, %f1902, %f1050, %f1056;
-	mul.f32 	%f1057, %f1898, %f1031;
-	fma.rn.f32 	%f1914, %f1900, %f1033, %f1057;
-	mul.f32 	%f1058, %f1900, %f1040;
-	fma.rn.f32 	%f1911, %f1898, %f1037, %f1058;
-	mul.f32 	%f1059, %f1900, %f1046;
-	fma.rn.f32 	%f1908, %f1898, %f1044, %f1059;
-	mul.f32 	%f1913, %f1897, %f1031;
-	mul.f32 	%f1910, %f1897, %f1037;
-	mul.f32 	%f1907, %f1897, %f1044;
-
-BB8_37:
-	mul.f32 	%f1091, %f1908, %f1912;
-	mul.f32 	%f1092, %f1909, %f1911;
-	sub.f32 	%f1093, %f1092, %f1091;
-	mul.f32 	%f1094, %f1913, %f1093;
-	mul.f32 	%f1095, %f1907, %f1912;
-	mul.f32 	%f1096, %f1909, %f1910;
-	sub.f32 	%f1097, %f1096, %f1095;
-	mul.f32 	%f1098, %f1097, %f1914;
-	sub.f32 	%f1099, %f1094, %f1098;
-	mul.f32 	%f1100, %f1907, %f1911;
-	mul.f32 	%f1101, %f1908, %f1910;
-	sub.f32 	%f1102, %f1101, %f1100;
-	fma.rn.f32 	%f1103, %f1102, %f1915, %f1099;
-	rcp.rn.f32 	%f1104, %f1103;
-	mul.f32 	%f1922, %f1093, %f1104;
-	mul.f32 	%f1105, %f1909, %f1914;
-	mul.f32 	%f1106, %f1908, %f1915;
-	sub.f32 	%f1107, %f1106, %f1105;
-	mul.f32 	%f1923, %f1104, %f1107;
-	mul.f32 	%f1108, %f1911, %f1915;
-	mul.f32 	%f1109, %f1912, %f1914;
-	sub.f32 	%f1110, %f1109, %f1108;
-	mul.f32 	%f1924, %f1104, %f1110;
-	sub.f32 	%f1111, %f1095, %f1096;
-	mul.f32 	%f1919, %f1111, %f1104;
-	mul.f32 	%f1112, %f1907, %f1915;
-	mul.f32 	%f1113, %f1909, %f1913;
-	sub.f32 	%f1114, %f1113, %f1112;
-	mul.f32 	%f1920, %f1104, %f1114;
-	mul.f32 	%f1115, %f1912, %f1913;
-	mul.f32 	%f1116, %f1910, %f1915;
-	sub.f32 	%f1117, %f1116, %f1115;
-	mul.f32 	%f1921, %f1104, %f1117;
-	mul.f32 	%f1916, %f1102, %f1104;
-	mul.f32 	%f1118, %f1908, %f1913;
-	mul.f32 	%f1119, %f1907, %f1914;
-	sub.f32 	%f1120, %f1119, %f1118;
-	mul.f32 	%f1917, %f1120, %f1104;
-	mul.f32 	%f1121, %f1910, %f1914;
-	mul.f32 	%f1122, %f1911, %f1913;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r275,%r276,%r277,%r278}, [%rd236];
+	// end inline asm
+	mov.b32 	%f1032, %r275;
+	mul.f32 	%f1033, %f218, %f1032;
+	fma.rn.f32 	%f1034, %f1010, %f1937, %f1033;
+	mul.f32 	%f1035, %f1030, %f1030;
+	fma.rn.f32 	%f1036, %f1029, %f1029, %f1035;
+	fma.rn.f32 	%f1037, %f1031, %f1031, %f1036;
+	fma.rn.f32 	%f1038, %f1034, %f1034, %f1037;
+	sqrt.rn.f32 	%f1039, %f1038;
+	rcp.rn.f32 	%f1040, %f1039;
+	mul.f32 	%f1934, %f1029, %f1040;
+	mul.f32 	%f1935, %f1030, %f1040;
+	mul.f32 	%f1936, %f1031, %f1040;
+	mul.f32 	%f1937, %f1040, %f1034;
+
+$L__BB8_35:
+	mul.f32 	%f1041, %f1935, %f1935;
+	fma.rn.f32 	%f1042, %f1934, %f1934, %f1041;
+	fma.rn.f32 	%f1043, %f1936, %f1936, %f1042;
+	fma.rn.f32 	%f1044, %f1937, %f1937, %f1043;
+	rcp.rn.f32 	%f1045, %f1044;
+	mul.f32 	%f1046, %f1934, %f1045;
+	mul.f32 	%f1047, %f1935, %f1045;
+	mul.f32 	%f1048, %f1936, %f1045;
+	mul.f32 	%f1049, %f1937, %f1045;
+	mul.f32 	%f1050, %f1934, %f1046;
+	mul.f32 	%f1051, %f1935, %f1047;
+	mul.f32 	%f1052, %f1936, %f1048;
+	mul.f32 	%f1053, %f1934, %f1047;
+	mul.f32 	%f1054, %f1936, %f1049;
+	mul.f32 	%f1055, %f1934, %f1048;
+	mul.f32 	%f1056, %f1935, %f1049;
+	mul.f32 	%f1057, %f1935, %f1048;
+	mul.f32 	%f1058, %f1934, %f1049;
+	sub.f32 	%f1059, %f1050, %f1051;
+	sub.f32 	%f1060, %f1059, %f1052;
+	fma.rn.f32 	%f1061, %f1937, %f1049, %f1060;
+	sub.f32 	%f1062, %f1053, %f1054;
+	add.f32 	%f1063, %f1062, %f1062;
+	add.f32 	%f1064, %f1055, %f1056;
+	add.f32 	%f1065, %f1064, %f1064;
+	add.f32 	%f1066, %f1053, %f1054;
+	add.f32 	%f1067, %f1066, %f1066;
+	sub.f32 	%f1068, %f1051, %f1050;
+	sub.f32 	%f1069, %f1068, %f1052;
+	fma.rn.f32 	%f1070, %f1937, %f1049, %f1069;
+	sub.f32 	%f1071, %f1057, %f1058;
+	add.f32 	%f1072, %f1071, %f1071;
+	sub.f32 	%f1073, %f1055, %f1056;
+	add.f32 	%f1074, %f1073, %f1073;
+	add.f32 	%f1075, %f1057, %f1058;
+	add.f32 	%f1076, %f1075, %f1075;
+	neg.f32 	%f1077, %f1050;
+	sub.f32 	%f1078, %f1077, %f1051;
+	add.f32 	%f1079, %f1052, %f1078;
+	fma.rn.f32 	%f1080, %f1937, %f1049, %f1079;
+	mul.f32 	%f1081, %f1930, %f1061;
+	fma.rn.f32 	%f1082, %f1932, %f1063, %f1081;
+	fma.rn.f32 	%f1946, %f1933, %f1065, %f1082;
+	mul.f32 	%f1083, %f1932, %f1070;
+	fma.rn.f32 	%f1084, %f1930, %f1067, %f1083;
+	fma.rn.f32 	%f1943, %f1933, %f1072, %f1084;
+	mul.f32 	%f1085, %f1932, %f1076;
+	fma.rn.f32 	%f1086, %f1930, %f1074, %f1085;
+	fma.rn.f32 	%f1940, %f1933, %f1080, %f1086;
+	mul.f32 	%f1087, %f1929, %f1061;
+	fma.rn.f32 	%f1945, %f1931, %f1063, %f1087;
+	mul.f32 	%f1088, %f1931, %f1070;
+	fma.rn.f32 	%f1942, %f1929, %f1067, %f1088;
+	mul.f32 	%f1089, %f1931, %f1076;
+	fma.rn.f32 	%f1939, %f1929, %f1074, %f1089;
+	mul.f32 	%f1944, %f1928, %f1061;
+	mul.f32 	%f1941, %f1928, %f1067;
+	mul.f32 	%f1938, %f1928, %f1074;
+
+$L__BB8_38:
+	mul.f32 	%f1121, %f1939, %f1943;
+	mul.f32 	%f1122, %f1940, %f1942;
 	sub.f32 	%f1123, %f1122, %f1121;
-	mul.f32 	%f1918, %f1123, %f1104;
-	bra.uni 	BB8_38;
-
-BB8_27:
-	setp.ne.s32	%p14, %r183, 1;
-	mov.f32 	%f1917, %f1916;
-	mov.f32 	%f1919, %f1916;
-	mov.f32 	%f1920, %f1918;
-	mov.f32 	%f1921, %f1916;
-	mov.f32 	%f1922, %f1918;
-	mov.f32 	%f1923, %f1916;
-	mov.f32 	%f1924, %f1916;
-	@%p14 bra 	BB8_38;
-
-	// inline asm
-	call (%rd175), _optix_get_static_transform_from_handle, (%rd173);
-	// inline asm
-	add.s64 	%rd663, %rd175, 64;
-
-BB8_30:
-	// inline asm
-	cvta.to.global.u64 %rd179, %rd663;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r185,%r186,%r187,%r188}, [%rd179];
-	// inline asm
-	mov.b32 	 %f1922, %r185;
-	mov.b32 	 %f1923, %r186;
-	mov.b32 	 %f1924, %r187;
-	add.s64 	%rd183, %rd663, 16;
-	// inline asm
-	cvta.to.global.u64 %rd182, %rd183;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r189,%r190,%r191,%r192}, [%rd182];
-	// inline asm
-	mov.b32 	 %f1919, %r189;
-	mov.b32 	 %f1920, %r190;
-	mov.b32 	 %f1921, %r191;
-	add.s64 	%rd186, %rd663, 32;
-	// inline asm
-	cvta.to.global.u64 %rd185, %rd186;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r193,%r194,%r195,%r196}, [%rd185];
-	// inline asm
-	mov.b32 	 %f1916, %r193;
-	mov.b32 	 %f1917, %r194;
-	mov.b32 	 %f1918, %r195;
-
-BB8_38:
-	setp.eq.s32	%p18, %r647, 0;
-	@%p18 bra 	BB8_39;
-	bra.uni 	BB8_40;
-
-BB8_39:
-	mov.f32 	%f1896, %f1916;
-	mov.f32 	%f1895, %f1917;
-	mov.f32 	%f1894, %f1918;
-	mov.f32 	%f1893, %f1919;
-	mov.f32 	%f1892, %f1920;
-	mov.f32 	%f1891, %f1921;
-	mov.f32 	%f1890, %f1922;
-	mov.f32 	%f1889, %f1923;
-	mov.f32 	%f1888, %f1924;
-	bra.uni 	BB8_41;
-
-BB8_40:
-	mul.f32 	%f1124, %f1893, %f1923;
-	fma.rn.f32 	%f1125, %f1890, %f1922, %f1124;
-	fma.rn.f32 	%f289, %f1896, %f1924, %f1125;
-	mul.f32 	%f1126, %f1892, %f1923;
-	fma.rn.f32 	%f1127, %f1889, %f1922, %f1126;
-	fma.rn.f32 	%f290, %f1895, %f1924, %f1127;
-	mul.f32 	%f1128, %f1891, %f1923;
-	fma.rn.f32 	%f1129, %f1888, %f1922, %f1128;
-	fma.rn.f32 	%f291, %f1894, %f1924, %f1129;
-	mul.f32 	%f1130, %f1893, %f1920;
-	fma.rn.f32 	%f1131, %f1890, %f1919, %f1130;
-	fma.rn.f32 	%f292, %f1896, %f1921, %f1131;
-	mul.f32 	%f1132, %f1892, %f1920;
-	fma.rn.f32 	%f1133, %f1889, %f1919, %f1132;
-	fma.rn.f32 	%f293, %f1895, %f1921, %f1133;
-	mul.f32 	%f1134, %f1891, %f1920;
-	fma.rn.f32 	%f1135, %f1888, %f1919, %f1134;
-	fma.rn.f32 	%f294, %f1894, %f1921, %f1135;
-	mul.f32 	%f1136, %f1893, %f1917;
-	fma.rn.f32 	%f1137, %f1890, %f1916, %f1136;
-	fma.rn.f32 	%f1896, %f1896, %f1918, %f1137;
-	mul.f32 	%f1138, %f1892, %f1917;
-	fma.rn.f32 	%f1139, %f1889, %f1916, %f1138;
-	fma.rn.f32 	%f1895, %f1895, %f1918, %f1139;
-	mul.f32 	%f1140, %f1891, %f1917;
-	fma.rn.f32 	%f1141, %f1888, %f1916, %f1140;
-	fma.rn.f32 	%f1894, %f1894, %f1918, %f1141;
-	mov.f32 	%f1893, %f292;
-	mov.f32 	%f1892, %f293;
-	mov.f32 	%f1891, %f294;
-	mov.f32 	%f1890, %f289;
-	mov.f32 	%f1889, %f290;
-	mov.f32 	%f1888, %f291;
-
-BB8_41:
-	add.s32 	%r647, %r647, 1;
-	setp.lt.u32	%p19, %r647, %r30;
-	@%p19 bra 	BB8_25;
-
-	mul.f32 	%f1142, %f955, %f1889;
-	fma.rn.f32 	%f1143, %f954, %f1890, %f1142;
-	fma.rn.f32 	%f1934, %f1936, %f1888, %f1143;
-	mul.f32 	%f1144, %f955, %f1892;
-	fma.rn.f32 	%f1145, %f954, %f1893, %f1144;
-	fma.rn.f32 	%f1935, %f1936, %f1891, %f1145;
-	mul.f32 	%f1146, %f955, %f1895;
-	fma.rn.f32 	%f1147, %f954, %f1896, %f1146;
-	fma.rn.f32 	%f1936, %f1936, %f1894, %f1147;
-	bra.uni 	BB8_43;
-
-BB8_24:
-	mov.f32 	%f1934, %f954;
-	mov.f32 	%f1935, %f955;
-
-BB8_43:
-	// inline asm
-	call (%f1149), _optix_get_ray_tmax, ();
-	// inline asm
-	ld.const.u64 	%rd294, [params+80];
-	setp.eq.s64	%p20, %rd294, 0;
-	@%p20 bra 	BB8_48;
-
-	ld.u64 	%rd295, [%rd51];
-	ld.const.u64 	%rd296, [params+328];
+	mul.f32 	%f1124, %f1944, %f1123;
+	mul.f32 	%f1125, %f1938, %f1943;
+	mul.f32 	%f1126, %f1940, %f1941;
+	sub.f32 	%f1127, %f1126, %f1125;
+	mul.f32 	%f1128, %f1127, %f1945;
+	sub.f32 	%f1129, %f1124, %f1128;
+	mul.f32 	%f1130, %f1938, %f1942;
+	mul.f32 	%f1131, %f1939, %f1941;
+	sub.f32 	%f1132, %f1131, %f1130;
+	fma.rn.f32 	%f1133, %f1132, %f1946, %f1129;
+	rcp.rn.f32 	%f1134, %f1133;
+	mul.f32 	%f1953, %f1123, %f1134;
+	mul.f32 	%f1135, %f1940, %f1945;
+	mul.f32 	%f1136, %f1939, %f1946;
+	sub.f32 	%f1137, %f1136, %f1135;
+	mul.f32 	%f1954, %f1137, %f1134;
+	mul.f32 	%f1138, %f1942, %f1946;
+	mul.f32 	%f1139, %f1943, %f1945;
+	sub.f32 	%f1140, %f1139, %f1138;
+	mul.f32 	%f1955, %f1140, %f1134;
+	sub.f32 	%f1141, %f1125, %f1126;
+	mul.f32 	%f1950, %f1141, %f1134;
+	mul.f32 	%f1142, %f1938, %f1946;
+	mul.f32 	%f1143, %f1940, %f1944;
+	sub.f32 	%f1144, %f1143, %f1142;
+	mul.f32 	%f1951, %f1144, %f1134;
+	mul.f32 	%f1145, %f1943, %f1944;
+	mul.f32 	%f1146, %f1941, %f1946;
+	sub.f32 	%f1147, %f1146, %f1145;
+	mul.f32 	%f1952, %f1147, %f1134;
+	mul.f32 	%f1947, %f1132, %f1134;
+	mul.f32 	%f1148, %f1939, %f1944;
+	mul.f32 	%f1149, %f1938, %f1945;
+	sub.f32 	%f1150, %f1149, %f1148;
+	mul.f32 	%f1948, %f1150, %f1134;
+	mul.f32 	%f1151, %f1941, %f1945;
+	mul.f32 	%f1152, %f1942, %f1944;
+	sub.f32 	%f1153, %f1152, %f1151;
+	mul.f32 	%f1949, %f1153, %f1134;
+	bra.uni 	$L__BB8_39;
+
+$L__BB8_30:
+	// begin inline asm
+	call (%rd650), _optix_get_instance_inverse_transform_from_handle, (%rd167);
+	// end inline asm
+
+$L__BB8_31:
+	// begin inline asm
+	cvta.to.global.u64 %rd173, %rd650;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r192,%r193,%r194,%r195}, [%rd173];
+	// end inline asm
+	mov.b32 	%f1953, %r192;
+	mov.b32 	%f1954, %r193;
+	mov.b32 	%f1955, %r194;
+	add.s64 	%rd177, %rd650, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd176, %rd177;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r196,%r197,%r198,%r199}, [%rd176];
+	// end inline asm
+	mov.b32 	%f1950, %r196;
+	mov.b32 	%f1951, %r197;
+	mov.b32 	%f1952, %r198;
+	add.s64 	%rd180, %rd650, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd179, %rd180;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r200,%r201,%r202,%r203}, [%rd179];
+	// end inline asm
+	mov.b32 	%f1947, %r200;
+	mov.b32 	%f1948, %r201;
+	mov.b32 	%f1949, %r202;
+
+$L__BB8_39:
+	setp.eq.s32 	%p20, %r656, 0;
+	@%p20 bra 	$L__BB8_41;
+
+	mul.f32 	%f1154, %f1924, %f1954;
+	fma.rn.f32 	%f1155, %f1921, %f1953, %f1154;
+	fma.rn.f32 	%f304, %f1927, %f1955, %f1155;
+	mul.f32 	%f1156, %f1923, %f1954;
+	fma.rn.f32 	%f1157, %f1920, %f1953, %f1156;
+	fma.rn.f32 	%f305, %f1926, %f1955, %f1157;
+	mul.f32 	%f1158, %f1922, %f1954;
+	fma.rn.f32 	%f1159, %f1919, %f1953, %f1158;
+	fma.rn.f32 	%f1955, %f1925, %f1955, %f1159;
+	mul.f32 	%f1160, %f1924, %f1951;
+	fma.rn.f32 	%f1161, %f1921, %f1950, %f1160;
+	fma.rn.f32 	%f307, %f1927, %f1952, %f1161;
+	mul.f32 	%f1162, %f1923, %f1951;
+	fma.rn.f32 	%f1163, %f1920, %f1950, %f1162;
+	fma.rn.f32 	%f308, %f1926, %f1952, %f1163;
+	mul.f32 	%f1164, %f1922, %f1951;
+	fma.rn.f32 	%f1165, %f1919, %f1950, %f1164;
+	fma.rn.f32 	%f1952, %f1925, %f1952, %f1165;
+	mul.f32 	%f1166, %f1924, %f1948;
+	fma.rn.f32 	%f1167, %f1921, %f1947, %f1166;
+	fma.rn.f32 	%f310, %f1927, %f1949, %f1167;
+	mul.f32 	%f1168, %f1923, %f1948;
+	fma.rn.f32 	%f1169, %f1920, %f1947, %f1168;
+	fma.rn.f32 	%f311, %f1926, %f1949, %f1169;
+	mul.f32 	%f1170, %f1922, %f1948;
+	fma.rn.f32 	%f1171, %f1919, %f1947, %f1170;
+	fma.rn.f32 	%f1949, %f1925, %f1949, %f1171;
+	mov.f32 	%f1947, %f310;
+	mov.f32 	%f1948, %f311;
+	mov.f32 	%f1950, %f307;
+	mov.f32 	%f1951, %f308;
+	mov.f32 	%f1953, %f304;
+	mov.f32 	%f1954, %f305;
+
+$L__BB8_41:
+	add.s32 	%r656, %r656, 1;
+	setp.lt.u32 	%p21, %r656, %r187;
+	mov.f32 	%f1919, %f1955;
+	mov.f32 	%f1920, %f1954;
+	mov.f32 	%f1921, %f1953;
+	mov.f32 	%f1922, %f1952;
+	mov.f32 	%f1923, %f1951;
+	mov.f32 	%f1924, %f1950;
+	mov.f32 	%f1925, %f1949;
+	mov.f32 	%f1926, %f1948;
+	mov.f32 	%f1927, %f1947;
+	@%p21 bra 	$L__BB8_26;
+
+$L__BB8_42:
+	mul.f32 	%f1172, %f1975, %f1954;
+	fma.rn.f32 	%f1173, %f1974, %f1953, %f1172;
+	mul.f32 	%f1174, %f1975, %f1951;
+	fma.rn.f32 	%f1175, %f1974, %f1950, %f1174;
+	mul.f32 	%f1176, %f1975, %f1948;
+	fma.rn.f32 	%f1177, %f1974, %f1947, %f1176;
+	fma.rn.f32 	%f1976, %f985, %f1949, %f1177;
+	fma.rn.f32 	%f1975, %f985, %f1952, %f1175;
+	fma.rn.f32 	%f1974, %f985, %f1955, %f1173;
+	bra.uni 	$L__BB8_44;
+
+$L__BB8_43:
+	mov.f32 	%f1976, %f985;
+
+$L__BB8_44:
+	// begin inline asm
+	call (%f1179), _optix_get_ray_tmax, ();
+	// end inline asm
+	ld.const.u64 	%rd286, [params+80];
+	setp.eq.s64 	%p22, %rd286, 0;
+	@%p22 bra 	$L__BB8_49;
+
+	ld.u64 	%rd287, [%rd47];
+	ld.const.u64 	%rd288, [params+328];
+	cvta.to.global.u64 	%rd289, %rd288;
+	cvt.u64.u32 	%rd18, %r1;
+	mul.wide.u32 	%rd290, %r1, 8;
+	add.s64 	%rd291, %rd289, %rd290;
+	st.global.u64 	[%rd291], %rd287;
+	ld.const.u64 	%rd292, [params+336];
+	cvta.to.global.u64 	%rd293, %rd292;
+	mul.wide.u32 	%rd294, %r1, 4;
+	add.s64 	%rd295, %rd293, %rd294;
+	mov.u32 	%r338, 0;
+	st.global.u32 	[%rd295], %r338;
+	ld.const.u64 	%rd296, [params+344];
 	cvta.to.global.u64 	%rd297, %rd296;
-	cvt.u64.u32	%rd20, %r1;
-	mul.wide.u32 	%rd298, %r1, 8;
-	add.s64 	%rd299, %rd297, %rd298;
-	st.global.u64 	[%rd299], %rd295;
-	ld.const.u64 	%rd300, [params+336];
-	cvta.to.global.u64 	%rd301, %rd300;
-	mul.wide.u32 	%rd302, %r1, 4;
-	add.s64 	%rd303, %rd301, %rd302;
-	mov.u32 	%r331, 0;
-	st.global.u32 	[%rd303], %r331;
-	ld.const.u64 	%rd304, [params+344];
-	cvta.to.global.u64 	%rd305, %rd304;
-	add.s64 	%rd21, %rd305, %rd302;
-	ld.global.u32 	%r9, [%rd21];
-	setp.eq.s32	%p21, %r9, 0;
-	@%p21 bra 	BB8_47;
-
-	// inline asm
-	call (%r332), _optix_read_instance_id, ();
-	// inline asm
-	setp.ge.u32	%p22, %r332, %r9;
-	@%p22 bra 	BB8_47;
-
-	st.global.u32 	[%rd21], %r332;
-
-BB8_47:
-	ld.const.u64 	%rd306, [params+72];
-	cvta.to.global.u64 	%rd307, %rd306;
-	shl.b64 	%rd308, %rd20, 2;
-	add.s64 	%rd309, %rd307, %rd308;
-	st.global.f32 	[%rd309], %f1149;
-	bra.uni 	BB8_116;
-
-BB8_48:
-	fma.rn.f32 	%f1151, %f1149, %f1934, %f1887;
-	ld.f32 	%f1152, [%rd3+288];
-	sub.f32 	%f1153, %f1151, %f1152;
-	ld.f32 	%f1154, [%rd3+292];
-	fma.rn.f32 	%f1155, %f1149, %f1935, %f1886;
-	sub.f32 	%f1156, %f1155, %f1154;
-	ld.f32 	%f1157, [%rd3+296];
-	fma.rn.f32 	%f1158, %f1149, %f1936, %f1885;
-	sub.f32 	%f1159, %f1158, %f1157;
-	mul.f32 	%f1160, %f1153, %f1153;
-	fma.rn.f32 	%f1161, %f1156, %f1156, %f1160;
-	fma.rn.f32 	%f1162, %f1159, %f1159, %f1161;
-	sqrt.rn.f32 	%f1163, %f1162;
-	div.rn.f32 	%f1164, %f1153, %f1163;
-	div.rn.f32 	%f1165, %f1156, %f1163;
-	div.rn.f32 	%f1166, %f1159, %f1163;
+	add.s64 	%rd19, %rd297, %rd294;
+	ld.global.u32 	%r10, [%rd19];
+	setp.eq.s32 	%p23, %r10, 0;
+	@%p23 bra 	$L__BB8_48;
+
+	// begin inline asm
+	call (%r339), _optix_read_instance_id, ();
+	// end inline asm
+	setp.ge.u32 	%p24, %r339, %r10;
+	@%p24 bra 	$L__BB8_48;
+
+	st.global.u32 	[%rd19], %r339;
+
+$L__BB8_48:
+	ld.const.u64 	%rd298, [params+72];
+	cvta.to.global.u64 	%rd299, %rd298;
+	shl.b64 	%rd300, %rd18, 2;
+	add.s64 	%rd301, %rd299, %rd300;
+	st.global.f32 	[%rd301], %f1179;
+	bra.uni 	$L__BB8_117;
+
+$L__BB8_49:
+	fma.rn.f32 	%f1182, %f1179, %f1974, %f1916;
+	fma.rn.f32 	%f1183, %f1179, %f1975, %f1917;
+	fma.rn.f32 	%f1184, %f1179, %f1976, %f1918;
+	add.s64 	%rd20, %rd3, 288;
+	ld.f32 	%f1185, [%rd3+288];
+	sub.f32 	%f1186, %f1182, %f1185;
+	ld.f32 	%f1187, [%rd3+292];
+	sub.f32 	%f1188, %f1183, %f1187;
+	ld.f32 	%f1189, [%rd3+296];
+	sub.f32 	%f1190, %f1184, %f1189;
+	mul.f32 	%f1191, %f1186, %f1186;
+	fma.rn.f32 	%f1192, %f1188, %f1188, %f1191;
+	fma.rn.f32 	%f1193, %f1190, %f1190, %f1192;
+	sqrt.rn.f32 	%f1194, %f1193;
+	div.rn.f32 	%f1195, %f1186, %f1194;
+	div.rn.f32 	%f1196, %f1188, %f1194;
+	div.rn.f32 	%f1197, %f1190, %f1194;
 	ld.u8 	%rs1, [%rd3+308];
-	setp.eq.s16	%p23, %rs1, 0;
-	neg.f32 	%f1167, %f1164;
-	neg.f32 	%f1168, %f1165;
-	neg.f32 	%f1169, %f1166;
-	selp.f32	%f2100, %f1166, %f1169, %p23;
-	selp.f32	%f2099, %f1165, %f1168, %p23;
-	selp.f32	%f2098, %f1164, %f1167, %p23;
-	ld.f32 	%f317, [%rd3+304];
-	fma.rn.f32 	%f2101, %f317, %f2098, %f1152;
-	fma.rn.f32 	%f2102, %f317, %f2099, %f1154;
-	fma.rn.f32 	%f2103, %f317, %f2100, %f1157;
-	ld.const.u64 	%rd23, [params+96];
-	setp.eq.s64	%p24, %rd23, 0;
-	@%p24 bra 	BB8_56;
-
-	ld.v4.f32 	{%f1170, %f1171, %f1172, %f1173}, [%rd3+208];
-	ld.v4.f32 	{%f1177, %f1178, %f1179, %f1180}, [%rd3+160];
-	fma.rn.f32 	%f1182, %f2101, %f1177, %f1170;
-	fma.rn.f32 	%f1184, %f2101, %f1178, %f1171;
-	fma.rn.f32 	%f1186, %f2101, %f1179, %f1172;
-	ld.v4.f32 	{%f1187, %f1188, %f1189, %f1190}, [%rd3+176];
-	fma.rn.f32 	%f1192, %f2102, %f1187, %f1182;
-	fma.rn.f32 	%f1194, %f2102, %f1188, %f1184;
-	fma.rn.f32 	%f1196, %f2102, %f1189, %f1186;
-	ld.v4.f32 	{%f1197, %f1198, %f1199, %f1200}, [%rd3+192];
-	fma.rn.f32 	%f321, %f2103, %f1197, %f1192;
-	fma.rn.f32 	%f322, %f2103, %f1198, %f1194;
-	fma.rn.f32 	%f323, %f2103, %f1199, %f1196;
-	mul.f32 	%f1204, %f322, %f322;
-	fma.rn.f32 	%f324, %f321, %f321, %f1204;
-	abs.f32 	%f1205, %f323;
-	mov.f32 	%f1206, 0f3F800000;
-	sub.f32 	%f1207, %f1206, %f1205;
-	mul.f32 	%f1208, %f1207, 0f3F000000;
-	sqrt.rn.f32 	%f1209, %f1208;
-	setp.gt.f32	%p25, %f1205, 0f3F11EB85;
-	selp.f32	%f1210, %f1209, %f1205, %p25;
-	mul.f32 	%f1211, %f1210, %f1210;
-	mov.f32 	%f1212, 0f3C94D2E9;
-	mov.f32 	%f1213, 0f3D53F941;
-	fma.rn.f32 	%f1214, %f1213, %f1211, %f1212;
-	mov.f32 	%f1215, 0f3D3F841F;
-	fma.rn.f32 	%f1216, %f1214, %f1211, %f1215;
-	mov.f32 	%f1217, 0f3D994929;
-	fma.rn.f32 	%f1218, %f1216, %f1211, %f1217;
-	mov.f32 	%f1219, 0f3E2AAB94;
-	fma.rn.f32 	%f1220, %f1218, %f1211, %f1219;
-	mul.f32 	%f1221, %f1211, %f1220;
-	fma.rn.f32 	%f1222, %f1221, %f1210, %f1210;
-	add.f32 	%f1223, %f1222, %f1222;
-	mov.f32 	%f1224, 0f3FC90FDB;
-	sub.f32 	%f1225, %f1224, %f1222;
-	selp.f32	%f325, %f1223, %f1225, %p25;
-	abs.f32 	%f326, %f321;
-	abs.f32 	%f327, %f322;
-	setp.eq.f32	%p26, %f326, 0f00000000;
-	setp.eq.f32	%p27, %f327, 0f00000000;
-	and.pred  	%p28, %p26, %p27;
-	mov.b32 	 %r11, %f321;
-	mov.b32 	 %r333, %f322;
-	and.b32  	%r12, %r333, -2147483648;
-	@%p28 bra 	BB8_53;
-	bra.uni 	BB8_50;
-
-BB8_53:
-	shr.s32 	%r340, %r11, 31;
-	and.b32  	%r341, %r340, 1078530011;
-	or.b32  	%r342, %r341, %r12;
-	mov.b32 	 %f1937, %r342;
-	bra.uni 	BB8_54;
-
-BB8_50:
-	setp.eq.f32	%p29, %f326, 0f7F800000;
-	setp.eq.f32	%p30, %f327, 0f7F800000;
-	and.pred  	%p31, %p29, %p30;
-	@%p31 bra 	BB8_52;
-	bra.uni 	BB8_51;
-
-BB8_52:
-	shr.s32 	%r336, %r11, 31;
-	and.b32  	%r337, %r336, 13483017;
-	add.s32 	%r338, %r337, 1061752795;
-	or.b32  	%r339, %r338, %r12;
-	mov.b32 	 %f1937, %r339;
-	bra.uni 	BB8_54;
-
-BB8_51:
-	max.f32 	%f1226, %f327, %f326;
-	min.f32 	%f1227, %f327, %f326;
-	div.rn.f32 	%f1228, %f1227, %f1226;
-	mul.rn.f32 	%f1229, %f1228, %f1228;
-	mov.f32 	%f1230, 0fC0B59883;
-	mov.f32 	%f1231, 0fBF52C7EA;
-	fma.rn.f32 	%f1232, %f1229, %f1231, %f1230;
-	mov.f32 	%f1233, 0fC0D21907;
-	fma.rn.f32 	%f1234, %f1232, %f1229, %f1233;
-	mul.f32 	%f1235, %f1229, %f1234;
-	mul.f32 	%f1236, %f1228, %f1235;
-	add.f32 	%f1237, %f1229, 0f41355DC0;
-	mov.f32 	%f1238, 0f41E6BD60;
-	fma.rn.f32 	%f1239, %f1237, %f1229, %f1238;
-	mov.f32 	%f1240, 0f419D92C8;
-	fma.rn.f32 	%f1241, %f1239, %f1229, %f1240;
-	rcp.rn.f32 	%f1242, %f1241;
-	fma.rn.f32 	%f1243, %f1236, %f1242, %f1228;
-	sub.f32 	%f1245, %f1224, %f1243;
-	setp.gt.f32	%p32, %f327, %f326;
-	selp.f32	%f1246, %f1245, %f1243, %p32;
-	mov.f32 	%f1247, 0f40490FDB;
-	sub.f32 	%f1248, %f1247, %f1246;
-	setp.lt.s32	%p33, %r11, 0;
-	selp.f32	%f1249, %f1248, %f1246, %p33;
-	mov.b32 	 %r334, %f1249;
-	or.b32  	%r335, %r334, %r12;
-	mov.b32 	 %f1250, %r335;
-	add.f32 	%f1251, %f326, %f327;
-	setp.gtu.f32	%p34, %f1251, 0f7F800000;
-	selp.f32	%f1937, %f1251, %f1250, %p34;
-
-BB8_54:
-	add.f32 	%f1253, %f1937, 0f40C90FDB;
-	setp.lt.f32	%p35, %f1937, 0f00000000;
-	selp.f32	%f1254, %f1253, %f1937, %p35;
-	mul.f32 	%f1945, %f1254, 0f3E22F983;
-	mov.f32 	%f1255, 0f40490FDB;
-	sub.f32 	%f1256, %f1255, %f325;
-	setp.lt.f32	%p36, %f323, 0f00000000;
-	selp.f32	%f1257, %f1256, %f325, %p36;
-	mul.f32 	%f1944, %f1257, 0f3EA2F983;
-	ld.const.u64 	%rd310, [params+184];
-	setp.eq.s64	%p37, %rd310, 0;
-	@%p37 bra 	BB8_56;
-
-	neg.f32 	%f1258, %f322;
-	sqrt.rn.f32 	%f1259, %f324;
-	rcp.rn.f32 	%f1260, %f1259;
-	mul.f32 	%f1261, %f321, %f1260;
-	mul.f32 	%f1262, %f322, %f1260;
-	mul.f32 	%f1263, %f323, %f1261;
-	mul.f32 	%f1264, %f323, %f1262;
-	neg.f32 	%f1265, %f1259;
-	setp.eq.f32	%p38, %f1259, 0f00000000;
-	selp.f32	%f1266, 0f00000000, %f1265, %p38;
-	selp.f32	%f1267, 0f00000000, %f1264, %p38;
-	selp.f32	%f1268, 0f3F800000, %f1263, %p38;
-	ld.v4.f32 	{%f1269, %f1270, %f1271, %f1272}, [%rd3+32];
-	mul.f32 	%f1276, %f1269, %f1258;
-	mul.f32 	%f1277, %f1270, %f1258;
-	mul.f32 	%f1278, %f1271, %f1258;
-	ld.v4.f32 	{%f1279, %f1280, %f1281, %f1282}, [%rd3+48];
-	fma.rn.f32 	%f1286, %f321, %f1279, %f1276;
-	fma.rn.f32 	%f1287, %f321, %f1280, %f1277;
-	fma.rn.f32 	%f1288, %f321, %f1281, %f1278;
-	ld.v4.f32 	{%f1289, %f1290, %f1291, %f1292}, [%rd3+64];
-	mov.f32 	%f1294, 0f00000000;
-	fma.rn.f32 	%f1295, %f1294, %f1289, %f1286;
-	fma.rn.f32 	%f1297, %f1294, %f1290, %f1287;
-	fma.rn.f32 	%f1299, %f1294, %f1291, %f1288;
-	mul.f32 	%f342, %f1295, 0f40C90FDB;
-	mul.f32 	%f341, %f1297, 0f40C90FDB;
-	mul.f32 	%f340, %f1299, 0f40C90FDB;
-	mul.f32 	%f1300, %f1268, %f1269;
-	mul.f32 	%f1301, %f1268, %f1270;
-	mul.f32 	%f1302, %f1268, %f1271;
-	fma.rn.f32 	%f1303, %f1267, %f1279, %f1300;
-	fma.rn.f32 	%f1304, %f1267, %f1280, %f1301;
-	fma.rn.f32 	%f1305, %f1267, %f1281, %f1302;
-	fma.rn.f32 	%f1306, %f1266, %f1289, %f1303;
-	fma.rn.f32 	%f1307, %f1266, %f1290, %f1304;
-	fma.rn.f32 	%f1308, %f1266, %f1291, %f1305;
-	mul.f32 	%f345, %f1306, 0f40490FDB;
-	mul.f32 	%f344, %f1307, 0f40490FDB;
-	mul.f32 	%f343, %f1308, 0f40490FDB;
-
-BB8_56:
-	selp.f32	%f1309, 0f3F800000, 0fBF800000, %p23;
-	div.rn.f32 	%f1310, %f1309, %f317;
-	mul.f32 	%f2086, %f342, %f1310;
-	mul.f32 	%f2087, %f341, %f1310;
-	mul.f32 	%f2088, %f340, %f1310;
-	mul.f32 	%f2083, %f345, %f1310;
-	mul.f32 	%f2084, %f344, %f1310;
-	mul.f32 	%f2085, %f343, %f1310;
-	ld.u64 	%rd24, [%rd51];
-	ld.const.u64 	%rd311, [params+344];
-	cvta.to.global.u64 	%rd312, %rd311;
-	cvt.u64.u32	%rd25, %r1;
-	mul.wide.u32 	%rd313, %r1, 4;
-	add.s64 	%rd26, %rd312, %rd313;
-	ld.global.u32 	%r13, [%rd26];
-	setp.eq.s32	%p40, %r13, 0;
-	@%p40 bra 	BB8_57;
-
-	// inline asm
-	call (%r343), _optix_read_instance_id, ();
-	// inline asm
-	setp.ge.u32	%p41, %r343, %r13;
-	@%p41 bra 	BB8_57;
-
-	mov.f32 	%f2011, 0f00000000;
-	mov.f32 	%f2012, 0f3F800000;
-	mov.f32 	%f1949, %f2012;
-	mov.f32 	%f1948, %f2011;
-	mov.f32 	%f1947, %f2011;
-	mov.f32 	%f1946, %f2011;
-	mov.f32 	%f1953, %f2011;
-	mov.f32 	%f1952, %f2012;
-	mov.f32 	%f1951, %f2011;
-	mov.f32 	%f1950, %f2011;
-	mov.f32 	%f1957, %f2011;
-	mov.f32 	%f1956, %f2011;
-	mov.f32 	%f1955, %f2012;
-	mov.f32 	%f1954, %f2011;
-	@%p2 bra 	BB8_77;
-
-	add.s32 	%r648, %r30, -1;
-	setp.lt.s32	%p43, %r648, 0;
-	@%p43 bra 	BB8_77;
-
-BB8_61:
+	setp.eq.s16 	%p25, %rs1, 0;
+	neg.f32 	%f1198, %f1195;
+	neg.f32 	%f1199, %f1196;
+	neg.f32 	%f1200, %f1197;
+	selp.f32 	%f2110, %f1197, %f1200, %p25;
+	selp.f32 	%f2109, %f1196, %f1199, %p25;
+	selp.f32 	%f2108, %f1195, %f1198, %p25;
+	ld.f32 	%f344, [%rd3+304];
+	fma.rn.f32 	%f2141, %f344, %f2108, %f1185;
+	fma.rn.f32 	%f2142, %f344, %f2109, %f1187;
+	fma.rn.f32 	%f2143, %f344, %f2110, %f1189;
+	ld.const.u64 	%rd21, [params+96];
+	setp.eq.s64 	%p26, %rd21, 0;
+	@%p26 bra 	$L__BB8_57;
+
+	ld.v4.f32 	{%f1201, %f1202, %f1203, %f1204}, [%rd20+-80];
+	ld.f32 	%f1208, [%rd20+-128];
+	fma.rn.f32 	%f1209, %f2141, %f1208, %f1201;
+	ld.f32 	%f1210, [%rd20+-124];
+	fma.rn.f32 	%f1211, %f2141, %f1210, %f1202;
+	ld.f32 	%f1212, [%rd20+-120];
+	fma.rn.f32 	%f1213, %f2141, %f1212, %f1203;
+	ld.f32 	%f1214, [%rd20+-112];
+	fma.rn.f32 	%f1215, %f2142, %f1214, %f1209;
+	ld.f32 	%f1216, [%rd20+-108];
+	fma.rn.f32 	%f1217, %f2142, %f1216, %f1211;
+	ld.f32 	%f1218, [%rd20+-104];
+	fma.rn.f32 	%f1219, %f2142, %f1218, %f1213;
+	ld.f32 	%f1220, [%rd20+-96];
+	fma.rn.f32 	%f348, %f2143, %f1220, %f1215;
+	ld.f32 	%f1221, [%rd20+-92];
+	fma.rn.f32 	%f349, %f2143, %f1221, %f1217;
+	ld.f32 	%f1222, [%rd20+-88];
+	fma.rn.f32 	%f350, %f2143, %f1222, %f1219;
+	mul.f32 	%f1223, %f349, %f349;
+	fma.rn.f32 	%f351, %f348, %f348, %f1223;
+	abs.f32 	%f352, %f348;
+	abs.f32 	%f353, %f349;
+	setp.eq.f32 	%p27, %f352, 0f00000000;
+	setp.eq.f32 	%p28, %f353, 0f00000000;
+	and.pred  	%p29, %p27, %p28;
+	mov.b32 	%r12, %f348;
+	mov.b32 	%r340, %f349;
+	and.b32  	%r13, %r340, -2147483648;
+	@%p29 bra 	$L__BB8_54;
+	bra.uni 	$L__BB8_51;
+
+$L__BB8_54:
+	shr.s32 	%r345, %r12, 31;
+	and.b32  	%r346, %r345, 1078530011;
+	or.b32  	%r347, %r346, %r13;
+	mov.b32 	%f1977, %r347;
+	bra.uni 	$L__BB8_55;
+
+$L__BB8_51:
+	setp.eq.f32 	%p30, %f352, 0f7F800000;
+	setp.eq.f32 	%p31, %f353, 0f7F800000;
+	and.pred  	%p32, %p30, %p31;
+	@%p32 bra 	$L__BB8_53;
+	bra.uni 	$L__BB8_52;
+
+$L__BB8_53:
+	setp.lt.s32 	%p36, %r12, 0;
+	selp.b32 	%r343, 1075235812, 1061752795, %p36;
+	or.b32  	%r344, %r343, %r13;
+	mov.b32 	%f1977, %r344;
+	bra.uni 	$L__BB8_55;
+
+$L__BB8_52:
+	setp.lt.s32 	%p33, %r12, 0;
+	min.f32 	%f1224, %f353, %f352;
+	max.f32 	%f1225, %f353, %f352;
+	div.rn.f32 	%f1226, %f1224, %f1225;
+	mul.rn.f32 	%f1227, %f1226, %f1226;
+	mov.f32 	%f1228, 0fC0B59883;
+	mov.f32 	%f1229, 0fBF52C7EA;
+	fma.rn.f32 	%f1230, %f1227, %f1229, %f1228;
+	mov.f32 	%f1231, 0fC0D21907;
+	fma.rn.f32 	%f1232, %f1230, %f1227, %f1231;
+	mul.f32 	%f1233, %f1227, %f1232;
+	mul.f32 	%f1234, %f1226, %f1233;
+	add.f32 	%f1235, %f1227, 0f41355DC0;
+	mov.f32 	%f1236, 0f41E6BD60;
+	fma.rn.f32 	%f1237, %f1235, %f1227, %f1236;
+	mov.f32 	%f1238, 0f419D92C8;
+	fma.rn.f32 	%f1239, %f1237, %f1227, %f1238;
+	rcp.rn.f32 	%f1240, %f1239;
+	fma.rn.f32 	%f1241, %f1234, %f1240, %f1226;
+	mov.f32 	%f1242, 0f3FC90FDB;
+	sub.f32 	%f1243, %f1242, %f1241;
+	setp.gt.f32 	%p34, %f353, %f352;
+	selp.f32 	%f1244, %f1243, %f1241, %p34;
+	mov.f32 	%f1245, 0f40490FDB;
+	sub.f32 	%f1246, %f1245, %f1244;
+	selp.f32 	%f1247, %f1246, %f1244, %p33;
+	mov.b32 	%r341, %f1247;
+	or.b32  	%r342, %r13, %r341;
+	mov.b32 	%f1248, %r342;
+	add.f32 	%f1249, %f352, %f353;
+	setp.le.f32 	%p35, %f1249, 0f7F800000;
+	selp.f32 	%f1977, %f1248, %f1249, %p35;
+
+$L__BB8_55:
+	abs.f32 	%f1251, %f350;
+	setp.gt.f32 	%p37, %f1251, 0f3F11EB85;
+	mov.f32 	%f1252, 0f3F800000;
+	sub.f32 	%f1253, %f1252, %f1251;
+	mul.f32 	%f1254, %f1253, 0f3F000000;
+	sqrt.rn.f32 	%f1255, %f1254;
+	selp.f32 	%f1256, %f1255, %f1251, %p37;
+	mul.f32 	%f1257, %f1256, %f1256;
+	mov.f32 	%f1258, 0f3C94D2E9;
+	mov.f32 	%f1259, 0f3D53F941;
+	fma.rn.f32 	%f1260, %f1259, %f1257, %f1258;
+	mov.f32 	%f1261, 0f3D3F841F;
+	fma.rn.f32 	%f1262, %f1260, %f1257, %f1261;
+	mov.f32 	%f1263, 0f3D994929;
+	fma.rn.f32 	%f1264, %f1262, %f1257, %f1263;
+	mov.f32 	%f1265, 0f3E2AAB94;
+	fma.rn.f32 	%f1266, %f1264, %f1257, %f1265;
+	mul.f32 	%f1267, %f1257, %f1266;
+	fma.rn.f32 	%f1268, %f1267, %f1256, %f1256;
+	mov.f32 	%f1269, 0f3FC90FDB;
+	sub.f32 	%f1270, %f1269, %f1268;
+	add.f32 	%f1271, %f1268, %f1268;
+	selp.f32 	%f1272, %f1271, %f1270, %p37;
+	add.f32 	%f1273, %f1977, 0f40C90FDB;
+	setp.lt.f32 	%p38, %f1977, 0f00000000;
+	selp.f32 	%f1274, %f1273, %f1977, %p38;
+	mul.f32 	%f1985, %f1274, 0f3E22F983;
+	mov.f32 	%f1275, 0f40490FDB;
+	sub.f32 	%f1276, %f1275, %f1272;
+	setp.lt.f32 	%p39, %f350, 0f00000000;
+	selp.f32 	%f1277, %f1276, %f1272, %p39;
+	mul.f32 	%f1984, %f1277, 0f3EA2F983;
+	ld.const.u64 	%rd302, [params+184];
+	setp.eq.s64 	%p40, %rd302, 0;
+	@%p40 bra 	$L__BB8_57;
+
+	neg.f32 	%f1278, %f349;
+	sqrt.rn.f32 	%f1279, %f351;
+	rcp.rn.f32 	%f1280, %f1279;
+	mul.f32 	%f1281, %f348, %f1280;
+	mul.f32 	%f1282, %f349, %f1280;
+	mul.f32 	%f1283, %f350, %f1281;
+	mul.f32 	%f1284, %f350, %f1282;
+	neg.f32 	%f1285, %f1279;
+	setp.eq.f32 	%p41, %f1279, 0f00000000;
+	mov.f32 	%f1286, 0f00000000;
+	selp.f32 	%f1287, 0f00000000, %f1285, %p41;
+	selp.f32 	%f1288, 0f00000000, %f1284, %p41;
+	selp.f32 	%f1289, 0f3F800000, %f1283, %p41;
+	ld.v4.f32 	{%f1290, %f1291, %f1292, %f1293}, [%rd20+-256];
+	mul.f32 	%f1297, %f1290, %f1278;
+	mul.f32 	%f1298, %f1291, %f1278;
+	mul.f32 	%f1299, %f1292, %f1278;
+	ld.v4.f32 	{%f1300, %f1301, %f1302, %f1303}, [%rd20+-240];
+	fma.rn.f32 	%f1307, %f348, %f1300, %f1297;
+	fma.rn.f32 	%f1308, %f348, %f1301, %f1298;
+	fma.rn.f32 	%f1309, %f348, %f1302, %f1299;
+	ld.f32 	%f1310, [%rd20+-224];
+	fma.rn.f32 	%f1311, %f1286, %f1310, %f1307;
+	ld.f32 	%f1312, [%rd20+-220];
+	fma.rn.f32 	%f1313, %f1286, %f1312, %f1308;
+	ld.f32 	%f1314, [%rd20+-216];
+	fma.rn.f32 	%f1315, %f1286, %f1314, %f1309;
+	mul.f32 	%f2132, %f1311, 0f40C90FDB;
+	mul.f32 	%f2133, %f1313, 0f40C90FDB;
+	mul.f32 	%f2134, %f1315, 0f40C90FDB;
+	mul.f32 	%f1316, %f1289, %f1290;
+	mul.f32 	%f1317, %f1289, %f1291;
+	mul.f32 	%f1318, %f1289, %f1292;
+	fma.rn.f32 	%f1319, %f1288, %f1300, %f1316;
+	fma.rn.f32 	%f1320, %f1288, %f1301, %f1317;
+	fma.rn.f32 	%f1321, %f1288, %f1302, %f1318;
+	fma.rn.f32 	%f1322, %f1287, %f1310, %f1319;
+	fma.rn.f32 	%f1323, %f1287, %f1312, %f1320;
+	fma.rn.f32 	%f1324, %f1287, %f1314, %f1321;
+	mul.f32 	%f2129, %f1322, 0f40490FDB;
+	mul.f32 	%f2130, %f1323, 0f40490FDB;
+	mul.f32 	%f2131, %f1324, 0f40490FDB;
+
+$L__BB8_57:
+	selp.f32 	%f1325, 0f3F800000, 0fBF800000, %p25;
+	div.rn.f32 	%f1326, %f1325, %f344;
+	mul.f32 	%f2120, %f2132, %f1326;
+	mul.f32 	%f2121, %f2133, %f1326;
+	mul.f32 	%f2122, %f2134, %f1326;
+	mul.f32 	%f2117, %f2129, %f1326;
+	mul.f32 	%f2118, %f2130, %f1326;
+	mul.f32 	%f2119, %f2131, %f1326;
+	ld.u64 	%rd22, [%rd47];
+	ld.const.u64 	%rd303, [params+344];
+	cvta.to.global.u64 	%rd304, %rd303;
+	cvt.u64.u32 	%rd23, %r1;
+	mul.wide.u32 	%rd305, %r1, 4;
+	add.s64 	%rd24, %rd304, %rd305;
+	ld.global.u32 	%r14, [%rd24];
+	setp.eq.s32 	%p43, %r14, 0;
+	mov.f32 	%f2138, %f2108;
+	mov.f32 	%f2139, %f2109;
+	mov.f32 	%f2140, %f2110;
+	@%p43 bra 	$L__BB8_105;
+
+	// begin inline asm
+	call (%r348), _optix_read_instance_id, ();
+	// end inline asm
+	setp.ge.u32 	%p44, %r348, %r14;
+	mov.f32 	%f2138, %f2108;
+	mov.f32 	%f2139, %f2109;
+	mov.f32 	%f2140, %f2110;
+	@%p44 bra 	$L__BB8_105;
+
+	// begin inline asm
+	call (%r349), _optix_get_transform_list_size, ();
+	// end inline asm
+	setp.eq.s32 	%p45, %r349, 0;
+	mov.f32 	%f2085, 0f00000000;
+	mov.f32 	%f2084, 0f3F800000;
+	mov.f32 	%f2022, %f2084;
+	mov.f32 	%f2023, %f2085;
+	mov.f32 	%f2024, %f2085;
+	mov.f32 	%f2025, %f2085;
+	mov.f32 	%f2018, %f2085;
+	mov.f32 	%f2019, %f2084;
+	mov.f32 	%f2020, %f2085;
+	mov.f32 	%f2021, %f2085;
+	mov.f32 	%f2014, %f2085;
+	mov.f32 	%f2015, %f2085;
+	mov.f32 	%f2016, %f2084;
+	mov.f32 	%f2017, %f2085;
+	@%p45 bra 	$L__BB8_77;
+
+	// begin inline asm
+	call (%r350), _optix_get_transform_list_size, ();
+	// end inline asm
+	// begin inline asm
+	call (%f1339), _optix_get_ray_time, ();
+	// end inline asm
+	setp.lt.s32 	%p46, %r350, 1;
+	@%p46 bra 	$L__BB8_77;
+
+	add.s32 	%r657, %r350, 1;
+	mov.u32 	%r658, 1;
+
+$L__BB8_62:
 	.pragma "nounroll";
-	// inline asm
-	call (%rd314), _optix_get_transform_list_handle, (%r648);
-	// inline asm
-	// inline asm
-	call (%r345), _optix_get_transform_type_from_handle, (%rd314);
-	// inline asm
-	and.b32  	%r346, %r345, -2;
-	setp.eq.s32	%p44, %r346, 2;
-	@%p44 bra 	BB8_67;
-	bra.uni 	BB8_62;
-
-BB8_67:
-	setp.eq.s32	%p47, %r345, 2;
-	@%p47 bra 	BB8_71;
-	bra.uni 	BB8_68;
-
-BB8_71:
-	// inline asm
-	call (%rd388), _optix_get_matrix_motion_transform_from_handle, (%rd314);
-	// inline asm
-	// inline asm
-	cvta.to.global.u64 %rd390, %rd388;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r434,%r435,%r436,%r437}, [%rd390];
-	// inline asm
-	mov.b32	{%rs13, %rs14}, %r436;
-	add.s64 	%rd394, %rd388, 16;
-	// inline asm
-	cvta.to.global.u64 %rd393, %rd394;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r438,%r439,%r440,%r441}, [%rd393];
-	// inline asm
-	add.s64 	%rd397, %rd388, 32;
-	// inline asm
-	cvta.to.global.u64 %rd396, %rd397;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r442,%r443,%r444,%r445}, [%rd396];
-	// inline asm
-	add.s64 	%rd400, %rd388, 48;
-	// inline asm
-	cvta.to.global.u64 %rd399, %rd400;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r446,%r447,%r448,%r449}, [%rd399];
-	// inline asm
-	add.s64 	%rd403, %rd388, 64;
-	// inline asm
-	cvta.to.global.u64 %rd402, %rd403;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r450,%r451,%r452,%r453}, [%rd402];
-	// inline asm
-	add.s64 	%rd406, %rd388, 80;
-	// inline asm
-	cvta.to.global.u64 %rd405, %rd406;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r454,%r455,%r456,%r457}, [%rd405];
-	// inline asm
-	add.s64 	%rd409, %rd388, 96;
-	// inline asm
-	cvta.to.global.u64 %rd408, %rd409;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r458,%r459,%r460,%r461}, [%rd408];
-	// inline asm
-	add.s64 	%rd412, %rd388, 112;
-	// inline asm
-	cvta.to.global.u64 %rd411, %rd412;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r462,%r463,%r464,%r465}, [%rd411];
-	// inline asm
-	mov.b32 	 %f1449, %r437;
-	mov.b32 	 %f1450, %r438;
-	cvt.u32.u16	%r478, %rs13;
-	add.s32 	%r479, %r478, -1;
-	cvt.rn.f32.s32	%f1451, %r479;
-	sub.f32 	%f1452, %f957, %f1449;
-	mul.f32 	%f1453, %f1452, %f1451;
-	sub.f32 	%f1454, %f1450, %f1449;
-	div.rn.f32 	%f1455, %f1453, %f1454;
-	min.f32 	%f1456, %f1451, %f1455;
-	mov.f32 	%f1457, 0f00000000;
-	max.f32 	%f1458, %f1457, %f1456;
-	cvt.rmi.f32.f32	%f1459, %f1458;
-	cvt.rzi.s32.f32	%r480, %f1459;
-	cvt.s64.s32	%rd34, %r480;
-	mul.wide.s32 	%rd423, %r480, 48;
-	add.s64 	%rd415, %rd397, %rd423;
-	// inline asm
+	add.s32 	%r352, %r657, -2;
+	// begin inline asm
+	call (%rd306), _optix_get_transform_list_handle, (%r352);
+	// end inline asm
+	// begin inline asm
+	call (%r353), _optix_get_transform_type_from_handle, (%rd306);
+	// end inline asm
+	or.b32  	%r354, %r353, 1;
+	setp.eq.s32 	%p47, %r354, 3;
+	@%p47 bra 	$L__BB8_68;
+	bra.uni 	$L__BB8_63;
+
+$L__BB8_68:
+	setp.eq.s32 	%p50, %r353, 2;
+	@%p50 bra 	$L__BB8_72;
+	bra.uni 	$L__BB8_69;
+
+$L__BB8_72:
+	// begin inline asm
+	call (%rd378), _optix_get_matrix_motion_transform_from_handle, (%rd306);
+	// end inline asm
+	// begin inline asm
+	cvta.to.global.u64 %rd380, %rd378;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r442,%r443,%r444,%r445}, [%rd380];
+	// end inline asm
+	add.s64 	%rd384, %rd378, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd383, %rd384;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r446,%r447,%r448,%r449}, [%rd383];
+	// end inline asm
+	add.s64 	%rd387, %rd378, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd386, %rd387;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r450,%r451,%r452,%r453}, [%rd386];
+	// end inline asm
+	add.s64 	%rd390, %rd378, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd389, %rd390;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r454,%r455,%r456,%r457}, [%rd389];
+	// end inline asm
+	add.s64 	%rd393, %rd378, 64;
+	// begin inline asm
+	cvta.to.global.u64 %rd392, %rd393;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r458,%r459,%r460,%r461}, [%rd392];
+	// end inline asm
+	add.s64 	%rd396, %rd378, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd395, %rd396;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r462,%r463,%r464,%r465}, [%rd395];
+	// end inline asm
+	add.s64 	%rd399, %rd378, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd398, %rd399;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r466,%r467,%r468,%r469}, [%rd398];
+	// end inline asm
+	add.s64 	%rd402, %rd378, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd401, %rd402;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r470,%r471,%r472,%r473}, [%rd401];
+	// end inline asm
+	mov.b32 	%f1467, %r445;
+	mov.b32 	%f1468, %r446;
+	and.b32  	%r486, %r444, 65535;
+	add.s32 	%r487, %r486, -1;
+	cvt.rn.f32.s32 	%f1469, %r487;
+	sub.f32 	%f1470, %f1339, %f1467;
+	mul.f32 	%f1471, %f1470, %f1469;
+	sub.f32 	%f1472, %f1468, %f1467;
+	div.rn.f32 	%f1473, %f1471, %f1472;
+	min.f32 	%f1474, %f1469, %f1473;
+	mov.f32 	%f1475, 0f00000000;
+	max.f32 	%f1476, %f1475, %f1474;
+	cvt.rmi.f32.f32 	%f1477, %f1476;
+	sub.f32 	%f466, %f1476, %f1477;
+	cvt.rzi.s32.f32 	%r488, %f1477;
+	cvt.s64.s32 	%rd31, %r488;
+	mul.wide.s32 	%rd413, %r488, 48;
+	add.s64 	%rd405, %rd387, %rd413;
+	// begin inline asm
+	cvta.to.global.u64 %rd404, %rd405;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r474,%r475,%r476,%r477}, [%rd404];
+	// end inline asm
+	mov.b32 	%f2022, %r474;
+	mov.b32 	%f2023, %r475;
+	mov.b32 	%f2024, %r476;
+	mov.b32 	%f2025, %r477;
+	add.s64 	%rd408, %rd405, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd407, %rd408;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r478,%r479,%r480,%r481}, [%rd407];
+	// end inline asm
+	mov.b32 	%f2018, %r478;
+	mov.b32 	%f2019, %r479;
+	mov.b32 	%f2020, %r480;
+	mov.b32 	%f2021, %r481;
+	add.s64 	%rd411, %rd405, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd410, %rd411;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r482,%r483,%r484,%r485}, [%rd410];
+	// end inline asm
+	mov.b32 	%f2014, %r482;
+	mov.b32 	%f2015, %r483;
+	mov.b32 	%f2016, %r484;
+	mov.b32 	%f2017, %r485;
+	setp.leu.f32 	%p52, %f466, 0f00000000;
+	@%p52 bra 	$L__BB8_74;
+
+	mov.f32 	%f1478, 0f3F800000;
+	sub.f32 	%f1479, %f1478, %f466;
+	mul.lo.s64 	%rd423, %rd31, 48;
+	add.s64 	%rd424, %rd378, %rd423;
+	add.s64 	%rd415, %rd424, 80;
+	// begin inline asm
 	cvta.to.global.u64 %rd414, %rd415;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r466,%r467,%r468,%r469}, [%rd414];
-	// inline asm
-	mov.b32 	 %f1982, %r466;
-	mov.b32 	 %f1983, %r467;
-	mov.b32 	 %f1984, %r468;
-	mov.b32 	 %f1985, %r469;
-	add.s64 	%rd418, %rd415, 16;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r489,%r490,%r491,%r492}, [%rd414];
+	// end inline asm
+	mov.b32 	%f1480, %r489;
+	mov.b32 	%f1481, %r490;
+	mov.b32 	%f1482, %r491;
+	mov.b32 	%f1483, %r492;
+	mul.f32 	%f1484, %f466, %f1480;
+	mul.f32 	%f1485, %f466, %f1481;
+	mul.f32 	%f1486, %f466, %f1482;
+	mul.f32 	%f1487, %f466, %f1483;
+	fma.rn.f32 	%f2022, %f1479, %f2022, %f1484;
+	fma.rn.f32 	%f2023, %f1479, %f2023, %f1485;
+	fma.rn.f32 	%f2024, %f1479, %f2024, %f1486;
+	fma.rn.f32 	%f2025, %f1479, %f2025, %f1487;
+	add.s64 	%rd418, %rd424, 96;
+	// begin inline asm
 	cvta.to.global.u64 %rd417, %rd418;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r470,%r471,%r472,%r473}, [%rd417];
-	// inline asm
-	mov.b32 	 %f1978, %r470;
-	mov.b32 	 %f1979, %r471;
-	mov.b32 	 %f1980, %r472;
-	mov.b32 	 %f1981, %r473;
-	add.s64 	%rd421, %rd415, 32;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r493,%r494,%r495,%r496}, [%rd417];
+	// end inline asm
+	mov.b32 	%f1488, %r493;
+	mov.b32 	%f1489, %r494;
+	mov.b32 	%f1490, %r495;
+	mov.b32 	%f1491, %r496;
+	mul.f32 	%f1492, %f466, %f1488;
+	mul.f32 	%f1493, %f466, %f1489;
+	mul.f32 	%f1494, %f466, %f1490;
+	mul.f32 	%f1495, %f466, %f1491;
+	fma.rn.f32 	%f2018, %f1479, %f2018, %f1492;
+	fma.rn.f32 	%f2019, %f1479, %f2019, %f1493;
+	fma.rn.f32 	%f2020, %f1479, %f2020, %f1494;
+	fma.rn.f32 	%f2021, %f1479, %f2021, %f1495;
+	add.s64 	%rd421, %rd424, 112;
+	// begin inline asm
 	cvta.to.global.u64 %rd420, %rd421;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r474,%r475,%r476,%r477}, [%rd420];
-	// inline asm
-	sub.f32 	%f447, %f1458, %f1459;
-	mov.b32 	 %f1974, %r474;
-	mov.b32 	 %f1975, %r475;
-	mov.b32 	 %f1976, %r476;
-	mov.b32 	 %f1977, %r477;
-	setp.leu.f32	%p49, %f447, 0f00000000;
-	@%p49 bra 	BB8_73;
-
-	mul.lo.s64 	%rd433, %rd34, 48;
-	add.s64 	%rd434, %rd388, %rd433;
-	add.s64 	%rd425, %rd434, 80;
-	// inline asm
-	cvta.to.global.u64 %rd424, %rd425;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r481,%r482,%r483,%r484}, [%rd424];
-	// inline asm
-	mov.b32 	 %f1460, %r481;
-	mov.b32 	 %f1461, %r482;
-	mov.b32 	 %f1462, %r483;
-	mov.b32 	 %f1463, %r484;
-	add.s64 	%rd428, %rd434, 96;
-	// inline asm
-	cvta.to.global.u64 %rd427, %rd428;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r485,%r486,%r487,%r488}, [%rd427];
-	// inline asm
-	mov.b32 	 %f1464, %r485;
-	mov.b32 	 %f1465, %r486;
-	mov.b32 	 %f1466, %r487;
-	mov.b32 	 %f1467, %r488;
-	add.s64 	%rd431, %rd434, 112;
-	// inline asm
-	cvta.to.global.u64 %rd430, %rd431;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r489,%r490,%r491,%r492}, [%rd430];
-	// inline asm
-	mov.f32 	%f1468, 0f3F800000;
-	sub.f32 	%f1469, %f1468, %f447;
-	mul.f32 	%f1470, %f447, %f1460;
-	mul.f32 	%f1471, %f447, %f1461;
-	mul.f32 	%f1472, %f447, %f1462;
-	mul.f32 	%f1473, %f447, %f1463;
-	fma.rn.f32 	%f1982, %f1469, %f1982, %f1470;
-	fma.rn.f32 	%f1983, %f1469, %f1983, %f1471;
-	fma.rn.f32 	%f1984, %f1469, %f1984, %f1472;
-	fma.rn.f32 	%f1985, %f1469, %f1985, %f1473;
-	mul.f32 	%f1474, %f447, %f1464;
-	mul.f32 	%f1475, %f447, %f1465;
-	mul.f32 	%f1476, %f447, %f1466;
-	mul.f32 	%f1477, %f447, %f1467;
-	fma.rn.f32 	%f1978, %f1469, %f1978, %f1474;
-	fma.rn.f32 	%f1979, %f1469, %f1979, %f1475;
-	fma.rn.f32 	%f1980, %f1469, %f1980, %f1476;
-	fma.rn.f32 	%f1981, %f1469, %f1981, %f1477;
-	mov.b32 	 %f1478, %r489;
-	mov.b32 	 %f1479, %r490;
-	mov.b32 	 %f1480, %r491;
-	mov.b32 	 %f1481, %r492;
-	mul.f32 	%f1482, %f447, %f1478;
-	mul.f32 	%f1483, %f447, %f1479;
-	mul.f32 	%f1484, %f447, %f1480;
-	mul.f32 	%f1485, %f447, %f1481;
-	fma.rn.f32 	%f1974, %f1469, %f1974, %f1482;
-	fma.rn.f32 	%f1975, %f1469, %f1975, %f1483;
-	fma.rn.f32 	%f1976, %f1469, %f1976, %f1484;
-	fma.rn.f32 	%f1977, %f1469, %f1977, %f1485;
-	bra.uni 	BB8_73;
-
-BB8_62:
-	mov.f32 	%f1974, 0f00000000;
-	mov.f32 	%f1976, 0f3F800000;
-	setp.eq.s32	%p45, %r345, 4;
-	@%p45 bra 	BB8_65;
-	bra.uni 	BB8_63;
-
-BB8_65:
-	// inline asm
-	call (%rd664), _optix_get_instance_transform_from_handle, (%rd314);
-	// inline asm
-	bra.uni 	BB8_66;
-
-BB8_68:
-	// inline asm
-	call (%rd329), _optix_get_srt_motion_transform_from_handle, (%rd314);
-	// inline asm
-	// inline asm
-	cvta.to.global.u64 %rd331, %rd329;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r359,%r360,%r361,%r362}, [%rd331];
-	// inline asm
-	mov.b32	{%rs11, %rs12}, %r361;
-	add.s64 	%rd335, %rd329, 16;
-	// inline asm
-	cvta.to.global.u64 %rd334, %rd335;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r363,%r364,%r365,%r366}, [%rd334];
-	// inline asm
-	add.s64 	%rd338, %rd329, 32;
-	// inline asm
-	cvta.to.global.u64 %rd337, %rd338;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r367,%r368,%r369,%r370}, [%rd337];
-	// inline asm
-	add.s64 	%rd341, %rd329, 48;
-	// inline asm
-	cvta.to.global.u64 %rd340, %rd341;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r371,%r372,%r373,%r374}, [%rd340];
-	// inline asm
-	add.s64 	%rd344, %rd329, 64;
-	// inline asm
-	cvta.to.global.u64 %rd343, %rd344;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r375,%r376,%r377,%r378}, [%rd343];
-	// inline asm
-	add.s64 	%rd347, %rd329, 80;
-	// inline asm
-	cvta.to.global.u64 %rd346, %rd347;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r379,%r380,%r381,%r382}, [%rd346];
-	// inline asm
-	add.s64 	%rd350, %rd329, 96;
-	// inline asm
-	cvta.to.global.u64 %rd349, %rd350;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r383,%r384,%r385,%r386}, [%rd349];
-	// inline asm
-	add.s64 	%rd353, %rd329, 112;
-	// inline asm
-	cvta.to.global.u64 %rd352, %rd353;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r387,%r388,%r389,%r390}, [%rd352];
-	// inline asm
-	add.s64 	%rd356, %rd329, 128;
-	// inline asm
-	cvta.to.global.u64 %rd355, %rd356;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r391,%r392,%r393,%r394}, [%rd355];
-	// inline asm
-	add.s64 	%rd359, %rd329, 144;
-	// inline asm
-	cvta.to.global.u64 %rd358, %rd359;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r395,%r396,%r397,%r398}, [%rd358];
-	// inline asm
-	mov.b32 	 %f1336, %r362;
-	mov.b32 	 %f1337, %r363;
-	cvt.u32.u16	%r415, %rs11;
-	add.s32 	%r416, %r415, -1;
-	cvt.rn.f32.s32	%f1338, %r416;
-	sub.f32 	%f1339, %f957, %f1336;
-	mul.f32 	%f1340, %f1339, %f1338;
-	sub.f32 	%f1341, %f1337, %f1336;
-	div.rn.f32 	%f1342, %f1340, %f1341;
-	min.f32 	%f1343, %f1338, %f1342;
-	mov.f32 	%f1344, 0f00000000;
-	max.f32 	%f1345, %f1344, %f1343;
-	cvt.rmi.f32.f32	%f1346, %f1345;
-	cvt.rzi.s32.f32	%r417, %f1346;
-	cvt.s64.s32	%rd32, %r417;
-	mul.wide.s32 	%rd373, %r417, 64;
-	add.s64 	%rd362, %rd338, %rd373;
-	// inline asm
-	cvta.to.global.u64 %rd361, %rd362;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r399,%r400,%r401,%r402}, [%rd361];
-	// inline asm
-	mov.b32 	 %f1958, %r399;
-	mov.b32 	 %f1959, %r400;
-	mov.b32 	 %f1960, %r401;
-	mov.b32 	 %f1961, %r402;
-	add.s64 	%rd365, %rd362, 16;
-	// inline asm
-	cvta.to.global.u64 %rd364, %rd365;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r403,%r404,%r405,%r406}, [%rd364];
-	// inline asm
-	mov.b32 	 %f1962, %r403;
-	mov.b32 	 %f1963, %r404;
-	mov.b32 	 %f1964, %r405;
-	mov.b32 	 %f1965, %r406;
-	add.s64 	%rd368, %rd362, 32;
-	// inline asm
-	cvta.to.global.u64 %rd367, %rd368;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r407,%r408,%r409,%r410}, [%rd367];
-	// inline asm
-	sub.f32 	%f386, %f1345, %f1346;
-	mov.b32 	 %f1966, %r407;
-	mov.b32 	 %f1967, %r408;
-	mov.b32 	 %f1968, %r409;
-	mov.b32 	 %f1969, %r410;
-	add.s64 	%rd371, %rd362, 48;
-	// inline asm
-	cvta.to.global.u64 %rd370, %rd371;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r411,%r412,%r413,%r414}, [%rd370];
-	// inline asm
-	mov.b32 	 %f1970, %r411;
-	mov.b32 	 %f1971, %r412;
-	mov.b32 	 %f1972, %r413;
-	mov.b32 	 %f1973, %r414;
-	setp.leu.f32	%p48, %f386, 0f00000000;
-	@%p48 bra 	BB8_70;
-
-	shl.b64 	%rd386, %rd32, 6;
-	add.s64 	%rd387, %rd386, %rd329;
-	add.s64 	%rd375, %rd387, 96;
-	// inline asm
-	cvta.to.global.u64 %rd374, %rd375;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r418,%r419,%r420,%r421}, [%rd374];
-	// inline asm
-	mov.b32 	 %f1347, %r418;
-	mov.b32 	 %f1348, %r419;
-	mov.b32 	 %f1349, %r420;
-	mov.b32 	 %f1350, %r421;
-	add.s64 	%rd378, %rd387, 112;
-	// inline asm
-	cvta.to.global.u64 %rd377, %rd378;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r422,%r423,%r424,%r425}, [%rd377];
-	// inline asm
-	mov.b32 	 %f1351, %r422;
-	mov.b32 	 %f1352, %r423;
-	mov.b32 	 %f1353, %r424;
-	mov.b32 	 %f1354, %r425;
-	add.s64 	%rd381, %rd387, 128;
-	// inline asm
-	cvta.to.global.u64 %rd380, %rd381;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r426,%r427,%r428,%r429}, [%rd380];
-	// inline asm
-	mov.b32 	 %f1355, %r426;
-	mov.b32 	 %f1356, %r427;
-	mov.b32 	 %f1357, %r428;
-	mov.b32 	 %f1358, %r429;
-	add.s64 	%rd384, %rd387, 144;
-	// inline asm
-	cvta.to.global.u64 %rd383, %rd384;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r430,%r431,%r432,%r433}, [%rd383];
-	// inline asm
-	mov.f32 	%f1359, 0f3F800000;
-	sub.f32 	%f1360, %f1359, %f386;
-	mul.f32 	%f1361, %f386, %f1347;
-	mul.f32 	%f1362, %f386, %f1348;
-	mul.f32 	%f1363, %f386, %f1349;
-	mul.f32 	%f1364, %f386, %f1350;
-	fma.rn.f32 	%f1958, %f1360, %f1958, %f1361;
-	fma.rn.f32 	%f1959, %f1360, %f1959, %f1362;
-	fma.rn.f32 	%f1960, %f1360, %f1960, %f1363;
-	fma.rn.f32 	%f1961, %f1360, %f1961, %f1364;
-	mul.f32 	%f1365, %f386, %f1351;
-	mul.f32 	%f1366, %f386, %f1352;
-	mul.f32 	%f1367, %f386, %f1353;
-	mul.f32 	%f1368, %f386, %f1354;
-	fma.rn.f32 	%f1962, %f1360, %f1962, %f1365;
-	fma.rn.f32 	%f1963, %f1360, %f1963, %f1366;
-	fma.rn.f32 	%f1964, %f1360, %f1964, %f1367;
-	fma.rn.f32 	%f1965, %f1360, %f1965, %f1368;
-	mul.f32 	%f1369, %f386, %f1355;
-	mul.f32 	%f1370, %f386, %f1356;
-	mul.f32 	%f1371, %f386, %f1357;
-	mul.f32 	%f1372, %f386, %f1358;
-	fma.rn.f32 	%f1966, %f1360, %f1966, %f1369;
-	fma.rn.f32 	%f1373, %f1360, %f1967, %f1370;
-	fma.rn.f32 	%f1374, %f1360, %f1968, %f1371;
-	fma.rn.f32 	%f1375, %f1360, %f1969, %f1372;
-	mov.b32 	 %f1376, %r430;
-	mov.b32 	 %f1377, %r431;
-	mov.b32 	 %f1378, %r432;
-	mov.b32 	 %f1379, %r433;
-	mul.f32 	%f1380, %f386, %f1376;
-	mul.f32 	%f1381, %f386, %f1377;
-	mul.f32 	%f1382, %f386, %f1378;
-	mul.f32 	%f1383, %f386, %f1379;
-	fma.rn.f32 	%f1384, %f1360, %f1970, %f1380;
-	fma.rn.f32 	%f1971, %f1360, %f1971, %f1381;
-	fma.rn.f32 	%f1972, %f1360, %f1972, %f1382;
-	fma.rn.f32 	%f1973, %f1360, %f1973, %f1383;
-	mul.f32 	%f1385, %f1374, %f1374;
-	fma.rn.f32 	%f1386, %f1373, %f1373, %f1385;
-	fma.rn.f32 	%f1387, %f1375, %f1375, %f1386;
-	fma.rn.f32 	%f1388, %f1384, %f1384, %f1387;
-	sqrt.rn.f32 	%f1389, %f1388;
-	rcp.rn.f32 	%f1390, %f1389;
-	mul.f32 	%f1967, %f1373, %f1390;
-	mul.f32 	%f1968, %f1374, %f1390;
-	mul.f32 	%f1969, %f1375, %f1390;
-	mul.f32 	%f1970, %f1384, %f1390;
-
-BB8_70:
-	mul.f32 	%f1391, %f1968, %f1968;
-	fma.rn.f32 	%f1392, %f1967, %f1967, %f1391;
-	fma.rn.f32 	%f1393, %f1969, %f1969, %f1392;
-	fma.rn.f32 	%f1394, %f1970, %f1970, %f1393;
-	rcp.rn.f32 	%f1395, %f1394;
-	mul.f32 	%f1396, %f1967, %f1395;
-	mul.f32 	%f1397, %f1968, %f1395;
-	mul.f32 	%f1398, %f1969, %f1395;
-	mul.f32 	%f1399, %f1970, %f1395;
-	mul.f32 	%f1400, %f1967, %f1396;
-	mul.f32 	%f1401, %f1968, %f1397;
-	mul.f32 	%f1402, %f1969, %f1398;
-	mul.f32 	%f1403, %f1967, %f1397;
-	mul.f32 	%f1404, %f1969, %f1399;
-	mul.f32 	%f1405, %f1967, %f1398;
-	mul.f32 	%f1406, %f1968, %f1399;
-	mul.f32 	%f1407, %f1968, %f1398;
-	mul.f32 	%f1408, %f1967, %f1399;
-	sub.f32 	%f1409, %f1400, %f1401;
-	sub.f32 	%f1410, %f1409, %f1402;
-	fma.rn.f32 	%f1411, %f1970, %f1399, %f1410;
-	sub.f32 	%f1412, %f1403, %f1404;
-	add.f32 	%f1413, %f1412, %f1412;
-	add.f32 	%f1414, %f1405, %f1406;
-	add.f32 	%f1415, %f1414, %f1414;
-	add.f32 	%f1416, %f1403, %f1404;
-	add.f32 	%f1417, %f1416, %f1416;
-	sub.f32 	%f1418, %f1401, %f1400;
-	sub.f32 	%f1419, %f1418, %f1402;
-	fma.rn.f32 	%f1420, %f1970, %f1399, %f1419;
-	sub.f32 	%f1421, %f1407, %f1408;
-	add.f32 	%f1422, %f1421, %f1421;
-	sub.f32 	%f1423, %f1405, %f1406;
-	add.f32 	%f1424, %f1423, %f1423;
-	add.f32 	%f1425, %f1407, %f1408;
-	add.f32 	%f1426, %f1425, %f1425;
-	neg.f32 	%f1427, %f1400;
-	sub.f32 	%f1428, %f1427, %f1401;
-	add.f32 	%f1429, %f1402, %f1428;
-	fma.rn.f32 	%f1430, %f1970, %f1399, %f1429;
-	mul.f32 	%f1431, %f1961, %f1411;
-	fma.rn.f32 	%f1432, %f1964, %f1413, %f1431;
-	fma.rn.f32 	%f1433, %f1966, %f1415, %f1432;
-	sub.f32 	%f1985, %f1971, %f1433;
-	mul.f32 	%f1434, %f1964, %f1420;
-	fma.rn.f32 	%f1435, %f1961, %f1417, %f1434;
-	fma.rn.f32 	%f1436, %f1966, %f1422, %f1435;
-	sub.f32 	%f1981, %f1972, %f1436;
-	mul.f32 	%f1437, %f1964, %f1426;
-	fma.rn.f32 	%f1438, %f1961, %f1424, %f1437;
-	fma.rn.f32 	%f1439, %f1966, %f1430, %f1438;
-	sub.f32 	%f1977, %f1973, %f1439;
-	mul.f32 	%f1440, %f1960, %f1411;
-	fma.rn.f32 	%f1441, %f1963, %f1413, %f1440;
-	fma.rn.f32 	%f1984, %f1965, %f1415, %f1441;
-	mul.f32 	%f1442, %f1963, %f1420;
-	fma.rn.f32 	%f1443, %f1960, %f1417, %f1442;
-	fma.rn.f32 	%f1980, %f1965, %f1422, %f1443;
-	mul.f32 	%f1444, %f1963, %f1426;
-	fma.rn.f32 	%f1445, %f1960, %f1424, %f1444;
-	fma.rn.f32 	%f1976, %f1965, %f1430, %f1445;
-	mul.f32 	%f1446, %f1959, %f1411;
-	fma.rn.f32 	%f1983, %f1962, %f1413, %f1446;
-	mul.f32 	%f1447, %f1962, %f1420;
-	fma.rn.f32 	%f1979, %f1959, %f1417, %f1447;
-	mul.f32 	%f1448, %f1962, %f1426;
-	fma.rn.f32 	%f1975, %f1959, %f1424, %f1448;
-	mul.f32 	%f1982, %f1958, %f1411;
-	mul.f32 	%f1978, %f1958, %f1417;
-	mul.f32 	%f1974, %f1958, %f1424;
-	bra.uni 	BB8_73;
-
-BB8_63:
-	setp.ne.s32	%p46, %r345, 1;
-	mov.f32 	%f1975, %f1974;
-	mov.f32 	%f1977, %f1974;
-	mov.f32 	%f1978, %f1974;
-	mov.f32 	%f1979, %f1976;
-	mov.f32 	%f1980, %f1974;
-	mov.f32 	%f1981, %f1974;
-	mov.f32 	%f1982, %f1976;
-	mov.f32 	%f1983, %f1974;
-	mov.f32 	%f1984, %f1974;
-	mov.f32 	%f1985, %f1974;
-	@%p46 bra 	BB8_73;
-
-	// inline asm
-	call (%rd316), _optix_get_static_transform_from_handle, (%rd314);
-	// inline asm
-	add.s64 	%rd664, %rd316, 16;
-
-BB8_66:
-	// inline asm
-	cvta.to.global.u64 %rd320, %rd664;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r347,%r348,%r349,%r350}, [%rd320];
-	// inline asm
-	mov.b32 	 %f1982, %r347;
-	mov.b32 	 %f1983, %r348;
-	mov.b32 	 %f1984, %r349;
-	mov.b32 	 %f1985, %r350;
-	add.s64 	%rd324, %rd664, 16;
-	// inline asm
-	cvta.to.global.u64 %rd323, %rd324;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r351,%r352,%r353,%r354}, [%rd323];
-	// inline asm
-	mov.b32 	 %f1978, %r351;
-	mov.b32 	 %f1979, %r352;
-	mov.b32 	 %f1980, %r353;
-	mov.b32 	 %f1981, %r354;
-	add.s64 	%rd327, %rd664, 32;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r497,%r498,%r499,%r500}, [%rd420];
+	// end inline asm
+	mov.b32 	%f1496, %r497;
+	mov.b32 	%f1497, %r498;
+	mov.b32 	%f1498, %r499;
+	mov.b32 	%f1499, %r500;
+	mul.f32 	%f1500, %f466, %f1496;
+	mul.f32 	%f1501, %f466, %f1497;
+	mul.f32 	%f1502, %f466, %f1498;
+	mul.f32 	%f1503, %f466, %f1499;
+	fma.rn.f32 	%f2014, %f1479, %f2014, %f1500;
+	fma.rn.f32 	%f2015, %f1479, %f2015, %f1501;
+	fma.rn.f32 	%f2016, %f1479, %f2016, %f1502;
+	fma.rn.f32 	%f2017, %f1479, %f2017, %f1503;
+	bra.uni 	$L__BB8_74;
+
+$L__BB8_63:
+	mov.f32 	%f2014, 0f00000000;
+	mov.f32 	%f2016, 0f3F800000;
+	setp.eq.s32 	%p48, %r353, 4;
+	@%p48 bra 	$L__BB8_66;
+
+	setp.ne.s32 	%p49, %r353, 1;
+	mov.f32 	%f2015, %f2014;
+	mov.f32 	%f2017, %f2014;
+	mov.f32 	%f2018, %f2014;
+	mov.f32 	%f2019, %f2016;
+	mov.f32 	%f2020, %f2014;
+	mov.f32 	%f2021, %f2014;
+	mov.f32 	%f2022, %f2016;
+	mov.f32 	%f2023, %f2014;
+	mov.f32 	%f2024, %f2014;
+	mov.f32 	%f2025, %f2014;
+	@%p49 bra 	$L__BB8_74;
+
+	// begin inline asm
+	call (%rd308), _optix_get_static_transform_from_handle, (%rd306);
+	// end inline asm
+	add.s64 	%rd651, %rd308, 16;
+	bra.uni 	$L__BB8_67;
+
+$L__BB8_69:
+	// begin inline asm
+	call (%rd321), _optix_get_srt_motion_transform_from_handle, (%rd306);
+	// end inline asm
+	// begin inline asm
+	cvta.to.global.u64 %rd323, %rd321;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r367,%r368,%r369,%r370}, [%rd323];
+	// end inline asm
+	add.s64 	%rd327, %rd321, 16;
+	// begin inline asm
 	cvta.to.global.u64 %rd326, %rd327;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r355,%r356,%r357,%r358}, [%rd326];
-	// inline asm
-	mov.b32 	 %f1974, %r355;
-	mov.b32 	 %f1975, %r356;
-	mov.b32 	 %f1976, %r357;
-	mov.b32 	 %f1977, %r358;
-
-BB8_73:
-	add.s32 	%r18, %r648, 1;
-	setp.eq.s32	%p50, %r18, %r30;
-	@%p50 bra 	BB8_74;
-	bra.uni 	BB8_75;
-
-BB8_74:
-	mov.f32 	%f1957, %f1974;
-	mov.f32 	%f1956, %f1975;
-	mov.f32 	%f1955, %f1976;
-	mov.f32 	%f1954, %f1977;
-	mov.f32 	%f1953, %f1978;
-	mov.f32 	%f1952, %f1979;
-	mov.f32 	%f1951, %f1980;
-	mov.f32 	%f1950, %f1981;
-	mov.f32 	%f1949, %f1982;
-	mov.f32 	%f1948, %f1983;
-	mov.f32 	%f1947, %f1984;
-	mov.f32 	%f1946, %f1985;
-	bra.uni 	BB8_76;
-
-BB8_75:
-	mul.f32 	%f1486, %f1953, %f1983;
-	fma.rn.f32 	%f1487, %f1949, %f1982, %f1486;
-	fma.rn.f32 	%f476, %f1957, %f1984, %f1487;
-	mul.f32 	%f1488, %f1952, %f1983;
-	fma.rn.f32 	%f1489, %f1948, %f1982, %f1488;
-	fma.rn.f32 	%f477, %f1956, %f1984, %f1489;
-	mul.f32 	%f1490, %f1951, %f1983;
-	fma.rn.f32 	%f1491, %f1947, %f1982, %f1490;
-	fma.rn.f32 	%f478, %f1955, %f1984, %f1491;
-	mul.f32 	%f1492, %f1950, %f1983;
-	fma.rn.f32 	%f1493, %f1946, %f1982, %f1492;
-	fma.rn.f32 	%f1494, %f1954, %f1984, %f1493;
-	add.f32 	%f479, %f1985, %f1494;
-	mul.f32 	%f1495, %f1953, %f1979;
-	fma.rn.f32 	%f1496, %f1949, %f1978, %f1495;
-	fma.rn.f32 	%f480, %f1957, %f1980, %f1496;
-	mul.f32 	%f1497, %f1952, %f1979;
-	fma.rn.f32 	%f1498, %f1948, %f1978, %f1497;
-	fma.rn.f32 	%f481, %f1956, %f1980, %f1498;
-	mul.f32 	%f1499, %f1951, %f1979;
-	fma.rn.f32 	%f1500, %f1947, %f1978, %f1499;
-	fma.rn.f32 	%f482, %f1955, %f1980, %f1500;
-	mul.f32 	%f1501, %f1950, %f1979;
-	fma.rn.f32 	%f1502, %f1946, %f1978, %f1501;
-	fma.rn.f32 	%f1503, %f1954, %f1980, %f1502;
-	add.f32 	%f483, %f1981, %f1503;
-	mul.f32 	%f1504, %f1953, %f1975;
-	fma.rn.f32 	%f1505, %f1949, %f1974, %f1504;
-	fma.rn.f32 	%f1957, %f1957, %f1976, %f1505;
-	mul.f32 	%f1506, %f1952, %f1975;
-	fma.rn.f32 	%f1507, %f1948, %f1974, %f1506;
-	fma.rn.f32 	%f1956, %f1956, %f1976, %f1507;
-	mul.f32 	%f1508, %f1951, %f1975;
-	fma.rn.f32 	%f1509, %f1947, %f1974, %f1508;
-	fma.rn.f32 	%f1955, %f1955, %f1976, %f1509;
-	mul.f32 	%f1510, %f1950, %f1975;
-	fma.rn.f32 	%f1511, %f1946, %f1974, %f1510;
-	fma.rn.f32 	%f1512, %f1954, %f1976, %f1511;
-	add.f32 	%f1954, %f1977, %f1512;
-	mov.f32 	%f1953, %f480;
-	mov.f32 	%f1952, %f481;
-	mov.f32 	%f1951, %f482;
-	mov.f32 	%f1950, %f483;
-	mov.f32 	%f1949, %f476;
-	mov.f32 	%f1948, %f477;
-	mov.f32 	%f1947, %f478;
-	mov.f32 	%f1946, %f479;
-
-BB8_76:
-	add.s32 	%r648, %r18, -2;
-	setp.gt.s32	%p51, %r648, -1;
-	@%p51 bra 	BB8_61;
-
-BB8_77:
-	mov.u32 	%r649, 0;
-	mov.f32 	%f2010, %f2011;
-	mov.f32 	%f2015, %f2011;
-	mov.f32 	%f2014, %f2012;
-	mov.f32 	%f2013, %f2011;
-	mov.f32 	%f2018, %f2011;
-	mov.f32 	%f2017, %f2011;
-	mov.f32 	%f2016, %f2012;
-	@%p2 bra 	BB8_95;
-
-BB8_78:
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r371,%r372,%r373,%r374}, [%rd326];
+	// end inline asm
+	add.s64 	%rd330, %rd321, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd329, %rd330;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r375,%r376,%r377,%r378}, [%rd329];
+	// end inline asm
+	add.s64 	%rd333, %rd321, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd332, %rd333;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r379,%r380,%r381,%r382}, [%rd332];
+	// end inline asm
+	add.s64 	%rd336, %rd321, 64;
+	// begin inline asm
+	cvta.to.global.u64 %rd335, %rd336;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r383,%r384,%r385,%r386}, [%rd335];
+	// end inline asm
+	add.s64 	%rd339, %rd321, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd338, %rd339;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r387,%r388,%r389,%r390}, [%rd338];
+	// end inline asm
+	add.s64 	%rd342, %rd321, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd341, %rd342;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r391,%r392,%r393,%r394}, [%rd341];
+	// end inline asm
+	add.s64 	%rd345, %rd321, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd344, %rd345;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r395,%r396,%r397,%r398}, [%rd344];
+	// end inline asm
+	add.s64 	%rd348, %rd321, 128;
+	// begin inline asm
+	cvta.to.global.u64 %rd347, %rd348;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r399,%r400,%r401,%r402}, [%rd347];
+	// end inline asm
+	add.s64 	%rd351, %rd321, 144;
+	// begin inline asm
+	cvta.to.global.u64 %rd350, %rd351;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r403,%r404,%r405,%r406}, [%rd350];
+	// end inline asm
+	mov.b32 	%f1354, %r370;
+	mov.b32 	%f1355, %r371;
+	and.b32  	%r423, %r369, 65535;
+	add.s32 	%r424, %r423, -1;
+	cvt.rn.f32.s32 	%f1356, %r424;
+	sub.f32 	%f1357, %f1339, %f1354;
+	mul.f32 	%f1358, %f1357, %f1356;
+	sub.f32 	%f1359, %f1355, %f1354;
+	div.rn.f32 	%f1360, %f1358, %f1359;
+	min.f32 	%f1361, %f1356, %f1360;
+	mov.f32 	%f1362, 0f00000000;
+	max.f32 	%f1363, %f1362, %f1361;
+	cvt.rmi.f32.f32 	%f1364, %f1363;
+	sub.f32 	%f405, %f1363, %f1364;
+	cvt.rzi.s32.f32 	%r425, %f1364;
+	mul.wide.s32 	%rd365, %r425, 64;
+	add.s64 	%rd354, %rd330, %rd365;
+	// begin inline asm
+	cvta.to.global.u64 %rd353, %rd354;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r407,%r408,%r409,%r410}, [%rd353];
+	// end inline asm
+	mov.b32 	%f1998, %r407;
+	mov.b32 	%f1999, %r408;
+	mov.b32 	%f2000, %r409;
+	mov.b32 	%f2001, %r410;
+	add.s64 	%rd357, %rd354, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd356, %rd357;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r411,%r412,%r413,%r414}, [%rd356];
+	// end inline asm
+	mov.b32 	%f2002, %r411;
+	mov.b32 	%f2003, %r412;
+	mov.b32 	%f2004, %r413;
+	mov.b32 	%f2005, %r414;
+	add.s64 	%rd360, %rd354, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd359, %rd360;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r415,%r416,%r417,%r418}, [%rd359];
+	// end inline asm
+	mov.b32 	%f2006, %r415;
+	mov.b32 	%f2007, %r416;
+	mov.b32 	%f2008, %r417;
+	mov.b32 	%f2009, %r418;
+	add.s64 	%rd363, %rd354, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd362, %rd363;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r419,%r420,%r421,%r422}, [%rd362];
+	// end inline asm
+	mov.b32 	%f2010, %r419;
+	mov.b32 	%f2011, %r420;
+	mov.b32 	%f2012, %r421;
+	mov.b32 	%f2013, %r422;
+	setp.leu.f32 	%p51, %f405, 0f00000000;
+	@%p51 bra 	$L__BB8_71;
+
+	mov.f32 	%f1365, 0f3F800000;
+	sub.f32 	%f1366, %f1365, %f405;
+	add.s64 	%rd367, %rd354, 64;
+	// begin inline asm
+	cvta.to.global.u64 %rd366, %rd367;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r426,%r427,%r428,%r429}, [%rd366];
+	// end inline asm
+	mov.b32 	%f1367, %r426;
+	mov.b32 	%f1368, %r427;
+	mov.b32 	%f1369, %r428;
+	mov.b32 	%f1370, %r429;
+	mul.f32 	%f1371, %f405, %f1367;
+	mul.f32 	%f1372, %f405, %f1368;
+	mul.f32 	%f1373, %f405, %f1369;
+	mul.f32 	%f1374, %f405, %f1370;
+	fma.rn.f32 	%f1998, %f1366, %f1998, %f1371;
+	fma.rn.f32 	%f1999, %f1366, %f1999, %f1372;
+	fma.rn.f32 	%f2000, %f1366, %f2000, %f1373;
+	fma.rn.f32 	%f2001, %f1366, %f2001, %f1374;
+	add.s64 	%rd370, %rd354, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd369, %rd370;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r430,%r431,%r432,%r433}, [%rd369];
+	// end inline asm
+	mov.b32 	%f1375, %r430;
+	mov.b32 	%f1376, %r431;
+	mov.b32 	%f1377, %r432;
+	mov.b32 	%f1378, %r433;
+	mul.f32 	%f1379, %f405, %f1375;
+	mul.f32 	%f1380, %f405, %f1376;
+	mul.f32 	%f1381, %f405, %f1377;
+	mul.f32 	%f1382, %f405, %f1378;
+	fma.rn.f32 	%f2002, %f1366, %f2002, %f1379;
+	fma.rn.f32 	%f2003, %f1366, %f2003, %f1380;
+	fma.rn.f32 	%f2004, %f1366, %f2004, %f1381;
+	fma.rn.f32 	%f2005, %f1366, %f2005, %f1382;
+	add.s64 	%rd373, %rd354, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd372, %rd373;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r434,%r435,%r436,%r437}, [%rd372];
+	// end inline asm
+	mov.b32 	%f1383, %r434;
+	mov.b32 	%f1384, %r435;
+	mov.b32 	%f1385, %r436;
+	mov.b32 	%f1386, %r437;
+	mul.f32 	%f1387, %f405, %f1383;
+	mul.f32 	%f1388, %f405, %f1384;
+	mul.f32 	%f1389, %f405, %f1385;
+	mul.f32 	%f1390, %f405, %f1386;
+	fma.rn.f32 	%f2006, %f1366, %f2006, %f1387;
+	fma.rn.f32 	%f1391, %f1366, %f2007, %f1388;
+	fma.rn.f32 	%f1392, %f1366, %f2008, %f1389;
+	fma.rn.f32 	%f1393, %f1366, %f2009, %f1390;
+	add.s64 	%rd376, %rd354, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd375, %rd376;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r438,%r439,%r440,%r441}, [%rd375];
+	// end inline asm
+	mov.b32 	%f1394, %r438;
+	mov.b32 	%f1395, %r439;
+	mov.b32 	%f1396, %r440;
+	mov.b32 	%f1397, %r441;
+	mul.f32 	%f1398, %f405, %f1394;
+	mul.f32 	%f1399, %f405, %f1395;
+	mul.f32 	%f1400, %f405, %f1396;
+	mul.f32 	%f1401, %f405, %f1397;
+	fma.rn.f32 	%f1402, %f1366, %f2010, %f1398;
+	fma.rn.f32 	%f2011, %f1366, %f2011, %f1399;
+	fma.rn.f32 	%f2012, %f1366, %f2012, %f1400;
+	fma.rn.f32 	%f2013, %f1366, %f2013, %f1401;
+	mul.f32 	%f1403, %f1392, %f1392;
+	fma.rn.f32 	%f1404, %f1391, %f1391, %f1403;
+	fma.rn.f32 	%f1405, %f1393, %f1393, %f1404;
+	fma.rn.f32 	%f1406, %f1402, %f1402, %f1405;
+	sqrt.rn.f32 	%f1407, %f1406;
+	rcp.rn.f32 	%f1408, %f1407;
+	mul.f32 	%f2007, %f1391, %f1408;
+	mul.f32 	%f2008, %f1392, %f1408;
+	mul.f32 	%f2009, %f1393, %f1408;
+	mul.f32 	%f2010, %f1408, %f1402;
+
+$L__BB8_71:
+	mul.f32 	%f1409, %f2008, %f2008;
+	fma.rn.f32 	%f1410, %f2007, %f2007, %f1409;
+	fma.rn.f32 	%f1411, %f2009, %f2009, %f1410;
+	fma.rn.f32 	%f1412, %f2010, %f2010, %f1411;
+	rcp.rn.f32 	%f1413, %f1412;
+	mul.f32 	%f1414, %f2007, %f1413;
+	mul.f32 	%f1415, %f2008, %f1413;
+	mul.f32 	%f1416, %f2009, %f1413;
+	mul.f32 	%f1417, %f2010, %f1413;
+	mul.f32 	%f1418, %f2007, %f1414;
+	mul.f32 	%f1419, %f2008, %f1415;
+	mul.f32 	%f1420, %f2009, %f1416;
+	mul.f32 	%f1421, %f2007, %f1415;
+	mul.f32 	%f1422, %f2009, %f1417;
+	mul.f32 	%f1423, %f2007, %f1416;
+	mul.f32 	%f1424, %f2008, %f1417;
+	mul.f32 	%f1425, %f2008, %f1416;
+	mul.f32 	%f1426, %f2007, %f1417;
+	sub.f32 	%f1427, %f1418, %f1419;
+	sub.f32 	%f1428, %f1427, %f1420;
+	fma.rn.f32 	%f1429, %f2010, %f1417, %f1428;
+	sub.f32 	%f1430, %f1421, %f1422;
+	add.f32 	%f1431, %f1430, %f1430;
+	add.f32 	%f1432, %f1423, %f1424;
+	add.f32 	%f1433, %f1432, %f1432;
+	add.f32 	%f1434, %f1421, %f1422;
+	add.f32 	%f1435, %f1434, %f1434;
+	sub.f32 	%f1436, %f1419, %f1418;
+	sub.f32 	%f1437, %f1436, %f1420;
+	fma.rn.f32 	%f1438, %f2010, %f1417, %f1437;
+	sub.f32 	%f1439, %f1425, %f1426;
+	add.f32 	%f1440, %f1439, %f1439;
+	sub.f32 	%f1441, %f1423, %f1424;
+	add.f32 	%f1442, %f1441, %f1441;
+	add.f32 	%f1443, %f1425, %f1426;
+	add.f32 	%f1444, %f1443, %f1443;
+	neg.f32 	%f1445, %f1418;
+	sub.f32 	%f1446, %f1445, %f1419;
+	add.f32 	%f1447, %f1420, %f1446;
+	fma.rn.f32 	%f1448, %f2010, %f1417, %f1447;
+	mul.f32 	%f1449, %f2001, %f1429;
+	fma.rn.f32 	%f1450, %f2004, %f1431, %f1449;
+	fma.rn.f32 	%f1451, %f2006, %f1433, %f1450;
+	sub.f32 	%f2025, %f2011, %f1451;
+	mul.f32 	%f1452, %f2004, %f1438;
+	fma.rn.f32 	%f1453, %f2001, %f1435, %f1452;
+	fma.rn.f32 	%f1454, %f2006, %f1440, %f1453;
+	sub.f32 	%f2021, %f2012, %f1454;
+	mul.f32 	%f1455, %f2004, %f1444;
+	fma.rn.f32 	%f1456, %f2001, %f1442, %f1455;
+	fma.rn.f32 	%f1457, %f2006, %f1448, %f1456;
+	sub.f32 	%f2017, %f2013, %f1457;
+	mul.f32 	%f1458, %f2000, %f1429;
+	fma.rn.f32 	%f1459, %f2003, %f1431, %f1458;
+	fma.rn.f32 	%f2024, %f2005, %f1433, %f1459;
+	mul.f32 	%f1460, %f2003, %f1438;
+	fma.rn.f32 	%f1461, %f2000, %f1435, %f1460;
+	fma.rn.f32 	%f2020, %f2005, %f1440, %f1461;
+	mul.f32 	%f1462, %f2003, %f1444;
+	fma.rn.f32 	%f1463, %f2000, %f1442, %f1462;
+	fma.rn.f32 	%f2016, %f2005, %f1448, %f1463;
+	mul.f32 	%f1464, %f1999, %f1429;
+	fma.rn.f32 	%f2023, %f2002, %f1431, %f1464;
+	mul.f32 	%f1465, %f2002, %f1438;
+	fma.rn.f32 	%f2019, %f1999, %f1435, %f1465;
+	mul.f32 	%f1466, %f2002, %f1444;
+	fma.rn.f32 	%f2015, %f1999, %f1442, %f1466;
+	mul.f32 	%f2022, %f1998, %f1429;
+	mul.f32 	%f2018, %f1998, %f1435;
+	mul.f32 	%f2014, %f1998, %f1442;
+	bra.uni 	$L__BB8_74;
+
+$L__BB8_66:
+	// begin inline asm
+	call (%rd651), _optix_get_instance_transform_from_handle, (%rd306);
+	// end inline asm
+
+$L__BB8_67:
+	// begin inline asm
+	cvta.to.global.u64 %rd312, %rd651;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r355,%r356,%r357,%r358}, [%rd312];
+	// end inline asm
+	mov.b32 	%f2022, %r355;
+	mov.b32 	%f2023, %r356;
+	mov.b32 	%f2024, %r357;
+	mov.b32 	%f2025, %r358;
+	add.s64 	%rd316, %rd651, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd315, %rd316;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r359,%r360,%r361,%r362}, [%rd315];
+	// end inline asm
+	mov.b32 	%f2018, %r359;
+	mov.b32 	%f2019, %r360;
+	mov.b32 	%f2020, %r361;
+	mov.b32 	%f2021, %r362;
+	add.s64 	%rd319, %rd651, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd318, %rd319;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r363,%r364,%r365,%r366}, [%rd318];
+	// end inline asm
+	mov.b32 	%f2014, %r363;
+	mov.b32 	%f2015, %r364;
+	mov.b32 	%f2016, %r365;
+	mov.b32 	%f2017, %r366;
+
+$L__BB8_74:
+	setp.eq.s32 	%p53, %r658, 1;
+	@%p53 bra 	$L__BB8_76;
+
+	mul.f32 	%f1504, %f1993, %f2023;
+	fma.rn.f32 	%f1505, %f1989, %f2022, %f1504;
+	fma.rn.f32 	%f503, %f1997, %f2024, %f1505;
+	mul.f32 	%f1506, %f1992, %f2023;
+	fma.rn.f32 	%f1507, %f1988, %f2022, %f1506;
+	fma.rn.f32 	%f504, %f1996, %f2024, %f1507;
+	mul.f32 	%f1508, %f1991, %f2023;
+	fma.rn.f32 	%f1509, %f1987, %f2022, %f1508;
+	fma.rn.f32 	%f505, %f1995, %f2024, %f1509;
+	mul.f32 	%f1510, %f1990, %f2023;
+	fma.rn.f32 	%f1511, %f1986, %f2022, %f1510;
+	fma.rn.f32 	%f1512, %f1994, %f2024, %f1511;
+	add.f32 	%f2025, %f2025, %f1512;
+	mul.f32 	%f1513, %f1993, %f2019;
+	fma.rn.f32 	%f1514, %f1989, %f2018, %f1513;
+	fma.rn.f32 	%f507, %f1997, %f2020, %f1514;
+	mul.f32 	%f1515, %f1992, %f2019;
+	fma.rn.f32 	%f1516, %f1988, %f2018, %f1515;
+	fma.rn.f32 	%f508, %f1996, %f2020, %f1516;
+	mul.f32 	%f1517, %f1991, %f2019;
+	fma.rn.f32 	%f1518, %f1987, %f2018, %f1517;
+	fma.rn.f32 	%f509, %f1995, %f2020, %f1518;
+	mul.f32 	%f1519, %f1990, %f2019;
+	fma.rn.f32 	%f1520, %f1986, %f2018, %f1519;
+	fma.rn.f32 	%f1521, %f1994, %f2020, %f1520;
+	add.f32 	%f2021, %f2021, %f1521;
+	mul.f32 	%f1522, %f1993, %f2015;
+	fma.rn.f32 	%f1523, %f1989, %f2014, %f1522;
+	fma.rn.f32 	%f511, %f1997, %f2016, %f1523;
+	mul.f32 	%f1524, %f1992, %f2015;
+	fma.rn.f32 	%f1525, %f1988, %f2014, %f1524;
+	fma.rn.f32 	%f512, %f1996, %f2016, %f1525;
+	mul.f32 	%f1526, %f1991, %f2015;
+	fma.rn.f32 	%f1527, %f1987, %f2014, %f1526;
+	fma.rn.f32 	%f513, %f1995, %f2016, %f1527;
+	mul.f32 	%f1528, %f1990, %f2015;
+	fma.rn.f32 	%f1529, %f1986, %f2014, %f1528;
+	fma.rn.f32 	%f1530, %f1994, %f2016, %f1529;
+	add.f32 	%f2017, %f2017, %f1530;
+	mov.f32 	%f2014, %f511;
+	mov.f32 	%f2015, %f512;
+	mov.f32 	%f2016, %f513;
+	mov.f32 	%f2018, %f507;
+	mov.f32 	%f2019, %f508;
+	mov.f32 	%f2020, %f509;
+	mov.f32 	%f2022, %f503;
+	mov.f32 	%f2023, %f504;
+	mov.f32 	%f2024, %f505;
+
+$L__BB8_76:
+	add.s32 	%r658, %r658, -1;
+	add.s32 	%r657, %r657, -1;
+	setp.gt.s32 	%p54, %r657, 1;
+	mov.f32 	%f1986, %f2025;
+	mov.f32 	%f1987, %f2024;
+	mov.f32 	%f1988, %f2023;
+	mov.f32 	%f1989, %f2022;
+	mov.f32 	%f1990, %f2021;
+	mov.f32 	%f1991, %f2020;
+	mov.f32 	%f1992, %f2019;
+	mov.f32 	%f1993, %f2018;
+	mov.f32 	%f1994, %f2017;
+	mov.f32 	%f1995, %f2016;
+	mov.f32 	%f1996, %f2015;
+	mov.f32 	%f1997, %f2014;
+	@%p54 bra 	$L__BB8_62;
+
+$L__BB8_77:
+	// begin inline asm
+	call (%r501), _optix_get_transform_list_size, ();
+	// end inline asm
+	setp.eq.s32 	%p55, %r501, 0;
+	mov.f32 	%f2086, %f2085;
+	mov.f32 	%f2081, %f2085;
+	mov.f32 	%f2082, %f2084;
+	mov.f32 	%f2083, %f2085;
+	mov.f32 	%f2078, %f2085;
+	mov.f32 	%f2079, %f2085;
+	mov.f32 	%f2080, %f2084;
+	@%p55 bra 	$L__BB8_96;
+
+	// begin inline asm
+	call (%r502), _optix_get_transform_list_size, ();
+	// end inline asm
+	// begin inline asm
+	call (%f1540), _optix_get_ray_time, ();
+	// end inline asm
+	setp.eq.s32 	%p56, %r502, 0;
+	@%p56 bra 	$L__BB8_96;
+
+	mov.u32 	%r659, 0;
+
+$L__BB8_80:
 	.pragma "nounroll";
-	// inline asm
-	call (%rd435), _optix_get_transform_list_handle, (%r649);
-	// inline asm
-	// inline asm
-	call (%r495), _optix_get_transform_type_from_handle, (%rd435);
-	// inline asm
-	and.b32  	%r496, %r495, -2;
-	setp.eq.s32	%p53, %r496, 2;
-	@%p53 bra 	BB8_84;
-	bra.uni 	BB8_79;
-
-BB8_84:
-	setp.eq.s32	%p56, %r495, 2;
-	@%p56 bra 	BB8_88;
-	bra.uni 	BB8_85;
-
-BB8_88:
-	// inline asm
-	call (%rd509), _optix_get_matrix_motion_transform_from_handle, (%rd435);
-	// inline asm
-	// inline asm
-	cvta.to.global.u64 %rd511, %rd509;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r584,%r585,%r586,%r587}, [%rd511];
-	// inline asm
-	mov.b32	{%rs17, %rs18}, %r586;
-	add.s64 	%rd515, %rd509, 16;
-	// inline asm
+	// begin inline asm
+	call (%rd425), _optix_get_transform_list_handle, (%r659);
+	// end inline asm
+	// begin inline asm
+	call (%r505), _optix_get_transform_type_from_handle, (%rd425);
+	// end inline asm
+	or.b32  	%r506, %r505, 1;
+	setp.eq.s32 	%p57, %r506, 3;
+	@%p57 bra 	$L__BB8_86;
+	bra.uni 	$L__BB8_81;
+
+$L__BB8_86:
+	setp.eq.s32 	%p60, %r505, 2;
+	@%p60 bra 	$L__BB8_90;
+	bra.uni 	$L__BB8_87;
+
+$L__BB8_90:
+	// begin inline asm
+	call (%rd497), _optix_get_matrix_motion_transform_from_handle, (%rd425);
+	// end inline asm
+	// begin inline asm
+	cvta.to.global.u64 %rd499, %rd497;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r594,%r595,%r596,%r597}, [%rd499];
+	// end inline asm
+	add.s64 	%rd503, %rd497, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd502, %rd503;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r598,%r599,%r600,%r601}, [%rd502];
+	// end inline asm
+	add.s64 	%rd506, %rd497, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd505, %rd506;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r602,%r603,%r604,%r605}, [%rd505];
+	// end inline asm
+	add.s64 	%rd509, %rd497, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd508, %rd509;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r606,%r607,%r608,%r609}, [%rd508];
+	// end inline asm
+	add.s64 	%rd512, %rd497, 64;
+	// begin inline asm
+	cvta.to.global.u64 %rd511, %rd512;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r610,%r611,%r612,%r613}, [%rd511];
+	// end inline asm
+	add.s64 	%rd515, %rd497, 80;
+	// begin inline asm
 	cvta.to.global.u64 %rd514, %rd515;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r588,%r589,%r590,%r591}, [%rd514];
-	// inline asm
-	add.s64 	%rd518, %rd509, 32;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r614,%r615,%r616,%r617}, [%rd514];
+	// end inline asm
+	add.s64 	%rd518, %rd497, 96;
+	// begin inline asm
 	cvta.to.global.u64 %rd517, %rd518;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r592,%r593,%r594,%r595}, [%rd517];
-	// inline asm
-	add.s64 	%rd521, %rd509, 48;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r618,%r619,%r620,%r621}, [%rd517];
+	// end inline asm
+	add.s64 	%rd521, %rd497, 112;
+	// begin inline asm
 	cvta.to.global.u64 %rd520, %rd521;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r596,%r597,%r598,%r599}, [%rd520];
-	// inline asm
-	add.s64 	%rd524, %rd509, 64;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r622,%r623,%r624,%r625}, [%rd520];
+	// end inline asm
+	mov.b32 	%f1644, %r597;
+	mov.b32 	%f1645, %r598;
+	and.b32  	%r638, %r596, 65535;
+	add.s32 	%r639, %r638, -1;
+	cvt.rn.f32.s32 	%f1646, %r639;
+	sub.f32 	%f1647, %f1540, %f1644;
+	mul.f32 	%f1648, %f1647, %f1646;
+	sub.f32 	%f1649, %f1645, %f1644;
+	div.rn.f32 	%f1650, %f1648, %f1649;
+	min.f32 	%f1651, %f1646, %f1650;
+	mov.f32 	%f1652, 0f00000000;
+	max.f32 	%f1653, %f1652, %f1651;
+	cvt.rmi.f32.f32 	%f1654, %f1653;
+	sub.f32 	%f598, %f1653, %f1654;
+	cvt.rzi.s32.f32 	%r640, %f1654;
+	cvt.s64.s32 	%rd38, %r640;
+	mul.wide.s32 	%rd532, %r640, 48;
+	add.s64 	%rd524, %rd506, %rd532;
+	// begin inline asm
 	cvta.to.global.u64 %rd523, %rd524;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r600,%r601,%r602,%r603}, [%rd523];
-	// inline asm
-	add.s64 	%rd527, %rd509, 80;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r626,%r627,%r628,%r629}, [%rd523];
+	// end inline asm
+	mov.b32 	%f2075, %r626;
+	mov.b32 	%f2076, %r627;
+	mov.b32 	%f2077, %r628;
+	add.s64 	%rd527, %rd524, 16;
+	// begin inline asm
 	cvta.to.global.u64 %rd526, %rd527;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r604,%r605,%r606,%r607}, [%rd526];
-	// inline asm
-	add.s64 	%rd530, %rd509, 96;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r630,%r631,%r632,%r633}, [%rd526];
+	// end inline asm
+	mov.b32 	%f2072, %r630;
+	mov.b32 	%f2073, %r631;
+	mov.b32 	%f2074, %r632;
+	add.s64 	%rd530, %rd524, 32;
+	// begin inline asm
 	cvta.to.global.u64 %rd529, %rd530;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r608,%r609,%r610,%r611}, [%rd529];
-	// inline asm
-	add.s64 	%rd533, %rd509, 112;
-	// inline asm
-	cvta.to.global.u64 %rd532, %rd533;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r612,%r613,%r614,%r615}, [%rd532];
-	// inline asm
-	mov.b32 	 %f1624, %r587;
-	mov.b32 	 %f1625, %r588;
-	cvt.u32.u16	%r628, %rs17;
-	add.s32 	%r629, %r628, -1;
-	cvt.rn.f32.s32	%f1626, %r629;
-	sub.f32 	%f1627, %f957, %f1624;
-	mul.f32 	%f1628, %f1627, %f1626;
-	sub.f32 	%f1629, %f1625, %f1624;
-	div.rn.f32 	%f1630, %f1628, %f1629;
-	min.f32 	%f1631, %f1626, %f1630;
-	mov.f32 	%f1632, 0f00000000;
-	max.f32 	%f1633, %f1632, %f1631;
-	cvt.rmi.f32.f32	%f1634, %f1633;
-	cvt.rzi.s32.f32	%r630, %f1634;
-	cvt.s64.s32	%rd42, %r630;
-	mul.wide.s32 	%rd544, %r630, 48;
-	add.s64 	%rd536, %rd518, %rd544;
-	// inline asm
-	cvta.to.global.u64 %rd535, %rd536;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r616,%r617,%r618,%r619}, [%rd535];
-	// inline asm
-	mov.b32 	 %f2035, %r616;
-	mov.b32 	 %f2036, %r617;
-	mov.b32 	 %f2037, %r618;
-	add.s64 	%rd539, %rd536, 16;
-	// inline asm
-	cvta.to.global.u64 %rd538, %rd539;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r620,%r621,%r622,%r623}, [%rd538];
-	// inline asm
-	mov.b32 	 %f2032, %r620;
-	mov.b32 	 %f2033, %r621;
-	mov.b32 	 %f2034, %r622;
-	add.s64 	%rd542, %rd536, 32;
-	// inline asm
-	cvta.to.global.u64 %rd541, %rd542;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r624,%r625,%r626,%r627}, [%rd541];
-	// inline asm
-	sub.f32 	%f576, %f1633, %f1634;
-	mov.b32 	 %f2029, %r624;
-	mov.b32 	 %f2030, %r625;
-	mov.b32 	 %f2031, %r626;
-	setp.leu.f32	%p58, %f576, 0f00000000;
-	@%p58 bra 	BB8_90;
-
-	mul.lo.s64 	%rd554, %rd42, 48;
-	add.s64 	%rd555, %rd509, %rd554;
-	add.s64 	%rd546, %rd555, 80;
-	// inline asm
-	cvta.to.global.u64 %rd545, %rd546;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r631,%r632,%r633,%r634}, [%rd545];
-	// inline asm
-	mov.b32 	 %f1635, %r631;
-	mov.b32 	 %f1636, %r632;
-	mov.b32 	 %f1637, %r633;
-	add.s64 	%rd549, %rd555, 96;
-	// inline asm
-	cvta.to.global.u64 %rd548, %rd549;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r635,%r636,%r637,%r638}, [%rd548];
-	// inline asm
-	mov.b32 	 %f1638, %r635;
-	mov.b32 	 %f1639, %r636;
-	mov.b32 	 %f1640, %r637;
-	add.s64 	%rd552, %rd555, 112;
-	// inline asm
-	cvta.to.global.u64 %rd551, %rd552;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r639,%r640,%r641,%r642}, [%rd551];
-	// inline asm
-	mov.f32 	%f1641, 0f3F800000;
-	sub.f32 	%f1642, %f1641, %f576;
-	mul.f32 	%f1643, %f576, %f1635;
-	mul.f32 	%f1644, %f576, %f1636;
-	mul.f32 	%f1645, %f576, %f1637;
-	fma.rn.f32 	%f2035, %f1642, %f2035, %f1643;
-	fma.rn.f32 	%f2036, %f1642, %f2036, %f1644;
-	fma.rn.f32 	%f2037, %f1642, %f2037, %f1645;
-	mul.f32 	%f1646, %f576, %f1638;
-	mul.f32 	%f1647, %f576, %f1639;
-	mul.f32 	%f1648, %f576, %f1640;
-	fma.rn.f32 	%f2032, %f1642, %f2032, %f1646;
-	fma.rn.f32 	%f2033, %f1642, %f2033, %f1647;
-	fma.rn.f32 	%f2034, %f1642, %f2034, %f1648;
-	mov.b32 	 %f1649, %r639;
-	mov.b32 	 %f1650, %r640;
-	mov.b32 	 %f1651, %r641;
-	mul.f32 	%f1652, %f576, %f1649;
-	mul.f32 	%f1653, %f576, %f1650;
-	mul.f32 	%f1654, %f576, %f1651;
-	fma.rn.f32 	%f2029, %f1642, %f2029, %f1652;
-	fma.rn.f32 	%f2030, %f1642, %f2030, %f1653;
-	fma.rn.f32 	%f2031, %f1642, %f2031, %f1654;
-	bra.uni 	BB8_90;
-
-BB8_79:
-	mov.f32 	%f2038, 0f00000000;
-	mov.f32 	%f2040, 0f3F800000;
-	setp.eq.s32	%p54, %r495, 4;
-	@%p54 bra 	BB8_82;
-	bra.uni 	BB8_80;
-
-BB8_82:
-	// inline asm
-	call (%rd665), _optix_get_instance_inverse_transform_from_handle, (%rd435);
-	// inline asm
-	bra.uni 	BB8_83;
-
-BB8_85:
-	// inline asm
-	call (%rd450), _optix_get_srt_motion_transform_from_handle, (%rd435);
-	// inline asm
-	// inline asm
-	cvta.to.global.u64 %rd452, %rd450;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r509,%r510,%r511,%r512}, [%rd452];
-	// inline asm
-	mov.b32	{%rs15, %rs16}, %r511;
-	add.s64 	%rd456, %rd450, 16;
-	// inline asm
-	cvta.to.global.u64 %rd455, %rd456;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r513,%r514,%r515,%r516}, [%rd455];
-	// inline asm
-	add.s64 	%rd459, %rd450, 32;
-	// inline asm
-	cvta.to.global.u64 %rd458, %rd459;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r517,%r518,%r519,%r520}, [%rd458];
-	// inline asm
-	add.s64 	%rd462, %rd450, 48;
-	// inline asm
-	cvta.to.global.u64 %rd461, %rd462;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r521,%r522,%r523,%r524}, [%rd461];
-	// inline asm
-	add.s64 	%rd465, %rd450, 64;
-	// inline asm
-	cvta.to.global.u64 %rd464, %rd465;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r525,%r526,%r527,%r528}, [%rd464];
-	// inline asm
-	add.s64 	%rd468, %rd450, 80;
-	// inline asm
-	cvta.to.global.u64 %rd467, %rd468;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r529,%r530,%r531,%r532}, [%rd467];
-	// inline asm
-	add.s64 	%rd471, %rd450, 96;
-	// inline asm
-	cvta.to.global.u64 %rd470, %rd471;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r533,%r534,%r535,%r536}, [%rd470];
-	// inline asm
-	add.s64 	%rd474, %rd450, 112;
-	// inline asm
-	cvta.to.global.u64 %rd473, %rd474;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r537,%r538,%r539,%r540}, [%rd473];
-	// inline asm
-	add.s64 	%rd477, %rd450, 128;
-	// inline asm
-	cvta.to.global.u64 %rd476, %rd477;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r541,%r542,%r543,%r544}, [%rd476];
-	// inline asm
-	add.s64 	%rd480, %rd450, 144;
-	// inline asm
-	cvta.to.global.u64 %rd479, %rd480;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r545,%r546,%r547,%r548}, [%rd479];
-	// inline asm
-	mov.b32 	 %f1532, %r512;
-	mov.b32 	 %f1533, %r513;
-	cvt.u32.u16	%r565, %rs15;
-	add.s32 	%r566, %r565, -1;
-	cvt.rn.f32.s32	%f1534, %r566;
-	sub.f32 	%f1535, %f957, %f1532;
-	mul.f32 	%f1536, %f1535, %f1534;
-	sub.f32 	%f1537, %f1533, %f1532;
-	div.rn.f32 	%f1538, %f1536, %f1537;
-	min.f32 	%f1539, %f1534, %f1538;
-	mov.f32 	%f1540, 0f00000000;
-	max.f32 	%f1541, %f1540, %f1539;
-	cvt.rmi.f32.f32	%f1542, %f1541;
-	cvt.rzi.s32.f32	%r567, %f1542;
-	cvt.s64.s32	%rd40, %r567;
-	mul.wide.s32 	%rd494, %r567, 64;
-	add.s64 	%rd483, %rd459, %rd494;
-	// inline asm
-	cvta.to.global.u64 %rd482, %rd483;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r549,%r550,%r551,%r552}, [%rd482];
-	// inline asm
-	mov.b32 	 %f2019, %r549;
-	mov.b32 	 %f2020, %r550;
-	mov.b32 	 %f2021, %r551;
-	add.s64 	%rd486, %rd483, 16;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r634,%r635,%r636,%r637}, [%rd529];
+	// end inline asm
+	mov.b32 	%f2069, %r634;
+	mov.b32 	%f2070, %r635;
+	mov.b32 	%f2071, %r636;
+	setp.leu.f32 	%p62, %f598, 0f00000000;
+	@%p62 bra 	$L__BB8_92;
+
+	mov.f32 	%f1655, 0f3F800000;
+	sub.f32 	%f1656, %f1655, %f598;
+	mul.lo.s64 	%rd542, %rd38, 48;
+	add.s64 	%rd543, %rd497, %rd542;
+	add.s64 	%rd534, %rd543, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd533, %rd534;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r641,%r642,%r643,%r644}, [%rd533];
+	// end inline asm
+	mov.b32 	%f1657, %r641;
+	mov.b32 	%f1658, %r642;
+	mov.b32 	%f1659, %r643;
+	mul.f32 	%f1660, %f598, %f1657;
+	mul.f32 	%f1661, %f598, %f1658;
+	mul.f32 	%f1662, %f598, %f1659;
+	fma.rn.f32 	%f2075, %f1656, %f2075, %f1660;
+	fma.rn.f32 	%f2076, %f1656, %f2076, %f1661;
+	fma.rn.f32 	%f2077, %f1656, %f2077, %f1662;
+	add.s64 	%rd537, %rd543, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd536, %rd537;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r645,%r646,%r647,%r648}, [%rd536];
+	// end inline asm
+	mov.b32 	%f1663, %r645;
+	mov.b32 	%f1664, %r646;
+	mov.b32 	%f1665, %r647;
+	mul.f32 	%f1666, %f598, %f1663;
+	mul.f32 	%f1667, %f598, %f1664;
+	mul.f32 	%f1668, %f598, %f1665;
+	fma.rn.f32 	%f2072, %f1656, %f2072, %f1666;
+	fma.rn.f32 	%f2073, %f1656, %f2073, %f1667;
+	fma.rn.f32 	%f2074, %f1656, %f2074, %f1668;
+	add.s64 	%rd540, %rd543, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd539, %rd540;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r649,%r650,%r651,%r652}, [%rd539];
+	// end inline asm
+	mov.b32 	%f1669, %r649;
+	mov.b32 	%f1670, %r650;
+	mov.b32 	%f1671, %r651;
+	mul.f32 	%f1672, %f598, %f1669;
+	mul.f32 	%f1673, %f598, %f1670;
+	mul.f32 	%f1674, %f598, %f1671;
+	fma.rn.f32 	%f2069, %f1656, %f2069, %f1672;
+	fma.rn.f32 	%f2070, %f1656, %f2070, %f1673;
+	fma.rn.f32 	%f2071, %f1656, %f2071, %f1674;
+	bra.uni 	$L__BB8_92;
+
+$L__BB8_81:
+	mov.f32 	%f2078, 0f00000000;
+	mov.f32 	%f2080, 0f3F800000;
+	setp.eq.s32 	%p58, %r505, 4;
+	@%p58 bra 	$L__BB8_84;
+
+	setp.ne.s32 	%p59, %r505, 1;
+	mov.f32 	%f2079, %f2078;
+	mov.f32 	%f2081, %f2078;
+	mov.f32 	%f2082, %f2080;
+	mov.f32 	%f2083, %f2078;
+	mov.f32 	%f2084, %f2080;
+	mov.f32 	%f2085, %f2078;
+	mov.f32 	%f2086, %f2078;
+	@%p59 bra 	$L__BB8_93;
+
+	// begin inline asm
+	call (%rd427), _optix_get_static_transform_from_handle, (%rd425);
+	// end inline asm
+	add.s64 	%rd652, %rd427, 64;
+	bra.uni 	$L__BB8_85;
+
+$L__BB8_87:
+	// begin inline asm
+	call (%rd440), _optix_get_srt_motion_transform_from_handle, (%rd425);
+	// end inline asm
+	// begin inline asm
+	cvta.to.global.u64 %rd442, %rd440;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r519,%r520,%r521,%r522}, [%rd442];
+	// end inline asm
+	add.s64 	%rd446, %rd440, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd445, %rd446;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r523,%r524,%r525,%r526}, [%rd445];
+	// end inline asm
+	add.s64 	%rd449, %rd440, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd448, %rd449;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r527,%r528,%r529,%r530}, [%rd448];
+	// end inline asm
+	add.s64 	%rd452, %rd440, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd451, %rd452;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r531,%r532,%r533,%r534}, [%rd451];
+	// end inline asm
+	add.s64 	%rd455, %rd440, 64;
+	// begin inline asm
+	cvta.to.global.u64 %rd454, %rd455;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r535,%r536,%r537,%r538}, [%rd454];
+	// end inline asm
+	add.s64 	%rd458, %rd440, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd457, %rd458;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r539,%r540,%r541,%r542}, [%rd457];
+	// end inline asm
+	add.s64 	%rd461, %rd440, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd460, %rd461;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r543,%r544,%r545,%r546}, [%rd460];
+	// end inline asm
+	add.s64 	%rd464, %rd440, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd463, %rd464;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r547,%r548,%r549,%r550}, [%rd463];
+	// end inline asm
+	add.s64 	%rd467, %rd440, 128;
+	// begin inline asm
+	cvta.to.global.u64 %rd466, %rd467;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r551,%r552,%r553,%r554}, [%rd466];
+	// end inline asm
+	add.s64 	%rd470, %rd440, 144;
+	// begin inline asm
+	cvta.to.global.u64 %rd469, %rd470;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r555,%r556,%r557,%r558}, [%rd469];
+	// end inline asm
+	mov.b32 	%f1552, %r522;
+	mov.b32 	%f1553, %r523;
+	and.b32  	%r575, %r521, 65535;
+	add.s32 	%r576, %r575, -1;
+	cvt.rn.f32.s32 	%f1554, %r576;
+	sub.f32 	%f1555, %f1540, %f1552;
+	mul.f32 	%f1556, %f1555, %f1554;
+	sub.f32 	%f1557, %f1553, %f1552;
+	div.rn.f32 	%f1558, %f1556, %f1557;
+	min.f32 	%f1559, %f1554, %f1558;
+	mov.f32 	%f1560, 0f00000000;
+	max.f32 	%f1561, %f1560, %f1559;
+	cvt.rmi.f32.f32 	%f1562, %f1561;
+	sub.f32 	%f558, %f1561, %f1562;
+	cvt.rzi.s32.f32 	%r577, %f1562;
+	mul.wide.s32 	%rd484, %r577, 64;
+	add.s64 	%rd473, %rd449, %rd484;
+	// begin inline asm
+	cvta.to.global.u64 %rd472, %rd473;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r559,%r560,%r561,%r562}, [%rd472];
+	// end inline asm
+	mov.b32 	%f2059, %r559;
+	mov.b32 	%f2060, %r560;
+	mov.b32 	%f2061, %r561;
+	add.s64 	%rd476, %rd473, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd475, %rd476;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r563,%r564,%r565,%r566}, [%rd475];
+	// end inline asm
+	mov.b32 	%f2062, %r563;
+	mov.b32 	%f2063, %r564;
+	mov.b32 	%f2064, %r566;
+	add.s64 	%rd479, %rd473, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd478, %rd479;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r567,%r568,%r569,%r570}, [%rd478];
+	// end inline asm
+	mov.b32 	%f2065, %r568;
+	mov.b32 	%f2066, %r569;
+	mov.b32 	%f2067, %r570;
+	add.s64 	%rd482, %rd473, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd481, %rd482;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r571,%r572,%r573,%r574}, [%rd481];
+	// end inline asm
+	mov.b32 	%f2068, %r571;
+	setp.leu.f32 	%p61, %f558, 0f00000000;
+	@%p61 bra 	$L__BB8_89;
+
+	mov.f32 	%f1563, 0f3F800000;
+	sub.f32 	%f1564, %f1563, %f558;
+	add.s64 	%rd486, %rd473, 64;
+	// begin inline asm
 	cvta.to.global.u64 %rd485, %rd486;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r553,%r554,%r555,%r556}, [%rd485];
-	// inline asm
-	mov.b32 	 %f2022, %r553;
-	mov.b32 	 %f2023, %r554;
-	mov.b32 	 %f2024, %r556;
-	add.s64 	%rd489, %rd483, 32;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r578,%r579,%r580,%r581}, [%rd485];
+	// end inline asm
+	mov.b32 	%f1565, %r578;
+	mov.b32 	%f1566, %r579;
+	mov.b32 	%f1567, %r580;
+	mul.f32 	%f1568, %f558, %f1565;
+	mul.f32 	%f1569, %f558, %f1566;
+	mul.f32 	%f1570, %f558, %f1567;
+	fma.rn.f32 	%f2059, %f1564, %f2059, %f1568;
+	fma.rn.f32 	%f2060, %f1564, %f2060, %f1569;
+	fma.rn.f32 	%f2061, %f1564, %f2061, %f1570;
+	add.s64 	%rd489, %rd473, 80;
+	// begin inline asm
 	cvta.to.global.u64 %rd488, %rd489;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r557,%r558,%r559,%r560}, [%rd488];
-	// inline asm
-	sub.f32 	%f536, %f1541, %f1542;
-	mov.b32 	 %f2025, %r558;
-	mov.b32 	 %f2026, %r559;
-	mov.b32 	 %f2027, %r560;
-	add.s64 	%rd492, %rd483, 48;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r582,%r583,%r584,%r585}, [%rd488];
+	// end inline asm
+	mov.b32 	%f1571, %r582;
+	mov.b32 	%f1572, %r583;
+	mov.b32 	%f1573, %r585;
+	mul.f32 	%f1574, %f558, %f1571;
+	mul.f32 	%f1575, %f558, %f1572;
+	mul.f32 	%f1576, %f558, %f1573;
+	fma.rn.f32 	%f2062, %f1564, %f2062, %f1574;
+	fma.rn.f32 	%f2063, %f1564, %f2063, %f1575;
+	fma.rn.f32 	%f2064, %f1564, %f2064, %f1576;
+	add.s64 	%rd492, %rd473, 96;
+	// begin inline asm
 	cvta.to.global.u64 %rd491, %rd492;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r561,%r562,%r563,%r564}, [%rd491];
-	// inline asm
-	mov.b32 	 %f2028, %r561;
-	setp.leu.f32	%p57, %f536, 0f00000000;
-	@%p57 bra 	BB8_87;
-
-	shl.b64 	%rd507, %rd40, 6;
-	add.s64 	%rd508, %rd507, %rd450;
-	add.s64 	%rd496, %rd508, 96;
-	// inline asm
-	cvta.to.global.u64 %rd495, %rd496;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r568,%r569,%r570,%r571}, [%rd495];
-	// inline asm
-	mov.b32 	 %f1543, %r568;
-	mov.b32 	 %f1544, %r569;
-	mov.b32 	 %f1545, %r570;
-	add.s64 	%rd499, %rd508, 112;
-	// inline asm
-	cvta.to.global.u64 %rd498, %rd499;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r572,%r573,%r574,%r575}, [%rd498];
-	// inline asm
-	mov.b32 	 %f1546, %r572;
-	mov.b32 	 %f1547, %r573;
-	mov.b32 	 %f1548, %r575;
-	add.s64 	%rd502, %rd508, 128;
-	// inline asm
-	cvta.to.global.u64 %rd501, %rd502;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r576,%r577,%r578,%r579}, [%rd501];
-	// inline asm
-	mov.b32 	 %f1549, %r577;
-	mov.b32 	 %f1550, %r578;
-	mov.b32 	 %f1551, %r579;
-	add.s64 	%rd505, %rd508, 144;
-	// inline asm
-	cvta.to.global.u64 %rd504, %rd505;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r580,%r581,%r582,%r583}, [%rd504];
-	// inline asm
-	mov.f32 	%f1552, 0f3F800000;
-	sub.f32 	%f1553, %f1552, %f536;
-	mul.f32 	%f1554, %f536, %f1543;
-	mul.f32 	%f1555, %f536, %f1544;
-	mul.f32 	%f1556, %f536, %f1545;
-	fma.rn.f32 	%f2019, %f1553, %f2019, %f1554;
-	fma.rn.f32 	%f2020, %f1553, %f2020, %f1555;
-	fma.rn.f32 	%f2021, %f1553, %f2021, %f1556;
-	mul.f32 	%f1557, %f536, %f1546;
-	mul.f32 	%f1558, %f536, %f1547;
-	mul.f32 	%f1559, %f536, %f1548;
-	fma.rn.f32 	%f2022, %f1553, %f2022, %f1557;
-	fma.rn.f32 	%f2023, %f1553, %f2023, %f1558;
-	fma.rn.f32 	%f2024, %f1553, %f2024, %f1559;
-	mul.f32 	%f1560, %f536, %f1549;
-	mul.f32 	%f1561, %f536, %f1550;
-	mul.f32 	%f1562, %f536, %f1551;
-	fma.rn.f32 	%f1563, %f1553, %f2025, %f1560;
-	fma.rn.f32 	%f1564, %f1553, %f2026, %f1561;
-	fma.rn.f32 	%f1565, %f1553, %f2027, %f1562;
-	mov.b32 	 %f1566, %r580;
-	mul.f32 	%f1567, %f536, %f1566;
-	fma.rn.f32 	%f1568, %f1553, %f2028, %f1567;
-	mul.f32 	%f1569, %f1564, %f1564;
-	fma.rn.f32 	%f1570, %f1563, %f1563, %f1569;
-	fma.rn.f32 	%f1571, %f1565, %f1565, %f1570;
-	fma.rn.f32 	%f1572, %f1568, %f1568, %f1571;
-	sqrt.rn.f32 	%f1573, %f1572;
-	rcp.rn.f32 	%f1574, %f1573;
-	mul.f32 	%f2025, %f1563, %f1574;
-	mul.f32 	%f2026, %f1564, %f1574;
-	mul.f32 	%f2027, %f1565, %f1574;
-	mul.f32 	%f2028, %f1568, %f1574;
-
-BB8_87:
-	mul.f32 	%f1575, %f2026, %f2026;
-	fma.rn.f32 	%f1576, %f2025, %f2025, %f1575;
-	fma.rn.f32 	%f1577, %f2027, %f2027, %f1576;
-	fma.rn.f32 	%f1578, %f2028, %f2028, %f1577;
-	rcp.rn.f32 	%f1579, %f1578;
-	mul.f32 	%f1580, %f2025, %f1579;
-	mul.f32 	%f1581, %f2026, %f1579;
-	mul.f32 	%f1582, %f2027, %f1579;
-	mul.f32 	%f1583, %f2028, %f1579;
-	mul.f32 	%f1584, %f2025, %f1580;
-	mul.f32 	%f1585, %f2026, %f1581;
-	mul.f32 	%f1586, %f2027, %f1582;
-	mul.f32 	%f1587, %f2025, %f1581;
-	mul.f32 	%f1588, %f2027, %f1583;
-	mul.f32 	%f1589, %f2025, %f1582;
-	mul.f32 	%f1590, %f2026, %f1583;
-	mul.f32 	%f1591, %f2026, %f1582;
-	mul.f32 	%f1592, %f2025, %f1583;
-	sub.f32 	%f1593, %f1584, %f1585;
-	sub.f32 	%f1594, %f1593, %f1586;
-	fma.rn.f32 	%f1595, %f2028, %f1583, %f1594;
-	sub.f32 	%f1596, %f1587, %f1588;
-	add.f32 	%f1597, %f1596, %f1596;
-	add.f32 	%f1598, %f1589, %f1590;
-	add.f32 	%f1599, %f1598, %f1598;
-	add.f32 	%f1600, %f1587, %f1588;
-	add.f32 	%f1601, %f1600, %f1600;
-	sub.f32 	%f1602, %f1585, %f1584;
-	sub.f32 	%f1603, %f1602, %f1586;
-	fma.rn.f32 	%f1604, %f2028, %f1583, %f1603;
-	sub.f32 	%f1605, %f1591, %f1592;
-	add.f32 	%f1606, %f1605, %f1605;
-	sub.f32 	%f1607, %f1589, %f1590;
-	add.f32 	%f1608, %f1607, %f1607;
-	add.f32 	%f1609, %f1591, %f1592;
-	add.f32 	%f1610, %f1609, %f1609;
-	neg.f32 	%f1611, %f1584;
-	sub.f32 	%f1612, %f1611, %f1585;
-	add.f32 	%f1613, %f1586, %f1612;
-	fma.rn.f32 	%f1614, %f2028, %f1583, %f1613;
-	mul.f32 	%f1615, %f2021, %f1595;
-	fma.rn.f32 	%f1616, %f2023, %f1597, %f1615;
-	fma.rn.f32 	%f2037, %f2024, %f1599, %f1616;
-	mul.f32 	%f1617, %f2023, %f1604;
-	fma.rn.f32 	%f1618, %f2021, %f1601, %f1617;
-	fma.rn.f32 	%f2034, %f2024, %f1606, %f1618;
-	mul.f32 	%f1619, %f2023, %f1610;
-	fma.rn.f32 	%f1620, %f2021, %f1608, %f1619;
-	fma.rn.f32 	%f2031, %f2024, %f1614, %f1620;
-	mul.f32 	%f1621, %f2020, %f1595;
-	fma.rn.f32 	%f2036, %f2022, %f1597, %f1621;
-	mul.f32 	%f1622, %f2022, %f1604;
-	fma.rn.f32 	%f2033, %f2020, %f1601, %f1622;
-	mul.f32 	%f1623, %f2022, %f1610;
-	fma.rn.f32 	%f2030, %f2020, %f1608, %f1623;
-	mul.f32 	%f2035, %f2019, %f1595;
-	mul.f32 	%f2032, %f2019, %f1601;
-	mul.f32 	%f2029, %f2019, %f1608;
-
-BB8_90:
-	mul.f32 	%f1655, %f2030, %f2034;
-	mul.f32 	%f1656, %f2031, %f2033;
-	sub.f32 	%f1657, %f1656, %f1655;
-	mul.f32 	%f1658, %f2035, %f1657;
-	mul.f32 	%f1659, %f2029, %f2034;
-	mul.f32 	%f1660, %f2031, %f2032;
-	sub.f32 	%f1661, %f1660, %f1659;
-	mul.f32 	%f1662, %f1661, %f2036;
-	sub.f32 	%f1663, %f1658, %f1662;
-	mul.f32 	%f1664, %f2029, %f2033;
-	mul.f32 	%f1665, %f2030, %f2032;
-	sub.f32 	%f1666, %f1665, %f1664;
-	fma.rn.f32 	%f1667, %f1666, %f2037, %f1663;
-	rcp.rn.f32 	%f1668, %f1667;
-	mul.f32 	%f2044, %f1657, %f1668;
-	mul.f32 	%f1669, %f2031, %f2036;
-	mul.f32 	%f1670, %f2030, %f2037;
-	sub.f32 	%f1671, %f1670, %f1669;
-	mul.f32 	%f2045, %f1668, %f1671;
-	mul.f32 	%f1672, %f2033, %f2037;
-	mul.f32 	%f1673, %f2034, %f2036;
-	sub.f32 	%f1674, %f1673, %f1672;
-	mul.f32 	%f2046, %f1668, %f1674;
-	sub.f32 	%f1675, %f1659, %f1660;
-	mul.f32 	%f2041, %f1675, %f1668;
-	mul.f32 	%f1676, %f2029, %f2037;
-	mul.f32 	%f1677, %f2031, %f2035;
-	sub.f32 	%f1678, %f1677, %f1676;
-	mul.f32 	%f2042, %f1668, %f1678;
-	mul.f32 	%f1679, %f2034, %f2035;
-	mul.f32 	%f1680, %f2032, %f2037;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r586,%r587,%r588,%r589}, [%rd491];
+	// end inline asm
+	mov.b32 	%f1577, %r587;
+	mov.b32 	%f1578, %r588;
+	mov.b32 	%f1579, %r589;
+	mul.f32 	%f1580, %f558, %f1577;
+	mul.f32 	%f1581, %f558, %f1578;
+	mul.f32 	%f1582, %f558, %f1579;
+	fma.rn.f32 	%f1583, %f1564, %f2065, %f1580;
+	fma.rn.f32 	%f1584, %f1564, %f2066, %f1581;
+	fma.rn.f32 	%f1585, %f1564, %f2067, %f1582;
+	add.s64 	%rd495, %rd473, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd494, %rd495;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r590,%r591,%r592,%r593}, [%rd494];
+	// end inline asm
+	mov.b32 	%f1586, %r590;
+	mul.f32 	%f1587, %f558, %f1586;
+	fma.rn.f32 	%f1588, %f1564, %f2068, %f1587;
+	mul.f32 	%f1589, %f1584, %f1584;
+	fma.rn.f32 	%f1590, %f1583, %f1583, %f1589;
+	fma.rn.f32 	%f1591, %f1585, %f1585, %f1590;
+	fma.rn.f32 	%f1592, %f1588, %f1588, %f1591;
+	sqrt.rn.f32 	%f1593, %f1592;
+	rcp.rn.f32 	%f1594, %f1593;
+	mul.f32 	%f2065, %f1583, %f1594;
+	mul.f32 	%f2066, %f1584, %f1594;
+	mul.f32 	%f2067, %f1585, %f1594;
+	mul.f32 	%f2068, %f1594, %f1588;
+
+$L__BB8_89:
+	mul.f32 	%f1595, %f2066, %f2066;
+	fma.rn.f32 	%f1596, %f2065, %f2065, %f1595;
+	fma.rn.f32 	%f1597, %f2067, %f2067, %f1596;
+	fma.rn.f32 	%f1598, %f2068, %f2068, %f1597;
+	rcp.rn.f32 	%f1599, %f1598;
+	mul.f32 	%f1600, %f2065, %f1599;
+	mul.f32 	%f1601, %f2066, %f1599;
+	mul.f32 	%f1602, %f2067, %f1599;
+	mul.f32 	%f1603, %f2068, %f1599;
+	mul.f32 	%f1604, %f2065, %f1600;
+	mul.f32 	%f1605, %f2066, %f1601;
+	mul.f32 	%f1606, %f2067, %f1602;
+	mul.f32 	%f1607, %f2065, %f1601;
+	mul.f32 	%f1608, %f2067, %f1603;
+	mul.f32 	%f1609, %f2065, %f1602;
+	mul.f32 	%f1610, %f2066, %f1603;
+	mul.f32 	%f1611, %f2066, %f1602;
+	mul.f32 	%f1612, %f2065, %f1603;
+	sub.f32 	%f1613, %f1604, %f1605;
+	sub.f32 	%f1614, %f1613, %f1606;
+	fma.rn.f32 	%f1615, %f2068, %f1603, %f1614;
+	sub.f32 	%f1616, %f1607, %f1608;
+	add.f32 	%f1617, %f1616, %f1616;
+	add.f32 	%f1618, %f1609, %f1610;
+	add.f32 	%f1619, %f1618, %f1618;
+	add.f32 	%f1620, %f1607, %f1608;
+	add.f32 	%f1621, %f1620, %f1620;
+	sub.f32 	%f1622, %f1605, %f1604;
+	sub.f32 	%f1623, %f1622, %f1606;
+	fma.rn.f32 	%f1624, %f2068, %f1603, %f1623;
+	sub.f32 	%f1625, %f1611, %f1612;
+	add.f32 	%f1626, %f1625, %f1625;
+	sub.f32 	%f1627, %f1609, %f1610;
+	add.f32 	%f1628, %f1627, %f1627;
+	add.f32 	%f1629, %f1611, %f1612;
+	add.f32 	%f1630, %f1629, %f1629;
+	neg.f32 	%f1631, %f1604;
+	sub.f32 	%f1632, %f1631, %f1605;
+	add.f32 	%f1633, %f1606, %f1632;
+	fma.rn.f32 	%f1634, %f2068, %f1603, %f1633;
+	mul.f32 	%f1635, %f2061, %f1615;
+	fma.rn.f32 	%f1636, %f2063, %f1617, %f1635;
+	fma.rn.f32 	%f2077, %f2064, %f1619, %f1636;
+	mul.f32 	%f1637, %f2063, %f1624;
+	fma.rn.f32 	%f1638, %f2061, %f1621, %f1637;
+	fma.rn.f32 	%f2074, %f2064, %f1626, %f1638;
+	mul.f32 	%f1639, %f2063, %f1630;
+	fma.rn.f32 	%f1640, %f2061, %f1628, %f1639;
+	fma.rn.f32 	%f2071, %f2064, %f1634, %f1640;
+	mul.f32 	%f1641, %f2060, %f1615;
+	fma.rn.f32 	%f2076, %f2062, %f1617, %f1641;
+	mul.f32 	%f1642, %f2062, %f1624;
+	fma.rn.f32 	%f2073, %f2060, %f1621, %f1642;
+	mul.f32 	%f1643, %f2062, %f1630;
+	fma.rn.f32 	%f2070, %f2060, %f1628, %f1643;
+	mul.f32 	%f2075, %f2059, %f1615;
+	mul.f32 	%f2072, %f2059, %f1621;
+	mul.f32 	%f2069, %f2059, %f1628;
+
+$L__BB8_92:
+	mul.f32 	%f1675, %f2070, %f2074;
+	mul.f32 	%f1676, %f2071, %f2073;
+	sub.f32 	%f1677, %f1676, %f1675;
+	mul.f32 	%f1678, %f2075, %f1677;
+	mul.f32 	%f1679, %f2069, %f2074;
+	mul.f32 	%f1680, %f2071, %f2072;
 	sub.f32 	%f1681, %f1680, %f1679;
-	mul.f32 	%f2043, %f1668, %f1681;
-	mul.f32 	%f2038, %f1666, %f1668;
-	mul.f32 	%f1682, %f2030, %f2035;
-	mul.f32 	%f1683, %f2029, %f2036;
-	sub.f32 	%f1684, %f1683, %f1682;
-	mul.f32 	%f2039, %f1684, %f1668;
-	mul.f32 	%f1685, %f2032, %f2036;
-	mul.f32 	%f1686, %f2033, %f2035;
-	sub.f32 	%f1687, %f1686, %f1685;
-	mul.f32 	%f2040, %f1687, %f1668;
-	bra.uni 	BB8_91;
-
-BB8_80:
-	setp.ne.s32	%p55, %r495, 1;
-	mov.f32 	%f2039, %f2038;
-	mov.f32 	%f2041, %f2038;
-	mov.f32 	%f2042, %f2040;
-	mov.f32 	%f2043, %f2038;
-	mov.f32 	%f2044, %f2040;
-	mov.f32 	%f2045, %f2038;
-	mov.f32 	%f2046, %f2038;
-	@%p55 bra 	BB8_91;
-
-	// inline asm
-	call (%rd437), _optix_get_static_transform_from_handle, (%rd435);
-	// inline asm
-	add.s64 	%rd665, %rd437, 64;
-
-BB8_83:
-	// inline asm
-	cvta.to.global.u64 %rd441, %rd665;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r497,%r498,%r499,%r500}, [%rd441];
-	// inline asm
-	mov.b32 	 %f2044, %r497;
-	mov.b32 	 %f2045, %r498;
-	mov.b32 	 %f2046, %r499;
-	add.s64 	%rd445, %rd665, 16;
-	// inline asm
-	cvta.to.global.u64 %rd444, %rd445;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r501,%r502,%r503,%r504}, [%rd444];
-	// inline asm
-	mov.b32 	 %f2041, %r501;
-	mov.b32 	 %f2042, %r502;
-	mov.b32 	 %f2043, %r503;
-	add.s64 	%rd448, %rd665, 32;
-	// inline asm
-	cvta.to.global.u64 %rd447, %rd448;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r505,%r506,%r507,%r508}, [%rd447];
-	// inline asm
-	mov.b32 	 %f2038, %r505;
-	mov.b32 	 %f2039, %r506;
-	mov.b32 	 %f2040, %r507;
-
-BB8_91:
-	setp.eq.s32	%p59, %r649, 0;
-	@%p59 bra 	BB8_92;
-	bra.uni 	BB8_93;
-
-BB8_92:
-	mov.f32 	%f2018, %f2038;
-	mov.f32 	%f2017, %f2039;
-	mov.f32 	%f2016, %f2040;
-	mov.f32 	%f2015, %f2041;
-	mov.f32 	%f2014, %f2042;
-	mov.f32 	%f2013, %f2043;
-	mov.f32 	%f2012, %f2044;
-	mov.f32 	%f2011, %f2045;
-	mov.f32 	%f2010, %f2046;
-	bra.uni 	BB8_94;
-
-BB8_93:
-	mul.f32 	%f1688, %f2015, %f2045;
-	fma.rn.f32 	%f1689, %f2012, %f2044, %f1688;
-	fma.rn.f32 	%f616, %f2018, %f2046, %f1689;
-	mul.f32 	%f1690, %f2014, %f2045;
-	fma.rn.f32 	%f1691, %f2011, %f2044, %f1690;
-	fma.rn.f32 	%f617, %f2017, %f2046, %f1691;
-	mul.f32 	%f1692, %f2013, %f2045;
-	fma.rn.f32 	%f1693, %f2010, %f2044, %f1692;
-	fma.rn.f32 	%f618, %f2016, %f2046, %f1693;
-	mul.f32 	%f1694, %f2015, %f2042;
-	fma.rn.f32 	%f1695, %f2012, %f2041, %f1694;
-	fma.rn.f32 	%f619, %f2018, %f2043, %f1695;
-	mul.f32 	%f1696, %f2014, %f2042;
-	fma.rn.f32 	%f1697, %f2011, %f2041, %f1696;
-	fma.rn.f32 	%f620, %f2017, %f2043, %f1697;
-	mul.f32 	%f1698, %f2013, %f2042;
-	fma.rn.f32 	%f1699, %f2010, %f2041, %f1698;
-	fma.rn.f32 	%f621, %f2016, %f2043, %f1699;
-	mul.f32 	%f1700, %f2015, %f2039;
-	fma.rn.f32 	%f1701, %f2012, %f2038, %f1700;
-	fma.rn.f32 	%f2018, %f2018, %f2040, %f1701;
-	mul.f32 	%f1702, %f2014, %f2039;
-	fma.rn.f32 	%f1703, %f2011, %f2038, %f1702;
-	fma.rn.f32 	%f2017, %f2017, %f2040, %f1703;
-	mul.f32 	%f1704, %f2013, %f2039;
-	fma.rn.f32 	%f1705, %f2010, %f2038, %f1704;
-	fma.rn.f32 	%f2016, %f2016, %f2040, %f1705;
-	mov.f32 	%f2015, %f619;
-	mov.f32 	%f2014, %f620;
-	mov.f32 	%f2013, %f621;
-	mov.f32 	%f2012, %f616;
-	mov.f32 	%f2011, %f617;
-	mov.f32 	%f2010, %f618;
-
-BB8_94:
-	add.s32 	%r649, %r649, 1;
-	setp.lt.u32	%p60, %r649, %r30;
-	@%p60 bra 	BB8_78;
-
-BB8_95:
-	fma.rn.f32 	%f1706, %f2101, %f1949, %f1946;
-	fma.rn.f32 	%f1707, %f2102, %f1948, %f1706;
-	fma.rn.f32 	%f1708, %f2101, %f1953, %f1950;
-	fma.rn.f32 	%f1709, %f2102, %f1952, %f1708;
-	fma.rn.f32 	%f1710, %f2101, %f1957, %f1954;
-	fma.rn.f32 	%f1711, %f2102, %f1956, %f1710;
-	fma.rn.f32 	%f2101, %f2103, %f1947, %f1707;
-	fma.rn.f32 	%f2102, %f2103, %f1951, %f1709;
-	fma.rn.f32 	%f2103, %f2103, %f1955, %f1711;
-	ld.const.u64 	%rd556, [params+112];
-	setp.eq.s64	%p61, %rd556, 0;
-	mov.f32 	%f2095, %f2098;
-	mov.f32 	%f2096, %f2099;
-	mov.f32 	%f2097, %f2100;
-	@%p61 bra 	BB8_97;
-
-	mul.f32 	%f1712, %f2098, %f2012;
-	fma.rn.f32 	%f1713, %f2099, %f2015, %f1712;
-	mul.f32 	%f1714, %f2098, %f2011;
-	fma.rn.f32 	%f1715, %f2099, %f2014, %f1714;
-	mul.f32 	%f1716, %f2098, %f2010;
-	fma.rn.f32 	%f1717, %f2099, %f2013, %f1716;
-	fma.rn.f32 	%f1718, %f2100, %f2018, %f1713;
-	fma.rn.f32 	%f1719, %f2100, %f2017, %f1715;
-	fma.rn.f32 	%f1720, %f2100, %f2016, %f1717;
-	mul.f32 	%f1721, %f1718, %f1718;
-	fma.rn.f32 	%f1722, %f1719, %f1719, %f1721;
-	fma.rn.f32 	%f1723, %f1720, %f1720, %f1722;
-	sqrt.rn.f32 	%f1724, %f1723;
-	div.rn.f32 	%f2095, %f1718, %f1724;
-	div.rn.f32 	%f2096, %f1719, %f1724;
-	div.rn.f32 	%f2097, %f1720, %f1724;
-
-BB8_97:
-	ld.const.u64 	%rd557, [params+136];
-	setp.eq.s64	%p62, %rd557, 0;
-	@%p62 bra 	BB8_99;
-
-	mul.f32 	%f1725, %f2098, %f2012;
-	fma.rn.f32 	%f1726, %f2099, %f2015, %f1725;
-	mul.f32 	%f1727, %f2098, %f2011;
-	fma.rn.f32 	%f1728, %f2099, %f2014, %f1727;
-	mul.f32 	%f1729, %f2098, %f2010;
-	fma.rn.f32 	%f1730, %f2099, %f2013, %f1729;
-	fma.rn.f32 	%f1731, %f2100, %f2018, %f1726;
-	fma.rn.f32 	%f1732, %f2100, %f2017, %f1728;
-	fma.rn.f32 	%f1733, %f2100, %f2016, %f1730;
-	mul.f32 	%f1734, %f1731, %f1731;
-	fma.rn.f32 	%f1735, %f1732, %f1732, %f1734;
-	fma.rn.f32 	%f1736, %f1733, %f1733, %f1735;
-	sqrt.rn.f32 	%f1737, %f1736;
-	div.rn.f32 	%f2098, %f1731, %f1737;
-	div.rn.f32 	%f2099, %f1732, %f1737;
-	div.rn.f32 	%f2100, %f1733, %f1737;
-
-BB8_99:
-	ld.const.u64 	%rd558, [params+184];
-	setp.eq.s64	%p63, %rd558, 0;
-	@%p63 bra 	BB8_101;
-
-	mul.f32 	%f1738, %f342, %f1949;
-	fma.rn.f32 	%f1739, %f341, %f1948, %f1738;
-	mul.f32 	%f1740, %f342, %f1953;
-	fma.rn.f32 	%f1741, %f341, %f1952, %f1740;
-	mul.f32 	%f1742, %f342, %f1957;
-	fma.rn.f32 	%f1743, %f341, %f1956, %f1742;
-	fma.rn.f32 	%f342, %f340, %f1947, %f1739;
-	fma.rn.f32 	%f341, %f340, %f1951, %f1741;
-	fma.rn.f32 	%f340, %f340, %f1955, %f1743;
-	mul.f32 	%f1744, %f345, %f1949;
-	fma.rn.f32 	%f1745, %f344, %f1948, %f1744;
-	mul.f32 	%f1746, %f345, %f1953;
-	fma.rn.f32 	%f1747, %f344, %f1952, %f1746;
-	mul.f32 	%f1748, %f345, %f1957;
-	fma.rn.f32 	%f1749, %f344, %f1956, %f1748;
-	fma.rn.f32 	%f345, %f343, %f1947, %f1745;
-	fma.rn.f32 	%f344, %f343, %f1951, %f1747;
-	fma.rn.f32 	%f343, %f343, %f1955, %f1749;
-
-BB8_101:
-	ld.const.u64 	%rd559, [params+280];
-	ld.const.u64 	%rd560, [params+232];
-	or.b64  	%rd561, %rd559, %rd560;
-	setp.eq.s64	%p64, %rd561, 0;
-	@%p64 bra 	BB8_103;
-
-	mul.f32 	%f1750, %f2098, %f1949;
-	fma.rn.f32 	%f1751, %f2099, %f1953, %f1750;
-	mul.f32 	%f1752, %f2098, %f1948;
-	fma.rn.f32 	%f1753, %f2099, %f1952, %f1752;
-	mul.f32 	%f1754, %f2098, %f1947;
-	fma.rn.f32 	%f1755, %f2099, %f1951, %f1754;
-	fma.rn.f32 	%f1756, %f2100, %f1957, %f1751;
-	fma.rn.f32 	%f1757, %f2100, %f1956, %f1753;
-	fma.rn.f32 	%f1758, %f2100, %f1955, %f1755;
-	mul.f32 	%f1759, %f1756, %f1756;
-	fma.rn.f32 	%f1760, %f1757, %f1757, %f1759;
-	fma.rn.f32 	%f1761, %f1758, %f1758, %f1760;
-	sqrt.rn.f32 	%f1762, %f1761;
-	div.rn.f32 	%f1763, %f1756, %f1762;
-	div.rn.f32 	%f1764, %f1757, %f1762;
-	div.rn.f32 	%f1765, %f1758, %f1762;
-	mul.f32 	%f1766, %f1763, %f2012;
-	mul.f32 	%f1767, %f1763, %f2011;
-	mul.f32 	%f1768, %f1763, %f2010;
-	fma.rn.f32 	%f1769, %f1764, %f2015, %f1766;
-	fma.rn.f32 	%f1770, %f1764, %f2014, %f1767;
-	fma.rn.f32 	%f1771, %f1764, %f2013, %f1768;
-	fma.rn.f32 	%f1772, %f1765, %f2018, %f1769;
-	fma.rn.f32 	%f1773, %f1765, %f2017, %f1770;
-	fma.rn.f32 	%f1774, %f1765, %f2016, %f1771;
-	mul.f32 	%f1775, %f1772, %f1772;
-	fma.rn.f32 	%f1776, %f1773, %f1773, %f1775;
-	fma.rn.f32 	%f1777, %f1774, %f1774, %f1776;
-	sqrt.rn.f32 	%f1778, %f1777;
-	rcp.rn.f32 	%f1779, %f1778;
-	mul.f32 	%f1780, %f1779, %f1772;
-	mul.f32 	%f1781, %f1779, %f1773;
-	mul.f32 	%f1782, %f1779, %f1774;
-	mul.f32 	%f1783, %f2086, %f2012;
-	fma.rn.f32 	%f1784, %f2087, %f2015, %f1783;
-	mul.f32 	%f1785, %f2086, %f2011;
-	fma.rn.f32 	%f1786, %f2087, %f2014, %f1785;
-	mul.f32 	%f1787, %f2086, %f2010;
-	fma.rn.f32 	%f1788, %f2087, %f2013, %f1787;
-	fma.rn.f32 	%f1789, %f2088, %f2018, %f1784;
-	fma.rn.f32 	%f1790, %f2088, %f2017, %f1786;
-	fma.rn.f32 	%f1791, %f2088, %f2016, %f1788;
-	mul.f32 	%f1792, %f1789, %f1779;
-	mul.f32 	%f1793, %f1790, %f1779;
-	mul.f32 	%f1794, %f1791, %f1779;
-	mul.f32 	%f1795, %f2083, %f2012;
-	fma.rn.f32 	%f1796, %f2084, %f2015, %f1795;
-	mul.f32 	%f1797, %f2083, %f2011;
-	fma.rn.f32 	%f1798, %f2084, %f2014, %f1797;
-	mul.f32 	%f1799, %f2083, %f2010;
-	fma.rn.f32 	%f1800, %f2084, %f2013, %f1799;
-	fma.rn.f32 	%f1801, %f2085, %f2018, %f1796;
-	fma.rn.f32 	%f1802, %f2085, %f2017, %f1798;
-	fma.rn.f32 	%f1803, %f2085, %f2016, %f1800;
-	mul.f32 	%f1804, %f1801, %f1779;
-	mul.f32 	%f1805, %f1802, %f1779;
-	mul.f32 	%f1806, %f1803, %f1779;
-	mul.f32 	%f1807, %f1780, %f1792;
-	fma.rn.f32 	%f1808, %f1781, %f1793, %f1807;
-	fma.rn.f32 	%f1809, %f1782, %f1794, %f1808;
-	mul.f32 	%f1810, %f1780, %f1809;
-	mul.f32 	%f1811, %f1781, %f1809;
-	mul.f32 	%f1812, %f1782, %f1809;
-	sub.f32 	%f2086, %f1792, %f1810;
-	sub.f32 	%f2087, %f1793, %f1811;
-	sub.f32 	%f2088, %f1794, %f1812;
-	mul.f32 	%f1813, %f1780, %f1804;
-	fma.rn.f32 	%f1814, %f1781, %f1805, %f1813;
-	fma.rn.f32 	%f1815, %f1782, %f1806, %f1814;
-	mul.f32 	%f1816, %f1780, %f1815;
-	mul.f32 	%f1817, %f1781, %f1815;
-	mul.f32 	%f1818, %f1782, %f1815;
-	sub.f32 	%f2083, %f1804, %f1816;
-	sub.f32 	%f2084, %f1805, %f1817;
-	sub.f32 	%f2085, %f1806, %f1818;
-
-BB8_103:
-	st.global.u32 	[%rd26], %r343;
-	bra.uni 	BB8_104;
-
-BB8_57:
-	mov.f32 	%f2095, %f2098;
-	mov.f32 	%f2096, %f2099;
-	mov.f32 	%f2097, %f2100;
-
-BB8_104:
-	ld.const.u64 	%rd562, [params+328];
-	cvta.to.global.u64 	%rd563, %rd562;
-	shl.b64 	%rd564, %rd25, 3;
-	add.s64 	%rd565, %rd563, %rd564;
-	st.global.u64 	[%rd565], %rd24;
-	ld.const.u64 	%rd566, [params+336];
-	cvta.to.global.u64 	%rd567, %rd566;
-	shl.b64 	%rd568, %rd25, 2;
-	add.s64 	%rd569, %rd567, %rd568;
-	mov.u32 	%r643, 0;
-	st.global.u32 	[%rd569], %r643;
-	ld.const.u64 	%rd570, [params+160];
-	cvta.to.global.u64 	%rd571, %rd570;
-	add.s64 	%rd572, %rd571, %rd568;
-	st.global.f32 	[%rd572], %f2101;
-	ld.const.u64 	%rd573, [params+168];
+	mul.f32 	%f1682, %f1681, %f2076;
+	sub.f32 	%f1683, %f1678, %f1682;
+	mul.f32 	%f1684, %f2069, %f2073;
+	mul.f32 	%f1685, %f2070, %f2072;
+	sub.f32 	%f1686, %f1685, %f1684;
+	fma.rn.f32 	%f1687, %f1686, %f2077, %f1683;
+	rcp.rn.f32 	%f1688, %f1687;
+	mul.f32 	%f2084, %f1677, %f1688;
+	mul.f32 	%f1689, %f2071, %f2076;
+	mul.f32 	%f1690, %f2070, %f2077;
+	sub.f32 	%f1691, %f1690, %f1689;
+	mul.f32 	%f2085, %f1691, %f1688;
+	mul.f32 	%f1692, %f2073, %f2077;
+	mul.f32 	%f1693, %f2074, %f2076;
+	sub.f32 	%f1694, %f1693, %f1692;
+	mul.f32 	%f2086, %f1694, %f1688;
+	sub.f32 	%f1695, %f1679, %f1680;
+	mul.f32 	%f2081, %f1695, %f1688;
+	mul.f32 	%f1696, %f2069, %f2077;
+	mul.f32 	%f1697, %f2071, %f2075;
+	sub.f32 	%f1698, %f1697, %f1696;
+	mul.f32 	%f2082, %f1698, %f1688;
+	mul.f32 	%f1699, %f2074, %f2075;
+	mul.f32 	%f1700, %f2072, %f2077;
+	sub.f32 	%f1701, %f1700, %f1699;
+	mul.f32 	%f2083, %f1701, %f1688;
+	mul.f32 	%f2078, %f1686, %f1688;
+	mul.f32 	%f1702, %f2070, %f2075;
+	mul.f32 	%f1703, %f2069, %f2076;
+	sub.f32 	%f1704, %f1703, %f1702;
+	mul.f32 	%f2079, %f1704, %f1688;
+	mul.f32 	%f1705, %f2072, %f2076;
+	mul.f32 	%f1706, %f2073, %f2075;
+	sub.f32 	%f1707, %f1706, %f1705;
+	mul.f32 	%f2080, %f1707, %f1688;
+	bra.uni 	$L__BB8_93;
+
+$L__BB8_84:
+	// begin inline asm
+	call (%rd652), _optix_get_instance_inverse_transform_from_handle, (%rd425);
+	// end inline asm
+
+$L__BB8_85:
+	// begin inline asm
+	cvta.to.global.u64 %rd431, %rd652;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r507,%r508,%r509,%r510}, [%rd431];
+	// end inline asm
+	mov.b32 	%f2084, %r507;
+	mov.b32 	%f2085, %r508;
+	mov.b32 	%f2086, %r509;
+	add.s64 	%rd435, %rd652, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd434, %rd435;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r511,%r512,%r513,%r514}, [%rd434];
+	// end inline asm
+	mov.b32 	%f2081, %r511;
+	mov.b32 	%f2082, %r512;
+	mov.b32 	%f2083, %r513;
+	add.s64 	%rd438, %rd652, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd437, %rd438;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r515,%r516,%r517,%r518}, [%rd437];
+	// end inline asm
+	mov.b32 	%f2078, %r515;
+	mov.b32 	%f2079, %r516;
+	mov.b32 	%f2080, %r517;
+
+$L__BB8_93:
+	setp.eq.s32 	%p63, %r659, 0;
+	@%p63 bra 	$L__BB8_95;
+
+	mul.f32 	%f1708, %f2055, %f2085;
+	fma.rn.f32 	%f1709, %f2052, %f2084, %f1708;
+	fma.rn.f32 	%f644, %f2058, %f2086, %f1709;
+	mul.f32 	%f1710, %f2054, %f2085;
+	fma.rn.f32 	%f1711, %f2051, %f2084, %f1710;
+	fma.rn.f32 	%f645, %f2057, %f2086, %f1711;
+	mul.f32 	%f1712, %f2053, %f2085;
+	fma.rn.f32 	%f1713, %f2050, %f2084, %f1712;
+	fma.rn.f32 	%f2086, %f2056, %f2086, %f1713;
+	mul.f32 	%f1714, %f2055, %f2082;
+	fma.rn.f32 	%f1715, %f2052, %f2081, %f1714;
+	fma.rn.f32 	%f647, %f2058, %f2083, %f1715;
+	mul.f32 	%f1716, %f2054, %f2082;
+	fma.rn.f32 	%f1717, %f2051, %f2081, %f1716;
+	fma.rn.f32 	%f648, %f2057, %f2083, %f1717;
+	mul.f32 	%f1718, %f2053, %f2082;
+	fma.rn.f32 	%f1719, %f2050, %f2081, %f1718;
+	fma.rn.f32 	%f2083, %f2056, %f2083, %f1719;
+	mul.f32 	%f1720, %f2055, %f2079;
+	fma.rn.f32 	%f1721, %f2052, %f2078, %f1720;
+	fma.rn.f32 	%f650, %f2058, %f2080, %f1721;
+	mul.f32 	%f1722, %f2054, %f2079;
+	fma.rn.f32 	%f1723, %f2051, %f2078, %f1722;
+	fma.rn.f32 	%f651, %f2057, %f2080, %f1723;
+	mul.f32 	%f1724, %f2053, %f2079;
+	fma.rn.f32 	%f1725, %f2050, %f2078, %f1724;
+	fma.rn.f32 	%f2080, %f2056, %f2080, %f1725;
+	mov.f32 	%f2078, %f650;
+	mov.f32 	%f2079, %f651;
+	mov.f32 	%f2081, %f647;
+	mov.f32 	%f2082, %f648;
+	mov.f32 	%f2084, %f644;
+	mov.f32 	%f2085, %f645;
+
+$L__BB8_95:
+	add.s32 	%r659, %r659, 1;
+	setp.lt.u32 	%p64, %r659, %r502;
+	mov.f32 	%f2050, %f2086;
+	mov.f32 	%f2051, %f2085;
+	mov.f32 	%f2052, %f2084;
+	mov.f32 	%f2053, %f2083;
+	mov.f32 	%f2054, %f2082;
+	mov.f32 	%f2055, %f2081;
+	mov.f32 	%f2056, %f2080;
+	mov.f32 	%f2057, %f2079;
+	mov.f32 	%f2058, %f2078;
+	@%p64 bra 	$L__BB8_80;
+
+$L__BB8_96:
+	fma.rn.f32 	%f1726, %f2141, %f2022, %f2025;
+	fma.rn.f32 	%f1727, %f2142, %f2023, %f1726;
+	fma.rn.f32 	%f1728, %f2141, %f2018, %f2021;
+	fma.rn.f32 	%f1729, %f2142, %f2019, %f1728;
+	fma.rn.f32 	%f1730, %f2141, %f2014, %f2017;
+	fma.rn.f32 	%f1731, %f2142, %f2015, %f1730;
+	fma.rn.f32 	%f2141, %f2143, %f2024, %f1727;
+	fma.rn.f32 	%f2142, %f2143, %f2020, %f1729;
+	fma.rn.f32 	%f2143, %f2143, %f2016, %f1731;
+	ld.const.u64 	%rd544, [params+112];
+	setp.eq.s64 	%p65, %rd544, 0;
+	mov.f32 	%f2105, %f2108;
+	mov.f32 	%f2106, %f2109;
+	mov.f32 	%f2107, %f2110;
+	@%p65 bra 	$L__BB8_98;
+
+	mul.f32 	%f1732, %f2108, %f2084;
+	fma.rn.f32 	%f1733, %f2109, %f2081, %f1732;
+	mul.f32 	%f1734, %f2108, %f2085;
+	fma.rn.f32 	%f1735, %f2109, %f2082, %f1734;
+	mul.f32 	%f1736, %f2108, %f2086;
+	fma.rn.f32 	%f1737, %f2109, %f2083, %f1736;
+	fma.rn.f32 	%f1738, %f2110, %f2078, %f1733;
+	fma.rn.f32 	%f1739, %f2110, %f2079, %f1735;
+	fma.rn.f32 	%f1740, %f2110, %f2080, %f1737;
+	mul.f32 	%f1741, %f1738, %f1738;
+	fma.rn.f32 	%f1742, %f1739, %f1739, %f1741;
+	fma.rn.f32 	%f1743, %f1740, %f1740, %f1742;
+	sqrt.rn.f32 	%f1744, %f1743;
+	div.rn.f32 	%f2105, %f1738, %f1744;
+	div.rn.f32 	%f2106, %f1739, %f1744;
+	div.rn.f32 	%f2107, %f1740, %f1744;
+
+$L__BB8_98:
+	ld.const.u64 	%rd545, [params+136];
+	setp.eq.s64 	%p66, %rd545, 0;
+	@%p66 bra 	$L__BB8_100;
+
+	mul.f32 	%f1745, %f2108, %f2084;
+	fma.rn.f32 	%f1746, %f2109, %f2081, %f1745;
+	mul.f32 	%f1747, %f2108, %f2085;
+	fma.rn.f32 	%f1748, %f2109, %f2082, %f1747;
+	mul.f32 	%f1749, %f2108, %f2086;
+	fma.rn.f32 	%f1750, %f2109, %f2083, %f1749;
+	fma.rn.f32 	%f1751, %f2110, %f2078, %f1746;
+	fma.rn.f32 	%f1752, %f2110, %f2079, %f1748;
+	fma.rn.f32 	%f1753, %f2110, %f2080, %f1750;
+	mul.f32 	%f1754, %f1751, %f1751;
+	fma.rn.f32 	%f1755, %f1752, %f1752, %f1754;
+	fma.rn.f32 	%f1756, %f1753, %f1753, %f1755;
+	sqrt.rn.f32 	%f1757, %f1756;
+	div.rn.f32 	%f2108, %f1751, %f1757;
+	div.rn.f32 	%f2109, %f1752, %f1757;
+	div.rn.f32 	%f2110, %f1753, %f1757;
+
+$L__BB8_100:
+	mov.f32 	%f2140, %f2110;
+	mov.f32 	%f2139, %f2109;
+	mov.f32 	%f2138, %f2108;
+	ld.const.u64 	%rd546, [params+184];
+	setp.eq.s64 	%p67, %rd546, 0;
+	@%p67 bra 	$L__BB8_102;
+
+	mul.f32 	%f1758, %f2132, %f2022;
+	fma.rn.f32 	%f1759, %f2133, %f2023, %f1758;
+	mul.f32 	%f1760, %f2132, %f2018;
+	fma.rn.f32 	%f1761, %f2133, %f2019, %f1760;
+	mul.f32 	%f1762, %f2132, %f2014;
+	fma.rn.f32 	%f1763, %f2133, %f2015, %f1762;
+	fma.rn.f32 	%f2132, %f2134, %f2024, %f1759;
+	fma.rn.f32 	%f2133, %f2134, %f2020, %f1761;
+	fma.rn.f32 	%f2134, %f2134, %f2016, %f1763;
+	mul.f32 	%f1764, %f2129, %f2022;
+	fma.rn.f32 	%f1765, %f2130, %f2023, %f1764;
+	mul.f32 	%f1766, %f2129, %f2018;
+	fma.rn.f32 	%f1767, %f2130, %f2019, %f1766;
+	mul.f32 	%f1768, %f2129, %f2014;
+	fma.rn.f32 	%f1769, %f2130, %f2015, %f1768;
+	fma.rn.f32 	%f2129, %f2131, %f2024, %f1765;
+	fma.rn.f32 	%f2130, %f2131, %f2020, %f1767;
+	fma.rn.f32 	%f2131, %f2131, %f2016, %f1769;
+
+$L__BB8_102:
+	ld.const.u64 	%rd547, [params+232];
+	ld.const.u64 	%rd548, [params+280];
+	or.b64  	%rd549, %rd547, %rd548;
+	setp.eq.s64 	%p68, %rd549, 0;
+	@%p68 bra 	$L__BB8_104;
+
+	mul.f32 	%f1770, %f2138, %f2022;
+	fma.rn.f32 	%f1771, %f2139, %f2018, %f1770;
+	mul.f32 	%f1772, %f2138, %f2023;
+	fma.rn.f32 	%f1773, %f2139, %f2019, %f1772;
+	mul.f32 	%f1774, %f2138, %f2024;
+	fma.rn.f32 	%f1775, %f2139, %f2020, %f1774;
+	fma.rn.f32 	%f1776, %f2140, %f2014, %f1771;
+	fma.rn.f32 	%f1777, %f2140, %f2015, %f1773;
+	fma.rn.f32 	%f1778, %f2140, %f2016, %f1775;
+	mul.f32 	%f1779, %f1776, %f1776;
+	fma.rn.f32 	%f1780, %f1777, %f1777, %f1779;
+	fma.rn.f32 	%f1781, %f1778, %f1778, %f1780;
+	sqrt.rn.f32 	%f1782, %f1781;
+	div.rn.f32 	%f1783, %f1776, %f1782;
+	div.rn.f32 	%f1784, %f1777, %f1782;
+	div.rn.f32 	%f1785, %f1778, %f1782;
+	mul.f32 	%f1786, %f1783, %f2084;
+	mul.f32 	%f1787, %f1783, %f2085;
+	mul.f32 	%f1788, %f1783, %f2086;
+	fma.rn.f32 	%f1789, %f1784, %f2081, %f1786;
+	fma.rn.f32 	%f1790, %f1784, %f2082, %f1787;
+	fma.rn.f32 	%f1791, %f1784, %f2083, %f1788;
+	fma.rn.f32 	%f1792, %f1785, %f2078, %f1789;
+	fma.rn.f32 	%f1793, %f1785, %f2079, %f1790;
+	fma.rn.f32 	%f1794, %f1785, %f2080, %f1791;
+	mul.f32 	%f1795, %f1792, %f1792;
+	fma.rn.f32 	%f1796, %f1793, %f1793, %f1795;
+	fma.rn.f32 	%f1797, %f1794, %f1794, %f1796;
+	sqrt.rn.f32 	%f1798, %f1797;
+	rcp.rn.f32 	%f1799, %f1798;
+	mul.f32 	%f1800, %f1799, %f1792;
+	mul.f32 	%f1801, %f1799, %f1793;
+	mul.f32 	%f1802, %f1799, %f1794;
+	mul.f32 	%f1803, %f2120, %f2084;
+	fma.rn.f32 	%f1804, %f2121, %f2081, %f1803;
+	mul.f32 	%f1805, %f2120, %f2085;
+	fma.rn.f32 	%f1806, %f2121, %f2082, %f1805;
+	mul.f32 	%f1807, %f2120, %f2086;
+	fma.rn.f32 	%f1808, %f2121, %f2083, %f1807;
+	fma.rn.f32 	%f1809, %f2122, %f2078, %f1804;
+	fma.rn.f32 	%f1810, %f2122, %f2079, %f1806;
+	fma.rn.f32 	%f1811, %f2122, %f2080, %f1808;
+	mul.f32 	%f1812, %f1809, %f1799;
+	mul.f32 	%f1813, %f1810, %f1799;
+	mul.f32 	%f1814, %f1811, %f1799;
+	mul.f32 	%f1815, %f2117, %f2084;
+	fma.rn.f32 	%f1816, %f2118, %f2081, %f1815;
+	mul.f32 	%f1817, %f2117, %f2085;
+	fma.rn.f32 	%f1818, %f2118, %f2082, %f1817;
+	mul.f32 	%f1819, %f2117, %f2086;
+	fma.rn.f32 	%f1820, %f2118, %f2083, %f1819;
+	fma.rn.f32 	%f1821, %f2119, %f2078, %f1816;
+	fma.rn.f32 	%f1822, %f2119, %f2079, %f1818;
+	fma.rn.f32 	%f1823, %f2119, %f2080, %f1820;
+	mul.f32 	%f1824, %f1821, %f1799;
+	mul.f32 	%f1825, %f1822, %f1799;
+	mul.f32 	%f1826, %f1823, %f1799;
+	mul.f32 	%f1827, %f1800, %f1812;
+	fma.rn.f32 	%f1828, %f1801, %f1813, %f1827;
+	fma.rn.f32 	%f1829, %f1802, %f1814, %f1828;
+	mul.f32 	%f1830, %f1800, %f1829;
+	mul.f32 	%f1831, %f1801, %f1829;
+	mul.f32 	%f1832, %f1802, %f1829;
+	sub.f32 	%f2120, %f1812, %f1830;
+	sub.f32 	%f2121, %f1813, %f1831;
+	sub.f32 	%f2122, %f1814, %f1832;
+	mul.f32 	%f1833, %f1800, %f1824;
+	fma.rn.f32 	%f1834, %f1801, %f1825, %f1833;
+	fma.rn.f32 	%f1835, %f1802, %f1826, %f1834;
+	mul.f32 	%f1836, %f1800, %f1835;
+	mul.f32 	%f1837, %f1801, %f1835;
+	mul.f32 	%f1838, %f1802, %f1835;
+	sub.f32 	%f2117, %f1824, %f1836;
+	sub.f32 	%f2118, %f1825, %f1837;
+	sub.f32 	%f2119, %f1826, %f1838;
+
+$L__BB8_104:
+	st.global.u32 	[%rd24], %r348;
+	mov.f32 	%f2108, %f2105;
+	mov.f32 	%f2109, %f2106;
+	mov.f32 	%f2110, %f2107;
+
+$L__BB8_105:
+	ld.const.u64 	%rd550, [params+328];
+	cvta.to.global.u64 	%rd551, %rd550;
+	shl.b64 	%rd552, %rd23, 3;
+	add.s64 	%rd553, %rd551, %rd552;
+	st.global.u64 	[%rd553], %rd22;
+	ld.const.u64 	%rd554, [params+336];
+	cvta.to.global.u64 	%rd555, %rd554;
+	shl.b64 	%rd556, %rd23, 2;
+	add.s64 	%rd557, %rd555, %rd556;
+	mov.u32 	%r653, 0;
+	st.global.u32 	[%rd557], %r653;
+	ld.const.u64 	%rd558, [params+160];
+	cvta.to.global.u64 	%rd559, %rd558;
+	add.s64 	%rd560, %rd559, %rd556;
+	st.global.f32 	[%rd560], %f2141;
+	ld.const.u64 	%rd561, [params+168];
+	cvta.to.global.u64 	%rd562, %rd561;
+	add.s64 	%rd563, %rd562, %rd556;
+	st.global.f32 	[%rd563], %f2142;
+	ld.const.u64 	%rd564, [params+176];
+	cvta.to.global.u64 	%rd565, %rd564;
+	add.s64 	%rd566, %rd565, %rd556;
+	st.global.f32 	[%rd566], %f2143;
+	ld.const.u64 	%rd567, [params+72];
+	cvta.to.global.u64 	%rd568, %rd567;
+	add.s64 	%rd569, %rd568, %rd556;
+	st.global.f32 	[%rd569], %f1179;
+	@%p26 bra 	$L__BB8_107;
+
+	cvta.to.global.u64 	%rd570, %rd21;
+	add.s64 	%rd572, %rd570, %rd556;
+	st.global.f32 	[%rd572], %f1985;
+	ld.const.u64 	%rd573, [params+104];
 	cvta.to.global.u64 	%rd574, %rd573;
-	add.s64 	%rd575, %rd574, %rd568;
-	st.global.f32 	[%rd575], %f2102;
-	ld.const.u64 	%rd576, [params+176];
-	cvta.to.global.u64 	%rd577, %rd576;
-	add.s64 	%rd578, %rd577, %rd568;
-	st.global.f32 	[%rd578], %f2103;
-	ld.const.u64 	%rd579, [params+72];
+	add.s64 	%rd575, %rd574, %rd556;
+	st.global.f32 	[%rd575], %f1984;
+
+$L__BB8_107:
+	ld.const.u64 	%rd39, [params+112];
+	setp.eq.s64 	%p70, %rd39, 0;
+	@%p70 bra 	$L__BB8_109;
+
+	cvta.to.global.u64 	%rd576, %rd39;
+	add.s64 	%rd578, %rd576, %rd556;
+	st.global.f32 	[%rd578], %f2108;
+	ld.const.u64 	%rd579, [params+120];
 	cvta.to.global.u64 	%rd580, %rd579;
-	add.s64 	%rd581, %rd580, %rd568;
-	st.global.f32 	[%rd581], %f1149;
-	@%p24 bra 	BB8_106;
-
-	cvta.to.global.u64 	%rd582, %rd23;
-	add.s64 	%rd584, %rd582, %rd568;
-	st.global.f32 	[%rd584], %f1945;
-	ld.const.u64 	%rd585, [params+104];
-	cvta.to.global.u64 	%rd586, %rd585;
-	add.s64 	%rd587, %rd586, %rd568;
-	st.global.f32 	[%rd587], %f1944;
-
-BB8_106:
-	ld.const.u64 	%rd43, [params+112];
-	setp.eq.s64	%p66, %rd43, 0;
-	@%p66 bra 	BB8_108;
-
-	cvta.to.global.u64 	%rd588, %rd43;
-	add.s64 	%rd590, %rd588, %rd568;
-	st.global.f32 	[%rd590], %f2095;
-	ld.const.u64 	%rd591, [params+120];
+	add.s64 	%rd581, %rd580, %rd556;
+	st.global.f32 	[%rd581], %f2109;
+	ld.const.u64 	%rd582, [params+128];
+	cvta.to.global.u64 	%rd583, %rd582;
+	add.s64 	%rd584, %rd583, %rd556;
+	st.global.f32 	[%rd584], %f2110;
+
+$L__BB8_109:
+	ld.const.u64 	%rd40, [params+136];
+	setp.eq.s64 	%p71, %rd40, 0;
+	@%p71 bra 	$L__BB8_111;
+
+	cvta.to.global.u64 	%rd585, %rd40;
+	add.s64 	%rd587, %rd585, %rd556;
+	st.global.f32 	[%rd587], %f2138;
+	ld.const.u64 	%rd588, [params+144];
+	cvta.to.global.u64 	%rd589, %rd588;
+	add.s64 	%rd590, %rd589, %rd556;
+	st.global.f32 	[%rd590], %f2139;
+	ld.const.u64 	%rd591, [params+152];
 	cvta.to.global.u64 	%rd592, %rd591;
-	add.s64 	%rd593, %rd592, %rd568;
-	st.global.f32 	[%rd593], %f2096;
-	ld.const.u64 	%rd594, [params+128];
-	cvta.to.global.u64 	%rd595, %rd594;
-	add.s64 	%rd596, %rd595, %rd568;
-	st.global.f32 	[%rd596], %f2097;
-
-BB8_108:
-	ld.const.u64 	%rd44, [params+136];
-	setp.eq.s64	%p67, %rd44, 0;
-	@%p67 bra 	BB8_110;
-
-	cvta.to.global.u64 	%rd597, %rd44;
-	add.s64 	%rd599, %rd597, %rd568;
-	st.global.f32 	[%rd599], %f2098;
-	ld.const.u64 	%rd600, [params+144];
+	add.s64 	%rd593, %rd592, %rd556;
+	st.global.f32 	[%rd593], %f2140;
+
+$L__BB8_111:
+	ld.const.u64 	%rd41, [params+184];
+	setp.eq.s64 	%p72, %rd41, 0;
+	@%p72 bra 	$L__BB8_113;
+
+	cvta.to.global.u64 	%rd594, %rd41;
+	add.s64 	%rd596, %rd594, %rd556;
+	st.global.f32 	[%rd596], %f2132;
+	ld.const.u64 	%rd597, [params+192];
+	cvta.to.global.u64 	%rd598, %rd597;
+	add.s64 	%rd599, %rd598, %rd556;
+	st.global.f32 	[%rd599], %f2133;
+	ld.const.u64 	%rd600, [params+200];
 	cvta.to.global.u64 	%rd601, %rd600;
-	add.s64 	%rd602, %rd601, %rd568;
-	st.global.f32 	[%rd602], %f2099;
-	ld.const.u64 	%rd603, [params+152];
+	add.s64 	%rd602, %rd601, %rd556;
+	st.global.f32 	[%rd602], %f2134;
+	ld.const.u64 	%rd603, [params+208];
 	cvta.to.global.u64 	%rd604, %rd603;
-	add.s64 	%rd605, %rd604, %rd568;
-	st.global.f32 	[%rd605], %f2100;
-
-BB8_110:
-	ld.const.u64 	%rd45, [params+184];
-	setp.eq.s64	%p68, %rd45, 0;
-	@%p68 bra 	BB8_112;
-
-	cvta.to.global.u64 	%rd606, %rd45;
-	add.s64 	%rd608, %rd606, %rd568;
-	st.global.f32 	[%rd608], %f342;
-	ld.const.u64 	%rd609, [params+192];
+	add.s64 	%rd605, %rd604, %rd556;
+	st.global.f32 	[%rd605], %f2129;
+	ld.const.u64 	%rd606, [params+216];
+	cvta.to.global.u64 	%rd607, %rd606;
+	add.s64 	%rd608, %rd607, %rd556;
+	st.global.f32 	[%rd608], %f2130;
+	ld.const.u64 	%rd609, [params+224];
 	cvta.to.global.u64 	%rd610, %rd609;
-	add.s64 	%rd611, %rd610, %rd568;
-	st.global.f32 	[%rd611], %f341;
-	ld.const.u64 	%rd612, [params+200];
-	cvta.to.global.u64 	%rd613, %rd612;
-	add.s64 	%rd614, %rd613, %rd568;
-	st.global.f32 	[%rd614], %f340;
-	ld.const.u64 	%rd615, [params+208];
+	add.s64 	%rd611, %rd610, %rd556;
+	st.global.f32 	[%rd611], %f2131;
+
+$L__BB8_113:
+	ld.const.u64 	%rd42, [params+232];
+	setp.eq.s64 	%p73, %rd42, 0;
+	@%p73 bra 	$L__BB8_115;
+
+	cvta.to.global.u64 	%rd612, %rd42;
+	add.s64 	%rd614, %rd612, %rd556;
+	st.global.f32 	[%rd614], %f2120;
+	ld.const.u64 	%rd615, [params+240];
 	cvta.to.global.u64 	%rd616, %rd615;
-	add.s64 	%rd617, %rd616, %rd568;
-	st.global.f32 	[%rd617], %f345;
-	ld.const.u64 	%rd618, [params+216];
+	add.s64 	%rd617, %rd616, %rd556;
+	st.global.f32 	[%rd617], %f2121;
+	ld.const.u64 	%rd618, [params+248];
 	cvta.to.global.u64 	%rd619, %rd618;
-	add.s64 	%rd620, %rd619, %rd568;
-	st.global.f32 	[%rd620], %f344;
-	ld.const.u64 	%rd621, [params+224];
+	add.s64 	%rd620, %rd619, %rd556;
+	st.global.f32 	[%rd620], %f2122;
+	ld.const.u64 	%rd621, [params+256];
 	cvta.to.global.u64 	%rd622, %rd621;
-	add.s64 	%rd623, %rd622, %rd568;
-	st.global.f32 	[%rd623], %f343;
-
-BB8_112:
-	ld.const.u64 	%rd46, [params+232];
-	setp.eq.s64	%p69, %rd46, 0;
-	@%p69 bra 	BB8_114;
-
-	cvta.to.global.u64 	%rd624, %rd46;
-	add.s64 	%rd626, %rd624, %rd568;
-	st.global.f32 	[%rd626], %f2086;
-	ld.const.u64 	%rd627, [params+240];
+	add.s64 	%rd623, %rd622, %rd556;
+	st.global.f32 	[%rd623], %f2117;
+	ld.const.u64 	%rd624, [params+264];
+	cvta.to.global.u64 	%rd625, %rd624;
+	add.s64 	%rd626, %rd625, %rd556;
+	st.global.f32 	[%rd626], %f2118;
+	ld.const.u64 	%rd627, [params+272];
 	cvta.to.global.u64 	%rd628, %rd627;
-	add.s64 	%rd629, %rd628, %rd568;
-	st.global.f32 	[%rd629], %f2087;
-	ld.const.u64 	%rd630, [params+248];
-	cvta.to.global.u64 	%rd631, %rd630;
-	add.s64 	%rd632, %rd631, %rd568;
-	st.global.f32 	[%rd632], %f2088;
-	ld.const.u64 	%rd633, [params+256];
+	add.s64 	%rd629, %rd628, %rd556;
+	st.global.f32 	[%rd629], %f2119;
+
+$L__BB8_115:
+	ld.const.u64 	%rd43, [params+280];
+	setp.eq.s64 	%p74, %rd43, 0;
+	@%p74 bra 	$L__BB8_117;
+
+	cvta.to.global.u64 	%rd630, %rd43;
+	add.s64 	%rd632, %rd630, %rd556;
+	st.global.f32 	[%rd632], %f2120;
+	ld.const.u64 	%rd633, [params+288];
 	cvta.to.global.u64 	%rd634, %rd633;
-	add.s64 	%rd635, %rd634, %rd568;
-	st.global.f32 	[%rd635], %f2083;
-	ld.const.u64 	%rd636, [params+264];
+	add.s64 	%rd635, %rd634, %rd556;
+	st.global.f32 	[%rd635], %f2121;
+	ld.const.u64 	%rd636, [params+296];
 	cvta.to.global.u64 	%rd637, %rd636;
-	add.s64 	%rd638, %rd637, %rd568;
-	st.global.f32 	[%rd638], %f2084;
-	ld.const.u64 	%rd639, [params+272];
+	add.s64 	%rd638, %rd637, %rd556;
+	st.global.f32 	[%rd638], %f2122;
+	ld.const.u64 	%rd639, [params+304];
 	cvta.to.global.u64 	%rd640, %rd639;
-	add.s64 	%rd641, %rd640, %rd568;
-	st.global.f32 	[%rd641], %f2085;
-
-BB8_114:
-	ld.const.u64 	%rd47, [params+280];
-	setp.eq.s64	%p70, %rd47, 0;
-	@%p70 bra 	BB8_116;
-
-	cvta.to.global.u64 	%rd642, %rd47;
-	add.s64 	%rd644, %rd642, %rd568;
-	st.global.f32 	[%rd644], %f2086;
-	ld.const.u64 	%rd645, [params+288];
+	add.s64 	%rd641, %rd640, %rd556;
+	st.global.f32 	[%rd641], %f2117;
+	ld.const.u64 	%rd642, [params+312];
+	cvta.to.global.u64 	%rd643, %rd642;
+	add.s64 	%rd644, %rd643, %rd556;
+	st.global.f32 	[%rd644], %f2118;
+	ld.const.u64 	%rd645, [params+320];
 	cvta.to.global.u64 	%rd646, %rd645;
-	add.s64 	%rd647, %rd646, %rd568;
-	st.global.f32 	[%rd647], %f2087;
-	ld.const.u64 	%rd648, [params+296];
-	cvta.to.global.u64 	%rd649, %rd648;
-	add.s64 	%rd650, %rd649, %rd568;
-	st.global.f32 	[%rd650], %f2088;
-	ld.const.u64 	%rd651, [params+304];
-	cvta.to.global.u64 	%rd652, %rd651;
-	add.s64 	%rd653, %rd652, %rd568;
-	st.global.f32 	[%rd653], %f2083;
-	ld.const.u64 	%rd654, [params+312];
-	cvta.to.global.u64 	%rd655, %rd654;
-	add.s64 	%rd656, %rd655, %rd568;
-	st.global.f32 	[%rd656], %f2084;
-	ld.const.u64 	%rd657, [params+320];
-	cvta.to.global.u64 	%rd658, %rd657;
-	add.s64 	%rd659, %rd658, %rd568;
-	st.global.f32 	[%rd659], %f2085;
-
-BB8_116:
+	add.s64 	%rd647, %rd646, %rd556;
+	st.global.f32 	[%rd647], %f2119;
+
+$L__BB8_117:
 	ret;
-}
 
+}
 	// .globl	__intersection__asphsurf
-.visible .entry __intersection__asphsurf(
-
-)
+.visible .entry __intersection__asphsurf()
 {
-	.reg .pred 	%p<516>;
-	.reg .b16 	%rs<18>;
-	.reg .f32 	%f<1018>;
-	.reg .b32 	%r<904>;
-	.reg .f64 	%fd<482>;
-	.reg .b64 	%rd<271>;
-
-
-	// inline asm
-	call (%rd22), _optix_get_sbt_data_ptr_64, ();
-	// inline asm
-	ld.u64 	%rd1, [%rd22+8];
-	// inline asm
-	call (%f357), _optix_get_world_ray_origin_x, ();
-	// inline asm
-	// inline asm
-	call (%f358), _optix_get_world_ray_origin_y, ();
-	// inline asm
-	// inline asm
-	call (%f965), _optix_get_world_ray_origin_z, ();
-	// inline asm
-	// inline asm
-	call (%r32), _optix_get_transform_list_size, ();
-	// inline asm
-	setp.eq.s32	%p23, %r32, 0;
-	@%p23 bra 	BB9_1;
-
-	mov.u32 	%r902, 0;
-	// inline asm
-	call (%f360), _optix_get_ray_time, ();
-	// inline asm
-
-BB9_3:
+	.reg .pred 	%p<49>;
+	.reg .f32 	%f<1036>;
+	.reg .b32 	%r<323>;
+	.reg .b64 	%rd<259>;
+
+
+	// begin inline asm
+	call (%rd17), _optix_get_sbt_data_ptr_64, ();
+	// end inline asm
+	ld.u64 	%rd1, [%rd17+8];
+	// begin inline asm
+	call (%f973), _optix_get_world_ray_origin_x, ();
+	// end inline asm
+	// begin inline asm
+	call (%f974), _optix_get_world_ray_origin_y, ();
+	// end inline asm
+	// begin inline asm
+	call (%f975), _optix_get_world_ray_origin_z, ();
+	// end inline asm
+	// begin inline asm
+	call (%r9), _optix_get_transform_list_size, ();
+	// end inline asm
+	setp.eq.s32 	%p5, %r9, 0;
+	@%p5 bra 	$L__BB9_21;
+
+	// begin inline asm
+	call (%r10), _optix_get_transform_list_size, ();
+	// end inline asm
+	// begin inline asm
+	call (%f363), _optix_get_ray_time, ();
+	// end inline asm
+	setp.eq.s32 	%p6, %r10, 0;
+	@%p6 bra 	$L__BB9_19;
+
+	mov.u32 	%r321, 0;
+
+$L__BB9_3:
 	.pragma "nounroll";
-	// inline asm
-	call (%rd23), _optix_get_transform_list_handle, (%r902);
-	// inline asm
-	// inline asm
-	call (%r35), _optix_get_transform_type_from_handle, (%rd23);
-	// inline asm
-	and.b32  	%r36, %r35, -2;
-	setp.eq.s32	%p24, %r36, 2;
-	@%p24 bra 	BB9_9;
-	bra.uni 	BB9_4;
-
-BB9_9:
-	setp.eq.s32	%p27, %r35, 2;
-	@%p27 bra 	BB9_13;
-	bra.uni 	BB9_10;
-
-BB9_13:
-	// inline asm
-	call (%rd97), _optix_get_matrix_motion_transform_from_handle, (%rd23);
-	// inline asm
-	// inline asm
-	cvta.to.global.u64 %rd99, %rd97;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r124,%r125,%r126,%r127}, [%rd99];
-	// inline asm
-	mov.b32	{%rs4, %rs5}, %r126;
-	add.s64 	%rd103, %rd97, 16;
-	// inline asm
-	cvta.to.global.u64 %rd102, %rd103;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r128,%r129,%r130,%r131}, [%rd102];
-	// inline asm
-	add.s64 	%rd106, %rd97, 32;
-	// inline asm
-	cvta.to.global.u64 %rd105, %rd106;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r132,%r133,%r134,%r135}, [%rd105];
-	// inline asm
-	add.s64 	%rd109, %rd97, 48;
-	// inline asm
-	cvta.to.global.u64 %rd108, %rd109;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r136,%r137,%r138,%r139}, [%rd108];
-	// inline asm
-	add.s64 	%rd112, %rd97, 64;
-	// inline asm
-	cvta.to.global.u64 %rd111, %rd112;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r140,%r141,%r142,%r143}, [%rd111];
-	// inline asm
-	add.s64 	%rd115, %rd97, 80;
-	// inline asm
-	cvta.to.global.u64 %rd114, %rd115;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r144,%r145,%r146,%r147}, [%rd114];
-	// inline asm
-	add.s64 	%rd118, %rd97, 96;
-	// inline asm
-	cvta.to.global.u64 %rd117, %rd118;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r148,%r149,%r150,%r151}, [%rd117];
-	// inline asm
-	add.s64 	%rd121, %rd97, 112;
-	// inline asm
-	cvta.to.global.u64 %rd120, %rd121;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r152,%r153,%r154,%r155}, [%rd120];
-	// inline asm
-	mov.b32 	 %f487, %r127;
-	mov.b32 	 %f488, %r128;
-	cvt.u32.u16	%r168, %rs4;
-	add.s32 	%r169, %r168, -1;
-	cvt.rn.f32.s32	%f489, %r169;
-	sub.f32 	%f490, %f360, %f487;
-	mul.f32 	%f491, %f490, %f489;
-	sub.f32 	%f492, %f488, %f487;
-	div.rn.f32 	%f493, %f491, %f492;
-	min.f32 	%f494, %f489, %f493;
-	mov.f32 	%f495, 0f00000000;
-	max.f32 	%f496, %f495, %f494;
-	cvt.rmi.f32.f32	%f497, %f496;
-	cvt.rzi.s32.f32	%r170, %f497;
-	mul.wide.s32 	%rd132, %r170, 48;
-	add.s64 	%rd124, %rd106, %rd132;
-	// inline asm
-	cvta.to.global.u64 %rd123, %rd124;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r156,%r157,%r158,%r159}, [%rd123];
-	// inline asm
-	mov.b32 	 %f937, %r156;
-	mov.b32 	 %f938, %r157;
-	mov.b32 	 %f939, %r158;
-	mov.b32 	 %f940, %r159;
-	add.s64 	%rd127, %rd124, 16;
-	// inline asm
+	// begin inline asm
+	call (%rd18), _optix_get_transform_list_handle, (%r321);
+	// end inline asm
+	// begin inline asm
+	call (%r13), _optix_get_transform_type_from_handle, (%rd18);
+	// end inline asm
+	or.b32  	%r14, %r13, 1;
+	setp.eq.s32 	%p7, %r14, 3;
+	@%p7 bra 	$L__BB9_9;
+	bra.uni 	$L__BB9_4;
+
+$L__BB9_9:
+	setp.eq.s32 	%p10, %r13, 2;
+	@%p10 bra 	$L__BB9_13;
+	bra.uni 	$L__BB9_10;
+
+$L__BB9_13:
+	// begin inline asm
+	call (%rd90), _optix_get_matrix_motion_transform_from_handle, (%rd18);
+	// end inline asm
+	// begin inline asm
+	cvta.to.global.u64 %rd92, %rd90;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r102,%r103,%r104,%r105}, [%rd92];
+	// end inline asm
+	add.s64 	%rd96, %rd90, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd95, %rd96;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r106,%r107,%r108,%r109}, [%rd95];
+	// end inline asm
+	add.s64 	%rd99, %rd90, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd98, %rd99;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r110,%r111,%r112,%r113}, [%rd98];
+	// end inline asm
+	add.s64 	%rd102, %rd90, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd101, %rd102;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r114,%r115,%r116,%r117}, [%rd101];
+	// end inline asm
+	add.s64 	%rd105, %rd90, 64;
+	// begin inline asm
+	cvta.to.global.u64 %rd104, %rd105;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r118,%r119,%r120,%r121}, [%rd104];
+	// end inline asm
+	add.s64 	%rd108, %rd90, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd107, %rd108;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r122,%r123,%r124,%r125}, [%rd107];
+	// end inline asm
+	add.s64 	%rd111, %rd90, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd110, %rd111;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r126,%r127,%r128,%r129}, [%rd110];
+	// end inline asm
+	add.s64 	%rd114, %rd90, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd113, %rd114;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r130,%r131,%r132,%r133}, [%rd113];
+	// end inline asm
+	mov.b32 	%f491, %r105;
+	mov.b32 	%f492, %r106;
+	and.b32  	%r146, %r104, 65535;
+	add.s32 	%r147, %r146, -1;
+	cvt.rn.f32.s32 	%f493, %r147;
+	sub.f32 	%f494, %f363, %f491;
+	mul.f32 	%f495, %f494, %f493;
+	sub.f32 	%f496, %f492, %f491;
+	div.rn.f32 	%f497, %f495, %f496;
+	min.f32 	%f498, %f493, %f497;
+	mov.f32 	%f499, 0f00000000;
+	max.f32 	%f500, %f499, %f498;
+	cvt.rmi.f32.f32 	%f501, %f500;
+	sub.f32 	%f90, %f500, %f501;
+	cvt.rzi.s32.f32 	%r148, %f501;
+	mul.wide.s32 	%rd125, %r148, 48;
+	add.s64 	%rd117, %rd99, %rd125;
+	// begin inline asm
+	cvta.to.global.u64 %rd116, %rd117;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r134,%r135,%r136,%r137}, [%rd116];
+	// end inline asm
+	mov.b32 	%f928, %r134;
+	mov.b32 	%f927, %r135;
+	mov.b32 	%f926, %r136;
+	mov.b32 	%f925, %r137;
+	add.s64 	%rd120, %rd117, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd119, %rd120;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r138,%r139,%r140,%r141}, [%rd119];
+	// end inline asm
+	mov.b32 	%f932, %r138;
+	mov.b32 	%f931, %r139;
+	mov.b32 	%f930, %r140;
+	mov.b32 	%f929, %r141;
+	add.s64 	%rd123, %rd117, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd122, %rd123;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r142,%r143,%r144,%r145}, [%rd122];
+	// end inline asm
+	mov.b32 	%f936, %r142;
+	mov.b32 	%f935, %r143;
+	mov.b32 	%f934, %r144;
+	mov.b32 	%f933, %r145;
+	setp.leu.f32 	%p12, %f90, 0f00000000;
+	@%p12 bra 	$L__BB9_15;
+
+	cvt.rmi.f32.f32 	%f896, %f500;
+	cvt.rzi.s32.f32 	%r320, %f896;
+	cvt.s64.s32 	%rd256, %r320;
+	mov.f32 	%f502, 0f3F800000;
+	sub.f32 	%f503, %f502, %f90;
+	mul.lo.s64 	%rd135, %rd256, 48;
+	add.s64 	%rd136, %rd90, %rd135;
+	add.s64 	%rd127, %rd136, 80;
+	// begin inline asm
 	cvta.to.global.u64 %rd126, %rd127;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r160,%r161,%r162,%r163}, [%rd126];
-	// inline asm
-	mov.b32 	 %f933, %r160;
-	mov.b32 	 %f934, %r161;
-	mov.b32 	 %f935, %r162;
-	mov.b32 	 %f936, %r163;
-	add.s64 	%rd130, %rd124, 32;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r149,%r150,%r151,%r152}, [%rd126];
+	// end inline asm
+	mov.b32 	%f504, %r149;
+	mov.b32 	%f505, %r150;
+	mov.b32 	%f506, %r151;
+	mov.b32 	%f507, %r152;
+	mul.f32 	%f508, %f90, %f504;
+	mul.f32 	%f509, %f90, %f505;
+	mul.f32 	%f510, %f90, %f506;
+	mul.f32 	%f511, %f90, %f507;
+	fma.rn.f32 	%f928, %f503, %f928, %f508;
+	fma.rn.f32 	%f927, %f503, %f927, %f509;
+	fma.rn.f32 	%f926, %f503, %f926, %f510;
+	fma.rn.f32 	%f925, %f503, %f925, %f511;
+	add.s64 	%rd130, %rd136, 96;
+	// begin inline asm
 	cvta.to.global.u64 %rd129, %rd130;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r164,%r165,%r166,%r167}, [%rd129];
-	// inline asm
-	sub.f32 	%f98, %f496, %f497;
-	mov.b32 	 %f929, %r164;
-	mov.b32 	 %f930, %r165;
-	mov.b32 	 %f931, %r166;
-	mov.b32 	 %f932, %r167;
-	setp.leu.f32	%p29, %f98, 0f00000000;
-	@%p29 bra 	BB9_15;
-
-	cvt.rmi.f32.f32	%f900, %f496;
-	cvt.rzi.s32.f32	%r901, %f900;
-	cvt.s64.s32	%rd268, %r901;
-	mul.lo.s64 	%rd142, %rd268, 48;
-	add.s64 	%rd143, %rd97, %rd142;
-	add.s64 	%rd134, %rd143, 80;
-	// inline asm
-	cvta.to.global.u64 %rd133, %rd134;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r171,%r172,%r173,%r174}, [%rd133];
-	// inline asm
-	mov.b32 	 %f498, %r171;
-	mov.b32 	 %f499, %r172;
-	mov.b32 	 %f500, %r173;
-	mov.b32 	 %f501, %r174;
-	add.s64 	%rd137, %rd143, 96;
-	// inline asm
-	cvta.to.global.u64 %rd136, %rd137;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r175,%r176,%r177,%r178}, [%rd136];
-	// inline asm
-	mov.b32 	 %f502, %r175;
-	mov.b32 	 %f503, %r176;
-	mov.b32 	 %f504, %r177;
-	mov.b32 	 %f505, %r178;
-	add.s64 	%rd140, %rd143, 112;
-	// inline asm
-	cvta.to.global.u64 %rd139, %rd140;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r179,%r180,%r181,%r182}, [%rd139];
-	// inline asm
-	mov.f32 	%f506, 0f3F800000;
-	sub.f32 	%f507, %f506, %f98;
-	mul.f32 	%f508, %f98, %f498;
-	mul.f32 	%f509, %f98, %f499;
-	mul.f32 	%f510, %f98, %f500;
-	mul.f32 	%f511, %f98, %f501;
-	fma.rn.f32 	%f937, %f507, %f937, %f508;
-	fma.rn.f32 	%f938, %f507, %f938, %f509;
-	fma.rn.f32 	%f939, %f507, %f939, %f510;
-	fma.rn.f32 	%f940, %f507, %f940, %f511;
-	mul.f32 	%f512, %f98, %f502;
-	mul.f32 	%f513, %f98, %f503;
-	mul.f32 	%f514, %f98, %f504;
-	mul.f32 	%f515, %f98, %f505;
-	fma.rn.f32 	%f933, %f507, %f933, %f512;
-	fma.rn.f32 	%f934, %f507, %f934, %f513;
-	fma.rn.f32 	%f935, %f507, %f935, %f514;
-	fma.rn.f32 	%f936, %f507, %f936, %f515;
-	mov.b32 	 %f516, %r179;
-	mov.b32 	 %f517, %r180;
-	mov.b32 	 %f518, %r181;
-	mov.b32 	 %f519, %r182;
-	mul.f32 	%f520, %f98, %f516;
-	mul.f32 	%f521, %f98, %f517;
-	mul.f32 	%f522, %f98, %f518;
-	mul.f32 	%f523, %f98, %f519;
-	fma.rn.f32 	%f929, %f507, %f929, %f520;
-	fma.rn.f32 	%f930, %f507, %f930, %f521;
-	fma.rn.f32 	%f931, %f507, %f931, %f522;
-	fma.rn.f32 	%f932, %f507, %f932, %f523;
-	bra.uni 	BB9_15;
-
-BB9_4:
-	mov.f32 	%f941, 0f00000000;
-	mov.f32 	%f943, 0f3F800000;
-	setp.eq.s32	%p25, %r35, 4;
-	@%p25 bra 	BB9_7;
-	bra.uni 	BB9_5;
-
-BB9_7:
-	// inline asm
-	call (%rd269), _optix_get_instance_inverse_transform_from_handle, (%rd23);
-	// inline asm
-	bra.uni 	BB9_8;
-
-BB9_10:
-	// inline asm
-	call (%rd38), _optix_get_srt_motion_transform_from_handle, (%rd23);
-	// inline asm
-	// inline asm
-	cvta.to.global.u64 %rd40, %rd38;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r49,%r50,%r51,%r52}, [%rd40];
-	// inline asm
-	mov.b32	{%rs2, %rs3}, %r51;
-	add.s64 	%rd44, %rd38, 16;
-	// inline asm
-	cvta.to.global.u64 %rd43, %rd44;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r53,%r54,%r55,%r56}, [%rd43];
-	// inline asm
-	add.s64 	%rd47, %rd38, 32;
-	// inline asm
-	cvta.to.global.u64 %rd46, %rd47;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r57,%r58,%r59,%r60}, [%rd46];
-	// inline asm
-	add.s64 	%rd50, %rd38, 48;
-	// inline asm
-	cvta.to.global.u64 %rd49, %rd50;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r61,%r62,%r63,%r64}, [%rd49];
-	// inline asm
-	add.s64 	%rd53, %rd38, 64;
-	// inline asm
-	cvta.to.global.u64 %rd52, %rd53;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r65,%r66,%r67,%r68}, [%rd52];
-	// inline asm
-	add.s64 	%rd56, %rd38, 80;
-	// inline asm
-	cvta.to.global.u64 %rd55, %rd56;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r69,%r70,%r71,%r72}, [%rd55];
-	// inline asm
-	add.s64 	%rd59, %rd38, 96;
-	// inline asm
-	cvta.to.global.u64 %rd58, %rd59;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r73,%r74,%r75,%r76}, [%rd58];
-	// inline asm
-	add.s64 	%rd62, %rd38, 112;
-	// inline asm
-	cvta.to.global.u64 %rd61, %rd62;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r77,%r78,%r79,%r80}, [%rd61];
-	// inline asm
-	add.s64 	%rd65, %rd38, 128;
-	// inline asm
-	cvta.to.global.u64 %rd64, %rd65;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r81,%r82,%r83,%r84}, [%rd64];
-	// inline asm
-	add.s64 	%rd68, %rd38, 144;
-	// inline asm
-	cvta.to.global.u64 %rd67, %rd68;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r85,%r86,%r87,%r88}, [%rd67];
-	// inline asm
-	mov.b32 	 %f374, %r52;
-	mov.b32 	 %f375, %r53;
-	cvt.u32.u16	%r105, %rs2;
-	add.s32 	%r106, %r105, -1;
-	cvt.rn.f32.s32	%f376, %r106;
-	sub.f32 	%f377, %f360, %f374;
-	mul.f32 	%f378, %f377, %f376;
-	sub.f32 	%f379, %f375, %f374;
-	div.rn.f32 	%f380, %f378, %f379;
-	min.f32 	%f381, %f376, %f380;
-	mov.f32 	%f382, 0f00000000;
-	max.f32 	%f383, %f382, %f381;
-	cvt.rmi.f32.f32	%f384, %f383;
-	cvt.rzi.s32.f32	%r107, %f384;
-	mul.wide.s32 	%rd82, %r107, 64;
-	add.s64 	%rd71, %rd47, %rd82;
-	// inline asm
-	cvta.to.global.u64 %rd70, %rd71;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r89,%r90,%r91,%r92}, [%rd70];
-	// inline asm
-	mov.b32 	 %f913, %r89;
-	mov.b32 	 %f914, %r90;
-	mov.b32 	 %f915, %r91;
-	mov.b32 	 %f916, %r92;
-	add.s64 	%rd74, %rd71, 16;
-	// inline asm
-	cvta.to.global.u64 %rd73, %rd74;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r93,%r94,%r95,%r96}, [%rd73];
-	// inline asm
-	mov.b32 	 %f917, %r93;
-	mov.b32 	 %f918, %r94;
-	mov.b32 	 %f919, %r95;
-	mov.b32 	 %f920, %r96;
-	add.s64 	%rd77, %rd71, 32;
-	// inline asm
-	cvta.to.global.u64 %rd76, %rd77;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r97,%r98,%r99,%r100}, [%rd76];
-	// inline asm
-	sub.f32 	%f37, %f383, %f384;
-	mov.b32 	 %f921, %r97;
-	mov.b32 	 %f922, %r98;
-	mov.b32 	 %f923, %r99;
-	mov.b32 	 %f924, %r100;
-	add.s64 	%rd80, %rd71, 48;
-	// inline asm
-	cvta.to.global.u64 %rd79, %rd80;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r101,%r102,%r103,%r104}, [%rd79];
-	// inline asm
-	mov.b32 	 %f925, %r101;
-	mov.b32 	 %f926, %r102;
-	mov.b32 	 %f927, %r103;
-	mov.b32 	 %f928, %r104;
-	setp.leu.f32	%p28, %f37, 0f00000000;
-	@%p28 bra 	BB9_12;
-
-	cvt.rmi.f32.f32	%f899, %f383;
-	cvt.rzi.s32.f32	%r900, %f899;
-	cvt.s64.s32	%rd267, %r900;
-	shl.b64 	%rd95, %rd267, 6;
-	add.s64 	%rd96, %rd95, %rd38;
-	add.s64 	%rd84, %rd96, 96;
-	// inline asm
-	cvta.to.global.u64 %rd83, %rd84;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r108,%r109,%r110,%r111}, [%rd83];
-	// inline asm
-	mov.b32 	 %f385, %r108;
-	mov.b32 	 %f386, %r109;
-	mov.b32 	 %f387, %r110;
-	mov.b32 	 %f388, %r111;
-	add.s64 	%rd87, %rd96, 112;
-	// inline asm
-	cvta.to.global.u64 %rd86, %rd87;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r112,%r113,%r114,%r115}, [%rd86];
-	// inline asm
-	mov.b32 	 %f389, %r112;
-	mov.b32 	 %f390, %r113;
-	mov.b32 	 %f391, %r114;
-	mov.b32 	 %f392, %r115;
-	add.s64 	%rd90, %rd96, 128;
-	// inline asm
-	cvta.to.global.u64 %rd89, %rd90;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r116,%r117,%r118,%r119}, [%rd89];
-	// inline asm
-	mov.b32 	 %f393, %r116;
-	mov.b32 	 %f394, %r117;
-	mov.b32 	 %f395, %r118;
-	mov.b32 	 %f396, %r119;
-	add.s64 	%rd93, %rd96, 144;
-	// inline asm
-	cvta.to.global.u64 %rd92, %rd93;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r120,%r121,%r122,%r123}, [%rd92];
-	// inline asm
-	mov.f32 	%f397, 0f3F800000;
-	sub.f32 	%f398, %f397, %f37;
-	mul.f32 	%f399, %f37, %f385;
-	mul.f32 	%f400, %f37, %f386;
-	mul.f32 	%f401, %f37, %f387;
-	mul.f32 	%f402, %f37, %f388;
-	fma.rn.f32 	%f913, %f398, %f913, %f399;
-	fma.rn.f32 	%f914, %f398, %f914, %f400;
-	fma.rn.f32 	%f915, %f398, %f915, %f401;
-	fma.rn.f32 	%f916, %f398, %f916, %f402;
-	mul.f32 	%f403, %f37, %f389;
-	mul.f32 	%f404, %f37, %f390;
-	mul.f32 	%f405, %f37, %f391;
-	mul.f32 	%f406, %f37, %f392;
-	fma.rn.f32 	%f917, %f398, %f917, %f403;
-	fma.rn.f32 	%f918, %f398, %f918, %f404;
-	fma.rn.f32 	%f919, %f398, %f919, %f405;
-	fma.rn.f32 	%f920, %f398, %f920, %f406;
-	mul.f32 	%f407, %f37, %f393;
-	mul.f32 	%f408, %f37, %f394;
-	mul.f32 	%f409, %f37, %f395;
-	mul.f32 	%f410, %f37, %f396;
-	fma.rn.f32 	%f921, %f398, %f921, %f407;
-	fma.rn.f32 	%f411, %f398, %f922, %f408;
-	fma.rn.f32 	%f412, %f398, %f923, %f409;
-	fma.rn.f32 	%f413, %f398, %f924, %f410;
-	mov.b32 	 %f414, %r120;
-	mov.b32 	 %f415, %r121;
-	mov.b32 	 %f416, %r122;
-	mov.b32 	 %f417, %r123;
-	mul.f32 	%f418, %f37, %f414;
-	mul.f32 	%f419, %f37, %f415;
-	mul.f32 	%f420, %f37, %f416;
-	mul.f32 	%f421, %f37, %f417;
-	fma.rn.f32 	%f422, %f398, %f925, %f418;
-	fma.rn.f32 	%f926, %f398, %f926, %f419;
-	fma.rn.f32 	%f927, %f398, %f927, %f420;
-	fma.rn.f32 	%f928, %f398, %f928, %f421;
-	mul.f32 	%f423, %f412, %f412;
-	fma.rn.f32 	%f424, %f411, %f411, %f423;
-	fma.rn.f32 	%f425, %f413, %f413, %f424;
-	fma.rn.f32 	%f426, %f422, %f422, %f425;
-	sqrt.rn.f32 	%f427, %f426;
-	rcp.rn.f32 	%f428, %f427;
-	mul.f32 	%f922, %f411, %f428;
-	mul.f32 	%f923, %f412, %f428;
-	mul.f32 	%f924, %f413, %f428;
-	mul.f32 	%f925, %f422, %f428;
-
-BB9_12:
-	mul.f32 	%f429, %f923, %f923;
-	fma.rn.f32 	%f430, %f922, %f922, %f429;
-	fma.rn.f32 	%f431, %f924, %f924, %f430;
-	fma.rn.f32 	%f432, %f925, %f925, %f431;
-	rcp.rn.f32 	%f433, %f432;
-	mul.f32 	%f434, %f922, %f433;
-	mul.f32 	%f435, %f923, %f433;
-	mul.f32 	%f436, %f924, %f433;
-	mul.f32 	%f437, %f925, %f433;
-	mul.f32 	%f438, %f922, %f434;
-	mul.f32 	%f439, %f923, %f435;
-	mul.f32 	%f440, %f924, %f436;
-	mul.f32 	%f441, %f922, %f435;
-	mul.f32 	%f442, %f924, %f437;
-	mul.f32 	%f443, %f922, %f436;
-	mul.f32 	%f444, %f923, %f437;
-	mul.f32 	%f445, %f923, %f436;
-	mul.f32 	%f446, %f922, %f437;
-	sub.f32 	%f447, %f438, %f439;
-	sub.f32 	%f448, %f447, %f440;
-	fma.rn.f32 	%f449, %f925, %f437, %f448;
-	sub.f32 	%f450, %f441, %f442;
-	add.f32 	%f451, %f450, %f450;
-	add.f32 	%f452, %f443, %f444;
-	add.f32 	%f453, %f452, %f452;
-	add.f32 	%f454, %f441, %f442;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r153,%r154,%r155,%r156}, [%rd129];
+	// end inline asm
+	mov.b32 	%f512, %r153;
+	mov.b32 	%f513, %r154;
+	mov.b32 	%f514, %r155;
+	mov.b32 	%f515, %r156;
+	mul.f32 	%f516, %f90, %f512;
+	mul.f32 	%f517, %f90, %f513;
+	mul.f32 	%f518, %f90, %f514;
+	mul.f32 	%f519, %f90, %f515;
+	fma.rn.f32 	%f932, %f503, %f932, %f516;
+	fma.rn.f32 	%f931, %f503, %f931, %f517;
+	fma.rn.f32 	%f930, %f503, %f930, %f518;
+	fma.rn.f32 	%f929, %f503, %f929, %f519;
+	add.s64 	%rd133, %rd136, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd132, %rd133;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r157,%r158,%r159,%r160}, [%rd132];
+	// end inline asm
+	mov.b32 	%f520, %r157;
+	mov.b32 	%f521, %r158;
+	mov.b32 	%f522, %r159;
+	mov.b32 	%f523, %r160;
+	mul.f32 	%f524, %f90, %f520;
+	mul.f32 	%f525, %f90, %f521;
+	mul.f32 	%f526, %f90, %f522;
+	mul.f32 	%f527, %f90, %f523;
+	fma.rn.f32 	%f936, %f503, %f936, %f524;
+	fma.rn.f32 	%f935, %f503, %f935, %f525;
+	fma.rn.f32 	%f934, %f503, %f934, %f526;
+	fma.rn.f32 	%f933, %f503, %f933, %f527;
+	bra.uni 	$L__BB9_15;
+
+$L__BB9_4:
+	mov.f32 	%f937, 0f00000000;
+	mov.f32 	%f940, 0f3F800000;
+	setp.eq.s32 	%p8, %r13, 4;
+	@%p8 bra 	$L__BB9_7;
+
+	setp.ne.s32 	%p9, %r13, 1;
+	mov.f32 	%f938, %f937;
+	mov.f32 	%f939, %f937;
+	mov.f32 	%f941, %f937;
+	mov.f32 	%f942, %f937;
+	mov.f32 	%f943, %f940;
+	mov.f32 	%f944, %f937;
+	mov.f32 	%f945, %f937;
+	mov.f32 	%f946, %f940;
+	mov.f32 	%f947, %f937;
+	mov.f32 	%f948, %f937;
+	@%p9 bra 	$L__BB9_16;
+
+	// begin inline asm
+	call (%rd20), _optix_get_static_transform_from_handle, (%rd18);
+	// end inline asm
+	add.s64 	%rd257, %rd20, 64;
+	bra.uni 	$L__BB9_8;
+
+$L__BB9_10:
+	// begin inline asm
+	call (%rd33), _optix_get_srt_motion_transform_from_handle, (%rd18);
+	// end inline asm
+	// begin inline asm
+	cvta.to.global.u64 %rd35, %rd33;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r27,%r28,%r29,%r30}, [%rd35];
+	// end inline asm
+	add.s64 	%rd39, %rd33, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd38, %rd39;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r31,%r32,%r33,%r34}, [%rd38];
+	// end inline asm
+	add.s64 	%rd42, %rd33, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd41, %rd42;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r35,%r36,%r37,%r38}, [%rd41];
+	// end inline asm
+	add.s64 	%rd45, %rd33, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd44, %rd45;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r39,%r40,%r41,%r42}, [%rd44];
+	// end inline asm
+	add.s64 	%rd48, %rd33, 64;
+	// begin inline asm
+	cvta.to.global.u64 %rd47, %rd48;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r43,%r44,%r45,%r46}, [%rd47];
+	// end inline asm
+	add.s64 	%rd51, %rd33, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd50, %rd51;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r47,%r48,%r49,%r50}, [%rd50];
+	// end inline asm
+	add.s64 	%rd54, %rd33, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd53, %rd54;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r51,%r52,%r53,%r54}, [%rd53];
+	// end inline asm
+	add.s64 	%rd57, %rd33, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd56, %rd57;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r55,%r56,%r57,%r58}, [%rd56];
+	// end inline asm
+	add.s64 	%rd60, %rd33, 128;
+	// begin inline asm
+	cvta.to.global.u64 %rd59, %rd60;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r59,%r60,%r61,%r62}, [%rd59];
+	// end inline asm
+	add.s64 	%rd63, %rd33, 144;
+	// begin inline asm
+	cvta.to.global.u64 %rd62, %rd63;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r63,%r64,%r65,%r66}, [%rd62];
+	// end inline asm
+	mov.b32 	%f378, %r30;
+	mov.b32 	%f379, %r31;
+	and.b32  	%r83, %r29, 65535;
+	add.s32 	%r84, %r83, -1;
+	cvt.rn.f32.s32 	%f380, %r84;
+	sub.f32 	%f381, %f363, %f378;
+	mul.f32 	%f382, %f381, %f380;
+	sub.f32 	%f383, %f379, %f378;
+	div.rn.f32 	%f384, %f382, %f383;
+	min.f32 	%f385, %f380, %f384;
+	mov.f32 	%f386, 0f00000000;
+	max.f32 	%f387, %f386, %f385;
+	cvt.rmi.f32.f32 	%f388, %f387;
+	sub.f32 	%f29, %f387, %f388;
+	cvt.rzi.s32.f32 	%r85, %f388;
+	mul.wide.s32 	%rd77, %r85, 64;
+	add.s64 	%rd66, %rd42, %rd77;
+	// begin inline asm
+	cvta.to.global.u64 %rd65, %rd66;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r67,%r68,%r69,%r70}, [%rd65];
+	// end inline asm
+	mov.b32 	%f909, %r67;
+	mov.b32 	%f910, %r68;
+	mov.b32 	%f911, %r69;
+	mov.b32 	%f912, %r70;
+	add.s64 	%rd69, %rd66, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd68, %rd69;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r71,%r72,%r73,%r74}, [%rd68];
+	// end inline asm
+	mov.b32 	%f913, %r71;
+	mov.b32 	%f914, %r72;
+	mov.b32 	%f915, %r73;
+	mov.b32 	%f916, %r74;
+	add.s64 	%rd72, %rd66, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd71, %rd72;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r75,%r76,%r77,%r78}, [%rd71];
+	// end inline asm
+	mov.b32 	%f917, %r75;
+	mov.b32 	%f918, %r76;
+	mov.b32 	%f919, %r77;
+	mov.b32 	%f920, %r78;
+	add.s64 	%rd75, %rd66, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd74, %rd75;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r79,%r80,%r81,%r82}, [%rd74];
+	// end inline asm
+	mov.b32 	%f921, %r79;
+	mov.b32 	%f922, %r80;
+	mov.b32 	%f923, %r81;
+	mov.b32 	%f924, %r82;
+	setp.leu.f32 	%p11, %f29, 0f00000000;
+	@%p11 bra 	$L__BB9_12;
+
+	mov.f32 	%f389, 0f3F800000;
+	sub.f32 	%f390, %f389, %f29;
+	add.s64 	%rd79, %rd66, 64;
+	// begin inline asm
+	cvta.to.global.u64 %rd78, %rd79;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r86,%r87,%r88,%r89}, [%rd78];
+	// end inline asm
+	mov.b32 	%f391, %r86;
+	mov.b32 	%f392, %r87;
+	mov.b32 	%f393, %r88;
+	mov.b32 	%f394, %r89;
+	mul.f32 	%f395, %f29, %f391;
+	mul.f32 	%f396, %f29, %f392;
+	mul.f32 	%f397, %f29, %f393;
+	mul.f32 	%f398, %f29, %f394;
+	fma.rn.f32 	%f909, %f390, %f909, %f395;
+	fma.rn.f32 	%f910, %f390, %f910, %f396;
+	fma.rn.f32 	%f911, %f390, %f911, %f397;
+	fma.rn.f32 	%f912, %f390, %f912, %f398;
+	add.s64 	%rd82, %rd66, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd81, %rd82;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r90,%r91,%r92,%r93}, [%rd81];
+	// end inline asm
+	mov.b32 	%f399, %r90;
+	mov.b32 	%f400, %r91;
+	mov.b32 	%f401, %r92;
+	mov.b32 	%f402, %r93;
+	mul.f32 	%f403, %f29, %f399;
+	mul.f32 	%f404, %f29, %f400;
+	mul.f32 	%f405, %f29, %f401;
+	mul.f32 	%f406, %f29, %f402;
+	fma.rn.f32 	%f913, %f390, %f913, %f403;
+	fma.rn.f32 	%f914, %f390, %f914, %f404;
+	fma.rn.f32 	%f915, %f390, %f915, %f405;
+	fma.rn.f32 	%f916, %f390, %f916, %f406;
+	add.s64 	%rd85, %rd66, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd84, %rd85;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r94,%r95,%r96,%r97}, [%rd84];
+	// end inline asm
+	mov.b32 	%f407, %r94;
+	mov.b32 	%f408, %r95;
+	mov.b32 	%f409, %r96;
+	mov.b32 	%f410, %r97;
+	mul.f32 	%f411, %f29, %f407;
+	mul.f32 	%f412, %f29, %f408;
+	mul.f32 	%f413, %f29, %f409;
+	mul.f32 	%f414, %f29, %f410;
+	fma.rn.f32 	%f917, %f390, %f917, %f411;
+	fma.rn.f32 	%f415, %f390, %f918, %f412;
+	fma.rn.f32 	%f416, %f390, %f919, %f413;
+	fma.rn.f32 	%f417, %f390, %f920, %f414;
+	add.s64 	%rd88, %rd66, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd87, %rd88;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r98,%r99,%r100,%r101}, [%rd87];
+	// end inline asm
+	mov.b32 	%f418, %r98;
+	mov.b32 	%f419, %r99;
+	mov.b32 	%f420, %r100;
+	mov.b32 	%f421, %r101;
+	mul.f32 	%f422, %f29, %f418;
+	mul.f32 	%f423, %f29, %f419;
+	mul.f32 	%f424, %f29, %f420;
+	mul.f32 	%f425, %f29, %f421;
+	fma.rn.f32 	%f426, %f390, %f921, %f422;
+	fma.rn.f32 	%f922, %f390, %f922, %f423;
+	fma.rn.f32 	%f923, %f390, %f923, %f424;
+	fma.rn.f32 	%f924, %f390, %f924, %f425;
+	mul.f32 	%f427, %f416, %f416;
+	fma.rn.f32 	%f428, %f415, %f415, %f427;
+	fma.rn.f32 	%f429, %f417, %f417, %f428;
+	fma.rn.f32 	%f430, %f426, %f426, %f429;
+	sqrt.rn.f32 	%f431, %f430;
+	rcp.rn.f32 	%f432, %f431;
+	mul.f32 	%f918, %f415, %f432;
+	mul.f32 	%f919, %f416, %f432;
+	mul.f32 	%f920, %f417, %f432;
+	mul.f32 	%f921, %f432, %f426;
+
+$L__BB9_12:
+	mul.f32 	%f433, %f919, %f919;
+	fma.rn.f32 	%f434, %f918, %f918, %f433;
+	fma.rn.f32 	%f435, %f920, %f920, %f434;
+	fma.rn.f32 	%f436, %f921, %f921, %f435;
+	rcp.rn.f32 	%f437, %f436;
+	mul.f32 	%f438, %f918, %f437;
+	mul.f32 	%f439, %f919, %f437;
+	mul.f32 	%f440, %f920, %f437;
+	mul.f32 	%f441, %f921, %f437;
+	mul.f32 	%f442, %f918, %f438;
+	mul.f32 	%f443, %f919, %f439;
+	mul.f32 	%f444, %f920, %f440;
+	mul.f32 	%f445, %f918, %f439;
+	mul.f32 	%f446, %f920, %f441;
+	mul.f32 	%f447, %f918, %f440;
+	mul.f32 	%f448, %f919, %f441;
+	mul.f32 	%f449, %f919, %f440;
+	mul.f32 	%f450, %f918, %f441;
+	sub.f32 	%f451, %f442, %f443;
+	sub.f32 	%f452, %f451, %f444;
+	fma.rn.f32 	%f453, %f921, %f441, %f452;
+	sub.f32 	%f454, %f445, %f446;
 	add.f32 	%f455, %f454, %f454;
-	sub.f32 	%f456, %f439, %f438;
-	sub.f32 	%f457, %f456, %f440;
-	fma.rn.f32 	%f458, %f925, %f437, %f457;
-	sub.f32 	%f459, %f445, %f446;
-	add.f32 	%f460, %f459, %f459;
-	sub.f32 	%f461, %f443, %f444;
-	add.f32 	%f462, %f461, %f461;
-	add.f32 	%f463, %f445, %f446;
+	add.f32 	%f456, %f447, %f448;
+	add.f32 	%f457, %f456, %f456;
+	add.f32 	%f458, %f445, %f446;
+	add.f32 	%f459, %f458, %f458;
+	sub.f32 	%f460, %f443, %f442;
+	sub.f32 	%f461, %f460, %f444;
+	fma.rn.f32 	%f462, %f921, %f441, %f461;
+	sub.f32 	%f463, %f449, %f450;
 	add.f32 	%f464, %f463, %f463;
-	neg.f32 	%f465, %f438;
-	sub.f32 	%f466, %f465, %f439;
-	add.f32 	%f467, %f440, %f466;
-	fma.rn.f32 	%f468, %f925, %f437, %f467;
-	mul.f32 	%f469, %f916, %f449;
-	fma.rn.f32 	%f470, %f919, %f451, %f469;
-	fma.rn.f32 	%f471, %f921, %f453, %f470;
-	sub.f32 	%f940, %f926, %f471;
-	mul.f32 	%f472, %f919, %f458;
-	fma.rn.f32 	%f473, %f916, %f455, %f472;
-	fma.rn.f32 	%f474, %f921, %f460, %f473;
-	sub.f32 	%f936, %f927, %f474;
-	mul.f32 	%f475, %f919, %f464;
-	fma.rn.f32 	%f476, %f916, %f462, %f475;
-	fma.rn.f32 	%f477, %f921, %f468, %f476;
-	sub.f32 	%f932, %f928, %f477;
-	mul.f32 	%f478, %f915, %f449;
-	fma.rn.f32 	%f479, %f918, %f451, %f478;
-	fma.rn.f32 	%f939, %f920, %f453, %f479;
-	mul.f32 	%f480, %f918, %f458;
-	fma.rn.f32 	%f481, %f915, %f455, %f480;
-	fma.rn.f32 	%f935, %f920, %f460, %f481;
-	mul.f32 	%f482, %f918, %f464;
-	fma.rn.f32 	%f483, %f915, %f462, %f482;
-	fma.rn.f32 	%f931, %f920, %f468, %f483;
-	mul.f32 	%f484, %f914, %f449;
-	fma.rn.f32 	%f938, %f917, %f451, %f484;
-	mul.f32 	%f485, %f917, %f458;
-	fma.rn.f32 	%f934, %f914, %f455, %f485;
-	mul.f32 	%f486, %f917, %f464;
-	fma.rn.f32 	%f930, %f914, %f462, %f486;
-	mul.f32 	%f937, %f913, %f449;
-	mul.f32 	%f933, %f913, %f455;
-	mul.f32 	%f929, %f913, %f462;
-
-BB9_15:
-	mul.f32 	%f524, %f930, %f935;
-	mul.f32 	%f525, %f931, %f934;
-	sub.f32 	%f526, %f525, %f524;
-	mul.f32 	%f527, %f937, %f526;
-	mul.f32 	%f528, %f929, %f935;
-	mul.f32 	%f529, %f931, %f933;
+	sub.f32 	%f465, %f447, %f448;
+	add.f32 	%f466, %f465, %f465;
+	add.f32 	%f467, %f449, %f450;
+	add.f32 	%f468, %f467, %f467;
+	neg.f32 	%f469, %f442;
+	sub.f32 	%f470, %f469, %f443;
+	add.f32 	%f471, %f444, %f470;
+	fma.rn.f32 	%f472, %f921, %f441, %f471;
+	mul.f32 	%f473, %f912, %f453;
+	fma.rn.f32 	%f474, %f915, %f455, %f473;
+	fma.rn.f32 	%f475, %f917, %f457, %f474;
+	sub.f32 	%f925, %f922, %f475;
+	mul.f32 	%f476, %f915, %f462;
+	fma.rn.f32 	%f477, %f912, %f459, %f476;
+	fma.rn.f32 	%f478, %f917, %f464, %f477;
+	sub.f32 	%f929, %f923, %f478;
+	mul.f32 	%f479, %f915, %f468;
+	fma.rn.f32 	%f480, %f912, %f466, %f479;
+	fma.rn.f32 	%f481, %f917, %f472, %f480;
+	sub.f32 	%f933, %f924, %f481;
+	mul.f32 	%f482, %f911, %f453;
+	fma.rn.f32 	%f483, %f914, %f455, %f482;
+	fma.rn.f32 	%f926, %f916, %f457, %f483;
+	mul.f32 	%f484, %f914, %f462;
+	fma.rn.f32 	%f485, %f911, %f459, %f484;
+	fma.rn.f32 	%f930, %f916, %f464, %f485;
+	mul.f32 	%f486, %f914, %f468;
+	fma.rn.f32 	%f487, %f911, %f466, %f486;
+	fma.rn.f32 	%f934, %f916, %f472, %f487;
+	mul.f32 	%f488, %f910, %f453;
+	fma.rn.f32 	%f927, %f913, %f455, %f488;
+	mul.f32 	%f489, %f913, %f462;
+	fma.rn.f32 	%f931, %f910, %f459, %f489;
+	mul.f32 	%f490, %f913, %f468;
+	fma.rn.f32 	%f935, %f910, %f466, %f490;
+	mul.f32 	%f928, %f909, %f453;
+	mul.f32 	%f932, %f909, %f459;
+	mul.f32 	%f936, %f909, %f466;
+
+$L__BB9_15:
+	mul.f32 	%f528, %f930, %f935;
+	mul.f32 	%f529, %f931, %f934;
 	sub.f32 	%f530, %f529, %f528;
-	mul.f32 	%f531, %f530, %f938;
-	sub.f32 	%f532, %f527, %f531;
-	mul.f32 	%f533, %f929, %f934;
-	mul.f32 	%f534, %f930, %f933;
-	sub.f32 	%f535, %f534, %f533;
-	fma.rn.f32 	%f536, %f535, %f939, %f532;
-	rcp.rn.f32 	%f537, %f536;
-	mul.f32 	%f949, %f526, %f537;
-	mul.f32 	%f538, %f931, %f938;
-	mul.f32 	%f539, %f930, %f939;
-	sub.f32 	%f540, %f539, %f538;
-	mul.f32 	%f950, %f537, %f540;
-	mul.f32 	%f541, %f934, %f939;
-	mul.f32 	%f542, %f935, %f938;
-	sub.f32 	%f543, %f542, %f541;
-	mul.f32 	%f951, %f537, %f543;
-	sub.f32 	%f544, %f528, %f529;
-	mul.f32 	%f945, %f544, %f537;
-	mul.f32 	%f545, %f929, %f939;
-	mul.f32 	%f546, %f931, %f937;
+	mul.f32 	%f531, %f928, %f530;
+	mul.f32 	%f532, %f930, %f936;
+	mul.f32 	%f533, %f932, %f934;
+	sub.f32 	%f534, %f533, %f532;
+	mul.f32 	%f535, %f927, %f534;
+	sub.f32 	%f536, %f531, %f535;
+	mul.f32 	%f537, %f931, %f936;
+	mul.f32 	%f538, %f932, %f935;
+	sub.f32 	%f539, %f538, %f537;
+	fma.rn.f32 	%f540, %f926, %f539, %f536;
+	rcp.rn.f32 	%f541, %f540;
+	mul.f32 	%f940, %f530, %f541;
+	mul.f32 	%f542, %f927, %f934;
+	mul.f32 	%f543, %f926, %f935;
+	sub.f32 	%f544, %f543, %f542;
+	mul.f32 	%f939, %f544, %f541;
+	mul.f32 	%f545, %f926, %f931;
+	mul.f32 	%f546, %f927, %f930;
 	sub.f32 	%f547, %f546, %f545;
-	mul.f32 	%f946, %f537, %f547;
-	mul.f32 	%f548, %f935, %f937;
-	mul.f32 	%f549, %f933, %f939;
-	sub.f32 	%f550, %f549, %f548;
-	mul.f32 	%f947, %f537, %f550;
-	mul.f32 	%f941, %f535, %f537;
-	mul.f32 	%f551, %f930, %f937;
-	mul.f32 	%f552, %f929, %f938;
-	sub.f32 	%f553, %f552, %f551;
-	mul.f32 	%f942, %f553, %f537;
-	mul.f32 	%f554, %f933, %f938;
-	mul.f32 	%f555, %f934, %f937;
-	sub.f32 	%f556, %f555, %f554;
-	mul.f32 	%f943, %f556, %f537;
-	mul.f32 	%f557, %f940, %f949;
-	neg.f32 	%f558, %f557;
-	mul.f32 	%f559, %f936, %f950;
-	sub.f32 	%f560, %f558, %f559;
-	mul.f32 	%f561, %f932, %f951;
-	sub.f32 	%f952, %f560, %f561;
-	mul.f32 	%f562, %f940, %f945;
-	neg.f32 	%f563, %f562;
-	mul.f32 	%f564, %f936, %f946;
-	sub.f32 	%f565, %f563, %f564;
-	mul.f32 	%f566, %f932, %f947;
-	sub.f32 	%f948, %f565, %f566;
-	mul.f32 	%f567, %f940, %f941;
-	neg.f32 	%f568, %f567;
-	mul.f32 	%f569, %f936, %f942;
-	sub.f32 	%f570, %f568, %f569;
-	mul.f32 	%f571, %f932, %f943;
-	sub.f32 	%f944, %f570, %f571;
-	bra.uni 	BB9_16;
-
-BB9_5:
-	setp.ne.s32	%p26, %r35, 1;
-	mov.f32 	%f942, %f941;
-	mov.f32 	%f944, %f941;
-	mov.f32 	%f945, %f941;
-	mov.f32 	%f946, %f943;
-	mov.f32 	%f947, %f941;
-	mov.f32 	%f948, %f941;
-	mov.f32 	%f949, %f943;
-	mov.f32 	%f950, %f941;
-	mov.f32 	%f951, %f941;
-	mov.f32 	%f952, %f941;
-	@%p26 bra 	BB9_16;
-
-	// inline asm
-	call (%rd25), _optix_get_static_transform_from_handle, (%rd23);
-	// inline asm
-	add.s64 	%rd269, %rd25, 64;
-
-BB9_8:
-	// inline asm
-	cvta.to.global.u64 %rd29, %rd269;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r37,%r38,%r39,%r40}, [%rd29];
-	// inline asm
-	mov.b32 	 %f949, %r37;
-	mov.b32 	 %f950, %r38;
-	mov.b32 	 %f951, %r39;
-	mov.b32 	 %f952, %r40;
-	add.s64 	%rd33, %rd269, 16;
-	// inline asm
-	cvta.to.global.u64 %rd32, %rd33;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r41,%r42,%r43,%r44}, [%rd32];
-	// inline asm
-	mov.b32 	 %f945, %r41;
-	mov.b32 	 %f946, %r42;
-	mov.b32 	 %f947, %r43;
-	mov.b32 	 %f948, %r44;
-	add.s64 	%rd36, %rd269, 32;
-	// inline asm
-	cvta.to.global.u64 %rd35, %rd36;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r45,%r46,%r47,%r48}, [%rd35];
-	// inline asm
-	mov.b32 	 %f941, %r45;
-	mov.b32 	 %f942, %r46;
-	mov.b32 	 %f943, %r47;
-	mov.b32 	 %f944, %r48;
-
-BB9_16:
-	setp.eq.s32	%p30, %r902, 0;
-	@%p30 bra 	BB9_17;
-	bra.uni 	BB9_18;
-
-BB9_17:
-	mov.f32 	%f912, %f952;
-	mov.f32 	%f911, %f951;
-	mov.f32 	%f910, %f950;
-	mov.f32 	%f909, %f949;
-	mov.f32 	%f908, %f948;
-	mov.f32 	%f907, %f947;
-	mov.f32 	%f906, %f946;
-	mov.f32 	%f905, %f945;
-	mov.f32 	%f904, %f944;
-	mov.f32 	%f903, %f943;
-	mov.f32 	%f902, %f942;
-	mov.f32 	%f901, %f941;
-	bra.uni 	BB9_19;
-
-BB9_18:
-	mul.f32 	%f572, %f905, %f950;
-	fma.rn.f32 	%f573, %f909, %f949, %f572;
-	fma.rn.f32 	%f151, %f901, %f951, %f573;
-	mul.f32 	%f574, %f906, %f950;
-	fma.rn.f32 	%f575, %f910, %f949, %f574;
-	fma.rn.f32 	%f152, %f902, %f951, %f575;
-	mul.f32 	%f576, %f907, %f950;
-	fma.rn.f32 	%f577, %f911, %f949, %f576;
-	fma.rn.f32 	%f153, %f903, %f951, %f577;
-	mul.f32 	%f578, %f908, %f950;
-	fma.rn.f32 	%f579, %f912, %f949, %f578;
-	fma.rn.f32 	%f580, %f904, %f951, %f579;
-	add.f32 	%f154, %f952, %f580;
-	mul.f32 	%f581, %f905, %f946;
-	fma.rn.f32 	%f582, %f909, %f945, %f581;
-	fma.rn.f32 	%f155, %f901, %f947, %f582;
-	mul.f32 	%f583, %f906, %f946;
-	fma.rn.f32 	%f584, %f910, %f945, %f583;
-	fma.rn.f32 	%f156, %f902, %f947, %f584;
-	mul.f32 	%f585, %f907, %f946;
-	fma.rn.f32 	%f586, %f911, %f945, %f585;
-	fma.rn.f32 	%f157, %f903, %f947, %f586;
-	mul.f32 	%f587, %f908, %f946;
-	fma.rn.f32 	%f588, %f912, %f945, %f587;
-	fma.rn.f32 	%f589, %f904, %f947, %f588;
-	add.f32 	%f158, %f948, %f589;
-	mul.f32 	%f590, %f905, %f942;
-	fma.rn.f32 	%f591, %f909, %f941, %f590;
-	fma.rn.f32 	%f901, %f901, %f943, %f591;
-	mul.f32 	%f592, %f906, %f942;
-	fma.rn.f32 	%f593, %f910, %f941, %f592;
-	fma.rn.f32 	%f902, %f902, %f943, %f593;
-	mul.f32 	%f594, %f907, %f942;
-	fma.rn.f32 	%f595, %f911, %f941, %f594;
-	fma.rn.f32 	%f903, %f903, %f943, %f595;
-	mul.f32 	%f596, %f908, %f942;
-	fma.rn.f32 	%f597, %f912, %f941, %f596;
-	fma.rn.f32 	%f598, %f904, %f943, %f597;
-	add.f32 	%f904, %f944, %f598;
-	mov.f32 	%f912, %f154;
-	mov.f32 	%f911, %f153;
-	mov.f32 	%f910, %f152;
-	mov.f32 	%f909, %f151;
-	mov.f32 	%f908, %f158;
-	mov.f32 	%f907, %f157;
-	mov.f32 	%f906, %f156;
-	mov.f32 	%f905, %f155;
-
-BB9_19:
-	add.s32 	%r902, %r902, 1;
-	setp.lt.u32	%p31, %r902, %r32;
-	@%p31 bra 	BB9_3;
-
-	mul.f32 	%f599, %f357, %f909;
-	fma.rn.f32 	%f600, %f358, %f910, %f599;
-	fma.rn.f32 	%f601, %f965, %f911, %f600;
-	add.f32 	%f967, %f912, %f601;
-	mul.f32 	%f602, %f357, %f905;
-	fma.rn.f32 	%f603, %f358, %f906, %f602;
-	fma.rn.f32 	%f604, %f965, %f907, %f603;
-	add.f32 	%f966, %f908, %f604;
-	mul.f32 	%f605, %f357, %f901;
-	fma.rn.f32 	%f606, %f358, %f902, %f605;
-	fma.rn.f32 	%f607, %f965, %f903, %f606;
-	add.f32 	%f965, %f904, %f607;
-	bra.uni 	BB9_21;
-
-BB9_1:
-	mov.f32 	%f966, %f358;
-	mov.f32 	%f967, %f357;
-
-BB9_21:
-	setp.eq.s32	%p515, %r32, 0;
-	// inline asm
-	call (%f608), _optix_get_world_ray_direction_x, ();
-	// inline asm
-	// inline asm
-	call (%f609), _optix_get_world_ray_direction_y, ();
-	// inline asm
-	// inline asm
-	call (%f1016), _optix_get_world_ray_direction_z, ();
-	// inline asm
-	// inline asm
-	call (%f611), _optix_get_ray_time, ();
-	// inline asm
-	mov.u32 	%r903, 0;
-	@%p515 bra 	BB9_22;
-
-BB9_23:
+	mul.f32 	%f938, %f547, %f541;
+	sub.f32 	%f548, %f532, %f533;
+	mul.f32 	%f944, %f548, %f541;
+	mul.f32 	%f549, %f926, %f936;
+	mul.f32 	%f550, %f928, %f934;
+	sub.f32 	%f551, %f550, %f549;
+	mul.f32 	%f943, %f551, %f541;
+	mul.f32 	%f552, %f928, %f930;
+	mul.f32 	%f553, %f926, %f932;
+	sub.f32 	%f554, %f553, %f552;
+	mul.f32 	%f942, %f554, %f541;
+	mul.f32 	%f948, %f539, %f541;
+	mul.f32 	%f555, %f928, %f935;
+	mul.f32 	%f556, %f927, %f936;
+	sub.f32 	%f557, %f556, %f555;
+	mul.f32 	%f947, %f557, %f541;
+	mul.f32 	%f558, %f927, %f932;
+	mul.f32 	%f559, %f928, %f931;
+	sub.f32 	%f560, %f559, %f558;
+	mul.f32 	%f946, %f560, %f541;
+	mul.f32 	%f561, %f925, %f940;
+	neg.f32 	%f562, %f561;
+	mul.f32 	%f563, %f929, %f939;
+	sub.f32 	%f564, %f562, %f563;
+	mul.f32 	%f565, %f933, %f938;
+	sub.f32 	%f937, %f564, %f565;
+	mul.f32 	%f566, %f925, %f944;
+	neg.f32 	%f567, %f566;
+	mul.f32 	%f568, %f929, %f943;
+	sub.f32 	%f569, %f567, %f568;
+	mul.f32 	%f570, %f933, %f942;
+	sub.f32 	%f941, %f569, %f570;
+	mul.f32 	%f571, %f925, %f948;
+	neg.f32 	%f572, %f571;
+	mul.f32 	%f573, %f929, %f947;
+	sub.f32 	%f574, %f572, %f573;
+	mul.f32 	%f575, %f933, %f946;
+	sub.f32 	%f945, %f574, %f575;
+	bra.uni 	$L__BB9_16;
+
+$L__BB9_7:
+	// begin inline asm
+	call (%rd257), _optix_get_instance_inverse_transform_from_handle, (%rd18);
+	// end inline asm
+
+$L__BB9_8:
+	// begin inline asm
+	cvta.to.global.u64 %rd24, %rd257;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r15,%r16,%r17,%r18}, [%rd24];
+	// end inline asm
+	mov.b32 	%f940, %r15;
+	mov.b32 	%f939, %r16;
+	mov.b32 	%f938, %r17;
+	mov.b32 	%f937, %r18;
+	add.s64 	%rd28, %rd257, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd27, %rd28;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r19,%r20,%r21,%r22}, [%rd27];
+	// end inline asm
+	mov.b32 	%f944, %r19;
+	mov.b32 	%f943, %r20;
+	mov.b32 	%f942, %r21;
+	mov.b32 	%f941, %r22;
+	add.s64 	%rd31, %rd257, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd30, %rd31;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r23,%r24,%r25,%r26}, [%rd30];
+	// end inline asm
+	mov.b32 	%f948, %r23;
+	mov.b32 	%f947, %r24;
+	mov.b32 	%f946, %r25;
+	mov.b32 	%f945, %r26;
+
+$L__BB9_16:
+	setp.eq.s32 	%p13, %r321, 0;
+	@%p13 bra 	$L__BB9_18;
+
+	mul.f32 	%f576, %f905, %f940;
+	fma.rn.f32 	%f577, %f901, %f939, %f576;
+	fma.rn.f32 	%f151, %f897, %f938, %f577;
+	mul.f32 	%f578, %f906, %f940;
+	fma.rn.f32 	%f579, %f902, %f939, %f578;
+	fma.rn.f32 	%f152, %f898, %f938, %f579;
+	mul.f32 	%f580, %f907, %f940;
+	fma.rn.f32 	%f581, %f903, %f939, %f580;
+	fma.rn.f32 	%f153, %f899, %f938, %f581;
+	mul.f32 	%f582, %f908, %f940;
+	fma.rn.f32 	%f583, %f904, %f939, %f582;
+	fma.rn.f32 	%f584, %f900, %f938, %f583;
+	add.f32 	%f937, %f937, %f584;
+	mul.f32 	%f585, %f905, %f944;
+	fma.rn.f32 	%f586, %f901, %f943, %f585;
+	fma.rn.f32 	%f155, %f897, %f942, %f586;
+	mul.f32 	%f587, %f906, %f944;
+	fma.rn.f32 	%f588, %f902, %f943, %f587;
+	fma.rn.f32 	%f156, %f898, %f942, %f588;
+	mul.f32 	%f589, %f907, %f944;
+	fma.rn.f32 	%f590, %f903, %f943, %f589;
+	fma.rn.f32 	%f157, %f899, %f942, %f590;
+	mul.f32 	%f591, %f908, %f944;
+	fma.rn.f32 	%f592, %f904, %f943, %f591;
+	fma.rn.f32 	%f593, %f900, %f942, %f592;
+	add.f32 	%f941, %f941, %f593;
+	mul.f32 	%f594, %f905, %f948;
+	fma.rn.f32 	%f595, %f901, %f947, %f594;
+	fma.rn.f32 	%f159, %f897, %f946, %f595;
+	mul.f32 	%f596, %f906, %f948;
+	fma.rn.f32 	%f597, %f902, %f947, %f596;
+	fma.rn.f32 	%f160, %f898, %f946, %f597;
+	mul.f32 	%f598, %f907, %f948;
+	fma.rn.f32 	%f599, %f903, %f947, %f598;
+	fma.rn.f32 	%f161, %f899, %f946, %f599;
+	mul.f32 	%f600, %f908, %f948;
+	fma.rn.f32 	%f601, %f904, %f947, %f600;
+	fma.rn.f32 	%f602, %f900, %f946, %f601;
+	add.f32 	%f945, %f945, %f602;
+	mov.f32 	%f938, %f153;
+	mov.f32 	%f939, %f152;
+	mov.f32 	%f940, %f151;
+	mov.f32 	%f942, %f157;
+	mov.f32 	%f943, %f156;
+	mov.f32 	%f944, %f155;
+	mov.f32 	%f946, %f161;
+	mov.f32 	%f947, %f160;
+	mov.f32 	%f948, %f159;
+
+$L__BB9_18:
+	add.s32 	%r321, %r321, 1;
+	setp.lt.u32 	%p14, %r321, %r10;
+	mov.f32 	%f897, %f948;
+	mov.f32 	%f898, %f947;
+	mov.f32 	%f899, %f946;
+	mov.f32 	%f900, %f945;
+	mov.f32 	%f901, %f944;
+	mov.f32 	%f902, %f943;
+	mov.f32 	%f903, %f942;
+	mov.f32 	%f904, %f941;
+	mov.f32 	%f905, %f940;
+	mov.f32 	%f906, %f939;
+	mov.f32 	%f907, %f938;
+	mov.f32 	%f908, %f937;
+	@%p14 bra 	$L__BB9_3;
+
+$L__BB9_19:
+	mul.f32 	%f603, %f973, %f940;
+	fma.rn.f32 	%f604, %f974, %f939, %f603;
+	fma.rn.f32 	%f605, %f975, %f938, %f604;
+	mul.f32 	%f606, %f973, %f944;
+	fma.rn.f32 	%f607, %f974, %f943, %f606;
+	fma.rn.f32 	%f608, %f975, %f942, %f607;
+	mul.f32 	%f609, %f973, %f948;
+	fma.rn.f32 	%f610, %f974, %f947, %f609;
+	fma.rn.f32 	%f611, %f975, %f946, %f610;
+	add.f32 	%f975, %f945, %f611;
+	add.f32 	%f974, %f941, %f608;
+	add.f32 	%f973, %f937, %f605;
+
+$L__BB9_21:
+	// begin inline asm
+	call (%f1031), _optix_get_world_ray_direction_x, ();
+	// end inline asm
+	// begin inline asm
+	call (%f1032), _optix_get_world_ray_direction_y, ();
+	// end inline asm
+	// begin inline asm
+	call (%f614), _optix_get_world_ray_direction_z, ();
+	// end inline asm
+	// begin inline asm
+	call (%r161), _optix_get_transform_list_size, ();
+	// end inline asm
+	setp.eq.s32 	%p15, %r161, 0;
+	@%p15 bra 	$L__BB9_41;
+
+	// begin inline asm
+	call (%r162), _optix_get_transform_list_size, ();
+	// end inline asm
+	// begin inline asm
+	call (%f615), _optix_get_ray_time, ();
+	// end inline asm
+	setp.eq.s32 	%p16, %r162, 0;
+	@%p16 bra 	$L__BB9_40;
+
+	mov.u32 	%r322, 0;
+
+$L__BB9_24:
 	.pragma "nounroll";
-	// inline asm
-	call (%rd144), _optix_get_transform_list_handle, (%r903);
-	// inline asm
-	// inline asm
-	call (%r185), _optix_get_transform_type_from_handle, (%rd144);
-	// inline asm
-	and.b32  	%r186, %r185, -2;
-	setp.eq.s32	%p33, %r186, 2;
-	@%p33 bra 	BB9_29;
-	bra.uni 	BB9_24;
-
-BB9_29:
-	setp.eq.s32	%p36, %r185, 2;
-	@%p36 bra 	BB9_33;
-	bra.uni 	BB9_30;
-
-BB9_33:
-	// inline asm
-	call (%rd218), _optix_get_matrix_motion_transform_from_handle, (%rd144);
-	// inline asm
-	// inline asm
-	cvta.to.global.u64 %rd220, %rd218;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r274,%r275,%r276,%r277}, [%rd220];
-	// inline asm
-	mov.b32	{%rs8, %rs9}, %r276;
-	add.s64 	%rd224, %rd218, 16;
-	// inline asm
+	// begin inline asm
+	call (%rd137), _optix_get_transform_list_handle, (%r322);
+	// end inline asm
+	// begin inline asm
+	call (%r165), _optix_get_transform_type_from_handle, (%rd137);
+	// end inline asm
+	or.b32  	%r166, %r165, 1;
+	setp.eq.s32 	%p17, %r166, 3;
+	@%p17 bra 	$L__BB9_30;
+	bra.uni 	$L__BB9_25;
+
+$L__BB9_30:
+	setp.eq.s32 	%p20, %r165, 2;
+	@%p20 bra 	$L__BB9_34;
+	bra.uni 	$L__BB9_31;
+
+$L__BB9_34:
+	// begin inline asm
+	call (%rd209), _optix_get_matrix_motion_transform_from_handle, (%rd137);
+	// end inline asm
+	// begin inline asm
+	cvta.to.global.u64 %rd211, %rd209;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r254,%r255,%r256,%r257}, [%rd211];
+	// end inline asm
+	add.s64 	%rd215, %rd209, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd214, %rd215;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r258,%r259,%r260,%r261}, [%rd214];
+	// end inline asm
+	add.s64 	%rd218, %rd209, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd217, %rd218;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r262,%r263,%r264,%r265}, [%rd217];
+	// end inline asm
+	add.s64 	%rd221, %rd209, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd220, %rd221;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r266,%r267,%r268,%r269}, [%rd220];
+	// end inline asm
+	add.s64 	%rd224, %rd209, 64;
+	// begin inline asm
 	cvta.to.global.u64 %rd223, %rd224;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r278,%r279,%r280,%r281}, [%rd223];
-	// inline asm
-	add.s64 	%rd227, %rd218, 32;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r270,%r271,%r272,%r273}, [%rd223];
+	// end inline asm
+	add.s64 	%rd227, %rd209, 80;
+	// begin inline asm
 	cvta.to.global.u64 %rd226, %rd227;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r282,%r283,%r284,%r285}, [%rd226];
-	// inline asm
-	add.s64 	%rd230, %rd218, 48;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r274,%r275,%r276,%r277}, [%rd226];
+	// end inline asm
+	add.s64 	%rd230, %rd209, 96;
+	// begin inline asm
 	cvta.to.global.u64 %rd229, %rd230;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r286,%r287,%r288,%r289}, [%rd229];
-	// inline asm
-	add.s64 	%rd233, %rd218, 64;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r278,%r279,%r280,%r281}, [%rd229];
+	// end inline asm
+	add.s64 	%rd233, %rd209, 112;
+	// begin inline asm
 	cvta.to.global.u64 %rd232, %rd233;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r290,%r291,%r292,%r293}, [%rd232];
-	// inline asm
-	add.s64 	%rd236, %rd218, 80;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r282,%r283,%r284,%r285}, [%rd232];
+	// end inline asm
+	mov.b32 	%f719, %r257;
+	mov.b32 	%f720, %r258;
+	and.b32  	%r298, %r256, 65535;
+	add.s32 	%r299, %r298, -1;
+	cvt.rn.f32.s32 	%f721, %r299;
+	sub.f32 	%f722, %f615, %f719;
+	mul.f32 	%f723, %f722, %f721;
+	sub.f32 	%f724, %f720, %f719;
+	div.rn.f32 	%f725, %f723, %f724;
+	min.f32 	%f726, %f721, %f725;
+	mov.f32 	%f727, 0f00000000;
+	max.f32 	%f728, %f727, %f726;
+	cvt.rmi.f32.f32 	%f729, %f728;
+	sub.f32 	%f261, %f728, %f729;
+	cvt.rzi.s32.f32 	%r300, %f729;
+	cvt.s64.s32 	%rd15, %r300;
+	mul.wide.s32 	%rd244, %r300, 48;
+	add.s64 	%rd236, %rd218, %rd244;
+	// begin inline asm
 	cvta.to.global.u64 %rd235, %rd236;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r294,%r295,%r296,%r297}, [%rd235];
-	// inline asm
-	add.s64 	%rd239, %rd218, 96;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r286,%r287,%r288,%r289}, [%rd235];
+	// end inline asm
+	mov.b32 	%f1001, %r286;
+	mov.b32 	%f1002, %r287;
+	mov.b32 	%f1003, %r288;
+	add.s64 	%rd239, %rd236, 16;
+	// begin inline asm
 	cvta.to.global.u64 %rd238, %rd239;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r298,%r299,%r300,%r301}, [%rd238];
-	// inline asm
-	add.s64 	%rd242, %rd218, 112;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r290,%r291,%r292,%r293}, [%rd238];
+	// end inline asm
+	mov.b32 	%f998, %r290;
+	mov.b32 	%f999, %r291;
+	mov.b32 	%f1000, %r292;
+	add.s64 	%rd242, %rd236, 32;
+	// begin inline asm
 	cvta.to.global.u64 %rd241, %rd242;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r302,%r303,%r304,%r305}, [%rd241];
-	// inline asm
-	mov.b32 	 %f714, %r277;
-	mov.b32 	 %f715, %r278;
-	cvt.u32.u16	%r318, %rs8;
-	add.s32 	%r319, %r318, -1;
-	cvt.rn.f32.s32	%f716, %r319;
-	sub.f32 	%f717, %f611, %f714;
-	mul.f32 	%f718, %f717, %f716;
-	sub.f32 	%f719, %f715, %f714;
-	div.rn.f32 	%f720, %f718, %f719;
-	min.f32 	%f721, %f716, %f720;
-	mov.f32 	%f722, 0f00000000;
-	max.f32 	%f723, %f722, %f721;
-	cvt.rmi.f32.f32	%f724, %f723;
-	cvt.rzi.s32.f32	%r320, %f724;
-	cvt.s64.s32	%rd17, %r320;
-	mul.wide.s32 	%rd253, %r320, 48;
-	add.s64 	%rd245, %rd227, %rd253;
-	// inline asm
-	cvta.to.global.u64 %rd244, %rd245;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r306,%r307,%r308,%r309}, [%rd244];
-	// inline asm
-	mov.b32 	 %f993, %r306;
-	mov.b32 	 %f994, %r307;
-	mov.b32 	 %f995, %r308;
-	add.s64 	%rd248, %rd245, 16;
-	// inline asm
-	cvta.to.global.u64 %rd247, %rd248;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r310,%r311,%r312,%r313}, [%rd247];
-	// inline asm
-	mov.b32 	 %f990, %r310;
-	mov.b32 	 %f991, %r311;
-	mov.b32 	 %f992, %r312;
-	add.s64 	%rd251, %rd245, 32;
-	// inline asm
-	cvta.to.global.u64 %rd250, %rd251;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r314,%r315,%r316,%r317}, [%rd250];
-	// inline asm
-	sub.f32 	%f249, %f723, %f724;
-	mov.b32 	 %f987, %r314;
-	mov.b32 	 %f988, %r315;
-	mov.b32 	 %f989, %r316;
-	setp.leu.f32	%p38, %f249, 0f00000000;
-	@%p38 bra 	BB9_35;
-
-	mul.lo.s64 	%rd263, %rd17, 48;
-	add.s64 	%rd264, %rd218, %rd263;
-	add.s64 	%rd255, %rd264, 80;
-	// inline asm
-	cvta.to.global.u64 %rd254, %rd255;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r321,%r322,%r323,%r324}, [%rd254];
-	// inline asm
-	mov.b32 	 %f725, %r321;
-	mov.b32 	 %f726, %r322;
-	mov.b32 	 %f727, %r323;
-	add.s64 	%rd258, %rd264, 96;
-	// inline asm
-	cvta.to.global.u64 %rd257, %rd258;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r325,%r326,%r327,%r328}, [%rd257];
-	// inline asm
-	mov.b32 	 %f728, %r325;
-	mov.b32 	 %f729, %r326;
-	mov.b32 	 %f730, %r327;
-	add.s64 	%rd261, %rd264, 112;
-	// inline asm
-	cvta.to.global.u64 %rd260, %rd261;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r329,%r330,%r331,%r332}, [%rd260];
-	// inline asm
-	mov.f32 	%f731, 0f3F800000;
-	sub.f32 	%f732, %f731, %f249;
-	mul.f32 	%f733, %f249, %f725;
-	mul.f32 	%f734, %f249, %f726;
-	mul.f32 	%f735, %f249, %f727;
-	fma.rn.f32 	%f993, %f732, %f993, %f733;
-	fma.rn.f32 	%f994, %f732, %f994, %f734;
-	fma.rn.f32 	%f995, %f732, %f995, %f735;
-	mul.f32 	%f736, %f249, %f728;
-	mul.f32 	%f737, %f249, %f729;
-	mul.f32 	%f738, %f249, %f730;
-	fma.rn.f32 	%f990, %f732, %f990, %f736;
-	fma.rn.f32 	%f991, %f732, %f991, %f737;
-	fma.rn.f32 	%f992, %f732, %f992, %f738;
-	mov.b32 	 %f739, %r329;
-	mov.b32 	 %f740, %r330;
-	mov.b32 	 %f741, %r331;
-	mul.f32 	%f742, %f249, %f739;
-	mul.f32 	%f743, %f249, %f740;
-	mul.f32 	%f744, %f249, %f741;
-	fma.rn.f32 	%f987, %f732, %f987, %f742;
-	fma.rn.f32 	%f988, %f732, %f988, %f743;
-	fma.rn.f32 	%f989, %f732, %f989, %f744;
-	bra.uni 	BB9_35;
-
-BB9_24:
-	mov.f32 	%f996, 0f00000000;
-	mov.f32 	%f998, 0f3F800000;
-	setp.eq.s32	%p34, %r185, 4;
-	@%p34 bra 	BB9_27;
-	bra.uni 	BB9_25;
-
-BB9_27:
-	// inline asm
-	call (%rd270), _optix_get_instance_inverse_transform_from_handle, (%rd144);
-	// inline asm
-	bra.uni 	BB9_28;
-
-BB9_30:
-	// inline asm
-	call (%rd159), _optix_get_srt_motion_transform_from_handle, (%rd144);
-	// inline asm
-	// inline asm
-	cvta.to.global.u64 %rd161, %rd159;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r199,%r200,%r201,%r202}, [%rd161];
-	// inline asm
-	mov.b32	{%rs6, %rs7}, %r201;
-	add.s64 	%rd165, %rd159, 16;
-	// inline asm
-	cvta.to.global.u64 %rd164, %rd165;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r203,%r204,%r205,%r206}, [%rd164];
-	// inline asm
-	add.s64 	%rd168, %rd159, 32;
-	// inline asm
-	cvta.to.global.u64 %rd167, %rd168;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r207,%r208,%r209,%r210}, [%rd167];
-	// inline asm
-	add.s64 	%rd171, %rd159, 48;
-	// inline asm
-	cvta.to.global.u64 %rd170, %rd171;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r211,%r212,%r213,%r214}, [%rd170];
-	// inline asm
-	add.s64 	%rd174, %rd159, 64;
-	// inline asm
-	cvta.to.global.u64 %rd173, %rd174;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r215,%r216,%r217,%r218}, [%rd173];
-	// inline asm
-	add.s64 	%rd177, %rd159, 80;
-	// inline asm
-	cvta.to.global.u64 %rd176, %rd177;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r219,%r220,%r221,%r222}, [%rd176];
-	// inline asm
-	add.s64 	%rd180, %rd159, 96;
-	// inline asm
-	cvta.to.global.u64 %rd179, %rd180;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r223,%r224,%r225,%r226}, [%rd179];
-	// inline asm
-	add.s64 	%rd183, %rd159, 112;
-	// inline asm
-	cvta.to.global.u64 %rd182, %rd183;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r227,%r228,%r229,%r230}, [%rd182];
-	// inline asm
-	add.s64 	%rd186, %rd159, 128;
-	// inline asm
-	cvta.to.global.u64 %rd185, %rd186;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r231,%r232,%r233,%r234}, [%rd185];
-	// inline asm
-	add.s64 	%rd189, %rd159, 144;
-	// inline asm
-	cvta.to.global.u64 %rd188, %rd189;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r235,%r236,%r237,%r238}, [%rd188];
-	// inline asm
-	mov.b32 	 %f622, %r202;
-	mov.b32 	 %f623, %r203;
-	cvt.u32.u16	%r255, %rs6;
-	add.s32 	%r256, %r255, -1;
-	cvt.rn.f32.s32	%f624, %r256;
-	sub.f32 	%f625, %f611, %f622;
-	mul.f32 	%f626, %f625, %f624;
-	sub.f32 	%f627, %f623, %f622;
-	div.rn.f32 	%f628, %f626, %f627;
-	min.f32 	%f629, %f624, %f628;
-	mov.f32 	%f630, 0f00000000;
-	max.f32 	%f631, %f630, %f629;
-	cvt.rmi.f32.f32	%f632, %f631;
-	cvt.rzi.s32.f32	%r257, %f632;
-	cvt.s64.s32	%rd15, %r257;
-	mul.wide.s32 	%rd203, %r257, 64;
-	add.s64 	%rd192, %rd168, %rd203;
-	// inline asm
-	cvta.to.global.u64 %rd191, %rd192;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r239,%r240,%r241,%r242}, [%rd191];
-	// inline asm
-	mov.b32 	 %f977, %r239;
-	mov.b32 	 %f978, %r240;
-	mov.b32 	 %f979, %r241;
-	add.s64 	%rd195, %rd192, 16;
-	// inline asm
-	cvta.to.global.u64 %rd194, %rd195;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r243,%r244,%r245,%r246}, [%rd194];
-	// inline asm
-	mov.b32 	 %f980, %r243;
-	mov.b32 	 %f981, %r244;
-	mov.b32 	 %f982, %r246;
-	add.s64 	%rd198, %rd192, 32;
-	// inline asm
-	cvta.to.global.u64 %rd197, %rd198;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r247,%r248,%r249,%r250}, [%rd197];
-	// inline asm
-	sub.f32 	%f209, %f631, %f632;
-	mov.b32 	 %f983, %r248;
-	mov.b32 	 %f984, %r249;
-	mov.b32 	 %f985, %r250;
-	add.s64 	%rd201, %rd192, 48;
-	// inline asm
-	cvta.to.global.u64 %rd200, %rd201;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r251,%r252,%r253,%r254}, [%rd200];
-	// inline asm
-	mov.b32 	 %f986, %r251;
-	setp.leu.f32	%p37, %f209, 0f00000000;
-	@%p37 bra 	BB9_32;
-
-	shl.b64 	%rd216, %rd15, 6;
-	add.s64 	%rd217, %rd216, %rd159;
-	add.s64 	%rd205, %rd217, 96;
-	// inline asm
-	cvta.to.global.u64 %rd204, %rd205;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r258,%r259,%r260,%r261}, [%rd204];
-	// inline asm
-	mov.b32 	 %f633, %r258;
-	mov.b32 	 %f634, %r259;
-	mov.b32 	 %f635, %r260;
-	add.s64 	%rd208, %rd217, 112;
-	// inline asm
-	cvta.to.global.u64 %rd207, %rd208;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r262,%r263,%r264,%r265}, [%rd207];
-	// inline asm
-	mov.b32 	 %f636, %r262;
-	mov.b32 	 %f637, %r263;
-	mov.b32 	 %f638, %r265;
-	add.s64 	%rd211, %rd217, 128;
-	// inline asm
-	cvta.to.global.u64 %rd210, %rd211;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r266,%r267,%r268,%r269}, [%rd210];
-	// inline asm
-	mov.b32 	 %f639, %r267;
-	mov.b32 	 %f640, %r268;
-	mov.b32 	 %f641, %r269;
-	add.s64 	%rd214, %rd217, 144;
-	// inline asm
-	cvta.to.global.u64 %rd213, %rd214;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r270,%r271,%r272,%r273}, [%rd213];
-	// inline asm
-	mov.f32 	%f642, 0f3F800000;
-	sub.f32 	%f643, %f642, %f209;
-	mul.f32 	%f644, %f209, %f633;
-	mul.f32 	%f645, %f209, %f634;
-	mul.f32 	%f646, %f209, %f635;
-	fma.rn.f32 	%f977, %f643, %f977, %f644;
-	fma.rn.f32 	%f978, %f643, %f978, %f645;
-	fma.rn.f32 	%f979, %f643, %f979, %f646;
-	mul.f32 	%f647, %f209, %f636;
-	mul.f32 	%f648, %f209, %f637;
-	mul.f32 	%f649, %f209, %f638;
-	fma.rn.f32 	%f980, %f643, %f980, %f647;
-	fma.rn.f32 	%f981, %f643, %f981, %f648;
-	fma.rn.f32 	%f982, %f643, %f982, %f649;
-	mul.f32 	%f650, %f209, %f639;
-	mul.f32 	%f651, %f209, %f640;
-	mul.f32 	%f652, %f209, %f641;
-	fma.rn.f32 	%f653, %f643, %f983, %f650;
-	fma.rn.f32 	%f654, %f643, %f984, %f651;
-	fma.rn.f32 	%f655, %f643, %f985, %f652;
-	mov.b32 	 %f656, %r270;
-	mul.f32 	%f657, %f209, %f656;
-	fma.rn.f32 	%f658, %f643, %f986, %f657;
-	mul.f32 	%f659, %f654, %f654;
-	fma.rn.f32 	%f660, %f653, %f653, %f659;
-	fma.rn.f32 	%f661, %f655, %f655, %f660;
-	fma.rn.f32 	%f662, %f658, %f658, %f661;
-	sqrt.rn.f32 	%f663, %f662;
-	rcp.rn.f32 	%f664, %f663;
-	mul.f32 	%f983, %f653, %f664;
-	mul.f32 	%f984, %f654, %f664;
-	mul.f32 	%f985, %f655, %f664;
-	mul.f32 	%f986, %f658, %f664;
-
-BB9_32:
-	mul.f32 	%f665, %f984, %f984;
-	fma.rn.f32 	%f666, %f983, %f983, %f665;
-	fma.rn.f32 	%f667, %f985, %f985, %f666;
-	fma.rn.f32 	%f668, %f986, %f986, %f667;
-	rcp.rn.f32 	%f669, %f668;
-	mul.f32 	%f670, %f983, %f669;
-	mul.f32 	%f671, %f984, %f669;
-	mul.f32 	%f672, %f985, %f669;
-	mul.f32 	%f673, %f986, %f669;
-	mul.f32 	%f674, %f983, %f670;
-	mul.f32 	%f675, %f984, %f671;
-	mul.f32 	%f676, %f985, %f672;
-	mul.f32 	%f677, %f983, %f671;
-	mul.f32 	%f678, %f985, %f673;
-	mul.f32 	%f679, %f983, %f672;
-	mul.f32 	%f680, %f984, %f673;
-	mul.f32 	%f681, %f984, %f672;
-	mul.f32 	%f682, %f983, %f673;
-	sub.f32 	%f683, %f674, %f675;
-	sub.f32 	%f684, %f683, %f676;
-	fma.rn.f32 	%f685, %f986, %f673, %f684;
-	sub.f32 	%f686, %f677, %f678;
-	add.f32 	%f687, %f686, %f686;
-	add.f32 	%f688, %f679, %f680;
-	add.f32 	%f689, %f688, %f688;
-	add.f32 	%f690, %f677, %f678;
-	add.f32 	%f691, %f690, %f690;
-	sub.f32 	%f692, %f675, %f674;
-	sub.f32 	%f693, %f692, %f676;
-	fma.rn.f32 	%f694, %f986, %f673, %f693;
-	sub.f32 	%f695, %f681, %f682;
-	add.f32 	%f696, %f695, %f695;
-	sub.f32 	%f697, %f679, %f680;
-	add.f32 	%f698, %f697, %f697;
-	add.f32 	%f699, %f681, %f682;
-	add.f32 	%f700, %f699, %f699;
-	neg.f32 	%f701, %f674;
-	sub.f32 	%f702, %f701, %f675;
-	add.f32 	%f703, %f676, %f702;
-	fma.rn.f32 	%f704, %f986, %f673, %f703;
-	mul.f32 	%f705, %f979, %f685;
-	fma.rn.f32 	%f706, %f981, %f687, %f705;
-	fma.rn.f32 	%f995, %f982, %f689, %f706;
-	mul.f32 	%f707, %f981, %f694;
-	fma.rn.f32 	%f708, %f979, %f691, %f707;
-	fma.rn.f32 	%f992, %f982, %f696, %f708;
-	mul.f32 	%f709, %f981, %f700;
-	fma.rn.f32 	%f710, %f979, %f698, %f709;
-	fma.rn.f32 	%f989, %f982, %f704, %f710;
-	mul.f32 	%f711, %f978, %f685;
-	fma.rn.f32 	%f994, %f980, %f687, %f711;
-	mul.f32 	%f712, %f980, %f694;
-	fma.rn.f32 	%f991, %f978, %f691, %f712;
-	mul.f32 	%f713, %f980, %f700;
-	fma.rn.f32 	%f988, %f978, %f698, %f713;
-	mul.f32 	%f993, %f977, %f685;
-	mul.f32 	%f990, %f977, %f691;
-	mul.f32 	%f987, %f977, %f698;
-
-BB9_35:
-	mul.f32 	%f745, %f988, %f992;
-	mul.f32 	%f746, %f989, %f991;
-	sub.f32 	%f747, %f746, %f745;
-	mul.f32 	%f748, %f993, %f747;
-	mul.f32 	%f749, %f987, %f992;
-	mul.f32 	%f750, %f989, %f990;
-	sub.f32 	%f751, %f750, %f749;
-	mul.f32 	%f752, %f751, %f994;
-	sub.f32 	%f753, %f748, %f752;
-	mul.f32 	%f754, %f987, %f991;
-	mul.f32 	%f755, %f988, %f990;
-	sub.f32 	%f756, %f755, %f754;
-	fma.rn.f32 	%f757, %f756, %f995, %f753;
-	rcp.rn.f32 	%f758, %f757;
-	mul.f32 	%f1002, %f747, %f758;
-	mul.f32 	%f759, %f989, %f994;
-	mul.f32 	%f760, %f988, %f995;
-	sub.f32 	%f761, %f760, %f759;
-	mul.f32 	%f1003, %f758, %f761;
-	mul.f32 	%f762, %f991, %f995;
-	mul.f32 	%f763, %f992, %f994;
-	sub.f32 	%f764, %f763, %f762;
-	mul.f32 	%f1004, %f758, %f764;
-	sub.f32 	%f765, %f749, %f750;
-	mul.f32 	%f999, %f765, %f758;
-	mul.f32 	%f766, %f987, %f995;
-	mul.f32 	%f767, %f989, %f993;
-	sub.f32 	%f768, %f767, %f766;
-	mul.f32 	%f1000, %f758, %f768;
-	mul.f32 	%f769, %f992, %f993;
-	mul.f32 	%f770, %f990, %f995;
-	sub.f32 	%f771, %f770, %f769;
-	mul.f32 	%f1001, %f758, %f771;
-	mul.f32 	%f996, %f756, %f758;
-	mul.f32 	%f772, %f988, %f993;
-	mul.f32 	%f773, %f987, %f994;
-	sub.f32 	%f774, %f773, %f772;
-	mul.f32 	%f997, %f774, %f758;
-	mul.f32 	%f775, %f990, %f994;
-	mul.f32 	%f776, %f991, %f993;
-	sub.f32 	%f777, %f776, %f775;
-	mul.f32 	%f998, %f777, %f758;
-	bra.uni 	BB9_36;
-
-BB9_25:
-	setp.ne.s32	%p35, %r185, 1;
-	mov.f32 	%f997, %f996;
-	mov.f32 	%f999, %f996;
-	mov.f32 	%f1000, %f998;
-	mov.f32 	%f1001, %f996;
-	mov.f32 	%f1002, %f998;
-	mov.f32 	%f1003, %f996;
-	mov.f32 	%f1004, %f996;
-	@%p35 bra 	BB9_36;
-
-	// inline asm
-	call (%rd146), _optix_get_static_transform_from_handle, (%rd144);
-	// inline asm
-	add.s64 	%rd270, %rd146, 64;
-
-BB9_28:
-	// inline asm
-	cvta.to.global.u64 %rd150, %rd270;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r187,%r188,%r189,%r190}, [%rd150];
-	// inline asm
-	mov.b32 	 %f1002, %r187;
-	mov.b32 	 %f1003, %r188;
-	mov.b32 	 %f1004, %r189;
-	add.s64 	%rd154, %rd270, 16;
-	// inline asm
-	cvta.to.global.u64 %rd153, %rd154;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r191,%r192,%r193,%r194}, [%rd153];
-	// inline asm
-	mov.b32 	 %f999, %r191;
-	mov.b32 	 %f1000, %r192;
-	mov.b32 	 %f1001, %r193;
-	add.s64 	%rd157, %rd270, 32;
-	// inline asm
-	cvta.to.global.u64 %rd156, %rd157;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r195,%r196,%r197,%r198}, [%rd156];
-	// inline asm
-	mov.b32 	 %f996, %r195;
-	mov.b32 	 %f997, %r196;
-	mov.b32 	 %f998, %r197;
-
-BB9_36:
-	setp.eq.s32	%p39, %r903, 0;
-	@%p39 bra 	BB9_37;
-	bra.uni 	BB9_38;
-
-BB9_37:
-	mov.f32 	%f976, %f996;
-	mov.f32 	%f975, %f997;
-	mov.f32 	%f974, %f998;
-	mov.f32 	%f973, %f999;
-	mov.f32 	%f972, %f1000;
-	mov.f32 	%f971, %f1001;
-	mov.f32 	%f970, %f1002;
-	mov.f32 	%f969, %f1003;
-	mov.f32 	%f968, %f1004;
-	bra.uni 	BB9_39;
-
-BB9_38:
-	mul.f32 	%f778, %f973, %f1003;
-	fma.rn.f32 	%f779, %f970, %f1002, %f778;
-	fma.rn.f32 	%f289, %f976, %f1004, %f779;
-	mul.f32 	%f780, %f972, %f1003;
-	fma.rn.f32 	%f781, %f969, %f1002, %f780;
-	fma.rn.f32 	%f290, %f975, %f1004, %f781;
-	mul.f32 	%f782, %f971, %f1003;
-	fma.rn.f32 	%f783, %f968, %f1002, %f782;
-	fma.rn.f32 	%f291, %f974, %f1004, %f783;
-	mul.f32 	%f784, %f973, %f1000;
-	fma.rn.f32 	%f785, %f970, %f999, %f784;
-	fma.rn.f32 	%f292, %f976, %f1001, %f785;
-	mul.f32 	%f786, %f972, %f1000;
-	fma.rn.f32 	%f787, %f969, %f999, %f786;
-	fma.rn.f32 	%f293, %f975, %f1001, %f787;
-	mul.f32 	%f788, %f971, %f1000;
-	fma.rn.f32 	%f789, %f968, %f999, %f788;
-	fma.rn.f32 	%f294, %f974, %f1001, %f789;
-	mul.f32 	%f790, %f973, %f997;
-	fma.rn.f32 	%f791, %f970, %f996, %f790;
-	fma.rn.f32 	%f976, %f976, %f998, %f791;
-	mul.f32 	%f792, %f972, %f997;
-	fma.rn.f32 	%f793, %f969, %f996, %f792;
-	fma.rn.f32 	%f975, %f975, %f998, %f793;
-	mul.f32 	%f794, %f971, %f997;
-	fma.rn.f32 	%f795, %f968, %f996, %f794;
-	fma.rn.f32 	%f974, %f974, %f998, %f795;
-	mov.f32 	%f973, %f292;
-	mov.f32 	%f972, %f293;
-	mov.f32 	%f971, %f294;
-	mov.f32 	%f970, %f289;
-	mov.f32 	%f969, %f290;
-	mov.f32 	%f968, %f291;
-
-BB9_39:
-	add.s32 	%r903, %r903, 1;
-	setp.lt.u32	%p40, %r903, %r32;
-	@%p40 bra 	BB9_23;
-
-	mul.f32 	%f796, %f609, %f969;
-	fma.rn.f32 	%f797, %f608, %f970, %f796;
-	fma.rn.f32 	%f1014, %f1016, %f968, %f797;
-	mul.f32 	%f798, %f609, %f972;
-	fma.rn.f32 	%f799, %f608, %f973, %f798;
-	fma.rn.f32 	%f1015, %f1016, %f971, %f799;
-	mul.f32 	%f800, %f609, %f975;
-	fma.rn.f32 	%f801, %f608, %f976, %f800;
-	fma.rn.f32 	%f1016, %f1016, %f974, %f801;
-	bra.uni 	BB9_41;
-
-BB9_22:
-	mov.f32 	%f1014, %f608;
-	mov.f32 	%f1015, %f609;
-
-BB9_41:
-	// inline asm
-	call (%f802), _optix_get_ray_tmin, ();
-	// inline asm
-	// inline asm
-	call (%f803), _optix_get_ray_tmax, ();
-	// inline asm
-	ld.u8 	%rs10, [%rd1+324];
-	setp.eq.s16	%p41, %rs10, 0;
-	cvt.f64.f32	%fd1, %f1016;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r8}, %fd1;
-	}
-	mov.f64 	%fd292, 0d4000000000000000;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r9}, %fd292;
-	}
-	bfe.u32 	%r333, %r9, 20, 11;
-	add.s32 	%r334, %r333, -1012;
-	cvt.u64.u32	%rd19, %r334;
-	mov.u64 	%rd265, 4611686018427387904;
-	shl.b64 	%rd20, %rd265, %r334;
-	setp.eq.s64	%p42, %rd20, -9223372036854775808;
-	abs.f64 	%fd2, %fd1;
-	// Callseq Start 0
-	{
-	.reg .b32 temp_param_reg;
-	// <end>}
-	.param .b64 param0;
-	st.param.f64	[param0+0], %fd2;
-	.param .b64 retval0;
-	call.uni (retval0), 
-	__internal_accurate_pow, 
-	(
-	param0
-	);
-	ld.param.f64	%fd413, [retval0+0];
-	
-	//{
-	}// Callseq End 0
-	setp.lt.s32	%p43, %r8, 0;
-	and.pred  	%p1, %p43, %p42;
-	@%p41 bra 	BB9_189;
-
-	ld.v2.f32 	{%f804, %f805}, [%rd1+312];
-	ld.v2.f32 	{%f807, %f808}, [%rd1+288];
-	ld.f32 	%f809, [%rd1+296];
-	fma.rn.f32 	%f317, %f805, 0fC0000000, %f809;
-	ld.f32 	%f810, [%rd1+308];
-	add.f32 	%f320, %f810, 0f3F800000;
-	neg.f32 	%f319, %f320;
-	cvt.f64.f32	%fd4, %f320;
-	@!%p1 bra 	BB9_44;
-	bra.uni 	BB9_43;
-
-BB9_43:
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r335}, %fd413;
-	}
-	xor.b32  	%r336, %r335, -2147483648;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r337, %temp}, %fd413;
-	}
-	mov.b64 	%fd413, {%r337, %r336};
-
-BB9_44:
-	setp.eq.f32	%p44, %f1016, 0f00000000;
-	@%p44 bra 	BB9_47;
-	bra.uni 	BB9_45;
-
-BB9_47:
-	selp.b32	%r338, %r8, 0, %p42;
-	mov.u32 	%r339, 0;
-	or.b32  	%r340, %r338, 2146435072;
-	setp.lt.s32	%p48, %r9, 0;
-	selp.b32	%r341, %r340, %r338, %p48;
-	mov.b64 	%fd413, {%r339, %r341};
-	bra.uni 	BB9_48;
-
-BB9_189:
-	ld.v4.f32 	{%f844, %f845, %f846, %f847}, [%rd1+288];
-	ld.f32 	%f336, [%rd1+312];
-	ld.f32 	%f848, [%rd1+308];
-	add.f32 	%f338, %f848, 0f3F800000;
-	neg.f32 	%f337, %f338;
-	cvt.f64.f32	%fd116, %f338;
-	@!%p1 bra 	BB9_191;
-	bra.uni 	BB9_190;
-
-BB9_190:
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r556}, %fd413;
-	}
-	xor.b32  	%r557, %r556, -2147483648;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r558, %temp}, %fd413;
-	}
-	mov.b64 	%fd413, {%r558, %r557};
-
-BB9_191:
-	setp.eq.f32	%p226, %f1016, 0f00000000;
-	@%p226 bra 	BB9_194;
-	bra.uni 	BB9_192;
-
-BB9_194:
-	selp.b32	%r559, %r8, 0, %p42;
-	mov.u32 	%r560, 0;
-	or.b32  	%r561, %r559, 2146435072;
-	setp.lt.s32	%p230, %r9, 0;
-	selp.b32	%r562, %r561, %r559, %p230;
-	mov.b64 	%fd413, {%r560, %r562};
-	bra.uni 	BB9_195;
-
-BB9_45:
-	setp.gt.s32	%p45, %r8, -1;
-	@%p45 bra 	BB9_48;
-
-	cvt.rzi.f64.f64	%fd294, %fd292;
-	setp.neu.f64	%p46, %fd294, 0d4000000000000000;
-	selp.f64	%fd413, 0dFFF8000000000000, %fd413, %p46;
-
-BB9_48:
-	add.f64 	%fd415, %fd1, 0d4000000000000000;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r342}, %fd415;
-	}
-	and.b32  	%r343, %r342, 2146435072;
-	setp.ne.s32	%p49, %r343, 2146435072;
-	@%p49 bra 	BB9_49;
-
-	setp.gtu.f64	%p50, %fd2, 0d7FF0000000000000;
-	@%p50 bra 	BB9_58;
-
-	and.b32  	%r344, %r9, 2147483647;
-	setp.ne.s32	%p51, %r344, 2146435072;
-	@%p51 bra 	BB9_53;
-
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r345, %temp}, %fd292;
-	}
-	setp.eq.s32	%p52, %r345, 0;
-	@%p52 bra 	BB9_57;
-
-BB9_53:
-	and.b32  	%r346, %r8, 2147483647;
-	setp.ne.s32	%p53, %r346, 2146435072;
-	@%p53 bra 	BB9_54;
-
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r347, %temp}, %fd1;
-	}
-	setp.ne.s32	%p54, %r347, 0;
-	mov.f64 	%fd415, %fd413;
-	@%p54 bra 	BB9_58;
-
-	shr.s32 	%r348, %r9, 31;
-	and.b32  	%r349, %r348, -2146435072;
-	add.s32 	%r350, %r349, 2146435072;
-	or.b32  	%r351, %r350, -2147483648;
-	selp.b32	%r352, %r351, %r350, %p1;
-	mov.u32 	%r353, 0;
-	mov.b64 	%fd415, {%r353, %r352};
-	bra.uni 	BB9_58;
-
-BB9_49:
-	mov.f64 	%fd415, %fd413;
-	bra.uni 	BB9_58;
-
-BB9_192:
-	setp.gt.s32	%p227, %r8, -1;
-	@%p227 bra 	BB9_195;
-
-	cvt.rzi.f64.f64	%fd343, %fd292;
-	setp.neu.f64	%p228, %fd343, 0d4000000000000000;
-	selp.f64	%fd413, 0dFFF8000000000000, %fd413, %p228;
-
-BB9_195:
-	add.f64 	%fd442, %fd1, 0d4000000000000000;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r563}, %fd442;
-	}
-	and.b32  	%r564, %r563, 2146435072;
-	setp.ne.s32	%p231, %r564, 2146435072;
-	@%p231 bra 	BB9_196;
-
-	setp.gtu.f64	%p232, %fd2, 0d7FF0000000000000;
-	@%p232 bra 	BB9_205;
-
-	and.b32  	%r565, %r9, 2147483647;
-	setp.ne.s32	%p233, %r565, 2146435072;
-	@%p233 bra 	BB9_200;
-
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r566, %temp}, %fd292;
-	}
-	setp.eq.s32	%p234, %r566, 0;
-	@%p234 bra 	BB9_204;
-
-BB9_200:
-	and.b32  	%r567, %r8, 2147483647;
-	setp.ne.s32	%p235, %r567, 2146435072;
-	@%p235 bra 	BB9_201;
-
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r568, %temp}, %fd1;
-	}
-	setp.ne.s32	%p236, %r568, 0;
-	mov.f64 	%fd442, %fd413;
-	@%p236 bra 	BB9_205;
-
-	shr.s32 	%r569, %r9, 31;
-	and.b32  	%r570, %r569, -2146435072;
-	add.s32 	%r571, %r570, 2146435072;
-	or.b32  	%r572, %r571, -2147483648;
-	selp.b32	%r573, %r572, %r571, %p1;
-	mov.u32 	%r574, 0;
-	mov.b64 	%fd442, {%r574, %r573};
-	bra.uni 	BB9_205;
-
-BB9_196:
-	mov.f64 	%fd442, %fd413;
-	bra.uni 	BB9_205;
-
-BB9_54:
-	mov.f64 	%fd415, %fd413;
-	bra.uni 	BB9_58;
-
-BB9_201:
-	mov.f64 	%fd442, %fd413;
-	bra.uni 	BB9_205;
-
-BB9_57:
-	setp.gt.f64	%p55, %fd2, 0d3FF0000000000000;
-	selp.b32	%r354, 2146435072, 0, %p55;
-	mov.u32 	%r355, 0;
-	xor.b32  	%r356, %r354, 2146435072;
-	setp.lt.s32	%p56, %r9, 0;
-	selp.b32	%r357, %r356, %r354, %p56;
-	setp.eq.f32	%p57, %f1016, 0fBF800000;
-	selp.b32	%r358, 1072693248, %r357, %p57;
-	mov.b64 	%fd415, {%r355, %r358};
-
-BB9_58:
-	cvt.f64.f32	%fd14, %f1014;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r10}, %fd14;
-	}
-	abs.f64 	%fd15, %fd14;
-	// Callseq Start 1
-	{
-	.reg .b32 temp_param_reg;
-	// <end>}
-	.param .b64 param0;
-	st.param.f64	[param0+0], %fd15;
-	.param .b64 retval0;
-	call.uni (retval0), 
-	__internal_accurate_pow, 
-	(
-	param0
-	);
-	ld.param.f64	%fd417, [retval0+0];
-	
-	//{
-	}// Callseq End 1
-	setp.gt.s32	%p58, %r10, -1;
-	setp.lt.s32	%p59, %r10, 0;
-	setp.ne.s64	%p60, %rd20, -9223372036854775808;
-	and.pred  	%p2, %p59, %p42;
-	or.pred  	%p62, %p58, %p60;
-	@%p62 bra 	BB9_60;
-
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r359}, %fd417;
-	}
-	xor.b32  	%r360, %r359, -2147483648;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r361, %temp}, %fd417;
-	}
-	mov.b64 	%fd417, {%r361, %r360};
-
-BB9_60:
-	setp.eq.f32	%p63, %f1014, 0f00000000;
-	@%p63 bra 	BB9_63;
-	bra.uni 	BB9_61;
-
-BB9_63:
-	selp.b32	%r362, %r10, 0, %p42;
-	mov.u32 	%r363, 0;
-	or.b32  	%r364, %r362, 2146435072;
-	setp.lt.s32	%p67, %r9, 0;
-	selp.b32	%r365, %r364, %r362, %p67;
-	mov.b64 	%fd417, {%r363, %r365};
-	bra.uni 	BB9_64;
-
-BB9_61:
-	@%p58 bra 	BB9_64;
-
-	cvt.rzi.f64.f64	%fd297, %fd292;
-	setp.neu.f64	%p65, %fd297, 0d4000000000000000;
-	selp.f64	%fd417, 0dFFF8000000000000, %fd417, %p65;
-
-BB9_64:
-	add.f64 	%fd418, %fd14, 0d4000000000000000;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r366}, %fd418;
-	}
-	and.b32  	%r367, %r366, 2146435072;
-	setp.ne.s32	%p68, %r367, 2146435072;
-	@%p68 bra 	BB9_65;
-
-	setp.gtu.f64	%p69, %fd15, 0d7FF0000000000000;
-	@%p69 bra 	BB9_74;
-
-	and.b32  	%r368, %r9, 2147483647;
-	setp.ne.s32	%p70, %r368, 2146435072;
-	@%p70 bra 	BB9_69;
-
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r369, %temp}, %fd292;
-	}
-	setp.eq.s32	%p71, %r369, 0;
-	@%p71 bra 	BB9_73;
-
-BB9_69:
-	and.b32  	%r370, %r10, 2147483647;
-	setp.ne.s32	%p72, %r370, 2146435072;
-	@%p72 bra 	BB9_70;
-
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r371, %temp}, %fd14;
-	}
-	setp.ne.s32	%p73, %r371, 0;
-	mov.f64 	%fd418, %fd417;
-	@%p73 bra 	BB9_74;
-
-	shr.s32 	%r372, %r9, 31;
-	and.b32  	%r373, %r372, -2146435072;
-	add.s32 	%r374, %r373, 2146435072;
-	or.b32  	%r375, %r374, -2147483648;
-	selp.b32	%r376, %r375, %r374, %p2;
-	mov.u32 	%r377, 0;
-	mov.b64 	%fd418, {%r377, %r376};
-	bra.uni 	BB9_74;
-
-BB9_65:
-	mov.f64 	%fd418, %fd417;
-	bra.uni 	BB9_74;
-
-BB9_70:
-	mov.f64 	%fd418, %fd417;
-	bra.uni 	BB9_74;
-
-BB9_73:
-	setp.gt.f64	%p74, %fd15, 0d3FF0000000000000;
-	selp.b32	%r378, 2146435072, 0, %p74;
-	mov.u32 	%r379, 0;
-	xor.b32  	%r380, %r378, 2146435072;
-	setp.lt.s32	%p75, %r9, 0;
-	selp.b32	%r381, %r380, %r378, %p75;
-	setp.eq.f32	%p76, %f1014, 0fBF800000;
-	selp.b32	%r382, 1072693248, %r381, %p76;
-	mov.b64 	%fd418, {%r379, %r382};
-
-BB9_74:
-	setp.eq.f32	%p77, %f1014, 0f3F800000;
-	selp.f64	%fd299, 0d3FF0000000000000, %fd418, %p77;
-	setp.eq.f32	%p78, %f1016, 0f3F800000;
-	selp.f64	%fd300, 0d3FF0000000000000, %fd415, %p78;
-	fma.rn.f64 	%fd26, %fd4, %fd300, %fd299;
-	cvt.f64.f32	%fd27, %f1015;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r11}, %fd27;
-	}
-	abs.f64 	%fd28, %fd27;
-	// Callseq Start 2
-	{
-	.reg .b32 temp_param_reg;
-	// <end>}
-	.param .b64 param0;
-	st.param.f64	[param0+0], %fd28;
-	.param .b64 retval0;
-	call.uni (retval0), 
-	__internal_accurate_pow, 
-	(
-	param0
-	);
-	ld.param.f64	%fd420, [retval0+0];
-	
-	//{
-	}// Callseq End 2
-	setp.gt.s32	%p79, %r11, -1;
-	setp.lt.s32	%p80, %r11, 0;
-	and.pred  	%p3, %p80, %p42;
-	or.pred  	%p83, %p79, %p60;
-	@%p83 bra 	BB9_76;
-
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r383}, %fd420;
-	}
-	xor.b32  	%r384, %r383, -2147483648;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r385, %temp}, %fd420;
-	}
-	mov.b64 	%fd420, {%r385, %r384};
-
-BB9_76:
-	setp.eq.f32	%p84, %f1015, 0f00000000;
-	@%p84 bra 	BB9_79;
-	bra.uni 	BB9_77;
-
-BB9_79:
-	selp.b32	%r386, %r11, 0, %p42;
-	mov.u32 	%r387, 0;
-	or.b32  	%r388, %r386, 2146435072;
-	setp.lt.s32	%p88, %r9, 0;
-	selp.b32	%r389, %r388, %r386, %p88;
-	mov.b64 	%fd420, {%r387, %r389};
-	bra.uni 	BB9_80;
-
-BB9_77:
-	@%p79 bra 	BB9_80;
-
-	cvt.rzi.f64.f64	%fd302, %fd292;
-	setp.neu.f64	%p86, %fd302, 0d4000000000000000;
-	selp.f64	%fd420, 0dFFF8000000000000, %fd420, %p86;
-
-BB9_80:
-	add.f64 	%fd421, %fd27, 0d4000000000000000;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r390}, %fd421;
-	}
-	and.b32  	%r391, %r390, 2146435072;
-	setp.ne.s32	%p89, %r391, 2146435072;
-	@%p89 bra 	BB9_81;
-
-	setp.gtu.f64	%p90, %fd28, 0d7FF0000000000000;
-	@%p90 bra 	BB9_90;
-
-	and.b32  	%r392, %r9, 2147483647;
-	setp.ne.s32	%p91, %r392, 2146435072;
-	@%p91 bra 	BB9_85;
-
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r393, %temp}, %fd292;
-	}
-	setp.eq.s32	%p92, %r393, 0;
-	@%p92 bra 	BB9_89;
-
-BB9_85:
-	and.b32  	%r394, %r11, 2147483647;
-	setp.ne.s32	%p93, %r394, 2146435072;
-	@%p93 bra 	BB9_86;
-
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r395, %temp}, %fd27;
-	}
-	setp.ne.s32	%p94, %r395, 0;
-	mov.f64 	%fd421, %fd420;
-	@%p94 bra 	BB9_90;
-
-	shr.s32 	%r396, %r9, 31;
-	and.b32  	%r397, %r396, -2146435072;
-	add.s32 	%r398, %r397, 2146435072;
-	or.b32  	%r399, %r398, -2147483648;
-	selp.b32	%r400, %r399, %r398, %p3;
-	mov.u32 	%r401, 0;
-	mov.b64 	%fd421, {%r401, %r400};
-	bra.uni 	BB9_90;
-
-BB9_81:
-	mov.f64 	%fd421, %fd420;
-	bra.uni 	BB9_90;
-
-BB9_86:
-	mov.f64 	%fd421, %fd420;
-	bra.uni 	BB9_90;
-
-BB9_89:
-	setp.gt.f64	%p95, %fd28, 0d3FF0000000000000;
-	selp.b32	%r402, 2146435072, 0, %p95;
-	mov.u32 	%r403, 0;
-	xor.b32  	%r404, %r402, 2146435072;
-	setp.lt.s32	%p96, %r9, 0;
-	selp.b32	%r405, %r404, %r402, %p96;
-	setp.eq.f32	%p97, %f1015, 0fBF800000;
-	selp.b32	%r406, 1072693248, %r405, %p97;
-	mov.b64 	%fd421, {%r403, %r406};
-
-BB9_90:
-	setp.eq.f32	%p98, %f1015, 0f3F800000;
-	selp.f64	%fd304, 0d3FF0000000000000, %fd421, %p98;
-	add.f64 	%fd305, %fd26, %fd304;
-	cvt.rn.f32.f64	%f321, %fd305;
-	add.f32 	%f811, %f320, %f320;
-	mul.f32 	%f812, %f965, %f811;
-	mul.f32 	%f813, %f1016, %f812;
-	add.f32 	%f814, %f319, %f319;
-	mul.f32 	%f322, %f317, %f814;
-	fma.rn.f32 	%f815, %f1016, %f322, %f813;
-	add.f32 	%f816, %f967, %f967;
-	fma.rn.f32 	%f817, %f1014, %f816, %f815;
-	add.f32 	%f323, %f807, %f807;
-	mul.f32 	%f818, %f323, %f1014;
-	sub.f32 	%f819, %f817, %f818;
-	add.f32 	%f820, %f966, %f966;
-	fma.rn.f32 	%f821, %f1015, %f820, %f819;
-	add.f32 	%f324, %f808, %f808;
-	mul.f32 	%f822, %f324, %f1015;
-	sub.f32 	%f823, %f821, %f822;
-	add.f32 	%f824, %f1016, %f1016;
-	div.rn.f32 	%f825, %f824, %f804;
-	sub.f32 	%f325, %f823, %f825;
-	cvt.f64.f32	%fd39, %f965;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r12}, %fd39;
-	}
-	abs.f64 	%fd40, %fd39;
-	// Callseq Start 3
-	{
-	.reg .b32 temp_param_reg;
-	// <end>}
-	.param .b64 param0;
-	st.param.f64	[param0+0], %fd40;
-	.param .b64 retval0;
-	call.uni (retval0), 
-	__internal_accurate_pow, 
-	(
-	param0
-	);
-	ld.param.f64	%fd423, [retval0+0];
-	
-	//{
-	}// Callseq End 3
-	setp.gt.s32	%p99, %r12, -1;
-	setp.lt.s32	%p100, %r12, 0;
-	and.pred  	%p4, %p100, %p42;
-	or.pred  	%p103, %p99, %p60;
-	@%p103 bra 	BB9_92;
-
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r407}, %fd423;
-	}
-	xor.b32  	%r408, %r407, -2147483648;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r409, %temp}, %fd423;
-	}
-	mov.b64 	%fd423, {%r409, %r408};
-
-BB9_92:
-	setp.eq.f32	%p104, %f965, 0f00000000;
-	@%p104 bra 	BB9_95;
-	bra.uni 	BB9_93;
-
-BB9_95:
-	selp.b32	%r410, %r12, 0, %p42;
-	mov.u32 	%r411, 0;
-	or.b32  	%r412, %r410, 2146435072;
-	setp.lt.s32	%p108, %r9, 0;
-	selp.b32	%r413, %r412, %r410, %p108;
-	mov.b64 	%fd423, {%r411, %r413};
-	bra.uni 	BB9_96;
-
-BB9_93:
-	@%p99 bra 	BB9_96;
-
-	cvt.rzi.f64.f64	%fd307, %fd292;
-	setp.neu.f64	%p106, %fd307, 0d4000000000000000;
-	selp.f64	%fd423, 0dFFF8000000000000, %fd423, %p106;
-
-BB9_96:
-	add.f64 	%fd424, %fd39, 0d4000000000000000;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r414}, %fd424;
-	}
-	and.b32  	%r415, %r414, 2146435072;
-	setp.ne.s32	%p109, %r415, 2146435072;
-	@%p109 bra 	BB9_97;
-
-	setp.gtu.f64	%p110, %fd40, 0d7FF0000000000000;
-	@%p110 bra 	BB9_106;
-
-	and.b32  	%r416, %r9, 2147483647;
-	setp.ne.s32	%p111, %r416, 2146435072;
-	@%p111 bra 	BB9_101;
-
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r417, %temp}, %fd292;
-	}
-	setp.eq.s32	%p112, %r417, 0;
-	@%p112 bra 	BB9_105;
-
-BB9_101:
-	and.b32  	%r418, %r12, 2147483647;
-	setp.ne.s32	%p113, %r418, 2146435072;
-	@%p113 bra 	BB9_102;
-
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r419, %temp}, %fd39;
-	}
-	setp.ne.s32	%p114, %r419, 0;
-	mov.f64 	%fd424, %fd423;
-	@%p114 bra 	BB9_106;
-
-	shr.s32 	%r420, %r9, 31;
-	and.b32  	%r421, %r420, -2146435072;
-	add.s32 	%r422, %r421, 2146435072;
-	or.b32  	%r423, %r422, -2147483648;
-	selp.b32	%r424, %r423, %r422, %p4;
-	mov.u32 	%r425, 0;
-	mov.b64 	%fd424, {%r425, %r424};
-	bra.uni 	BB9_106;
-
-BB9_97:
-	mov.f64 	%fd424, %fd423;
-	bra.uni 	BB9_106;
-
-BB9_102:
-	mov.f64 	%fd424, %fd423;
-	bra.uni 	BB9_106;
-
-BB9_105:
-	setp.gt.f64	%p115, %fd40, 0d3FF0000000000000;
-	selp.b32	%r426, 2146435072, 0, %p115;
-	mov.u32 	%r427, 0;
-	xor.b32  	%r428, %r426, 2146435072;
-	setp.lt.s32	%p116, %r9, 0;
-	selp.b32	%r429, %r428, %r426, %p116;
-	setp.eq.f32	%p117, %f965, 0fBF800000;
-	selp.b32	%r430, 1072693248, %r429, %p117;
-	mov.b64 	%fd424, {%r427, %r430};
-
-BB9_106:
-	setp.eq.f32	%p118, %f965, 0f3F800000;
-	selp.f64	%fd309, 0d3FF0000000000000, %fd424, %p118;
-	mul.f32 	%f826, %f965, %f322;
-	cvt.f64.f32	%fd310, %f826;
-	fma.rn.f64 	%fd51, %fd4, %fd309, %fd310;
-	neg.f32 	%f326, %f317;
-	cvt.f64.f32	%fd52, %f326;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r13}, %fd52;
-	}
-	abs.f64 	%fd53, %fd52;
-	// Callseq Start 4
-	{
-	.reg .b32 temp_param_reg;
-	// <end>}
-	.param .b64 param0;
-	st.param.f64	[param0+0], %fd53;
-	.param .b64 retval0;
-	call.uni (retval0), 
-	__internal_accurate_pow, 
-	(
-	param0
-	);
-	ld.param.f64	%fd426, [retval0+0];
-	
-	//{
-	}// Callseq End 4
-	setp.gt.s32	%p119, %r13, -1;
-	setp.lt.s32	%p120, %r13, 0;
-	and.pred  	%p5, %p120, %p42;
-	or.pred  	%p123, %p119, %p60;
-	@%p123 bra 	BB9_108;
-
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r431}, %fd426;
-	}
-	xor.b32  	%r432, %r431, -2147483648;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r433, %temp}, %fd426;
-	}
-	mov.b64 	%fd426, {%r433, %r432};
-
-BB9_108:
-	setp.eq.f32	%p124, %f326, 0f00000000;
-	@%p124 bra 	BB9_111;
-	bra.uni 	BB9_109;
-
-BB9_111:
-	selp.b32	%r434, %r13, 0, %p42;
-	mov.u32 	%r435, 0;
-	or.b32  	%r436, %r434, 2146435072;
-	setp.lt.s32	%p128, %r9, 0;
-	selp.b32	%r437, %r436, %r434, %p128;
-	mov.b64 	%fd426, {%r435, %r437};
-	bra.uni 	BB9_112;
-
-BB9_109:
-	@%p119 bra 	BB9_112;
-
-	cvt.rzi.f64.f64	%fd312, %fd292;
-	setp.neu.f64	%p126, %fd312, 0d4000000000000000;
-	selp.f64	%fd426, 0dFFF8000000000000, %fd426, %p126;
-
-BB9_112:
-	add.f64 	%fd427, %fd52, 0d4000000000000000;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r438}, %fd427;
-	}
-	and.b32  	%r439, %r438, 2146435072;
-	setp.ne.s32	%p129, %r439, 2146435072;
-	@%p129 bra 	BB9_113;
-
-	setp.gtu.f64	%p130, %fd53, 0d7FF0000000000000;
-	@%p130 bra 	BB9_122;
-
-	and.b32  	%r440, %r9, 2147483647;
-	setp.ne.s32	%p131, %r440, 2146435072;
-	@%p131 bra 	BB9_117;
-
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r441, %temp}, %fd292;
-	}
-	setp.eq.s32	%p132, %r441, 0;
-	@%p132 bra 	BB9_121;
-
-BB9_117:
-	and.b32  	%r442, %r13, 2147483647;
-	setp.ne.s32	%p133, %r442, 2146435072;
-	@%p133 bra 	BB9_118;
-
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r443, %temp}, %fd52;
-	}
-	setp.ne.s32	%p134, %r443, 0;
-	mov.f64 	%fd427, %fd426;
-	@%p134 bra 	BB9_122;
-
-	shr.s32 	%r444, %r9, 31;
-	and.b32  	%r445, %r444, -2146435072;
-	add.s32 	%r446, %r445, 2146435072;
-	or.b32  	%r447, %r446, -2147483648;
-	selp.b32	%r448, %r447, %r446, %p5;
-	mov.u32 	%r449, 0;
-	mov.b64 	%fd427, {%r449, %r448};
-	bra.uni 	BB9_122;
-
-BB9_113:
-	mov.f64 	%fd427, %fd426;
-	bra.uni 	BB9_122;
-
-BB9_118:
-	mov.f64 	%fd427, %fd426;
-	bra.uni 	BB9_122;
-
-BB9_121:
-	setp.gt.f64	%p135, %fd53, 0d3FF0000000000000;
-	selp.b32	%r450, 2146435072, 0, %p135;
-	mov.u32 	%r451, 0;
-	xor.b32  	%r452, %r450, 2146435072;
-	setp.lt.s32	%p136, %r9, 0;
-	selp.b32	%r453, %r452, %r450, %p136;
-	setp.eq.f32	%p137, %f326, 0fBF800000;
-	selp.b32	%r454, 1072693248, %r453, %p137;
-	mov.b64 	%fd427, {%r451, %r454};
-
-BB9_122:
-	setp.eq.f32	%p138, %f326, 0f3F800000;
-	selp.f64	%fd314, 0d3FF0000000000000, %fd427, %p138;
-	cvt.f64.f32	%fd315, %f319;
-	mul.f64 	%fd316, %fd315, %fd314;
-	sub.f64 	%fd64, %fd51, %fd316;
-	cvt.f64.f32	%fd65, %f967;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r14}, %fd65;
-	}
-	abs.f64 	%fd66, %fd65;
-	// Callseq Start 5
-	{
-	.reg .b32 temp_param_reg;
-	// <end>}
-	.param .b64 param0;
-	st.param.f64	[param0+0], %fd66;
-	.param .b64 retval0;
-	call.uni (retval0), 
-	__internal_accurate_pow, 
-	(
-	param0
-	);
-	ld.param.f64	%fd429, [retval0+0];
-	
-	//{
-	}// Callseq End 5
-	setp.gt.s32	%p139, %r14, -1;
-	setp.lt.s32	%p140, %r14, 0;
-	and.pred  	%p6, %p140, %p42;
-	or.pred  	%p143, %p139, %p60;
-	@%p143 bra 	BB9_124;
-
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r455}, %fd429;
-	}
-	xor.b32  	%r456, %r455, -2147483648;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r457, %temp}, %fd429;
-	}
-	mov.b64 	%fd429, {%r457, %r456};
-
-BB9_124:
-	setp.eq.f32	%p144, %f967, 0f00000000;
-	@%p144 bra 	BB9_127;
-	bra.uni 	BB9_125;
-
-BB9_127:
-	selp.b32	%r458, %r14, 0, %p42;
-	mov.u32 	%r459, 0;
-	or.b32  	%r460, %r458, 2146435072;
-	setp.lt.s32	%p148, %r9, 0;
-	selp.b32	%r461, %r460, %r458, %p148;
-	mov.b64 	%fd429, {%r459, %r461};
-	bra.uni 	BB9_128;
-
-BB9_125:
-	@%p139 bra 	BB9_128;
-
-	cvt.rzi.f64.f64	%fd318, %fd292;
-	setp.neu.f64	%p146, %fd318, 0d4000000000000000;
-	selp.f64	%fd429, 0dFFF8000000000000, %fd429, %p146;
-
-BB9_128:
-	add.f64 	%fd430, %fd65, 0d4000000000000000;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r462}, %fd430;
-	}
-	and.b32  	%r463, %r462, 2146435072;
-	setp.ne.s32	%p149, %r463, 2146435072;
-	@%p149 bra 	BB9_129;
-
-	setp.gtu.f64	%p150, %fd66, 0d7FF0000000000000;
-	@%p150 bra 	BB9_138;
-
-	and.b32  	%r464, %r9, 2147483647;
-	setp.ne.s32	%p151, %r464, 2146435072;
-	@%p151 bra 	BB9_133;
-
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r465, %temp}, %fd292;
-	}
-	setp.eq.s32	%p152, %r465, 0;
-	@%p152 bra 	BB9_137;
-
-BB9_133:
-	and.b32  	%r466, %r14, 2147483647;
-	setp.ne.s32	%p153, %r466, 2146435072;
-	@%p153 bra 	BB9_134;
-
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r467, %temp}, %fd65;
-	}
-	setp.ne.s32	%p154, %r467, 0;
-	mov.f64 	%fd430, %fd429;
-	@%p154 bra 	BB9_138;
-
-	shr.s32 	%r468, %r9, 31;
-	and.b32  	%r469, %r468, -2146435072;
-	add.s32 	%r470, %r469, 2146435072;
-	or.b32  	%r471, %r470, -2147483648;
-	selp.b32	%r472, %r471, %r470, %p6;
-	mov.u32 	%r473, 0;
-	mov.b64 	%fd430, {%r473, %r472};
-	bra.uni 	BB9_138;
-
-BB9_129:
-	mov.f64 	%fd430, %fd429;
-	bra.uni 	BB9_138;
-
-BB9_134:
-	mov.f64 	%fd430, %fd429;
-	bra.uni 	BB9_138;
-
-BB9_137:
-	setp.gt.f64	%p155, %fd66, 0d3FF0000000000000;
-	selp.b32	%r474, 2146435072, 0, %p155;
-	mov.u32 	%r475, 0;
-	xor.b32  	%r476, %r474, 2146435072;
-	setp.lt.s32	%p156, %r9, 0;
-	selp.b32	%r477, %r476, %r474, %p156;
-	setp.eq.f32	%p157, %f967, 0fBF800000;
-	selp.b32	%r478, 1072693248, %r477, %p157;
-	mov.b64 	%fd430, {%r475, %r478};
-
-BB9_138:
-	setp.eq.f32	%p158, %f967, 0f3F800000;
-	selp.f64	%fd320, 0d3FF0000000000000, %fd430, %p158;
-	add.f64 	%fd321, %fd64, %fd320;
-	mul.f32 	%f827, %f323, %f967;
-	cvt.f64.f32	%fd322, %f827;
-	sub.f64 	%fd77, %fd321, %fd322;
-	neg.f32 	%f327, %f807;
-	cvt.f64.f32	%fd78, %f327;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r15}, %fd78;
-	}
-	abs.f64 	%fd79, %fd78;
-	// Callseq Start 6
-	{
-	.reg .b32 temp_param_reg;
-	// <end>}
-	.param .b64 param0;
-	st.param.f64	[param0+0], %fd79;
-	.param .b64 retval0;
-	call.uni (retval0), 
-	__internal_accurate_pow, 
-	(
-	param0
-	);
-	ld.param.f64	%fd432, [retval0+0];
-	
-	//{
-	}// Callseq End 6
-	setp.gt.s32	%p159, %r15, -1;
-	setp.lt.s32	%p160, %r15, 0;
-	and.pred  	%p7, %p160, %p42;
-	or.pred  	%p163, %p159, %p60;
-	@%p163 bra 	BB9_140;
-
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r479}, %fd432;
-	}
-	xor.b32  	%r480, %r479, -2147483648;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r481, %temp}, %fd432;
-	}
-	mov.b64 	%fd432, {%r481, %r480};
-
-BB9_140:
-	setp.eq.f32	%p164, %f327, 0f00000000;
-	@%p164 bra 	BB9_143;
-	bra.uni 	BB9_141;
-
-BB9_143:
-	selp.b32	%r482, %r15, 0, %p42;
-	mov.u32 	%r483, 0;
-	or.b32  	%r484, %r482, 2146435072;
-	setp.lt.s32	%p168, %r9, 0;
-	selp.b32	%r485, %r484, %r482, %p168;
-	mov.b64 	%fd432, {%r483, %r485};
-	bra.uni 	BB9_144;
-
-BB9_141:
-	@%p159 bra 	BB9_144;
-
-	cvt.rzi.f64.f64	%fd324, %fd292;
-	setp.neu.f64	%p166, %fd324, 0d4000000000000000;
-	selp.f64	%fd432, 0dFFF8000000000000, %fd432, %p166;
-
-BB9_144:
-	add.f64 	%fd433, %fd78, 0d4000000000000000;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r486}, %fd433;
-	}
-	and.b32  	%r487, %r486, 2146435072;
-	setp.ne.s32	%p169, %r487, 2146435072;
-	@%p169 bra 	BB9_145;
-
-	setp.gtu.f64	%p170, %fd79, 0d7FF0000000000000;
-	@%p170 bra 	BB9_154;
-
-	and.b32  	%r488, %r9, 2147483647;
-	setp.ne.s32	%p171, %r488, 2146435072;
-	@%p171 bra 	BB9_149;
-
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r489, %temp}, %fd292;
-	}
-	setp.eq.s32	%p172, %r489, 0;
-	@%p172 bra 	BB9_153;
-
-BB9_149:
-	and.b32  	%r490, %r15, 2147483647;
-	setp.ne.s32	%p173, %r490, 2146435072;
-	@%p173 bra 	BB9_150;
-
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r491, %temp}, %fd78;
-	}
-	setp.ne.s32	%p174, %r491, 0;
-	mov.f64 	%fd433, %fd432;
-	@%p174 bra 	BB9_154;
-
-	shr.s32 	%r492, %r9, 31;
-	and.b32  	%r493, %r492, -2146435072;
-	add.s32 	%r494, %r493, 2146435072;
-	or.b32  	%r495, %r494, -2147483648;
-	selp.b32	%r496, %r495, %r494, %p7;
-	mov.u32 	%r497, 0;
-	mov.b64 	%fd433, {%r497, %r496};
-	bra.uni 	BB9_154;
-
-BB9_145:
-	mov.f64 	%fd433, %fd432;
-	bra.uni 	BB9_154;
-
-BB9_150:
-	mov.f64 	%fd433, %fd432;
-	bra.uni 	BB9_154;
-
-BB9_153:
-	setp.gt.f64	%p175, %fd79, 0d3FF0000000000000;
-	selp.b32	%r498, 2146435072, 0, %p175;
-	mov.u32 	%r499, 0;
-	xor.b32  	%r500, %r498, 2146435072;
-	setp.lt.s32	%p176, %r9, 0;
-	selp.b32	%r501, %r500, %r498, %p176;
-	setp.eq.f32	%p177, %f327, 0fBF800000;
-	selp.b32	%r502, 1072693248, %r501, %p177;
-	mov.b64 	%fd433, {%r499, %r502};
-
-BB9_154:
-	setp.eq.f32	%p178, %f327, 0f3F800000;
-	selp.f64	%fd326, 0d3FF0000000000000, %fd433, %p178;
-	add.f64 	%fd90, %fd77, %fd326;
-	cvt.f64.f32	%fd91, %f966;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r16}, %fd91;
-	}
-	abs.f64 	%fd92, %fd91;
-	// Callseq Start 7
-	{
-	.reg .b32 temp_param_reg;
-	// <end>}
-	.param .b64 param0;
-	st.param.f64	[param0+0], %fd92;
-	.param .b64 retval0;
-	call.uni (retval0), 
-	__internal_accurate_pow, 
-	(
-	param0
-	);
-	ld.param.f64	%fd435, [retval0+0];
-	
-	//{
-	}// Callseq End 7
-	setp.gt.s32	%p179, %r16, -1;
-	setp.lt.s32	%p180, %r16, 0;
-	and.pred  	%p8, %p180, %p42;
-	or.pred  	%p183, %p179, %p60;
-	@%p183 bra 	BB9_156;
-
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r503}, %fd435;
-	}
-	xor.b32  	%r504, %r503, -2147483648;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r505, %temp}, %fd435;
-	}
-	mov.b64 	%fd435, {%r505, %r504};
-
-BB9_156:
-	setp.eq.f32	%p184, %f966, 0f00000000;
-	@%p184 bra 	BB9_159;
-	bra.uni 	BB9_157;
-
-BB9_159:
-	selp.b32	%r506, %r16, 0, %p42;
-	mov.u32 	%r507, 0;
-	or.b32  	%r508, %r506, 2146435072;
-	setp.lt.s32	%p188, %r9, 0;
-	selp.b32	%r509, %r508, %r506, %p188;
-	mov.b64 	%fd435, {%r507, %r509};
-	bra.uni 	BB9_160;
-
-BB9_157:
-	@%p179 bra 	BB9_160;
-
-	cvt.rzi.f64.f64	%fd328, %fd292;
-	setp.neu.f64	%p186, %fd328, 0d4000000000000000;
-	selp.f64	%fd435, 0dFFF8000000000000, %fd435, %p186;
-
-BB9_160:
-	add.f64 	%fd436, %fd91, 0d4000000000000000;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r510}, %fd436;
-	}
-	and.b32  	%r511, %r510, 2146435072;
-	setp.ne.s32	%p189, %r511, 2146435072;
-	@%p189 bra 	BB9_161;
-
-	setp.gtu.f64	%p190, %fd92, 0d7FF0000000000000;
-	@%p190 bra 	BB9_170;
-
-	and.b32  	%r512, %r9, 2147483647;
-	setp.ne.s32	%p191, %r512, 2146435072;
-	@%p191 bra 	BB9_165;
-
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r513, %temp}, %fd292;
-	}
-	setp.eq.s32	%p192, %r513, 0;
-	@%p192 bra 	BB9_169;
-
-BB9_165:
-	and.b32  	%r514, %r16, 2147483647;
-	setp.ne.s32	%p193, %r514, 2146435072;
-	@%p193 bra 	BB9_166;
-
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r515, %temp}, %fd91;
-	}
-	setp.ne.s32	%p194, %r515, 0;
-	mov.f64 	%fd436, %fd435;
-	@%p194 bra 	BB9_170;
-
-	shr.s32 	%r516, %r9, 31;
-	and.b32  	%r517, %r516, -2146435072;
-	add.s32 	%r518, %r517, 2146435072;
-	or.b32  	%r519, %r518, -2147483648;
-	selp.b32	%r520, %r519, %r518, %p8;
-	mov.u32 	%r521, 0;
-	mov.b64 	%fd436, {%r521, %r520};
-	bra.uni 	BB9_170;
-
-BB9_161:
-	mov.f64 	%fd436, %fd435;
-	bra.uni 	BB9_170;
-
-BB9_166:
-	mov.f64 	%fd436, %fd435;
-	bra.uni 	BB9_170;
-
-BB9_169:
-	setp.gt.f64	%p195, %fd92, 0d3FF0000000000000;
-	selp.b32	%r522, 2146435072, 0, %p195;
-	mov.u32 	%r523, 0;
-	xor.b32  	%r524, %r522, 2146435072;
-	setp.lt.s32	%p196, %r9, 0;
-	selp.b32	%r525, %r524, %r522, %p196;
-	setp.eq.f32	%p197, %f966, 0fBF800000;
-	selp.b32	%r526, 1072693248, %r525, %p197;
-	mov.b64 	%fd436, {%r523, %r526};
-
-BB9_170:
-	setp.eq.f32	%p198, %f966, 0f3F800000;
-	selp.f64	%fd330, 0d3FF0000000000000, %fd436, %p198;
-	add.f64 	%fd331, %fd90, %fd330;
-	mul.f32 	%f828, %f324, %f966;
-	cvt.f64.f32	%fd332, %f828;
-	sub.f64 	%fd103, %fd331, %fd332;
-	neg.f32 	%f328, %f808;
-	cvt.f64.f32	%fd104, %f328;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r17}, %fd104;
-	}
-	abs.f64 	%fd105, %fd104;
-	// Callseq Start 8
-	{
-	.reg .b32 temp_param_reg;
-	// <end>}
-	.param .b64 param0;
-	st.param.f64	[param0+0], %fd105;
-	.param .b64 retval0;
-	call.uni (retval0), 
-	__internal_accurate_pow, 
-	(
-	param0
-	);
-	ld.param.f64	%fd438, [retval0+0];
-	
-	//{
-	}// Callseq End 8
-	setp.gt.s32	%p199, %r17, -1;
-	setp.lt.s32	%p200, %r17, 0;
-	and.pred  	%p9, %p200, %p42;
-	or.pred  	%p203, %p199, %p60;
-	@%p203 bra 	BB9_172;
-
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r527}, %fd438;
-	}
-	xor.b32  	%r528, %r527, -2147483648;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r529, %temp}, %fd438;
-	}
-	mov.b64 	%fd438, {%r529, %r528};
-
-BB9_172:
-	setp.eq.f32	%p204, %f328, 0f00000000;
-	@%p204 bra 	BB9_175;
-	bra.uni 	BB9_173;
-
-BB9_175:
-	selp.b32	%r530, %r17, 0, %p42;
-	mov.u32 	%r531, 0;
-	or.b32  	%r532, %r530, 2146435072;
-	setp.lt.s32	%p208, %r9, 0;
-	selp.b32	%r533, %r532, %r530, %p208;
-	mov.b64 	%fd438, {%r531, %r533};
-	bra.uni 	BB9_176;
-
-BB9_173:
-	@%p199 bra 	BB9_176;
-
-	cvt.rzi.f64.f64	%fd334, %fd292;
-	setp.neu.f64	%p206, %fd334, 0d4000000000000000;
-	selp.f64	%fd438, 0dFFF8000000000000, %fd438, %p206;
-
-BB9_176:
-	add.f64 	%fd439, %fd104, 0d4000000000000000;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r534}, %fd439;
-	}
-	and.b32  	%r535, %r534, 2146435072;
-	setp.ne.s32	%p209, %r535, 2146435072;
-	@%p209 bra 	BB9_177;
-
-	setp.gtu.f64	%p210, %fd105, 0d7FF0000000000000;
-	@%p210 bra 	BB9_186;
-
-	and.b32  	%r536, %r9, 2147483647;
-	setp.ne.s32	%p211, %r536, 2146435072;
-	@%p211 bra 	BB9_181;
-
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r537, %temp}, %fd292;
-	}
-	setp.eq.s32	%p212, %r537, 0;
-	@%p212 bra 	BB9_185;
-
-BB9_181:
-	and.b32  	%r538, %r17, 2147483647;
-	setp.ne.s32	%p213, %r538, 2146435072;
-	@%p213 bra 	BB9_182;
-
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r539, %temp}, %fd104;
-	}
-	setp.ne.s32	%p214, %r539, 0;
-	mov.f64 	%fd439, %fd438;
-	@%p214 bra 	BB9_186;
-
-	shr.s32 	%r540, %r9, 31;
-	and.b32  	%r541, %r540, -2146435072;
-	add.s32 	%r542, %r541, 2146435072;
-	or.b32  	%r543, %r542, -2147483648;
-	selp.b32	%r544, %r543, %r542, %p9;
-	mov.u32 	%r545, 0;
-	mov.b64 	%fd439, {%r545, %r544};
-	bra.uni 	BB9_186;
-
-BB9_177:
-	mov.f64 	%fd439, %fd438;
-	bra.uni 	BB9_186;
-
-BB9_182:
-	mov.f64 	%fd439, %fd438;
-	bra.uni 	BB9_186;
-
-BB9_185:
-	setp.gt.f64	%p215, %fd105, 0d3FF0000000000000;
-	selp.b32	%r546, 2146435072, 0, %p215;
-	mov.u32 	%r547, 0;
-	xor.b32  	%r548, %r546, 2146435072;
-	setp.lt.s32	%p216, %r9, 0;
-	selp.b32	%r549, %r548, %r546, %p216;
-	setp.eq.f32	%p217, %f328, 0fBF800000;
-	selp.b32	%r550, 1072693248, %r549, %p217;
-	mov.b64 	%fd439, {%r547, %r550};
-
-BB9_186:
-	setp.eq.f32	%p218, %f328, 0f3F800000;
-	selp.f64	%fd336, 0d3FF0000000000000, %fd439, %p218;
-	add.f64 	%fd337, %fd103, %fd336;
-	add.f32 	%f830, %f965, %f965;
-	div.rn.f32 	%f831, %f830, %f804;
-	cvt.f64.f32	%fd338, %f831;
-	sub.f64 	%fd339, %fd337, %fd338;
-	add.f32 	%f832, %f317, %f317;
-	div.rn.f32 	%f833, %f832, %f804;
-	cvt.f64.f32	%fd340, %f833;
-	add.f64 	%fd341, %fd339, %fd340;
-	cvt.rn.f32.f64	%f329, %fd341;
-	setp.eq.f32	%p219, %f325, 0f00000000;
-	setp.eq.f32	%p220, %f321, 0f00000000;
-	and.pred  	%p221, %p220, %p219;
-	mov.u16 	%rs17, 0;
-	@%p221 bra 	BB9_337;
-
-	neg.f32 	%f834, %f329;
-	div.rn.f32 	%f1017, %f834, %f325;
-	mul.f32 	%f835, %f321, 0fC0800000;
-	mul.f32 	%f836, %f835, %f329;
-	fma.rn.f32 	%f331, %f325, %f325, %f836;
-	setp.lt.f32	%p222, %f331, 0f00000000;
-	setp.neu.f32	%p223, %f321, 0f00000000;
-	and.pred  	%p224, %p222, %p223;
-	@%p224 bra 	BB9_337;
-
-	mov.b32 	 %r551, %f325;
-	and.b32  	%r552, %r551, -2147483648;
-	sqrt.rn.f32 	%f837, %f331;
-	mov.b32 	 %r553, %f837;
-	and.b32  	%r554, %r553, 2147483647;
-	or.b32  	%r555, %r554, %r552;
-	mov.b32 	 %f838, %r555;
-	add.f32 	%f839, %f325, %f838;
-	mul.f32 	%f840, %f839, 0fBF000000;
-	div.rn.f32 	%f841, %f840, %f321;
-	div.rn.f32 	%f842, %f329, %f840;
-	max.f32 	%f843, %f841, %f842;
-	selp.f32	%f1017, %f1017, %f843, %p220;
-	bra.uni 	BB9_336;
-
-BB9_204:
-	setp.gt.f64	%p237, %fd2, 0d3FF0000000000000;
-	selp.b32	%r575, 2146435072, 0, %p237;
-	mov.u32 	%r576, 0;
-	xor.b32  	%r577, %r575, 2146435072;
-	setp.lt.s32	%p238, %r9, 0;
-	selp.b32	%r578, %r577, %r575, %p238;
-	setp.eq.f32	%p239, %f1016, 0fBF800000;
-	selp.b32	%r579, 1072693248, %r578, %p239;
-	mov.b64 	%fd442, {%r576, %r579};
-
-BB9_205:
-	cvt.f64.f32	%fd126, %f1014;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r18}, %fd126;
-	}
-	abs.f64 	%fd127, %fd126;
-	// Callseq Start 9
-	{
-	.reg .b32 temp_param_reg;
-	// <end>}
-	.param .b64 param0;
-	st.param.f64	[param0+0], %fd127;
-	.param .b64 retval0;
-	call.uni (retval0), 
-	__internal_accurate_pow, 
-	(
-	param0
-	);
-	ld.param.f64	%fd444, [retval0+0];
-	
-	//{
-	}// Callseq End 9
-	setp.gt.s32	%p240, %r18, -1;
-	setp.lt.s32	%p241, %r18, 0;
-	setp.ne.s64	%p242, %rd20, -9223372036854775808;
-	and.pred  	%p10, %p241, %p42;
-	or.pred  	%p244, %p240, %p242;
-	@%p244 bra 	BB9_207;
-
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r580}, %fd444;
-	}
-	xor.b32  	%r581, %r580, -2147483648;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r582, %temp}, %fd444;
-	}
-	mov.b64 	%fd444, {%r582, %r581};
-
-BB9_207:
-	setp.eq.f32	%p245, %f1014, 0f00000000;
-	@%p245 bra 	BB9_210;
-	bra.uni 	BB9_208;
-
-BB9_210:
-	selp.b32	%r583, %r18, 0, %p42;
-	mov.u32 	%r584, 0;
-	or.b32  	%r585, %r583, 2146435072;
-	setp.lt.s32	%p249, %r9, 0;
-	selp.b32	%r586, %r585, %r583, %p249;
-	mov.b64 	%fd444, {%r584, %r586};
-	bra.uni 	BB9_211;
-
-BB9_208:
-	@%p240 bra 	BB9_211;
-
-	cvt.rzi.f64.f64	%fd346, %fd292;
-	setp.neu.f64	%p247, %fd346, 0d4000000000000000;
-	selp.f64	%fd444, 0dFFF8000000000000, %fd444, %p247;
-
-BB9_211:
-	add.f64 	%fd445, %fd126, 0d4000000000000000;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r587}, %fd445;
-	}
-	and.b32  	%r588, %r587, 2146435072;
-	setp.ne.s32	%p250, %r588, 2146435072;
-	@%p250 bra 	BB9_212;
-
-	setp.gtu.f64	%p251, %fd127, 0d7FF0000000000000;
-	@%p251 bra 	BB9_221;
-
-	and.b32  	%r589, %r9, 2147483647;
-	setp.ne.s32	%p252, %r589, 2146435072;
-	@%p252 bra 	BB9_216;
-
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r590, %temp}, %fd292;
-	}
-	setp.eq.s32	%p253, %r590, 0;
-	@%p253 bra 	BB9_220;
-
-BB9_216:
-	and.b32  	%r591, %r18, 2147483647;
-	setp.ne.s32	%p254, %r591, 2146435072;
-	@%p254 bra 	BB9_217;
-
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r592, %temp}, %fd126;
-	}
-	setp.ne.s32	%p255, %r592, 0;
-	mov.f64 	%fd445, %fd444;
-	@%p255 bra 	BB9_221;
-
-	shr.s32 	%r593, %r9, 31;
-	and.b32  	%r594, %r593, -2146435072;
-	add.s32 	%r595, %r594, 2146435072;
-	or.b32  	%r596, %r595, -2147483648;
-	selp.b32	%r597, %r596, %r595, %p10;
-	mov.u32 	%r598, 0;
-	mov.b64 	%fd445, {%r598, %r597};
-	bra.uni 	BB9_221;
-
-BB9_212:
-	mov.f64 	%fd445, %fd444;
-	bra.uni 	BB9_221;
-
-BB9_217:
-	mov.f64 	%fd445, %fd444;
-	bra.uni 	BB9_221;
-
-BB9_220:
-	setp.gt.f64	%p256, %fd127, 0d3FF0000000000000;
-	selp.b32	%r599, 2146435072, 0, %p256;
-	mov.u32 	%r600, 0;
-	xor.b32  	%r601, %r599, 2146435072;
-	setp.lt.s32	%p257, %r9, 0;
-	selp.b32	%r602, %r601, %r599, %p257;
-	setp.eq.f32	%p258, %f1014, 0fBF800000;
-	selp.b32	%r603, 1072693248, %r602, %p258;
-	mov.b64 	%fd445, {%r600, %r603};
-
-BB9_221:
-	setp.eq.f32	%p259, %f1014, 0f3F800000;
-	selp.f64	%fd348, 0d3FF0000000000000, %fd445, %p259;
-	setp.eq.f32	%p260, %f1016, 0f3F800000;
-	selp.f64	%fd349, 0d3FF0000000000000, %fd442, %p260;
-	fma.rn.f64 	%fd138, %fd116, %fd349, %fd348;
-	cvt.f64.f32	%fd139, %f1015;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r19}, %fd139;
-	}
-	abs.f64 	%fd140, %fd139;
-	// Callseq Start 10
-	{
-	.reg .b32 temp_param_reg;
-	// <end>}
-	.param .b64 param0;
-	st.param.f64	[param0+0], %fd140;
-	.param .b64 retval0;
-	call.uni (retval0), 
-	__internal_accurate_pow, 
-	(
-	param0
-	);
-	ld.param.f64	%fd447, [retval0+0];
-	
-	//{
-	}// Callseq End 10
-	setp.gt.s32	%p261, %r19, -1;
-	setp.lt.s32	%p262, %r19, 0;
-	and.pred  	%p11, %p262, %p42;
-	or.pred  	%p265, %p261, %p242;
-	@%p265 bra 	BB9_223;
-
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r604}, %fd447;
-	}
-	xor.b32  	%r605, %r604, -2147483648;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r606, %temp}, %fd447;
-	}
-	mov.b64 	%fd447, {%r606, %r605};
-
-BB9_223:
-	setp.eq.f32	%p266, %f1015, 0f00000000;
-	@%p266 bra 	BB9_226;
-	bra.uni 	BB9_224;
-
-BB9_226:
-	selp.b32	%r607, %r19, 0, %p42;
-	mov.u32 	%r608, 0;
-	or.b32  	%r609, %r607, 2146435072;
-	setp.lt.s32	%p270, %r9, 0;
-	selp.b32	%r610, %r609, %r607, %p270;
-	mov.b64 	%fd447, {%r608, %r610};
-	bra.uni 	BB9_227;
-
-BB9_224:
-	@%p261 bra 	BB9_227;
-
-	cvt.rzi.f64.f64	%fd351, %fd292;
-	setp.neu.f64	%p268, %fd351, 0d4000000000000000;
-	selp.f64	%fd447, 0dFFF8000000000000, %fd447, %p268;
-
-BB9_227:
-	add.f64 	%fd448, %fd139, 0d4000000000000000;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r611}, %fd448;
-	}
-	and.b32  	%r612, %r611, 2146435072;
-	setp.ne.s32	%p271, %r612, 2146435072;
-	@%p271 bra 	BB9_228;
-
-	setp.gtu.f64	%p272, %fd140, 0d7FF0000000000000;
-	@%p272 bra 	BB9_237;
-
-	and.b32  	%r613, %r9, 2147483647;
-	setp.ne.s32	%p273, %r613, 2146435072;
-	@%p273 bra 	BB9_232;
-
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r614, %temp}, %fd292;
-	}
-	setp.eq.s32	%p274, %r614, 0;
-	@%p274 bra 	BB9_236;
-
-BB9_232:
-	and.b32  	%r615, %r19, 2147483647;
-	setp.ne.s32	%p275, %r615, 2146435072;
-	@%p275 bra 	BB9_233;
-
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r616, %temp}, %fd139;
-	}
-	setp.ne.s32	%p276, %r616, 0;
-	mov.f64 	%fd448, %fd447;
-	@%p276 bra 	BB9_237;
-
-	shr.s32 	%r617, %r9, 31;
-	and.b32  	%r618, %r617, -2146435072;
-	add.s32 	%r619, %r618, 2146435072;
-	or.b32  	%r620, %r619, -2147483648;
-	selp.b32	%r621, %r620, %r619, %p11;
-	mov.u32 	%r622, 0;
-	mov.b64 	%fd448, {%r622, %r621};
-	bra.uni 	BB9_237;
-
-BB9_228:
-	mov.f64 	%fd448, %fd447;
-	bra.uni 	BB9_237;
-
-BB9_233:
-	mov.f64 	%fd448, %fd447;
-	bra.uni 	BB9_237;
-
-BB9_236:
-	setp.gt.f64	%p277, %fd140, 0d3FF0000000000000;
-	selp.b32	%r623, 2146435072, 0, %p277;
-	mov.u32 	%r624, 0;
-	xor.b32  	%r625, %r623, 2146435072;
-	setp.lt.s32	%p278, %r9, 0;
-	selp.b32	%r626, %r625, %r623, %p278;
-	setp.eq.f32	%p279, %f1015, 0fBF800000;
-	selp.b32	%r627, 1072693248, %r626, %p279;
-	mov.b64 	%fd448, {%r624, %r627};
-
-BB9_237:
-	setp.eq.f32	%p280, %f1015, 0f3F800000;
-	selp.f64	%fd353, 0d3FF0000000000000, %fd448, %p280;
-	add.f64 	%fd354, %fd138, %fd353;
-	cvt.rn.f32.f64	%f339, %fd354;
-	add.f32 	%f849, %f338, %f338;
-	mul.f32 	%f850, %f965, %f849;
-	mul.f32 	%f851, %f1016, %f850;
-	add.f32 	%f852, %f337, %f337;
-	mul.f32 	%f340, %f846, %f852;
-	fma.rn.f32 	%f853, %f1016, %f340, %f851;
-	add.f32 	%f854, %f967, %f967;
-	fma.rn.f32 	%f855, %f1014, %f854, %f853;
-	add.f32 	%f341, %f844, %f844;
-	mul.f32 	%f856, %f341, %f1014;
-	sub.f32 	%f857, %f855, %f856;
-	add.f32 	%f858, %f966, %f966;
-	fma.rn.f32 	%f859, %f1015, %f858, %f857;
-	add.f32 	%f342, %f845, %f845;
-	mul.f32 	%f860, %f342, %f1015;
-	sub.f32 	%f861, %f859, %f860;
-	add.f32 	%f862, %f1016, %f1016;
-	div.rn.f32 	%f863, %f862, %f336;
-	sub.f32 	%f343, %f861, %f863;
-	cvt.f64.f32	%fd151, %f965;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r20}, %fd151;
-	}
-	abs.f64 	%fd152, %fd151;
-	// Callseq Start 11
-	{
-	.reg .b32 temp_param_reg;
-	// <end>}
-	.param .b64 param0;
-	st.param.f64	[param0+0], %fd152;
-	.param .b64 retval0;
-	call.uni (retval0), 
-	__internal_accurate_pow, 
-	(
-	param0
-	);
-	ld.param.f64	%fd450, [retval0+0];
-	
-	//{
-	}// Callseq End 11
-	setp.gt.s32	%p281, %r20, -1;
-	setp.lt.s32	%p282, %r20, 0;
-	and.pred  	%p12, %p282, %p42;
-	or.pred  	%p285, %p281, %p242;
-	@%p285 bra 	BB9_239;
-
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r628}, %fd450;
-	}
-	xor.b32  	%r629, %r628, -2147483648;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r630, %temp}, %fd450;
-	}
-	mov.b64 	%fd450, {%r630, %r629};
-
-BB9_239:
-	setp.eq.f32	%p286, %f965, 0f00000000;
-	@%p286 bra 	BB9_242;
-	bra.uni 	BB9_240;
-
-BB9_242:
-	selp.b32	%r631, %r20, 0, %p42;
-	mov.u32 	%r632, 0;
-	or.b32  	%r633, %r631, 2146435072;
-	setp.lt.s32	%p290, %r9, 0;
-	selp.b32	%r634, %r633, %r631, %p290;
-	mov.b64 	%fd450, {%r632, %r634};
-	bra.uni 	BB9_243;
-
-BB9_240:
-	@%p281 bra 	BB9_243;
-
-	cvt.rzi.f64.f64	%fd356, %fd292;
-	setp.neu.f64	%p288, %fd356, 0d4000000000000000;
-	selp.f64	%fd450, 0dFFF8000000000000, %fd450, %p288;
-
-BB9_243:
-	add.f64 	%fd451, %fd151, 0d4000000000000000;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r635}, %fd451;
-	}
-	and.b32  	%r636, %r635, 2146435072;
-	setp.ne.s32	%p291, %r636, 2146435072;
-	@%p291 bra 	BB9_244;
-
-	setp.gtu.f64	%p292, %fd152, 0d7FF0000000000000;
-	@%p292 bra 	BB9_253;
-
-	and.b32  	%r637, %r9, 2147483647;
-	setp.ne.s32	%p293, %r637, 2146435072;
-	@%p293 bra 	BB9_248;
-
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r638, %temp}, %fd292;
-	}
-	setp.eq.s32	%p294, %r638, 0;
-	@%p294 bra 	BB9_252;
-
-BB9_248:
-	and.b32  	%r639, %r20, 2147483647;
-	setp.ne.s32	%p295, %r639, 2146435072;
-	@%p295 bra 	BB9_249;
-
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r640, %temp}, %fd151;
-	}
-	setp.ne.s32	%p296, %r640, 0;
-	mov.f64 	%fd451, %fd450;
-	@%p296 bra 	BB9_253;
-
-	shr.s32 	%r641, %r9, 31;
-	and.b32  	%r642, %r641, -2146435072;
-	add.s32 	%r643, %r642, 2146435072;
-	or.b32  	%r644, %r643, -2147483648;
-	selp.b32	%r645, %r644, %r643, %p12;
-	mov.u32 	%r646, 0;
-	mov.b64 	%fd451, {%r646, %r645};
-	bra.uni 	BB9_253;
-
-BB9_244:
-	mov.f64 	%fd451, %fd450;
-	bra.uni 	BB9_253;
-
-BB9_249:
-	mov.f64 	%fd451, %fd450;
-	bra.uni 	BB9_253;
-
-BB9_252:
-	setp.gt.f64	%p297, %fd152, 0d3FF0000000000000;
-	selp.b32	%r647, 2146435072, 0, %p297;
-	mov.u32 	%r648, 0;
-	xor.b32  	%r649, %r647, 2146435072;
-	setp.lt.s32	%p298, %r9, 0;
-	selp.b32	%r650, %r649, %r647, %p298;
-	setp.eq.f32	%p299, %f965, 0fBF800000;
-	selp.b32	%r651, 1072693248, %r650, %p299;
-	mov.b64 	%fd451, {%r648, %r651};
-
-BB9_253:
-	setp.eq.f32	%p300, %f965, 0f3F800000;
-	selp.f64	%fd358, 0d3FF0000000000000, %fd451, %p300;
-	mul.f32 	%f864, %f965, %f340;
-	cvt.f64.f32	%fd359, %f864;
-	fma.rn.f64 	%fd163, %fd116, %fd358, %fd359;
-	neg.f32 	%f344, %f846;
-	cvt.f64.f32	%fd164, %f344;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r21}, %fd164;
-	}
-	abs.f64 	%fd165, %fd164;
-	// Callseq Start 12
-	{
-	.reg .b32 temp_param_reg;
-	// <end>}
-	.param .b64 param0;
-	st.param.f64	[param0+0], %fd165;
-	.param .b64 retval0;
-	call.uni (retval0), 
-	__internal_accurate_pow, 
-	(
-	param0
-	);
-	ld.param.f64	%fd453, [retval0+0];
-	
-	//{
-	}// Callseq End 12
-	setp.gt.s32	%p301, %r21, -1;
-	setp.lt.s32	%p302, %r21, 0;
-	and.pred  	%p13, %p302, %p42;
-	or.pred  	%p305, %p301, %p242;
-	@%p305 bra 	BB9_255;
-
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r652}, %fd453;
-	}
-	xor.b32  	%r653, %r652, -2147483648;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r654, %temp}, %fd453;
-	}
-	mov.b64 	%fd453, {%r654, %r653};
-
-BB9_255:
-	setp.eq.f32	%p306, %f344, 0f00000000;
-	@%p306 bra 	BB9_258;
-	bra.uni 	BB9_256;
-
-BB9_258:
-	selp.b32	%r655, %r21, 0, %p42;
-	mov.u32 	%r656, 0;
-	or.b32  	%r657, %r655, 2146435072;
-	setp.lt.s32	%p310, %r9, 0;
-	selp.b32	%r658, %r657, %r655, %p310;
-	mov.b64 	%fd453, {%r656, %r658};
-	bra.uni 	BB9_259;
-
-BB9_256:
-	@%p301 bra 	BB9_259;
-
-	cvt.rzi.f64.f64	%fd361, %fd292;
-	setp.neu.f64	%p308, %fd361, 0d4000000000000000;
-	selp.f64	%fd453, 0dFFF8000000000000, %fd453, %p308;
-
-BB9_259:
-	add.f64 	%fd454, %fd164, 0d4000000000000000;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r659}, %fd454;
-	}
-	and.b32  	%r660, %r659, 2146435072;
-	setp.ne.s32	%p311, %r660, 2146435072;
-	@%p311 bra 	BB9_260;
-
-	setp.gtu.f64	%p312, %fd165, 0d7FF0000000000000;
-	@%p312 bra 	BB9_269;
-
-	and.b32  	%r661, %r9, 2147483647;
-	setp.ne.s32	%p313, %r661, 2146435072;
-	@%p313 bra 	BB9_264;
-
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r662, %temp}, %fd292;
-	}
-	setp.eq.s32	%p314, %r662, 0;
-	@%p314 bra 	BB9_268;
-
-BB9_264:
-	and.b32  	%r663, %r21, 2147483647;
-	setp.ne.s32	%p315, %r663, 2146435072;
-	@%p315 bra 	BB9_265;
-
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r664, %temp}, %fd164;
-	}
-	setp.ne.s32	%p316, %r664, 0;
-	mov.f64 	%fd454, %fd453;
-	@%p316 bra 	BB9_269;
-
-	shr.s32 	%r665, %r9, 31;
-	and.b32  	%r666, %r665, -2146435072;
-	add.s32 	%r667, %r666, 2146435072;
-	or.b32  	%r668, %r667, -2147483648;
-	selp.b32	%r669, %r668, %r667, %p13;
-	mov.u32 	%r670, 0;
-	mov.b64 	%fd454, {%r670, %r669};
-	bra.uni 	BB9_269;
-
-BB9_260:
-	mov.f64 	%fd454, %fd453;
-	bra.uni 	BB9_269;
-
-BB9_265:
-	mov.f64 	%fd454, %fd453;
-	bra.uni 	BB9_269;
-
-BB9_268:
-	setp.gt.f64	%p317, %fd165, 0d3FF0000000000000;
-	selp.b32	%r671, 2146435072, 0, %p317;
-	mov.u32 	%r672, 0;
-	xor.b32  	%r673, %r671, 2146435072;
-	setp.lt.s32	%p318, %r9, 0;
-	selp.b32	%r674, %r673, %r671, %p318;
-	setp.eq.f32	%p319, %f344, 0fBF800000;
-	selp.b32	%r675, 1072693248, %r674, %p319;
-	mov.b64 	%fd454, {%r672, %r675};
-
-BB9_269:
-	setp.eq.f32	%p320, %f344, 0f3F800000;
-	selp.f64	%fd363, 0d3FF0000000000000, %fd454, %p320;
-	cvt.f64.f32	%fd364, %f337;
-	mul.f64 	%fd365, %fd364, %fd363;
-	sub.f64 	%fd176, %fd163, %fd365;
-	cvt.f64.f32	%fd177, %f967;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r22}, %fd177;
-	}
-	abs.f64 	%fd178, %fd177;
-	// Callseq Start 13
-	{
-	.reg .b32 temp_param_reg;
-	// <end>}
-	.param .b64 param0;
-	st.param.f64	[param0+0], %fd178;
-	.param .b64 retval0;
-	call.uni (retval0), 
-	__internal_accurate_pow, 
-	(
-	param0
-	);
-	ld.param.f64	%fd456, [retval0+0];
-	
-	//{
-	}// Callseq End 13
-	setp.gt.s32	%p321, %r22, -1;
-	setp.lt.s32	%p322, %r22, 0;
-	and.pred  	%p14, %p322, %p42;
-	or.pred  	%p325, %p321, %p242;
-	@%p325 bra 	BB9_271;
-
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r676}, %fd456;
-	}
-	xor.b32  	%r677, %r676, -2147483648;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r678, %temp}, %fd456;
-	}
-	mov.b64 	%fd456, {%r678, %r677};
-
-BB9_271:
-	setp.eq.f32	%p326, %f967, 0f00000000;
-	@%p326 bra 	BB9_274;
-	bra.uni 	BB9_272;
-
-BB9_274:
-	selp.b32	%r679, %r22, 0, %p42;
-	mov.u32 	%r680, 0;
-	or.b32  	%r681, %r679, 2146435072;
-	setp.lt.s32	%p330, %r9, 0;
-	selp.b32	%r682, %r681, %r679, %p330;
-	mov.b64 	%fd456, {%r680, %r682};
-	bra.uni 	BB9_275;
-
-BB9_272:
-	@%p321 bra 	BB9_275;
-
-	cvt.rzi.f64.f64	%fd367, %fd292;
-	setp.neu.f64	%p328, %fd367, 0d4000000000000000;
-	selp.f64	%fd456, 0dFFF8000000000000, %fd456, %p328;
-
-BB9_275:
-	add.f64 	%fd457, %fd177, 0d4000000000000000;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r683}, %fd457;
-	}
-	and.b32  	%r684, %r683, 2146435072;
-	setp.ne.s32	%p331, %r684, 2146435072;
-	@%p331 bra 	BB9_276;
-
-	setp.gtu.f64	%p332, %fd178, 0d7FF0000000000000;
-	@%p332 bra 	BB9_285;
-
-	and.b32  	%r685, %r9, 2147483647;
-	setp.ne.s32	%p333, %r685, 2146435072;
-	@%p333 bra 	BB9_280;
-
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r686, %temp}, %fd292;
-	}
-	setp.eq.s32	%p334, %r686, 0;
-	@%p334 bra 	BB9_284;
-
-BB9_280:
-	and.b32  	%r687, %r22, 2147483647;
-	setp.ne.s32	%p335, %r687, 2146435072;
-	@%p335 bra 	BB9_281;
-
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r688, %temp}, %fd177;
-	}
-	setp.ne.s32	%p336, %r688, 0;
-	mov.f64 	%fd457, %fd456;
-	@%p336 bra 	BB9_285;
-
-	shr.s32 	%r689, %r9, 31;
-	and.b32  	%r690, %r689, -2146435072;
-	add.s32 	%r691, %r690, 2146435072;
-	or.b32  	%r692, %r691, -2147483648;
-	selp.b32	%r693, %r692, %r691, %p14;
-	mov.u32 	%r694, 0;
-	mov.b64 	%fd457, {%r694, %r693};
-	bra.uni 	BB9_285;
-
-BB9_276:
-	mov.f64 	%fd457, %fd456;
-	bra.uni 	BB9_285;
-
-BB9_281:
-	mov.f64 	%fd457, %fd456;
-	bra.uni 	BB9_285;
-
-BB9_284:
-	setp.gt.f64	%p337, %fd178, 0d3FF0000000000000;
-	selp.b32	%r695, 2146435072, 0, %p337;
-	mov.u32 	%r696, 0;
-	xor.b32  	%r697, %r695, 2146435072;
-	setp.lt.s32	%p338, %r9, 0;
-	selp.b32	%r698, %r697, %r695, %p338;
-	setp.eq.f32	%p339, %f967, 0fBF800000;
-	selp.b32	%r699, 1072693248, %r698, %p339;
-	mov.b64 	%fd457, {%r696, %r699};
-
-BB9_285:
-	setp.eq.f32	%p340, %f967, 0f3F800000;
-	selp.f64	%fd369, 0d3FF0000000000000, %fd457, %p340;
-	add.f64 	%fd370, %fd176, %fd369;
-	mul.f32 	%f865, %f341, %f967;
-	cvt.f64.f32	%fd371, %f865;
-	sub.f64 	%fd189, %fd370, %fd371;
-	neg.f32 	%f345, %f844;
-	cvt.f64.f32	%fd190, %f345;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r23}, %fd190;
-	}
-	abs.f64 	%fd191, %fd190;
-	// Callseq Start 14
-	{
-	.reg .b32 temp_param_reg;
-	// <end>}
-	.param .b64 param0;
-	st.param.f64	[param0+0], %fd191;
-	.param .b64 retval0;
-	call.uni (retval0), 
-	__internal_accurate_pow, 
-	(
-	param0
-	);
-	ld.param.f64	%fd459, [retval0+0];
-	
-	//{
-	}// Callseq End 14
-	setp.gt.s32	%p341, %r23, -1;
-	setp.lt.s32	%p342, %r23, 0;
-	and.pred  	%p15, %p342, %p42;
-	or.pred  	%p345, %p341, %p242;
-	@%p345 bra 	BB9_287;
-
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r700}, %fd459;
-	}
-	xor.b32  	%r701, %r700, -2147483648;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r702, %temp}, %fd459;
-	}
-	mov.b64 	%fd459, {%r702, %r701};
-
-BB9_287:
-	setp.eq.f32	%p346, %f345, 0f00000000;
-	@%p346 bra 	BB9_290;
-	bra.uni 	BB9_288;
-
-BB9_290:
-	selp.b32	%r703, %r23, 0, %p42;
-	mov.u32 	%r704, 0;
-	or.b32  	%r705, %r703, 2146435072;
-	setp.lt.s32	%p350, %r9, 0;
-	selp.b32	%r706, %r705, %r703, %p350;
-	mov.b64 	%fd459, {%r704, %r706};
-	bra.uni 	BB9_291;
-
-BB9_288:
-	@%p341 bra 	BB9_291;
-
-	cvt.rzi.f64.f64	%fd373, %fd292;
-	setp.neu.f64	%p348, %fd373, 0d4000000000000000;
-	selp.f64	%fd459, 0dFFF8000000000000, %fd459, %p348;
-
-BB9_291:
-	add.f64 	%fd460, %fd190, 0d4000000000000000;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r707}, %fd460;
-	}
-	and.b32  	%r708, %r707, 2146435072;
-	setp.ne.s32	%p351, %r708, 2146435072;
-	@%p351 bra 	BB9_292;
-
-	setp.gtu.f64	%p352, %fd191, 0d7FF0000000000000;
-	@%p352 bra 	BB9_301;
-
-	and.b32  	%r709, %r9, 2147483647;
-	setp.ne.s32	%p353, %r709, 2146435072;
-	@%p353 bra 	BB9_296;
-
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r710, %temp}, %fd292;
-	}
-	setp.eq.s32	%p354, %r710, 0;
-	@%p354 bra 	BB9_300;
-
-BB9_296:
-	and.b32  	%r711, %r23, 2147483647;
-	setp.ne.s32	%p355, %r711, 2146435072;
-	@%p355 bra 	BB9_297;
-
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r712, %temp}, %fd190;
-	}
-	setp.ne.s32	%p356, %r712, 0;
-	mov.f64 	%fd460, %fd459;
-	@%p356 bra 	BB9_301;
-
-	shr.s32 	%r713, %r9, 31;
-	and.b32  	%r714, %r713, -2146435072;
-	add.s32 	%r715, %r714, 2146435072;
-	or.b32  	%r716, %r715, -2147483648;
-	selp.b32	%r717, %r716, %r715, %p15;
-	mov.u32 	%r718, 0;
-	mov.b64 	%fd460, {%r718, %r717};
-	bra.uni 	BB9_301;
-
-BB9_292:
-	mov.f64 	%fd460, %fd459;
-	bra.uni 	BB9_301;
-
-BB9_297:
-	mov.f64 	%fd460, %fd459;
-	bra.uni 	BB9_301;
-
-BB9_300:
-	setp.gt.f64	%p357, %fd191, 0d3FF0000000000000;
-	selp.b32	%r719, 2146435072, 0, %p357;
-	mov.u32 	%r720, 0;
-	xor.b32  	%r721, %r719, 2146435072;
-	setp.lt.s32	%p358, %r9, 0;
-	selp.b32	%r722, %r721, %r719, %p358;
-	setp.eq.f32	%p359, %f345, 0fBF800000;
-	selp.b32	%r723, 1072693248, %r722, %p359;
-	mov.b64 	%fd460, {%r720, %r723};
-
-BB9_301:
-	setp.eq.f32	%p360, %f345, 0f3F800000;
-	selp.f64	%fd375, 0d3FF0000000000000, %fd460, %p360;
-	add.f64 	%fd202, %fd189, %fd375;
-	cvt.f64.f32	%fd203, %f966;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r24}, %fd203;
-	}
-	abs.f64 	%fd204, %fd203;
-	// Callseq Start 15
-	{
-	.reg .b32 temp_param_reg;
-	// <end>}
-	.param .b64 param0;
-	st.param.f64	[param0+0], %fd204;
-	.param .b64 retval0;
-	call.uni (retval0), 
-	__internal_accurate_pow, 
-	(
-	param0
-	);
-	ld.param.f64	%fd462, [retval0+0];
-	
-	//{
-	}// Callseq End 15
-	setp.gt.s32	%p361, %r24, -1;
-	setp.lt.s32	%p362, %r24, 0;
-	and.pred  	%p16, %p362, %p42;
-	or.pred  	%p365, %p361, %p242;
-	@%p365 bra 	BB9_303;
-
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r724}, %fd462;
-	}
-	xor.b32  	%r725, %r724, -2147483648;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r726, %temp}, %fd462;
-	}
-	mov.b64 	%fd462, {%r726, %r725};
-
-BB9_303:
-	setp.eq.f32	%p366, %f966, 0f00000000;
-	@%p366 bra 	BB9_306;
-	bra.uni 	BB9_304;
-
-BB9_306:
-	selp.b32	%r727, %r24, 0, %p42;
-	mov.u32 	%r728, 0;
-	or.b32  	%r729, %r727, 2146435072;
-	setp.lt.s32	%p370, %r9, 0;
-	selp.b32	%r730, %r729, %r727, %p370;
-	mov.b64 	%fd462, {%r728, %r730};
-	bra.uni 	BB9_307;
-
-BB9_304:
-	@%p361 bra 	BB9_307;
-
-	cvt.rzi.f64.f64	%fd377, %fd292;
-	setp.neu.f64	%p368, %fd377, 0d4000000000000000;
-	selp.f64	%fd462, 0dFFF8000000000000, %fd462, %p368;
-
-BB9_307:
-	add.f64 	%fd463, %fd203, 0d4000000000000000;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r731}, %fd463;
-	}
-	and.b32  	%r732, %r731, 2146435072;
-	setp.ne.s32	%p371, %r732, 2146435072;
-	@%p371 bra 	BB9_308;
-
-	setp.gtu.f64	%p372, %fd204, 0d7FF0000000000000;
-	@%p372 bra 	BB9_317;
-
-	and.b32  	%r733, %r9, 2147483647;
-	setp.ne.s32	%p373, %r733, 2146435072;
-	@%p373 bra 	BB9_312;
-
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r734, %temp}, %fd292;
-	}
-	setp.eq.s32	%p374, %r734, 0;
-	@%p374 bra 	BB9_316;
-
-BB9_312:
-	and.b32  	%r735, %r24, 2147483647;
-	setp.ne.s32	%p375, %r735, 2146435072;
-	@%p375 bra 	BB9_313;
-
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r736, %temp}, %fd203;
-	}
-	setp.ne.s32	%p376, %r736, 0;
-	mov.f64 	%fd463, %fd462;
-	@%p376 bra 	BB9_317;
-
-	shr.s32 	%r737, %r9, 31;
-	and.b32  	%r738, %r737, -2146435072;
-	add.s32 	%r739, %r738, 2146435072;
-	or.b32  	%r740, %r739, -2147483648;
-	selp.b32	%r741, %r740, %r739, %p16;
-	mov.u32 	%r742, 0;
-	mov.b64 	%fd463, {%r742, %r741};
-	bra.uni 	BB9_317;
-
-BB9_308:
-	mov.f64 	%fd463, %fd462;
-	bra.uni 	BB9_317;
-
-BB9_313:
-	mov.f64 	%fd463, %fd462;
-	bra.uni 	BB9_317;
-
-BB9_316:
-	setp.gt.f64	%p377, %fd204, 0d3FF0000000000000;
-	selp.b32	%r743, 2146435072, 0, %p377;
-	mov.u32 	%r744, 0;
-	xor.b32  	%r745, %r743, 2146435072;
-	setp.lt.s32	%p378, %r9, 0;
-	selp.b32	%r746, %r745, %r743, %p378;
-	setp.eq.f32	%p379, %f966, 0fBF800000;
-	selp.b32	%r747, 1072693248, %r746, %p379;
-	mov.b64 	%fd463, {%r744, %r747};
-
-BB9_317:
-	setp.eq.f32	%p380, %f966, 0f3F800000;
-	selp.f64	%fd379, 0d3FF0000000000000, %fd463, %p380;
-	add.f64 	%fd380, %fd202, %fd379;
-	mul.f32 	%f866, %f342, %f966;
-	cvt.f64.f32	%fd381, %f866;
-	sub.f64 	%fd215, %fd380, %fd381;
-	neg.f32 	%f346, %f845;
-	cvt.f64.f32	%fd216, %f346;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r25}, %fd216;
-	}
-	abs.f64 	%fd217, %fd216;
-	// Callseq Start 16
-	{
-	.reg .b32 temp_param_reg;
-	// <end>}
-	.param .b64 param0;
-	st.param.f64	[param0+0], %fd217;
-	.param .b64 retval0;
-	call.uni (retval0), 
-	__internal_accurate_pow, 
-	(
-	param0
-	);
-	ld.param.f64	%fd465, [retval0+0];
-	
-	//{
-	}// Callseq End 16
-	setp.gt.s32	%p381, %r25, -1;
-	setp.lt.s32	%p382, %r25, 0;
-	and.pred  	%p17, %p382, %p42;
-	or.pred  	%p385, %p381, %p242;
-	@%p385 bra 	BB9_319;
-
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r748}, %fd465;
-	}
-	xor.b32  	%r749, %r748, -2147483648;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r750, %temp}, %fd465;
-	}
-	mov.b64 	%fd465, {%r750, %r749};
-
-BB9_319:
-	setp.eq.f32	%p386, %f346, 0f00000000;
-	@%p386 bra 	BB9_322;
-	bra.uni 	BB9_320;
-
-BB9_322:
-	selp.b32	%r751, %r25, 0, %p42;
-	mov.u32 	%r752, 0;
-	or.b32  	%r753, %r751, 2146435072;
-	setp.lt.s32	%p390, %r9, 0;
-	selp.b32	%r754, %r753, %r751, %p390;
-	mov.b64 	%fd465, {%r752, %r754};
-	bra.uni 	BB9_323;
-
-BB9_320:
-	@%p381 bra 	BB9_323;
-
-	cvt.rzi.f64.f64	%fd383, %fd292;
-	setp.neu.f64	%p388, %fd383, 0d4000000000000000;
-	selp.f64	%fd465, 0dFFF8000000000000, %fd465, %p388;
-
-BB9_323:
-	add.f64 	%fd466, %fd216, 0d4000000000000000;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r755}, %fd466;
-	}
-	and.b32  	%r756, %r755, 2146435072;
-	setp.ne.s32	%p391, %r756, 2146435072;
-	@%p391 bra 	BB9_324;
-
-	setp.gtu.f64	%p392, %fd217, 0d7FF0000000000000;
-	@%p392 bra 	BB9_333;
-
-	and.b32  	%r757, %r9, 2147483647;
-	setp.ne.s32	%p393, %r757, 2146435072;
-	@%p393 bra 	BB9_328;
-
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r758, %temp}, %fd292;
-	}
-	setp.eq.s32	%p394, %r758, 0;
-	@%p394 bra 	BB9_332;
-
-BB9_328:
-	and.b32  	%r759, %r25, 2147483647;
-	setp.ne.s32	%p395, %r759, 2146435072;
-	@%p395 bra 	BB9_329;
-
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r760, %temp}, %fd216;
-	}
-	setp.ne.s32	%p396, %r760, 0;
-	mov.f64 	%fd466, %fd465;
-	@%p396 bra 	BB9_333;
-
-	shr.s32 	%r761, %r9, 31;
-	and.b32  	%r762, %r761, -2146435072;
-	add.s32 	%r763, %r762, 2146435072;
-	or.b32  	%r764, %r763, -2147483648;
-	selp.b32	%r765, %r764, %r763, %p17;
-	mov.u32 	%r766, 0;
-	mov.b64 	%fd466, {%r766, %r765};
-	bra.uni 	BB9_333;
-
-BB9_324:
-	mov.f64 	%fd466, %fd465;
-	bra.uni 	BB9_333;
-
-BB9_329:
-	mov.f64 	%fd466, %fd465;
-	bra.uni 	BB9_333;
-
-BB9_332:
-	setp.gt.f64	%p397, %fd217, 0d3FF0000000000000;
-	selp.b32	%r767, 2146435072, 0, %p397;
-	mov.u32 	%r768, 0;
-	xor.b32  	%r769, %r767, 2146435072;
-	setp.lt.s32	%p398, %r9, 0;
-	selp.b32	%r770, %r769, %r767, %p398;
-	setp.eq.f32	%p399, %f346, 0fBF800000;
-	selp.b32	%r771, 1072693248, %r770, %p399;
-	mov.b64 	%fd466, {%r768, %r771};
-
-BB9_333:
-	setp.eq.f32	%p400, %f346, 0f3F800000;
-	selp.f64	%fd385, 0d3FF0000000000000, %fd466, %p400;
-	add.f64 	%fd386, %fd215, %fd385;
-	add.f32 	%f868, %f965, %f965;
-	div.rn.f32 	%f869, %f868, %f336;
-	cvt.f64.f32	%fd387, %f869;
-	sub.f64 	%fd388, %fd386, %fd387;
-	add.f32 	%f870, %f846, %f846;
-	div.rn.f32 	%f871, %f870, %f336;
-	cvt.f64.f32	%fd389, %f871;
-	add.f64 	%fd390, %fd388, %fd389;
-	cvt.rn.f32.f64	%f347, %fd390;
-	setp.eq.f32	%p401, %f343, 0f00000000;
-	setp.eq.f32	%p402, %f339, 0f00000000;
-	and.pred  	%p403, %p402, %p401;
-	mov.u16 	%rs17, 0;
-	@%p403 bra 	BB9_337;
-
-	neg.f32 	%f872, %f347;
-	div.rn.f32 	%f1017, %f872, %f343;
-	mul.f32 	%f873, %f339, 0fC0800000;
-	mul.f32 	%f874, %f873, %f347;
-	fma.rn.f32 	%f349, %f343, %f343, %f874;
-	setp.lt.f32	%p404, %f349, 0f00000000;
-	setp.neu.f32	%p405, %f339, 0f00000000;
-	and.pred  	%p406, %p404, %p405;
-	@%p406 bra 	BB9_337;
-
-	mov.b32 	 %r772, %f343;
-	and.b32  	%r773, %r772, -2147483648;
-	sqrt.rn.f32 	%f875, %f349;
-	mov.b32 	 %r774, %f875;
-	and.b32  	%r775, %r774, 2147483647;
-	or.b32  	%r776, %r775, %r773;
-	mov.b32 	 %f876, %r776;
-	add.f32 	%f877, %f343, %f876;
-	mul.f32 	%f878, %f877, 0fBF000000;
-	div.rn.f32 	%f879, %f878, %f339;
-	div.rn.f32 	%f880, %f347, %f878;
-	min.f32 	%f881, %f879, %f880;
-	selp.f32	%f1017, %f1017, %f881, %p402;
-
-BB9_336:
-	mov.u16 	%rs17, 1;
-
-BB9_337:
-	ld.v4.f32 	{%f882, %f883, %f884, %f885}, [%rd1+288];
-	ld.v4.f32 	{%f889, %f890, %f891, %f892}, [%rd1+320];
-	fma.rn.f32 	%f893, %f1017, %f1014, %f967;
-	sub.f32 	%f354, %f893, %f882;
-	fma.rn.f32 	%f894, %f1017, %f1015, %f966;
-	sub.f32 	%f355, %f894, %f883;
-	fma.rn.f32 	%f895, %f1017, %f1016, %f965;
-	sub.f32 	%f356, %f895, %f884;
-	cvt.f64.f32	%fd228, %f354;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r27}, %fd228;
-	}
-	cvt.u32.u64	%r777, %rd19;
-	shl.b64 	%rd21, %rd265, %r777;
-	setp.ne.s64	%p408, %rd21, -9223372036854775808;
-	setp.eq.s64	%p409, %rd21, -9223372036854775808;
-	abs.f64 	%fd229, %fd228;
-	// Callseq Start 17
-	{
-	.reg .b32 temp_param_reg;
-	// <end>}
-	.param .b64 param0;
-	st.param.f64	[param0+0], %fd229;
-	.param .b64 retval0;
-	call.uni (retval0), 
-	__internal_accurate_pow, 
-	(
-	param0
-	);
-	ld.param.f64	%fd468, [retval0+0];
-	
-	//{
-	}// Callseq End 17
-	setp.gt.s32	%p410, %r27, -1;
-	setp.lt.s32	%p411, %r27, 0;
-	and.pred  	%p18, %p411, %p409;
-	or.pred  	%p412, %p410, %p408;
-	@%p412 bra 	BB9_339;
-
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r778}, %fd468;
-	}
-	xor.b32  	%r779, %r778, -2147483648;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r780, %temp}, %fd468;
-	}
-	mov.b64 	%fd468, {%r780, %r779};
-
-BB9_339:
-	setp.eq.f32	%p413, %f354, 0f00000000;
-	@%p413 bra 	BB9_342;
-	bra.uni 	BB9_340;
-
-BB9_342:
-	selp.b32	%r781, %r27, 0, %p409;
-	mov.u32 	%r782, 0;
-	or.b32  	%r783, %r781, 2146435072;
-	setp.lt.s32	%p417, %r9, 0;
-	selp.b32	%r784, %r783, %r781, %p417;
-	mov.b64 	%fd468, {%r782, %r784};
-	bra.uni 	BB9_343;
-
-BB9_340:
-	@%p410 bra 	BB9_343;
-
-	cvt.rzi.f64.f64	%fd393, %fd292;
-	setp.neu.f64	%p415, %fd393, 0d4000000000000000;
-	selp.f64	%fd468, 0dFFF8000000000000, %fd468, %p415;
-
-BB9_343:
-	add.f64 	%fd469, %fd228, 0d4000000000000000;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r785}, %fd469;
-	}
-	and.b32  	%r786, %r785, 2146435072;
-	setp.ne.s32	%p418, %r786, 2146435072;
-	@%p418 bra 	BB9_344;
-
-	setp.gtu.f64	%p419, %fd229, 0d7FF0000000000000;
-	@%p419 bra 	BB9_353;
-
-	and.b32  	%r787, %r9, 2147483647;
-	setp.ne.s32	%p420, %r787, 2146435072;
-	@%p420 bra 	BB9_348;
-
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r788, %temp}, %fd292;
-	}
-	setp.eq.s32	%p421, %r788, 0;
-	@%p421 bra 	BB9_352;
-
-BB9_348:
-	and.b32  	%r789, %r27, 2147483647;
-	setp.ne.s32	%p422, %r789, 2146435072;
-	@%p422 bra 	BB9_349;
-
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r790, %temp}, %fd228;
-	}
-	setp.ne.s32	%p423, %r790, 0;
-	mov.f64 	%fd469, %fd468;
-	@%p423 bra 	BB9_353;
-
-	shr.s32 	%r791, %r9, 31;
-	and.b32  	%r792, %r791, -2146435072;
-	add.s32 	%r793, %r792, 2146435072;
-	or.b32  	%r794, %r793, -2147483648;
-	selp.b32	%r795, %r794, %r793, %p18;
-	mov.u32 	%r796, 0;
-	mov.b64 	%fd469, {%r796, %r795};
-	bra.uni 	BB9_353;
-
-BB9_344:
-	mov.f64 	%fd469, %fd468;
-	bra.uni 	BB9_353;
-
-BB9_349:
-	mov.f64 	%fd469, %fd468;
-	bra.uni 	BB9_353;
-
-BB9_352:
-	setp.gt.f64	%p424, %fd229, 0d3FF0000000000000;
-	selp.b32	%r797, 2146435072, 0, %p424;
-	mov.u32 	%r798, 0;
-	xor.b32  	%r799, %r797, 2146435072;
-	setp.lt.s32	%p425, %r9, 0;
-	selp.b32	%r800, %r799, %r797, %p425;
-	setp.eq.f32	%p426, %f354, 0fBF800000;
-	selp.b32	%r801, 1072693248, %r800, %p426;
-	mov.b64 	%fd469, {%r798, %r801};
-
-BB9_353:
-	setp.eq.f32	%p427, %f354, 0f3F800000;
-	selp.f64	%fd240, 0d3FF0000000000000, %fd469, %p427;
-	cvt.f64.f32	%fd241, %f355;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r28}, %fd241;
-	}
-	abs.f64 	%fd242, %fd241;
-	// Callseq Start 18
-	{
-	.reg .b32 temp_param_reg;
-	// <end>}
-	.param .b64 param0;
-	st.param.f64	[param0+0], %fd242;
-	.param .b64 retval0;
-	call.uni (retval0), 
-	__internal_accurate_pow, 
-	(
-	param0
-	);
-	ld.param.f64	%fd471, [retval0+0];
-	
-	//{
-	}// Callseq End 18
-	setp.gt.s32	%p428, %r28, -1;
-	setp.lt.s32	%p429, %r28, 0;
-	and.pred  	%p19, %p429, %p409;
-	or.pred  	%p432, %p428, %p408;
-	@%p432 bra 	BB9_355;
-
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r802}, %fd471;
-	}
-	xor.b32  	%r803, %r802, -2147483648;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r804, %temp}, %fd471;
-	}
-	mov.b64 	%fd471, {%r804, %r803};
-
-BB9_355:
-	setp.eq.f32	%p433, %f355, 0f00000000;
-	@%p433 bra 	BB9_358;
-	bra.uni 	BB9_356;
-
-BB9_358:
-	selp.b32	%r805, %r28, 0, %p409;
-	mov.u32 	%r806, 0;
-	or.b32  	%r807, %r805, 2146435072;
-	setp.lt.s32	%p437, %r9, 0;
-	selp.b32	%r808, %r807, %r805, %p437;
-	mov.b64 	%fd471, {%r806, %r808};
-	bra.uni 	BB9_359;
-
-BB9_356:
-	@%p428 bra 	BB9_359;
-
-	cvt.rzi.f64.f64	%fd396, %fd292;
-	setp.neu.f64	%p435, %fd396, 0d4000000000000000;
-	selp.f64	%fd471, 0dFFF8000000000000, %fd471, %p435;
-
-BB9_359:
-	add.f64 	%fd472, %fd241, 0d4000000000000000;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r809}, %fd472;
-	}
-	and.b32  	%r810, %r809, 2146435072;
-	setp.ne.s32	%p438, %r810, 2146435072;
-	@%p438 bra 	BB9_360;
-
-	setp.gtu.f64	%p439, %fd242, 0d7FF0000000000000;
-	@%p439 bra 	BB9_369;
-
-	and.b32  	%r811, %r9, 2147483647;
-	setp.ne.s32	%p440, %r811, 2146435072;
-	@%p440 bra 	BB9_364;
-
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r812, %temp}, %fd292;
-	}
-	setp.eq.s32	%p441, %r812, 0;
-	@%p441 bra 	BB9_368;
-
-BB9_364:
-	and.b32  	%r813, %r28, 2147483647;
-	setp.ne.s32	%p442, %r813, 2146435072;
-	@%p442 bra 	BB9_365;
-
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r814, %temp}, %fd241;
-	}
-	setp.ne.s32	%p443, %r814, 0;
-	mov.f64 	%fd472, %fd471;
-	@%p443 bra 	BB9_369;
-
-	shr.s32 	%r815, %r9, 31;
-	and.b32  	%r816, %r815, -2146435072;
-	add.s32 	%r817, %r816, 2146435072;
-	or.b32  	%r818, %r817, -2147483648;
-	selp.b32	%r819, %r818, %r817, %p19;
-	mov.u32 	%r820, 0;
-	mov.b64 	%fd472, {%r820, %r819};
-	bra.uni 	BB9_369;
-
-BB9_360:
-	mov.f64 	%fd472, %fd471;
-	bra.uni 	BB9_369;
-
-BB9_365:
-	mov.f64 	%fd472, %fd471;
-	bra.uni 	BB9_369;
-
-BB9_368:
-	setp.gt.f64	%p444, %fd242, 0d3FF0000000000000;
-	selp.b32	%r821, 2146435072, 0, %p444;
-	mov.u32 	%r822, 0;
-	xor.b32  	%r823, %r821, 2146435072;
-	setp.lt.s32	%p445, %r9, 0;
-	selp.b32	%r824, %r823, %r821, %p445;
-	setp.eq.f32	%p446, %f355, 0fBF800000;
-	selp.b32	%r825, 1072693248, %r824, %p446;
-	mov.b64 	%fd472, {%r822, %r825};
-
-BB9_369:
-	setp.eq.f32	%p447, %f355, 0f3F800000;
-	selp.f64	%fd398, 0d3FF0000000000000, %fd472, %p447;
-	add.f64 	%fd253, %fd240, %fd398;
-	cvt.f64.f32	%fd254, %f356;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r29}, %fd254;
-	}
-	abs.f64 	%fd255, %fd254;
-	// Callseq Start 19
-	{
-	.reg .b32 temp_param_reg;
-	// <end>}
-	.param .b64 param0;
-	st.param.f64	[param0+0], %fd255;
-	.param .b64 retval0;
-	call.uni (retval0), 
-	__internal_accurate_pow, 
-	(
-	param0
-	);
-	ld.param.f64	%fd474, [retval0+0];
-	
-	//{
-	}// Callseq End 19
-	setp.gt.s32	%p448, %r29, -1;
-	setp.lt.s32	%p449, %r29, 0;
-	and.pred  	%p20, %p449, %p409;
-	or.pred  	%p452, %p448, %p408;
-	@%p452 bra 	BB9_371;
-
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r826}, %fd474;
-	}
-	xor.b32  	%r827, %r826, -2147483648;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r828, %temp}, %fd474;
-	}
-	mov.b64 	%fd474, {%r828, %r827};
-
-BB9_371:
-	setp.eq.f32	%p453, %f356, 0f00000000;
-	@%p453 bra 	BB9_374;
-	bra.uni 	BB9_372;
-
-BB9_374:
-	selp.b32	%r829, %r29, 0, %p409;
-	mov.u32 	%r830, 0;
-	or.b32  	%r831, %r829, 2146435072;
-	setp.lt.s32	%p457, %r9, 0;
-	selp.b32	%r832, %r831, %r829, %p457;
-	mov.b64 	%fd474, {%r830, %r832};
-	bra.uni 	BB9_375;
-
-BB9_372:
-	@%p448 bra 	BB9_375;
-
-	cvt.rzi.f64.f64	%fd400, %fd292;
-	setp.neu.f64	%p455, %fd400, 0d4000000000000000;
-	selp.f64	%fd474, 0dFFF8000000000000, %fd474, %p455;
-
-BB9_375:
-	add.f64 	%fd475, %fd254, 0d4000000000000000;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r833}, %fd475;
-	}
-	and.b32  	%r834, %r833, 2146435072;
-	setp.ne.s32	%p458, %r834, 2146435072;
-	@%p458 bra 	BB9_376;
-
-	setp.gtu.f64	%p459, %fd255, 0d7FF0000000000000;
-	@%p459 bra 	BB9_385;
-
-	and.b32  	%r835, %r9, 2147483647;
-	setp.ne.s32	%p460, %r835, 2146435072;
-	@%p460 bra 	BB9_380;
-
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r836, %temp}, %fd292;
-	}
-	setp.eq.s32	%p461, %r836, 0;
-	@%p461 bra 	BB9_384;
-
-BB9_380:
-	and.b32  	%r837, %r29, 2147483647;
-	setp.ne.s32	%p462, %r837, 2146435072;
-	@%p462 bra 	BB9_381;
-
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r838, %temp}, %fd254;
-	}
-	setp.ne.s32	%p463, %r838, 0;
-	mov.f64 	%fd475, %fd474;
-	@%p463 bra 	BB9_385;
-
-	shr.s32 	%r839, %r9, 31;
-	and.b32  	%r840, %r839, -2146435072;
-	add.s32 	%r841, %r840, 2146435072;
-	or.b32  	%r842, %r841, -2147483648;
-	selp.b32	%r843, %r842, %r841, %p20;
-	mov.u32 	%r844, 0;
-	mov.b64 	%fd475, {%r844, %r843};
-	bra.uni 	BB9_385;
-
-BB9_376:
-	mov.f64 	%fd475, %fd474;
-	bra.uni 	BB9_385;
-
-BB9_381:
-	mov.f64 	%fd475, %fd474;
-	bra.uni 	BB9_385;
-
-BB9_384:
-	setp.gt.f64	%p464, %fd255, 0d3FF0000000000000;
-	selp.b32	%r845, 2146435072, 0, %p464;
-	mov.u32 	%r846, 0;
-	xor.b32  	%r847, %r845, 2146435072;
-	setp.lt.s32	%p465, %r9, 0;
-	selp.b32	%r848, %r847, %r845, %p465;
-	setp.eq.f32	%p466, %f356, 0fBF800000;
-	selp.b32	%r849, 1072693248, %r848, %p466;
-	mov.b64 	%fd475, {%r846, %r849};
-
-BB9_385:
-	setp.eq.f32	%p467, %f356, 0f3F800000;
-	selp.f64	%fd402, 0d3FF0000000000000, %fd475, %p467;
-	add.f64 	%fd403, %fd253, %fd402;
-	sqrt.rn.f64 	%fd266, %fd403;
-	cvt.f64.f32	%fd267, %f889;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r30}, %fd267;
-	}
-	abs.f64 	%fd268, %fd267;
-	// Callseq Start 20
-	{
-	.reg .b32 temp_param_reg;
-	// <end>}
-	.param .b64 param0;
-	st.param.f64	[param0+0], %fd268;
-	.param .b64 retval0;
-	call.uni (retval0), 
-	__internal_accurate_pow, 
-	(
-	param0
-	);
-	ld.param.f64	%fd477, [retval0+0];
-	
-	//{
-	}// Callseq End 20
-	setp.gt.s32	%p468, %r30, -1;
-	setp.lt.s32	%p469, %r30, 0;
-	and.pred  	%p21, %p469, %p409;
-	or.pred  	%p472, %p468, %p408;
-	@%p472 bra 	BB9_387;
-
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r850}, %fd477;
-	}
-	xor.b32  	%r851, %r850, -2147483648;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r852, %temp}, %fd477;
-	}
-	mov.b64 	%fd477, {%r852, %r851};
-
-BB9_387:
-	setp.eq.f32	%p473, %f889, 0f00000000;
-	@%p473 bra 	BB9_390;
-	bra.uni 	BB9_388;
-
-BB9_390:
-	selp.b32	%r853, %r30, 0, %p409;
-	mov.u32 	%r854, 0;
-	or.b32  	%r855, %r853, 2146435072;
-	setp.lt.s32	%p477, %r9, 0;
-	selp.b32	%r856, %r855, %r853, %p477;
-	mov.b64 	%fd477, {%r854, %r856};
-	bra.uni 	BB9_391;
-
-BB9_388:
-	@%p468 bra 	BB9_391;
-
-	cvt.rzi.f64.f64	%fd405, %fd292;
-	setp.neu.f64	%p475, %fd405, 0d4000000000000000;
-	selp.f64	%fd477, 0dFFF8000000000000, %fd477, %p475;
-
-BB9_391:
-	add.f64 	%fd478, %fd267, 0d4000000000000000;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r857}, %fd478;
-	}
-	and.b32  	%r858, %r857, 2146435072;
-	setp.ne.s32	%p478, %r858, 2146435072;
-	@%p478 bra 	BB9_392;
-
-	setp.gtu.f64	%p479, %fd268, 0d7FF0000000000000;
-	@%p479 bra 	BB9_401;
-
-	and.b32  	%r859, %r9, 2147483647;
-	setp.ne.s32	%p480, %r859, 2146435072;
-	@%p480 bra 	BB9_396;
-
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r860, %temp}, %fd292;
-	}
-	setp.eq.s32	%p481, %r860, 0;
-	@%p481 bra 	BB9_400;
-
-BB9_396:
-	and.b32  	%r861, %r30, 2147483647;
-	setp.ne.s32	%p482, %r861, 2146435072;
-	@%p482 bra 	BB9_397;
-
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r862, %temp}, %fd267;
-	}
-	setp.ne.s32	%p483, %r862, 0;
-	mov.f64 	%fd478, %fd477;
-	@%p483 bra 	BB9_401;
-
-	shr.s32 	%r863, %r9, 31;
-	and.b32  	%r864, %r863, -2146435072;
-	add.s32 	%r865, %r864, 2146435072;
-	or.b32  	%r866, %r865, -2147483648;
-	selp.b32	%r867, %r866, %r865, %p21;
-	mov.u32 	%r868, 0;
-	mov.b64 	%fd478, {%r868, %r867};
-	bra.uni 	BB9_401;
-
-BB9_392:
-	mov.f64 	%fd478, %fd477;
-	bra.uni 	BB9_401;
-
-BB9_397:
-	mov.f64 	%fd478, %fd477;
-	bra.uni 	BB9_401;
-
-BB9_400:
-	setp.gt.f64	%p484, %fd268, 0d3FF0000000000000;
-	selp.b32	%r869, 2146435072, 0, %p484;
-	mov.u32 	%r870, 0;
-	xor.b32  	%r871, %r869, 2146435072;
-	setp.lt.s32	%p485, %r9, 0;
-	selp.b32	%r872, %r871, %r869, %p485;
-	setp.eq.f32	%p486, %f889, 0fBF800000;
-	selp.b32	%r873, 1072693248, %r872, %p486;
-	mov.b64 	%fd478, {%r870, %r873};
-
-BB9_401:
-	setp.eq.f32	%p487, %f889, 0f3F800000;
-	selp.f64	%fd279, 0d3FF0000000000000, %fd478, %p487;
-	cvt.f64.f32	%fd280, %f891;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r31}, %fd280;
-	}
-	abs.f64 	%fd281, %fd280;
-	// Callseq Start 21
-	{
-	.reg .b32 temp_param_reg;
-	// <end>}
-	.param .b64 param0;
-	st.param.f64	[param0+0], %fd281;
-	.param .b64 retval0;
-	call.uni (retval0), 
-	__internal_accurate_pow, 
-	(
-	param0
-	);
-	ld.param.f64	%fd480, [retval0+0];
-	
-	//{
-	}// Callseq End 21
-	setp.gt.s32	%p488, %r31, -1;
-	setp.lt.s32	%p489, %r31, 0;
-	and.pred  	%p22, %p489, %p409;
-	or.pred  	%p492, %p488, %p408;
-	@%p492 bra 	BB9_403;
-
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r874}, %fd480;
-	}
-	xor.b32  	%r875, %r874, -2147483648;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r876, %temp}, %fd480;
-	}
-	mov.b64 	%fd480, {%r876, %r875};
-
-BB9_403:
-	setp.eq.f32	%p493, %f891, 0f00000000;
-	@%p493 bra 	BB9_406;
-	bra.uni 	BB9_404;
-
-BB9_406:
-	selp.b32	%r877, %r31, 0, %p409;
-	mov.u32 	%r878, 0;
-	or.b32  	%r879, %r877, 2146435072;
-	setp.lt.s32	%p497, %r9, 0;
-	selp.b32	%r880, %r879, %r877, %p497;
-	mov.b64 	%fd480, {%r878, %r880};
-	bra.uni 	BB9_407;
-
-BB9_404:
-	@%p488 bra 	BB9_407;
-
-	cvt.rzi.f64.f64	%fd408, %fd292;
-	setp.neu.f64	%p495, %fd408, 0d4000000000000000;
-	selp.f64	%fd480, 0dFFF8000000000000, %fd480, %p495;
-
-BB9_407:
-	add.f64 	%fd481, %fd280, 0d4000000000000000;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r881}, %fd481;
-	}
-	and.b32  	%r882, %r881, 2146435072;
-	setp.ne.s32	%p498, %r882, 2146435072;
-	@%p498 bra 	BB9_408;
-
-	setp.gtu.f64	%p499, %fd281, 0d7FF0000000000000;
-	@%p499 bra 	BB9_417;
-
-	and.b32  	%r883, %r9, 2147483647;
-	setp.ne.s32	%p500, %r883, 2146435072;
-	@%p500 bra 	BB9_412;
-
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r884, %temp}, %fd292;
-	}
-	setp.eq.s32	%p501, %r884, 0;
-	@%p501 bra 	BB9_416;
-
-BB9_412:
-	and.b32  	%r885, %r31, 2147483647;
-	setp.ne.s32	%p502, %r885, 2146435072;
-	@%p502 bra 	BB9_413;
-
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r886, %temp}, %fd280;
-	}
-	setp.ne.s32	%p503, %r886, 0;
-	mov.f64 	%fd481, %fd480;
-	@%p503 bra 	BB9_417;
-
-	shr.s32 	%r887, %r9, 31;
-	and.b32  	%r888, %r887, -2146435072;
-	add.s32 	%r889, %r888, 2146435072;
-	or.b32  	%r890, %r889, -2147483648;
-	selp.b32	%r891, %r890, %r889, %p22;
-	mov.u32 	%r892, 0;
-	mov.b64 	%fd481, {%r892, %r891};
-	bra.uni 	BB9_417;
-
-BB9_408:
-	mov.f64 	%fd481, %fd480;
-	bra.uni 	BB9_417;
-
-BB9_413:
-	mov.f64 	%fd481, %fd480;
-	bra.uni 	BB9_417;
-
-BB9_416:
-	setp.gt.f64	%p504, %fd281, 0d3FF0000000000000;
-	selp.b32	%r893, 2146435072, 0, %p504;
-	mov.u32 	%r894, 0;
-	xor.b32  	%r895, %r893, 2146435072;
-	setp.lt.s32	%p505, %r9, 0;
-	selp.b32	%r896, %r895, %r893, %p505;
-	setp.eq.f32	%p506, %f891, 0fBF800000;
-	selp.b32	%r897, 1072693248, %r896, %p506;
-	mov.b64 	%fd481, {%r894, %r897};
-
-BB9_417:
-	setp.eq.f32	%p507, %f891, 0f3F800000;
-	selp.f64	%fd410, 0d3FF0000000000000, %fd481, %p507;
-	add.f64 	%fd411, %fd279, %fd410;
-	sqrt.rn.f64 	%fd412, %fd411;
-	cvt.rn.f32.f64	%f896, %fd412;
-	cvt.rn.f32.f64	%f897, %fd266;
-	setp.gtu.f32	%p508, %f897, %f896;
-	setp.eq.s16	%p509, %rs17, 0;
-	or.pred  	%p510, %p509, %p508;
-	setp.ltu.f32	%p511, %f1017, %f802;
-	or.pred  	%p512, %p510, %p511;
-	setp.geu.f32	%p513, %f1017, %f803;
-	or.pred  	%p514, %p513, %p512;
-	@%p514 bra 	BB9_419;
-
-	mov.u32 	%r899, 254;
-	// inline asm
-	call (%r898), _optix_report_intersection_0, (%f1017, %r899);
-	// inline asm
-
-BB9_419:
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r294,%r295,%r296,%r297}, [%rd241];
+	// end inline asm
+	mov.b32 	%f995, %r294;
+	mov.b32 	%f996, %r295;
+	mov.b32 	%f997, %r296;
+	setp.leu.f32 	%p22, %f261, 0f00000000;
+	@%p22 bra 	$L__BB9_36;
+
+	mov.f32 	%f730, 0f3F800000;
+	sub.f32 	%f731, %f730, %f261;
+	mul.lo.s64 	%rd254, %rd15, 48;
+	add.s64 	%rd255, %rd209, %rd254;
+	add.s64 	%rd246, %rd255, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd245, %rd246;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r301,%r302,%r303,%r304}, [%rd245];
+	// end inline asm
+	mov.b32 	%f732, %r301;
+	mov.b32 	%f733, %r302;
+	mov.b32 	%f734, %r303;
+	mul.f32 	%f735, %f261, %f732;
+	mul.f32 	%f736, %f261, %f733;
+	mul.f32 	%f737, %f261, %f734;
+	fma.rn.f32 	%f1001, %f731, %f1001, %f735;
+	fma.rn.f32 	%f1002, %f731, %f1002, %f736;
+	fma.rn.f32 	%f1003, %f731, %f1003, %f737;
+	add.s64 	%rd249, %rd255, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd248, %rd249;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r305,%r306,%r307,%r308}, [%rd248];
+	// end inline asm
+	mov.b32 	%f738, %r305;
+	mov.b32 	%f739, %r306;
+	mov.b32 	%f740, %r307;
+	mul.f32 	%f741, %f261, %f738;
+	mul.f32 	%f742, %f261, %f739;
+	mul.f32 	%f743, %f261, %f740;
+	fma.rn.f32 	%f998, %f731, %f998, %f741;
+	fma.rn.f32 	%f999, %f731, %f999, %f742;
+	fma.rn.f32 	%f1000, %f731, %f1000, %f743;
+	add.s64 	%rd252, %rd255, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd251, %rd252;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r309,%r310,%r311,%r312}, [%rd251];
+	// end inline asm
+	mov.b32 	%f744, %r309;
+	mov.b32 	%f745, %r310;
+	mov.b32 	%f746, %r311;
+	mul.f32 	%f747, %f261, %f744;
+	mul.f32 	%f748, %f261, %f745;
+	mul.f32 	%f749, %f261, %f746;
+	fma.rn.f32 	%f995, %f731, %f995, %f747;
+	fma.rn.f32 	%f996, %f731, %f996, %f748;
+	fma.rn.f32 	%f997, %f731, %f997, %f749;
+	bra.uni 	$L__BB9_36;
+
+$L__BB9_25:
+	mov.f32 	%f1004, 0f00000000;
+	mov.f32 	%f1006, 0f3F800000;
+	setp.eq.s32 	%p18, %r165, 4;
+	@%p18 bra 	$L__BB9_28;
+
+	setp.ne.s32 	%p19, %r165, 1;
+	mov.f32 	%f1005, %f1004;
+	mov.f32 	%f1007, %f1004;
+	mov.f32 	%f1008, %f1006;
+	mov.f32 	%f1009, %f1004;
+	mov.f32 	%f1010, %f1006;
+	mov.f32 	%f1011, %f1004;
+	mov.f32 	%f1012, %f1004;
+	@%p19 bra 	$L__BB9_37;
+
+	// begin inline asm
+	call (%rd139), _optix_get_static_transform_from_handle, (%rd137);
+	// end inline asm
+	add.s64 	%rd258, %rd139, 64;
+	bra.uni 	$L__BB9_29;
+
+$L__BB9_31:
+	// begin inline asm
+	call (%rd152), _optix_get_srt_motion_transform_from_handle, (%rd137);
+	// end inline asm
+	// begin inline asm
+	cvta.to.global.u64 %rd154, %rd152;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r179,%r180,%r181,%r182}, [%rd154];
+	// end inline asm
+	add.s64 	%rd158, %rd152, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd157, %rd158;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r183,%r184,%r185,%r186}, [%rd157];
+	// end inline asm
+	add.s64 	%rd161, %rd152, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd160, %rd161;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r187,%r188,%r189,%r190}, [%rd160];
+	// end inline asm
+	add.s64 	%rd164, %rd152, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd163, %rd164;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r191,%r192,%r193,%r194}, [%rd163];
+	// end inline asm
+	add.s64 	%rd167, %rd152, 64;
+	// begin inline asm
+	cvta.to.global.u64 %rd166, %rd167;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r195,%r196,%r197,%r198}, [%rd166];
+	// end inline asm
+	add.s64 	%rd170, %rd152, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd169, %rd170;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r199,%r200,%r201,%r202}, [%rd169];
+	// end inline asm
+	add.s64 	%rd173, %rd152, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd172, %rd173;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r203,%r204,%r205,%r206}, [%rd172];
+	// end inline asm
+	add.s64 	%rd176, %rd152, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd175, %rd176;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r207,%r208,%r209,%r210}, [%rd175];
+	// end inline asm
+	add.s64 	%rd179, %rd152, 128;
+	// begin inline asm
+	cvta.to.global.u64 %rd178, %rd179;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r211,%r212,%r213,%r214}, [%rd178];
+	// end inline asm
+	add.s64 	%rd182, %rd152, 144;
+	// begin inline asm
+	cvta.to.global.u64 %rd181, %rd182;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r215,%r216,%r217,%r218}, [%rd181];
+	// end inline asm
+	mov.b32 	%f627, %r182;
+	mov.b32 	%f628, %r183;
+	and.b32  	%r235, %r181, 65535;
+	add.s32 	%r236, %r235, -1;
+	cvt.rn.f32.s32 	%f629, %r236;
+	sub.f32 	%f630, %f615, %f627;
+	mul.f32 	%f631, %f630, %f629;
+	sub.f32 	%f632, %f628, %f627;
+	div.rn.f32 	%f633, %f631, %f632;
+	min.f32 	%f634, %f629, %f633;
+	mov.f32 	%f635, 0f00000000;
+	max.f32 	%f636, %f635, %f634;
+	cvt.rmi.f32.f32 	%f637, %f636;
+	sub.f32 	%f221, %f636, %f637;
+	cvt.rzi.s32.f32 	%r237, %f637;
+	mul.wide.s32 	%rd196, %r237, 64;
+	add.s64 	%rd185, %rd161, %rd196;
+	// begin inline asm
+	cvta.to.global.u64 %rd184, %rd185;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r219,%r220,%r221,%r222}, [%rd184];
+	// end inline asm
+	mov.b32 	%f985, %r219;
+	mov.b32 	%f986, %r220;
+	mov.b32 	%f987, %r221;
+	add.s64 	%rd188, %rd185, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd187, %rd188;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r223,%r224,%r225,%r226}, [%rd187];
+	// end inline asm
+	mov.b32 	%f988, %r223;
+	mov.b32 	%f989, %r224;
+	mov.b32 	%f990, %r226;
+	add.s64 	%rd191, %rd185, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd190, %rd191;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r227,%r228,%r229,%r230}, [%rd190];
+	// end inline asm
+	mov.b32 	%f991, %r228;
+	mov.b32 	%f992, %r229;
+	mov.b32 	%f993, %r230;
+	add.s64 	%rd194, %rd185, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd193, %rd194;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r231,%r232,%r233,%r234}, [%rd193];
+	// end inline asm
+	mov.b32 	%f994, %r231;
+	setp.leu.f32 	%p21, %f221, 0f00000000;
+	@%p21 bra 	$L__BB9_33;
+
+	mov.f32 	%f638, 0f3F800000;
+	sub.f32 	%f639, %f638, %f221;
+	add.s64 	%rd198, %rd185, 64;
+	// begin inline asm
+	cvta.to.global.u64 %rd197, %rd198;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r238,%r239,%r240,%r241}, [%rd197];
+	// end inline asm
+	mov.b32 	%f640, %r238;
+	mov.b32 	%f641, %r239;
+	mov.b32 	%f642, %r240;
+	mul.f32 	%f643, %f221, %f640;
+	mul.f32 	%f644, %f221, %f641;
+	mul.f32 	%f645, %f221, %f642;
+	fma.rn.f32 	%f985, %f639, %f985, %f643;
+	fma.rn.f32 	%f986, %f639, %f986, %f644;
+	fma.rn.f32 	%f987, %f639, %f987, %f645;
+	add.s64 	%rd201, %rd185, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd200, %rd201;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r242,%r243,%r244,%r245}, [%rd200];
+	// end inline asm
+	mov.b32 	%f646, %r242;
+	mov.b32 	%f647, %r243;
+	mov.b32 	%f648, %r245;
+	mul.f32 	%f649, %f221, %f646;
+	mul.f32 	%f650, %f221, %f647;
+	mul.f32 	%f651, %f221, %f648;
+	fma.rn.f32 	%f988, %f639, %f988, %f649;
+	fma.rn.f32 	%f989, %f639, %f989, %f650;
+	fma.rn.f32 	%f990, %f639, %f990, %f651;
+	add.s64 	%rd204, %rd185, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd203, %rd204;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r246,%r247,%r248,%r249}, [%rd203];
+	// end inline asm
+	mov.b32 	%f652, %r247;
+	mov.b32 	%f653, %r248;
+	mov.b32 	%f654, %r249;
+	mul.f32 	%f655, %f221, %f652;
+	mul.f32 	%f656, %f221, %f653;
+	mul.f32 	%f657, %f221, %f654;
+	fma.rn.f32 	%f658, %f639, %f991, %f655;
+	fma.rn.f32 	%f659, %f639, %f992, %f656;
+	fma.rn.f32 	%f660, %f639, %f993, %f657;
+	add.s64 	%rd207, %rd185, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd206, %rd207;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r250,%r251,%r252,%r253}, [%rd206];
+	// end inline asm
+	mov.b32 	%f661, %r250;
+	mul.f32 	%f662, %f221, %f661;
+	fma.rn.f32 	%f663, %f639, %f994, %f662;
+	mul.f32 	%f664, %f659, %f659;
+	fma.rn.f32 	%f665, %f658, %f658, %f664;
+	fma.rn.f32 	%f666, %f660, %f660, %f665;
+	fma.rn.f32 	%f667, %f663, %f663, %f666;
+	sqrt.rn.f32 	%f668, %f667;
+	rcp.rn.f32 	%f669, %f668;
+	mul.f32 	%f991, %f658, %f669;
+	mul.f32 	%f992, %f659, %f669;
+	mul.f32 	%f993, %f660, %f669;
+	mul.f32 	%f994, %f669, %f663;
+
+$L__BB9_33:
+	mul.f32 	%f670, %f992, %f992;
+	fma.rn.f32 	%f671, %f991, %f991, %f670;
+	fma.rn.f32 	%f672, %f993, %f993, %f671;
+	fma.rn.f32 	%f673, %f994, %f994, %f672;
+	rcp.rn.f32 	%f674, %f673;
+	mul.f32 	%f675, %f991, %f674;
+	mul.f32 	%f676, %f992, %f674;
+	mul.f32 	%f677, %f993, %f674;
+	mul.f32 	%f678, %f994, %f674;
+	mul.f32 	%f679, %f991, %f675;
+	mul.f32 	%f680, %f992, %f676;
+	mul.f32 	%f681, %f993, %f677;
+	mul.f32 	%f682, %f991, %f676;
+	mul.f32 	%f683, %f993, %f678;
+	mul.f32 	%f684, %f991, %f677;
+	mul.f32 	%f685, %f992, %f678;
+	mul.f32 	%f686, %f992, %f677;
+	mul.f32 	%f687, %f991, %f678;
+	sub.f32 	%f688, %f679, %f680;
+	sub.f32 	%f689, %f688, %f681;
+	fma.rn.f32 	%f690, %f994, %f678, %f689;
+	sub.f32 	%f691, %f682, %f683;
+	add.f32 	%f692, %f691, %f691;
+	add.f32 	%f693, %f684, %f685;
+	add.f32 	%f694, %f693, %f693;
+	add.f32 	%f695, %f682, %f683;
+	add.f32 	%f696, %f695, %f695;
+	sub.f32 	%f697, %f680, %f679;
+	sub.f32 	%f698, %f697, %f681;
+	fma.rn.f32 	%f699, %f994, %f678, %f698;
+	sub.f32 	%f700, %f686, %f687;
+	add.f32 	%f701, %f700, %f700;
+	sub.f32 	%f702, %f684, %f685;
+	add.f32 	%f703, %f702, %f702;
+	add.f32 	%f704, %f686, %f687;
+	add.f32 	%f705, %f704, %f704;
+	neg.f32 	%f706, %f679;
+	sub.f32 	%f707, %f706, %f680;
+	add.f32 	%f708, %f681, %f707;
+	fma.rn.f32 	%f709, %f994, %f678, %f708;
+	mul.f32 	%f710, %f987, %f690;
+	fma.rn.f32 	%f711, %f989, %f692, %f710;
+	fma.rn.f32 	%f1003, %f990, %f694, %f711;
+	mul.f32 	%f712, %f989, %f699;
+	fma.rn.f32 	%f713, %f987, %f696, %f712;
+	fma.rn.f32 	%f1000, %f990, %f701, %f713;
+	mul.f32 	%f714, %f989, %f705;
+	fma.rn.f32 	%f715, %f987, %f703, %f714;
+	fma.rn.f32 	%f997, %f990, %f709, %f715;
+	mul.f32 	%f716, %f986, %f690;
+	fma.rn.f32 	%f1002, %f988, %f692, %f716;
+	mul.f32 	%f717, %f988, %f699;
+	fma.rn.f32 	%f999, %f986, %f696, %f717;
+	mul.f32 	%f718, %f988, %f705;
+	fma.rn.f32 	%f996, %f986, %f703, %f718;
+	mul.f32 	%f1001, %f985, %f690;
+	mul.f32 	%f998, %f985, %f696;
+	mul.f32 	%f995, %f985, %f703;
+
+$L__BB9_36:
+	mul.f32 	%f750, %f996, %f1000;
+	mul.f32 	%f751, %f997, %f999;
+	sub.f32 	%f752, %f751, %f750;
+	mul.f32 	%f753, %f1001, %f752;
+	mul.f32 	%f754, %f995, %f1000;
+	mul.f32 	%f755, %f997, %f998;
+	sub.f32 	%f756, %f755, %f754;
+	mul.f32 	%f757, %f756, %f1002;
+	sub.f32 	%f758, %f753, %f757;
+	mul.f32 	%f759, %f995, %f999;
+	mul.f32 	%f760, %f996, %f998;
+	sub.f32 	%f761, %f760, %f759;
+	fma.rn.f32 	%f762, %f761, %f1003, %f758;
+	rcp.rn.f32 	%f763, %f762;
+	mul.f32 	%f1010, %f752, %f763;
+	mul.f32 	%f764, %f997, %f1002;
+	mul.f32 	%f765, %f996, %f1003;
+	sub.f32 	%f766, %f765, %f764;
+	mul.f32 	%f1011, %f766, %f763;
+	mul.f32 	%f767, %f999, %f1003;
+	mul.f32 	%f768, %f1000, %f1002;
+	sub.f32 	%f769, %f768, %f767;
+	mul.f32 	%f1012, %f769, %f763;
+	sub.f32 	%f770, %f754, %f755;
+	mul.f32 	%f1007, %f770, %f763;
+	mul.f32 	%f771, %f995, %f1003;
+	mul.f32 	%f772, %f997, %f1001;
+	sub.f32 	%f773, %f772, %f771;
+	mul.f32 	%f1008, %f773, %f763;
+	mul.f32 	%f774, %f1000, %f1001;
+	mul.f32 	%f775, %f998, %f1003;
+	sub.f32 	%f776, %f775, %f774;
+	mul.f32 	%f1009, %f776, %f763;
+	mul.f32 	%f1004, %f761, %f763;
+	mul.f32 	%f777, %f996, %f1001;
+	mul.f32 	%f778, %f995, %f1002;
+	sub.f32 	%f779, %f778, %f777;
+	mul.f32 	%f1005, %f779, %f763;
+	mul.f32 	%f780, %f998, %f1002;
+	mul.f32 	%f781, %f999, %f1001;
+	sub.f32 	%f782, %f781, %f780;
+	mul.f32 	%f1006, %f782, %f763;
+	bra.uni 	$L__BB9_37;
+
+$L__BB9_28:
+	// begin inline asm
+	call (%rd258), _optix_get_instance_inverse_transform_from_handle, (%rd137);
+	// end inline asm
+
+$L__BB9_29:
+	// begin inline asm
+	cvta.to.global.u64 %rd143, %rd258;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r167,%r168,%r169,%r170}, [%rd143];
+	// end inline asm
+	mov.b32 	%f1010, %r167;
+	mov.b32 	%f1011, %r168;
+	mov.b32 	%f1012, %r169;
+	add.s64 	%rd147, %rd258, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd146, %rd147;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r171,%r172,%r173,%r174}, [%rd146];
+	// end inline asm
+	mov.b32 	%f1007, %r171;
+	mov.b32 	%f1008, %r172;
+	mov.b32 	%f1009, %r173;
+	add.s64 	%rd150, %rd258, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd149, %rd150;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r175,%r176,%r177,%r178}, [%rd149];
+	// end inline asm
+	mov.b32 	%f1004, %r175;
+	mov.b32 	%f1005, %r176;
+	mov.b32 	%f1006, %r177;
+
+$L__BB9_37:
+	setp.eq.s32 	%p23, %r322, 0;
+	@%p23 bra 	$L__BB9_39;
+
+	mul.f32 	%f783, %f981, %f1011;
+	fma.rn.f32 	%f784, %f978, %f1010, %f783;
+	fma.rn.f32 	%f307, %f984, %f1012, %f784;
+	mul.f32 	%f785, %f980, %f1011;
+	fma.rn.f32 	%f786, %f977, %f1010, %f785;
+	fma.rn.f32 	%f308, %f983, %f1012, %f786;
+	mul.f32 	%f787, %f979, %f1011;
+	fma.rn.f32 	%f788, %f976, %f1010, %f787;
+	fma.rn.f32 	%f1012, %f982, %f1012, %f788;
+	mul.f32 	%f789, %f981, %f1008;
+	fma.rn.f32 	%f790, %f978, %f1007, %f789;
+	fma.rn.f32 	%f310, %f984, %f1009, %f790;
+	mul.f32 	%f791, %f980, %f1008;
+	fma.rn.f32 	%f792, %f977, %f1007, %f791;
+	fma.rn.f32 	%f311, %f983, %f1009, %f792;
+	mul.f32 	%f793, %f979, %f1008;
+	fma.rn.f32 	%f794, %f976, %f1007, %f793;
+	fma.rn.f32 	%f1009, %f982, %f1009, %f794;
+	mul.f32 	%f795, %f981, %f1005;
+	fma.rn.f32 	%f796, %f978, %f1004, %f795;
+	fma.rn.f32 	%f313, %f984, %f1006, %f796;
+	mul.f32 	%f797, %f980, %f1005;
+	fma.rn.f32 	%f798, %f977, %f1004, %f797;
+	fma.rn.f32 	%f314, %f983, %f1006, %f798;
+	mul.f32 	%f799, %f979, %f1005;
+	fma.rn.f32 	%f800, %f976, %f1004, %f799;
+	fma.rn.f32 	%f1006, %f982, %f1006, %f800;
+	mov.f32 	%f1004, %f313;
+	mov.f32 	%f1005, %f314;
+	mov.f32 	%f1007, %f310;
+	mov.f32 	%f1008, %f311;
+	mov.f32 	%f1010, %f307;
+	mov.f32 	%f1011, %f308;
+
+$L__BB9_39:
+	add.s32 	%r322, %r322, 1;
+	setp.lt.u32 	%p24, %r322, %r162;
+	mov.f32 	%f976, %f1012;
+	mov.f32 	%f977, %f1011;
+	mov.f32 	%f978, %f1010;
+	mov.f32 	%f979, %f1009;
+	mov.f32 	%f980, %f1008;
+	mov.f32 	%f981, %f1007;
+	mov.f32 	%f982, %f1006;
+	mov.f32 	%f983, %f1005;
+	mov.f32 	%f984, %f1004;
+	@%p24 bra 	$L__BB9_24;
+
+$L__BB9_40:
+	mul.f32 	%f801, %f1032, %f1011;
+	fma.rn.f32 	%f802, %f1031, %f1010, %f801;
+	mul.f32 	%f803, %f1032, %f1008;
+	fma.rn.f32 	%f804, %f1031, %f1007, %f803;
+	mul.f32 	%f805, %f1032, %f1005;
+	fma.rn.f32 	%f806, %f1031, %f1004, %f805;
+	fma.rn.f32 	%f1033, %f614, %f1006, %f806;
+	fma.rn.f32 	%f1032, %f614, %f1009, %f804;
+	fma.rn.f32 	%f1031, %f614, %f1012, %f802;
+	bra.uni 	$L__BB9_42;
+
+$L__BB9_41:
+	mov.f32 	%f1033, %f614;
+
+$L__BB9_42:
+	// begin inline asm
+	call (%f807), _optix_get_ray_tmin, ();
+	// end inline asm
+	// begin inline asm
+	call (%f808), _optix_get_ray_tmax, ();
+	// end inline asm
+	add.s64 	%rd16, %rd1, 288;
+	ld.v4.f32 	{%f811, %f812, %f813, %f814}, [%rd1+288];
+	ld.v2.f32 	{%f815, %f816}, [%rd1+304];
+	add.f32 	%f819, %f815, 0f3F800000;
+	mul.f32 	%f820, %f1033, %f1033;
+	mul.f32 	%f821, %f819, %f820;
+	fma.rn.f32 	%f822, %f1031, %f1031, %f821;
+	fma.rn.f32 	%f351, %f1032, %f1032, %f822;
+	add.f32 	%f823, %f819, %f819;
+	mul.f32 	%f824, %f823, %f975;
+	mul.f32 	%f825, %f1033, %f824;
+	mul.f32 	%f826, %f819, 0fC0000000;
+	mul.f32 	%f827, %f813, %f826;
+	fma.rn.f32 	%f828, %f1033, %f827, %f825;
+	add.f32 	%f829, %f973, %f973;
+	fma.rn.f32 	%f830, %f1031, %f829, %f828;
+	add.f32 	%f831, %f811, %f811;
+	mul.f32 	%f832, %f831, %f1031;
+	sub.f32 	%f833, %f830, %f832;
+	add.f32 	%f834, %f974, %f974;
+	fma.rn.f32 	%f835, %f1032, %f834, %f833;
+	add.f32 	%f836, %f812, %f812;
+	mul.f32 	%f837, %f836, %f1032;
+	sub.f32 	%f838, %f835, %f837;
+	add.f32 	%f839, %f1033, %f1033;
+	div.rn.f32 	%f840, %f839, %f816;
+	sub.f32 	%f352, %f838, %f840;
+	mul.f32 	%f841, %f975, %f975;
+	mul.f32 	%f842, %f819, %f841;
+	fma.rn.f32 	%f843, %f827, %f975, %f842;
+	mul.f32 	%f844, %f813, %f813;
+	fma.rn.f32 	%f845, %f844, %f819, %f843;
+	fma.rn.f32 	%f846, %f973, %f973, %f845;
+	mul.f32 	%f847, %f831, %f973;
+	sub.f32 	%f848, %f846, %f847;
+	fma.rn.f32 	%f849, %f811, %f811, %f848;
+	fma.rn.f32 	%f850, %f974, %f974, %f849;
+	mul.f32 	%f851, %f836, %f974;
+	sub.f32 	%f852, %f850, %f851;
+	fma.rn.f32 	%f853, %f812, %f812, %f852;
+	add.f32 	%f854, %f975, %f975;
+	div.rn.f32 	%f855, %f854, %f816;
+	sub.f32 	%f856, %f853, %f855;
+	mul.f32 	%f857, %f813, 0fC0000000;
+	div.rn.f32 	%f858, %f857, %f816;
+	sub.f32 	%f353, %f856, %f858;
+	setp.eq.f32 	%p26, %f351, 0f00000000;
+	setp.eq.f32 	%p27, %f352, 0f00000000;
+	and.pred  	%p28, %p26, %p27;
+	mov.pred 	%p47, -1;
+	@%p28 bra 	$L__BB9_45;
+
+	neg.f32 	%f859, %f353;
+	div.rn.f32 	%f1034, %f859, %f352;
+	mul.f32 	%f860, %f351, 0fC0800000;
+	mul.f32 	%f861, %f860, %f353;
+	fma.rn.f32 	%f355, %f352, %f352, %f861;
+	setp.neu.f32 	%p30, %f351, 0f00000000;
+	setp.lt.f32 	%p31, %f355, 0f00000000;
+	and.pred  	%p32, %p31, %p30;
+	mov.f32 	%f1035, %f1034;
+	@%p32 bra 	$L__BB9_45;
+
+	mov.b32 	%r313, %f352;
+	and.b32  	%r314, %r313, -2147483648;
+	sqrt.rn.f32 	%f862, %f355;
+	mov.b32 	%r315, %f862;
+	and.b32  	%r316, %r315, 2147483647;
+	or.b32  	%r317, %r316, %r314;
+	mov.b32 	%f863, %r317;
+	add.f32 	%f864, %f352, %f863;
+	mul.f32 	%f865, %f864, 0fBF000000;
+	div.rn.f32 	%f866, %f865, %f351;
+	div.rn.f32 	%f867, %f353, %f865;
+	min.f32 	%f868, %f866, %f867;
+	max.f32 	%f869, %f866, %f867;
+	selp.f32 	%f356, %f1034, %f868, %p26;
+	selp.f32 	%f1035, %f1034, %f869, %p26;
+	mov.pred 	%p47, 0;
+	mov.f32 	%f1034, %f356;
+
+$L__BB9_45:
+	@%p47 bra 	$L__BB9_50;
+
+	fma.rn.f32 	%f870, %f1034, %f1031, %f973;
+	sub.f32 	%f871, %f870, %f811;
+	fma.rn.f32 	%f872, %f1034, %f1032, %f974;
+	sub.f32 	%f873, %f872, %f812;
+	fma.rn.f32 	%f874, %f1034, %f1033, %f975;
+	sub.f32 	%f875, %f874, %f813;
+	mul.f32 	%f876, %f873, %f873;
+	fma.rn.f32 	%f877, %f871, %f871, %f876;
+	fma.rn.f32 	%f878, %f875, %f875, %f877;
+	sqrt.rn.f32 	%f879, %f878;
+	ld.f32 	%f880, [%rd16+28];
+	mul.f32 	%f881, %f880, %f880;
+	ld.f32 	%f882, [%rd16+32];
+	fma.rn.f32 	%f883, %f882, %f882, %f881;
+	sqrt.rn.f32 	%f884, %f883;
+	setp.le.f32 	%p36, %f879, %f884;
+	setp.lt.f32 	%p37, %f1034, %f808;
+	setp.ge.f32 	%p38, %f1034, %f807;
+	and.pred  	%p39, %p38, %p37;
+	and.pred  	%p2, %p39, %p36;
+	fma.rn.f32 	%f885, %f1035, %f1031, %f973;
+	sub.f32 	%f886, %f885, %f811;
+	fma.rn.f32 	%f887, %f1035, %f1032, %f974;
+	sub.f32 	%f888, %f887, %f812;
+	fma.rn.f32 	%f889, %f1035, %f1033, %f975;
+	sub.f32 	%f890, %f889, %f813;
+	mul.f32 	%f891, %f888, %f888;
+	fma.rn.f32 	%f892, %f886, %f886, %f891;
+	fma.rn.f32 	%f893, %f890, %f890, %f892;
+	sqrt.rn.f32 	%f894, %f893;
+	setp.gtu.f32 	%p40, %f894, %f884;
+	mov.pred 	%p48, 0;
+	@%p40 bra 	$L__BB9_48;
+
+	setp.ge.f32 	%p41, %f1035, %f807;
+	setp.lt.f32 	%p42, %f1035, %f808;
+	and.pred  	%p48, %p41, %p42;
+
+$L__BB9_48:
+	or.pred  	%p43, %p2, %p48;
+	not.pred 	%p44, %p43;
+	@%p44 bra 	$L__BB9_50;
+
+	not.pred 	%p45, %p2;
+	and.pred  	%p46, %p48, %p45;
+	selp.f32 	%f895, %f1035, %f1034, %p46;
+	mov.u32 	%r319, 254;
+	// begin inline asm
+	call (%r318), _optix_report_intersection_0, (%f895, %r319);
+	// end inline asm
+
+$L__BB9_50:
 	ret;
-}
 
+}
 	// .globl	__closesthit__asphsurf
-.visible .entry __closesthit__asphsurf(
-
-)
+.visible .entry __closesthit__asphsurf()
 {
-	.reg .pred 	%p<135>;
-	.reg .b16 	%rs<21>;
-	.reg .f32 	%f<1994>;
-	.reg .b32 	%r<738>;
-	.reg .f64 	%fd<89>;
-	.reg .b64 	%rd<667>;
-
-
-	// inline asm
-	call (%r26), _optix_get_launch_dimension_x, ();
-	// inline asm
-	// inline asm
-	call (%r27), _optix_get_launch_dimension_y, ();
-	// inline asm
-	// inline asm
-	call (%r29), _optix_get_launch_index_x, ();
-	// inline asm
-	// inline asm
-	call (%r30), _optix_get_launch_index_y, ();
-	// inline asm
-	// inline asm
-	call (%r31), _optix_get_launch_index_z, ();
-	// inline asm
-	mad.lo.s32 	%r32, %r31, %r27, %r30;
-	mad.lo.s32 	%r1, %r32, %r26, %r29;
+	.reg .pred 	%p<59>;
+	.reg .b16 	%rs<3>;
+	.reg .f32 	%f<2059>;
+	.reg .b32 	%r<650>;
+	.reg .b64 	%rd<652>;
+
+
+	// begin inline asm
+	call (%r25), _optix_get_launch_dimension_x, ();
+	// end inline asm
+	// begin inline asm
+	call (%r26), _optix_get_launch_dimension_y, ();
+	// end inline asm
+	// begin inline asm
+	call (%r28), _optix_get_launch_index_x, ();
+	// end inline asm
+	// begin inline asm
+	call (%r29), _optix_get_launch_index_y, ();
+	// end inline asm
+	// begin inline asm
+	call (%r30), _optix_get_launch_index_z, ();
+	// end inline asm
+	mad.lo.s32 	%r31, %r30, %r26, %r29;
+	mad.lo.s32 	%r1, %r31, %r25, %r28;
 	ld.const.u64 	%rd1, [params+352];
-	setp.eq.s64	%p4, %rd1, 0;
-	@%p4 bra 	BB10_2;
+	setp.eq.s64 	%p1, %rd1, 0;
+	@%p1 bra 	$L__BB10_2;
 
-	cvta.to.global.u64 	%rd49, %rd1;
-	cvt.u64.u32	%rd50, %r1;
-	add.s64 	%rd51, %rd49, %rd50;
-	mov.u16 	%rs2, 1;
-	st.global.u8 	[%rd51], %rs2;
-	bra.uni 	BB10_178;
-
-BB10_2:
-	// inline asm
-	call (%rd52), _optix_get_sbt_data_ptr_64, ();
-	// inline asm
-	ld.u64 	%rd3, [%rd52+8];
-	// inline asm
-	call (%f686), _optix_get_world_ray_origin_x, ();
-	// inline asm
-	// inline asm
-	call (%f687), _optix_get_world_ray_origin_y, ();
-	// inline asm
-	// inline asm
-	call (%f1779), _optix_get_world_ray_origin_z, ();
-	// inline asm
-	// inline asm
+	cvta.to.global.u64 	%rd44, %rd1;
+	cvt.u64.u32 	%rd45, %r1;
+	add.s64 	%rd46, %rd44, %rd45;
+	mov.u16 	%rs1, 1;
+	st.global.u8 	[%rd46], %rs1;
+	bra.uni 	$L__BB10_114;
+
+$L__BB10_2:
+	// begin inline asm
+	call (%rd47), _optix_get_sbt_data_ptr_64, ();
+	// end inline asm
+	ld.u64 	%rd3, [%rd47+8];
+	// begin inline asm
+	call (%f1835), _optix_get_world_ray_origin_x, ();
+	// end inline asm
+	// begin inline asm
+	call (%f1836), _optix_get_world_ray_origin_y, ();
+	// end inline asm
+	// begin inline asm
+	call (%f1837), _optix_get_world_ray_origin_z, ();
+	// end inline asm
+	// begin inline asm
+	call (%r32), _optix_get_transform_list_size, ();
+	// end inline asm
+	setp.eq.s32 	%p2, %r32, 0;
+	@%p2 bra 	$L__BB10_23;
+
+	// begin inline asm
 	call (%r33), _optix_get_transform_list_size, ();
-	// inline asm
-	setp.eq.s32	%p5, %r33, 0;
-	@%p5 bra 	BB10_3;
+	// end inline asm
+	// begin inline asm
+	call (%f714), _optix_get_ray_time, ();
+	// end inline asm
+	setp.eq.s32 	%p3, %r33, 0;
+	@%p3 bra 	$L__BB10_21;
 
-	mov.u32 	%r734, 0;
-	// inline asm
-	call (%f689), _optix_get_ray_time, ();
-	// inline asm
+	mov.u32 	%r645, 0;
 
-BB10_5:
+$L__BB10_5:
 	.pragma "nounroll";
-	// inline asm
-	call (%rd53), _optix_get_transform_list_handle, (%r734);
-	// inline asm
-	// inline asm
-	call (%r36), _optix_get_transform_type_from_handle, (%rd53);
-	// inline asm
-	and.b32  	%r37, %r36, -2;
-	setp.eq.s32	%p6, %r37, 2;
-	@%p6 bra 	BB10_11;
-	bra.uni 	BB10_6;
-
-BB10_11:
-	setp.eq.s32	%p9, %r36, 2;
-	@%p9 bra 	BB10_15;
-	bra.uni 	BB10_12;
-
-BB10_15:
-	// inline asm
-	call (%rd127), _optix_get_matrix_motion_transform_from_handle, (%rd53);
-	// inline asm
-	// inline asm
-	cvta.to.global.u64 %rd129, %rd127;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r125,%r126,%r127,%r128}, [%rd129];
-	// inline asm
-	mov.b32	{%rs5, %rs6}, %r127;
-	add.s64 	%rd133, %rd127, 16;
-	// inline asm
-	cvta.to.global.u64 %rd132, %rd133;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r129,%r130,%r131,%r132}, [%rd132];
-	// inline asm
-	add.s64 	%rd136, %rd127, 32;
-	// inline asm
-	cvta.to.global.u64 %rd135, %rd136;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r133,%r134,%r135,%r136}, [%rd135];
-	// inline asm
-	add.s64 	%rd139, %rd127, 48;
-	// inline asm
-	cvta.to.global.u64 %rd138, %rd139;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r137,%r138,%r139,%r140}, [%rd138];
-	// inline asm
-	add.s64 	%rd142, %rd127, 64;
-	// inline asm
-	cvta.to.global.u64 %rd141, %rd142;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r141,%r142,%r143,%r144}, [%rd141];
-	// inline asm
-	add.s64 	%rd145, %rd127, 80;
-	// inline asm
-	cvta.to.global.u64 %rd144, %rd145;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r145,%r146,%r147,%r148}, [%rd144];
-	// inline asm
-	add.s64 	%rd148, %rd127, 96;
-	// inline asm
-	cvta.to.global.u64 %rd147, %rd148;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r149,%r150,%r151,%r152}, [%rd147];
-	// inline asm
-	add.s64 	%rd151, %rd127, 112;
-	// inline asm
-	cvta.to.global.u64 %rd150, %rd151;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r153,%r154,%r155,%r156}, [%rd150];
-	// inline asm
-	mov.b32 	 %f816, %r128;
-	mov.b32 	 %f817, %r129;
-	cvt.u32.u16	%r169, %rs5;
+	// begin inline asm
+	call (%rd48), _optix_get_transform_list_handle, (%r645);
+	// end inline asm
+	// begin inline asm
+	call (%r36), _optix_get_transform_type_from_handle, (%rd48);
+	// end inline asm
+	or.b32  	%r37, %r36, 1;
+	setp.eq.s32 	%p4, %r37, 3;
+	@%p4 bra 	$L__BB10_11;
+	bra.uni 	$L__BB10_6;
+
+$L__BB10_11:
+	setp.eq.s32 	%p7, %r36, 2;
+	@%p7 bra 	$L__BB10_15;
+	bra.uni 	$L__BB10_12;
+
+$L__BB10_15:
+	// begin inline asm
+	call (%rd120), _optix_get_matrix_motion_transform_from_handle, (%rd48);
+	// end inline asm
+	// begin inline asm
+	cvta.to.global.u64 %rd122, %rd120;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r125,%r126,%r127,%r128}, [%rd122];
+	// end inline asm
+	add.s64 	%rd126, %rd120, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd125, %rd126;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r129,%r130,%r131,%r132}, [%rd125];
+	// end inline asm
+	add.s64 	%rd129, %rd120, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd128, %rd129;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r133,%r134,%r135,%r136}, [%rd128];
+	// end inline asm
+	add.s64 	%rd132, %rd120, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd131, %rd132;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r137,%r138,%r139,%r140}, [%rd131];
+	// end inline asm
+	add.s64 	%rd135, %rd120, 64;
+	// begin inline asm
+	cvta.to.global.u64 %rd134, %rd135;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r141,%r142,%r143,%r144}, [%rd134];
+	// end inline asm
+	add.s64 	%rd138, %rd120, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd137, %rd138;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r145,%r146,%r147,%r148}, [%rd137];
+	// end inline asm
+	add.s64 	%rd141, %rd120, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd140, %rd141;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r149,%r150,%r151,%r152}, [%rd140];
+	// end inline asm
+	add.s64 	%rd144, %rd120, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd143, %rd144;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r153,%r154,%r155,%r156}, [%rd143];
+	// end inline asm
+	mov.b32 	%f842, %r128;
+	mov.b32 	%f843, %r129;
+	and.b32  	%r169, %r127, 65535;
 	add.s32 	%r170, %r169, -1;
-	cvt.rn.f32.s32	%f818, %r170;
-	sub.f32 	%f819, %f689, %f816;
-	mul.f32 	%f820, %f819, %f818;
-	sub.f32 	%f821, %f817, %f816;
-	div.rn.f32 	%f822, %f820, %f821;
-	min.f32 	%f823, %f818, %f822;
-	mov.f32 	%f824, 0f00000000;
-	max.f32 	%f825, %f824, %f823;
-	cvt.rmi.f32.f32	%f826, %f825;
-	cvt.rzi.s32.f32	%r171, %f826;
-	mul.wide.s32 	%rd162, %r171, 48;
-	add.s64 	%rd154, %rd136, %rd162;
-	// inline asm
-	cvta.to.global.u64 %rd153, %rd154;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r157,%r158,%r159,%r160}, [%rd153];
-	// inline asm
-	mov.b32 	 %f1746, %r157;
-	mov.b32 	 %f1745, %r158;
-	mov.b32 	 %f1744, %r159;
-	mov.b32 	 %f1743, %r160;
-	add.s64 	%rd157, %rd154, 16;
-	// inline asm
+	cvt.rn.f32.s32 	%f844, %r170;
+	sub.f32 	%f845, %f714, %f842;
+	mul.f32 	%f846, %f845, %f844;
+	sub.f32 	%f847, %f843, %f842;
+	div.rn.f32 	%f848, %f846, %f847;
+	min.f32 	%f849, %f844, %f848;
+	mov.f32 	%f850, 0f00000000;
+	max.f32 	%f851, %f850, %f849;
+	cvt.rmi.f32.f32 	%f852, %f851;
+	sub.f32 	%f90, %f851, %f852;
+	cvt.rzi.s32.f32 	%r171, %f852;
+	mul.wide.s32 	%rd155, %r171, 48;
+	add.s64 	%rd147, %rd129, %rd155;
+	// begin inline asm
+	cvta.to.global.u64 %rd146, %rd147;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r157,%r158,%r159,%r160}, [%rd146];
+	// end inline asm
+	mov.b32 	%f1790, %r157;
+	mov.b32 	%f1789, %r158;
+	mov.b32 	%f1788, %r159;
+	mov.b32 	%f1787, %r160;
+	add.s64 	%rd150, %rd147, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd149, %rd150;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r161,%r162,%r163,%r164}, [%rd149];
+	// end inline asm
+	mov.b32 	%f1794, %r161;
+	mov.b32 	%f1793, %r162;
+	mov.b32 	%f1792, %r163;
+	mov.b32 	%f1791, %r164;
+	add.s64 	%rd153, %rd147, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd152, %rd153;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r165,%r166,%r167,%r168}, [%rd152];
+	// end inline asm
+	mov.b32 	%f1798, %r165;
+	mov.b32 	%f1797, %r166;
+	mov.b32 	%f1796, %r167;
+	mov.b32 	%f1795, %r168;
+	setp.leu.f32 	%p9, %f90, 0f00000000;
+	@%p9 bra 	$L__BB10_17;
+
+	cvt.rmi.f32.f32 	%f1758, %f851;
+	cvt.rzi.s32.f32 	%r644, %f1758;
+	cvt.s64.s32 	%rd647, %r644;
+	mov.f32 	%f853, 0f3F800000;
+	sub.f32 	%f854, %f853, %f90;
+	mul.lo.s64 	%rd165, %rd647, 48;
+	add.s64 	%rd166, %rd120, %rd165;
+	add.s64 	%rd157, %rd166, 80;
+	// begin inline asm
 	cvta.to.global.u64 %rd156, %rd157;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r161,%r162,%r163,%r164}, [%rd156];
-	// inline asm
-	mov.b32 	 %f1750, %r161;
-	mov.b32 	 %f1749, %r162;
-	mov.b32 	 %f1748, %r163;
-	mov.b32 	 %f1747, %r164;
-	add.s64 	%rd160, %rd154, 32;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r172,%r173,%r174,%r175}, [%rd156];
+	// end inline asm
+	mov.b32 	%f855, %r172;
+	mov.b32 	%f856, %r173;
+	mov.b32 	%f857, %r174;
+	mov.b32 	%f858, %r175;
+	mul.f32 	%f859, %f90, %f855;
+	mul.f32 	%f860, %f90, %f856;
+	mul.f32 	%f861, %f90, %f857;
+	mul.f32 	%f862, %f90, %f858;
+	fma.rn.f32 	%f1790, %f854, %f1790, %f859;
+	fma.rn.f32 	%f1789, %f854, %f1789, %f860;
+	fma.rn.f32 	%f1788, %f854, %f1788, %f861;
+	fma.rn.f32 	%f1787, %f854, %f1787, %f862;
+	add.s64 	%rd160, %rd166, 96;
+	// begin inline asm
 	cvta.to.global.u64 %rd159, %rd160;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r165,%r166,%r167,%r168}, [%rd159];
-	// inline asm
-	sub.f32 	%f98, %f825, %f826;
-	mov.b32 	 %f1754, %r165;
-	mov.b32 	 %f1753, %r166;
-	mov.b32 	 %f1752, %r167;
-	mov.b32 	 %f1751, %r168;
-	setp.leu.f32	%p11, %f98, 0f00000000;
-	@%p11 bra 	BB10_17;
-
-	cvt.rmi.f32.f32	%f1714, %f825;
-	cvt.rzi.s32.f32	%r733, %f1714;
-	cvt.s64.s32	%rd662, %r733;
-	mul.lo.s64 	%rd172, %rd662, 48;
-	add.s64 	%rd173, %rd127, %rd172;
-	add.s64 	%rd164, %rd173, 80;
-	// inline asm
-	cvta.to.global.u64 %rd163, %rd164;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r172,%r173,%r174,%r175}, [%rd163];
-	// inline asm
-	mov.b32 	 %f827, %r172;
-	mov.b32 	 %f828, %r173;
-	mov.b32 	 %f829, %r174;
-	mov.b32 	 %f830, %r175;
-	add.s64 	%rd167, %rd173, 96;
-	// inline asm
-	cvta.to.global.u64 %rd166, %rd167;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r176,%r177,%r178,%r179}, [%rd166];
-	// inline asm
-	mov.b32 	 %f831, %r176;
-	mov.b32 	 %f832, %r177;
-	mov.b32 	 %f833, %r178;
-	mov.b32 	 %f834, %r179;
-	add.s64 	%rd170, %rd173, 112;
-	// inline asm
-	cvta.to.global.u64 %rd169, %rd170;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r180,%r181,%r182,%r183}, [%rd169];
-	// inline asm
-	mov.f32 	%f835, 0f3F800000;
-	sub.f32 	%f836, %f835, %f98;
-	mul.f32 	%f837, %f98, %f827;
-	mul.f32 	%f838, %f98, %f828;
-	mul.f32 	%f839, %f98, %f829;
-	mul.f32 	%f840, %f98, %f830;
-	fma.rn.f32 	%f1746, %f836, %f1746, %f837;
-	fma.rn.f32 	%f1745, %f836, %f1745, %f838;
-	fma.rn.f32 	%f1744, %f836, %f1744, %f839;
-	fma.rn.f32 	%f1743, %f836, %f1743, %f840;
-	mul.f32 	%f841, %f98, %f831;
-	mul.f32 	%f842, %f98, %f832;
-	mul.f32 	%f843, %f98, %f833;
-	mul.f32 	%f844, %f98, %f834;
-	fma.rn.f32 	%f1750, %f836, %f1750, %f841;
-	fma.rn.f32 	%f1749, %f836, %f1749, %f842;
-	fma.rn.f32 	%f1748, %f836, %f1748, %f843;
-	fma.rn.f32 	%f1747, %f836, %f1747, %f844;
-	mov.b32 	 %f845, %r180;
-	mov.b32 	 %f846, %r181;
-	mov.b32 	 %f847, %r182;
-	mov.b32 	 %f848, %r183;
-	mul.f32 	%f849, %f98, %f845;
-	mul.f32 	%f850, %f98, %f846;
-	mul.f32 	%f851, %f98, %f847;
-	mul.f32 	%f852, %f98, %f848;
-	fma.rn.f32 	%f1754, %f836, %f1754, %f849;
-	fma.rn.f32 	%f1753, %f836, %f1753, %f850;
-	fma.rn.f32 	%f1752, %f836, %f1752, %f851;
-	fma.rn.f32 	%f1751, %f836, %f1751, %f852;
-	bra.uni 	BB10_17;
-
-BB10_6:
-	mov.f32 	%f1755, 0f00000000;
-	mov.f32 	%f1758, 0f3F800000;
-	setp.eq.s32	%p7, %r36, 4;
-	@%p7 bra 	BB10_9;
-	bra.uni 	BB10_7;
-
-BB10_9:
-	// inline asm
-	call (%rd663), _optix_get_instance_inverse_transform_from_handle, (%rd53);
-	// inline asm
-	bra.uni 	BB10_10;
-
-BB10_12:
-	// inline asm
-	call (%rd68), _optix_get_srt_motion_transform_from_handle, (%rd53);
-	// inline asm
-	// inline asm
-	cvta.to.global.u64 %rd70, %rd68;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r50,%r51,%r52,%r53}, [%rd70];
-	// inline asm
-	mov.b32	{%rs3, %rs4}, %r52;
-	add.s64 	%rd74, %rd68, 16;
-	// inline asm
-	cvta.to.global.u64 %rd73, %rd74;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r54,%r55,%r56,%r57}, [%rd73];
-	// inline asm
-	add.s64 	%rd77, %rd68, 32;
-	// inline asm
-	cvta.to.global.u64 %rd76, %rd77;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r58,%r59,%r60,%r61}, [%rd76];
-	// inline asm
-	add.s64 	%rd80, %rd68, 48;
-	// inline asm
-	cvta.to.global.u64 %rd79, %rd80;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r62,%r63,%r64,%r65}, [%rd79];
-	// inline asm
-	add.s64 	%rd83, %rd68, 64;
-	// inline asm
-	cvta.to.global.u64 %rd82, %rd83;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r66,%r67,%r68,%r69}, [%rd82];
-	// inline asm
-	add.s64 	%rd86, %rd68, 80;
-	// inline asm
-	cvta.to.global.u64 %rd85, %rd86;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r70,%r71,%r72,%r73}, [%rd85];
-	// inline asm
-	add.s64 	%rd89, %rd68, 96;
-	// inline asm
-	cvta.to.global.u64 %rd88, %rd89;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r74,%r75,%r76,%r77}, [%rd88];
-	// inline asm
-	add.s64 	%rd92, %rd68, 112;
-	// inline asm
-	cvta.to.global.u64 %rd91, %rd92;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r78,%r79,%r80,%r81}, [%rd91];
-	// inline asm
-	add.s64 	%rd95, %rd68, 128;
-	// inline asm
-	cvta.to.global.u64 %rd94, %rd95;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r82,%r83,%r84,%r85}, [%rd94];
-	// inline asm
-	add.s64 	%rd98, %rd68, 144;
-	// inline asm
-	cvta.to.global.u64 %rd97, %rd98;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r86,%r87,%r88,%r89}, [%rd97];
-	// inline asm
-	mov.b32 	 %f703, %r53;
-	mov.b32 	 %f704, %r54;
-	cvt.u32.u16	%r106, %rs3;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r176,%r177,%r178,%r179}, [%rd159];
+	// end inline asm
+	mov.b32 	%f863, %r176;
+	mov.b32 	%f864, %r177;
+	mov.b32 	%f865, %r178;
+	mov.b32 	%f866, %r179;
+	mul.f32 	%f867, %f90, %f863;
+	mul.f32 	%f868, %f90, %f864;
+	mul.f32 	%f869, %f90, %f865;
+	mul.f32 	%f870, %f90, %f866;
+	fma.rn.f32 	%f1794, %f854, %f1794, %f867;
+	fma.rn.f32 	%f1793, %f854, %f1793, %f868;
+	fma.rn.f32 	%f1792, %f854, %f1792, %f869;
+	fma.rn.f32 	%f1791, %f854, %f1791, %f870;
+	add.s64 	%rd163, %rd166, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd162, %rd163;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r180,%r181,%r182,%r183}, [%rd162];
+	// end inline asm
+	mov.b32 	%f871, %r180;
+	mov.b32 	%f872, %r181;
+	mov.b32 	%f873, %r182;
+	mov.b32 	%f874, %r183;
+	mul.f32 	%f875, %f90, %f871;
+	mul.f32 	%f876, %f90, %f872;
+	mul.f32 	%f877, %f90, %f873;
+	mul.f32 	%f878, %f90, %f874;
+	fma.rn.f32 	%f1798, %f854, %f1798, %f875;
+	fma.rn.f32 	%f1797, %f854, %f1797, %f876;
+	fma.rn.f32 	%f1796, %f854, %f1796, %f877;
+	fma.rn.f32 	%f1795, %f854, %f1795, %f878;
+	bra.uni 	$L__BB10_17;
+
+$L__BB10_6:
+	mov.f32 	%f1799, 0f00000000;
+	mov.f32 	%f1802, 0f3F800000;
+	setp.eq.s32 	%p5, %r36, 4;
+	@%p5 bra 	$L__BB10_9;
+
+	setp.ne.s32 	%p6, %r36, 1;
+	mov.f32 	%f1800, %f1799;
+	mov.f32 	%f1801, %f1799;
+	mov.f32 	%f1803, %f1799;
+	mov.f32 	%f1804, %f1799;
+	mov.f32 	%f1805, %f1802;
+	mov.f32 	%f1806, %f1799;
+	mov.f32 	%f1807, %f1799;
+	mov.f32 	%f1808, %f1802;
+	mov.f32 	%f1809, %f1799;
+	mov.f32 	%f1810, %f1799;
+	@%p6 bra 	$L__BB10_18;
+
+	// begin inline asm
+	call (%rd50), _optix_get_static_transform_from_handle, (%rd48);
+	// end inline asm
+	add.s64 	%rd648, %rd50, 64;
+	bra.uni 	$L__BB10_10;
+
+$L__BB10_12:
+	// begin inline asm
+	call (%rd63), _optix_get_srt_motion_transform_from_handle, (%rd48);
+	// end inline asm
+	// begin inline asm
+	cvta.to.global.u64 %rd65, %rd63;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r50,%r51,%r52,%r53}, [%rd65];
+	// end inline asm
+	add.s64 	%rd69, %rd63, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd68, %rd69;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r54,%r55,%r56,%r57}, [%rd68];
+	// end inline asm
+	add.s64 	%rd72, %rd63, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd71, %rd72;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r58,%r59,%r60,%r61}, [%rd71];
+	// end inline asm
+	add.s64 	%rd75, %rd63, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd74, %rd75;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r62,%r63,%r64,%r65}, [%rd74];
+	// end inline asm
+	add.s64 	%rd78, %rd63, 64;
+	// begin inline asm
+	cvta.to.global.u64 %rd77, %rd78;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r66,%r67,%r68,%r69}, [%rd77];
+	// end inline asm
+	add.s64 	%rd81, %rd63, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd80, %rd81;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r70,%r71,%r72,%r73}, [%rd80];
+	// end inline asm
+	add.s64 	%rd84, %rd63, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd83, %rd84;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r74,%r75,%r76,%r77}, [%rd83];
+	// end inline asm
+	add.s64 	%rd87, %rd63, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd86, %rd87;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r78,%r79,%r80,%r81}, [%rd86];
+	// end inline asm
+	add.s64 	%rd90, %rd63, 128;
+	// begin inline asm
+	cvta.to.global.u64 %rd89, %rd90;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r82,%r83,%r84,%r85}, [%rd89];
+	// end inline asm
+	add.s64 	%rd93, %rd63, 144;
+	// begin inline asm
+	cvta.to.global.u64 %rd92, %rd93;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r86,%r87,%r88,%r89}, [%rd92];
+	// end inline asm
+	mov.b32 	%f729, %r53;
+	mov.b32 	%f730, %r54;
+	and.b32  	%r106, %r52, 65535;
 	add.s32 	%r107, %r106, -1;
-	cvt.rn.f32.s32	%f705, %r107;
-	sub.f32 	%f706, %f689, %f703;
-	mul.f32 	%f707, %f706, %f705;
-	sub.f32 	%f708, %f704, %f703;
-	div.rn.f32 	%f709, %f707, %f708;
-	min.f32 	%f710, %f705, %f709;
-	mov.f32 	%f711, 0f00000000;
-	max.f32 	%f712, %f711, %f710;
-	cvt.rmi.f32.f32	%f713, %f712;
-	cvt.rzi.s32.f32	%r108, %f713;
-	mul.wide.s32 	%rd112, %r108, 64;
-	add.s64 	%rd101, %rd77, %rd112;
-	// inline asm
-	cvta.to.global.u64 %rd100, %rd101;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r90,%r91,%r92,%r93}, [%rd100];
-	// inline asm
-	mov.b32 	 %f1727, %r90;
-	mov.b32 	 %f1728, %r91;
-	mov.b32 	 %f1729, %r92;
-	mov.b32 	 %f1730, %r93;
-	add.s64 	%rd104, %rd101, 16;
-	// inline asm
-	cvta.to.global.u64 %rd103, %rd104;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r94,%r95,%r96,%r97}, [%rd103];
-	// inline asm
-	mov.b32 	 %f1731, %r94;
-	mov.b32 	 %f1732, %r95;
-	mov.b32 	 %f1733, %r96;
-	mov.b32 	 %f1734, %r97;
-	add.s64 	%rd107, %rd101, 32;
-	// inline asm
-	cvta.to.global.u64 %rd106, %rd107;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r98,%r99,%r100,%r101}, [%rd106];
-	// inline asm
-	sub.f32 	%f37, %f712, %f713;
-	mov.b32 	 %f1735, %r98;
-	mov.b32 	 %f1736, %r99;
-	mov.b32 	 %f1737, %r100;
-	mov.b32 	 %f1738, %r101;
-	add.s64 	%rd110, %rd101, 48;
-	// inline asm
-	cvta.to.global.u64 %rd109, %rd110;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r102,%r103,%r104,%r105}, [%rd109];
-	// inline asm
-	mov.b32 	 %f1739, %r102;
-	mov.b32 	 %f1740, %r103;
-	mov.b32 	 %f1741, %r104;
-	mov.b32 	 %f1742, %r105;
-	setp.leu.f32	%p10, %f37, 0f00000000;
-	@%p10 bra 	BB10_14;
-
-	cvt.rmi.f32.f32	%f1713, %f712;
-	cvt.rzi.s32.f32	%r732, %f1713;
-	cvt.s64.s32	%rd661, %r732;
-	shl.b64 	%rd125, %rd661, 6;
-	add.s64 	%rd126, %rd125, %rd68;
-	add.s64 	%rd114, %rd126, 96;
-	// inline asm
-	cvta.to.global.u64 %rd113, %rd114;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r109,%r110,%r111,%r112}, [%rd113];
-	// inline asm
-	mov.b32 	 %f714, %r109;
-	mov.b32 	 %f715, %r110;
-	mov.b32 	 %f716, %r111;
-	mov.b32 	 %f717, %r112;
-	add.s64 	%rd117, %rd126, 112;
-	// inline asm
-	cvta.to.global.u64 %rd116, %rd117;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r113,%r114,%r115,%r116}, [%rd116];
-	// inline asm
-	mov.b32 	 %f718, %r113;
-	mov.b32 	 %f719, %r114;
-	mov.b32 	 %f720, %r115;
-	mov.b32 	 %f721, %r116;
-	add.s64 	%rd120, %rd126, 128;
-	// inline asm
-	cvta.to.global.u64 %rd119, %rd120;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r117,%r118,%r119,%r120}, [%rd119];
-	// inline asm
-	mov.b32 	 %f722, %r117;
-	mov.b32 	 %f723, %r118;
-	mov.b32 	 %f724, %r119;
-	mov.b32 	 %f725, %r120;
-	add.s64 	%rd123, %rd126, 144;
-	// inline asm
-	cvta.to.global.u64 %rd122, %rd123;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r121,%r122,%r123,%r124}, [%rd122];
-	// inline asm
-	mov.f32 	%f726, 0f3F800000;
-	sub.f32 	%f727, %f726, %f37;
-	mul.f32 	%f728, %f37, %f714;
-	mul.f32 	%f729, %f37, %f715;
-	mul.f32 	%f730, %f37, %f716;
-	mul.f32 	%f731, %f37, %f717;
-	fma.rn.f32 	%f1727, %f727, %f1727, %f728;
-	fma.rn.f32 	%f1728, %f727, %f1728, %f729;
-	fma.rn.f32 	%f1729, %f727, %f1729, %f730;
-	fma.rn.f32 	%f1730, %f727, %f1730, %f731;
-	mul.f32 	%f732, %f37, %f718;
-	mul.f32 	%f733, %f37, %f719;
-	mul.f32 	%f734, %f37, %f720;
-	mul.f32 	%f735, %f37, %f721;
-	fma.rn.f32 	%f1731, %f727, %f1731, %f732;
-	fma.rn.f32 	%f1732, %f727, %f1732, %f733;
-	fma.rn.f32 	%f1733, %f727, %f1733, %f734;
-	fma.rn.f32 	%f1734, %f727, %f1734, %f735;
-	mul.f32 	%f736, %f37, %f722;
-	mul.f32 	%f737, %f37, %f723;
-	mul.f32 	%f738, %f37, %f724;
-	mul.f32 	%f739, %f37, %f725;
-	fma.rn.f32 	%f1735, %f727, %f1735, %f736;
-	fma.rn.f32 	%f740, %f727, %f1736, %f737;
-	fma.rn.f32 	%f741, %f727, %f1737, %f738;
-	fma.rn.f32 	%f742, %f727, %f1738, %f739;
-	mov.b32 	 %f743, %r121;
-	mov.b32 	 %f744, %r122;
-	mov.b32 	 %f745, %r123;
-	mov.b32 	 %f746, %r124;
-	mul.f32 	%f747, %f37, %f743;
-	mul.f32 	%f748, %f37, %f744;
-	mul.f32 	%f749, %f37, %f745;
-	mul.f32 	%f750, %f37, %f746;
-	fma.rn.f32 	%f751, %f727, %f1739, %f747;
-	fma.rn.f32 	%f1740, %f727, %f1740, %f748;
-	fma.rn.f32 	%f1741, %f727, %f1741, %f749;
-	fma.rn.f32 	%f1742, %f727, %f1742, %f750;
-	mul.f32 	%f752, %f741, %f741;
-	fma.rn.f32 	%f753, %f740, %f740, %f752;
-	fma.rn.f32 	%f754, %f742, %f742, %f753;
-	fma.rn.f32 	%f755, %f751, %f751, %f754;
-	sqrt.rn.f32 	%f756, %f755;
-	rcp.rn.f32 	%f757, %f756;
-	mul.f32 	%f1736, %f740, %f757;
-	mul.f32 	%f1737, %f741, %f757;
-	mul.f32 	%f1738, %f742, %f757;
-	mul.f32 	%f1739, %f751, %f757;
-
-BB10_14:
-	mul.f32 	%f758, %f1737, %f1737;
-	fma.rn.f32 	%f759, %f1736, %f1736, %f758;
-	fma.rn.f32 	%f760, %f1738, %f1738, %f759;
-	fma.rn.f32 	%f761, %f1739, %f1739, %f760;
-	rcp.rn.f32 	%f762, %f761;
-	mul.f32 	%f763, %f1736, %f762;
-	mul.f32 	%f764, %f1737, %f762;
-	mul.f32 	%f765, %f1738, %f762;
-	mul.f32 	%f766, %f1739, %f762;
-	mul.f32 	%f767, %f1736, %f763;
-	mul.f32 	%f768, %f1737, %f764;
-	mul.f32 	%f769, %f1738, %f765;
-	mul.f32 	%f770, %f1736, %f764;
-	mul.f32 	%f771, %f1738, %f766;
-	mul.f32 	%f772, %f1736, %f765;
-	mul.f32 	%f773, %f1737, %f766;
-	mul.f32 	%f774, %f1737, %f765;
-	mul.f32 	%f775, %f1736, %f766;
-	sub.f32 	%f776, %f767, %f768;
-	sub.f32 	%f777, %f776, %f769;
-	fma.rn.f32 	%f778, %f1739, %f766, %f777;
-	sub.f32 	%f779, %f770, %f771;
-	add.f32 	%f780, %f779, %f779;
-	add.f32 	%f781, %f772, %f773;
-	add.f32 	%f782, %f781, %f781;
-	add.f32 	%f783, %f770, %f771;
-	add.f32 	%f784, %f783, %f783;
-	sub.f32 	%f785, %f768, %f767;
-	sub.f32 	%f786, %f785, %f769;
-	fma.rn.f32 	%f787, %f1739, %f766, %f786;
-	sub.f32 	%f788, %f774, %f775;
-	add.f32 	%f789, %f788, %f788;
-	sub.f32 	%f790, %f772, %f773;
-	add.f32 	%f791, %f790, %f790;
-	add.f32 	%f792, %f774, %f775;
-	add.f32 	%f793, %f792, %f792;
-	neg.f32 	%f794, %f767;
-	sub.f32 	%f795, %f794, %f768;
-	add.f32 	%f796, %f769, %f795;
-	fma.rn.f32 	%f797, %f1739, %f766, %f796;
-	mul.f32 	%f798, %f1730, %f778;
-	fma.rn.f32 	%f799, %f1733, %f780, %f798;
-	fma.rn.f32 	%f800, %f1735, %f782, %f799;
-	sub.f32 	%f1743, %f1740, %f800;
-	mul.f32 	%f801, %f1733, %f787;
-	fma.rn.f32 	%f802, %f1730, %f784, %f801;
-	fma.rn.f32 	%f803, %f1735, %f789, %f802;
-	sub.f32 	%f1747, %f1741, %f803;
-	mul.f32 	%f804, %f1733, %f793;
-	fma.rn.f32 	%f805, %f1730, %f791, %f804;
-	fma.rn.f32 	%f806, %f1735, %f797, %f805;
-	sub.f32 	%f1751, %f1742, %f806;
-	mul.f32 	%f807, %f1729, %f778;
-	fma.rn.f32 	%f808, %f1732, %f780, %f807;
-	fma.rn.f32 	%f1744, %f1734, %f782, %f808;
-	mul.f32 	%f809, %f1732, %f787;
-	fma.rn.f32 	%f810, %f1729, %f784, %f809;
-	fma.rn.f32 	%f1748, %f1734, %f789, %f810;
-	mul.f32 	%f811, %f1732, %f793;
-	fma.rn.f32 	%f812, %f1729, %f791, %f811;
-	fma.rn.f32 	%f1752, %f1734, %f797, %f812;
-	mul.f32 	%f813, %f1728, %f778;
-	fma.rn.f32 	%f1745, %f1731, %f780, %f813;
-	mul.f32 	%f814, %f1731, %f787;
-	fma.rn.f32 	%f1749, %f1728, %f784, %f814;
-	mul.f32 	%f815, %f1731, %f793;
-	fma.rn.f32 	%f1753, %f1728, %f791, %f815;
-	mul.f32 	%f1746, %f1727, %f778;
-	mul.f32 	%f1750, %f1727, %f784;
-	mul.f32 	%f1754, %f1727, %f791;
-
-BB10_17:
-	mul.f32 	%f853, %f1748, %f1753;
-	mul.f32 	%f854, %f1749, %f1752;
-	sub.f32 	%f855, %f854, %f853;
-	mul.f32 	%f856, %f1746, %f855;
-	mul.f32 	%f857, %f1748, %f1754;
-	mul.f32 	%f858, %f1750, %f1752;
-	sub.f32 	%f859, %f858, %f857;
-	mul.f32 	%f860, %f1745, %f859;
-	sub.f32 	%f861, %f856, %f860;
-	mul.f32 	%f862, %f1749, %f1754;
-	mul.f32 	%f863, %f1750, %f1753;
-	sub.f32 	%f864, %f863, %f862;
-	fma.rn.f32 	%f865, %f1744, %f864, %f861;
-	rcp.rn.f32 	%f866, %f865;
-	mul.f32 	%f1758, %f866, %f855;
-	mul.f32 	%f867, %f1745, %f1752;
-	mul.f32 	%f868, %f1744, %f1753;
-	sub.f32 	%f869, %f868, %f867;
-	mul.f32 	%f1757, %f866, %f869;
-	mul.f32 	%f870, %f1744, %f1749;
-	mul.f32 	%f871, %f1745, %f1748;
-	sub.f32 	%f872, %f871, %f870;
-	mul.f32 	%f1756, %f872, %f866;
-	sub.f32 	%f873, %f857, %f858;
-	mul.f32 	%f1762, %f866, %f873;
-	mul.f32 	%f874, %f1744, %f1754;
-	mul.f32 	%f875, %f1746, %f1752;
-	sub.f32 	%f876, %f875, %f874;
-	mul.f32 	%f1761, %f866, %f876;
-	mul.f32 	%f877, %f1746, %f1748;
-	mul.f32 	%f878, %f1744, %f1750;
-	sub.f32 	%f879, %f878, %f877;
-	mul.f32 	%f1760, %f879, %f866;
-	mul.f32 	%f1766, %f866, %f864;
-	mul.f32 	%f880, %f1746, %f1753;
-	mul.f32 	%f881, %f1745, %f1754;
-	sub.f32 	%f882, %f881, %f880;
-	mul.f32 	%f1765, %f866, %f882;
-	mul.f32 	%f883, %f1745, %f1750;
-	mul.f32 	%f884, %f1746, %f1749;
+	cvt.rn.f32.s32 	%f731, %r107;
+	sub.f32 	%f732, %f714, %f729;
+	mul.f32 	%f733, %f732, %f731;
+	sub.f32 	%f734, %f730, %f729;
+	div.rn.f32 	%f735, %f733, %f734;
+	min.f32 	%f736, %f731, %f735;
+	mov.f32 	%f737, 0f00000000;
+	max.f32 	%f738, %f737, %f736;
+	cvt.rmi.f32.f32 	%f739, %f738;
+	sub.f32 	%f29, %f738, %f739;
+	cvt.rzi.s32.f32 	%r108, %f739;
+	mul.wide.s32 	%rd107, %r108, 64;
+	add.s64 	%rd96, %rd72, %rd107;
+	// begin inline asm
+	cvta.to.global.u64 %rd95, %rd96;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r90,%r91,%r92,%r93}, [%rd95];
+	// end inline asm
+	mov.b32 	%f1771, %r90;
+	mov.b32 	%f1772, %r91;
+	mov.b32 	%f1773, %r92;
+	mov.b32 	%f1774, %r93;
+	add.s64 	%rd99, %rd96, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd98, %rd99;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r94,%r95,%r96,%r97}, [%rd98];
+	// end inline asm
+	mov.b32 	%f1775, %r94;
+	mov.b32 	%f1776, %r95;
+	mov.b32 	%f1777, %r96;
+	mov.b32 	%f1778, %r97;
+	add.s64 	%rd102, %rd96, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd101, %rd102;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r98,%r99,%r100,%r101}, [%rd101];
+	// end inline asm
+	mov.b32 	%f1779, %r98;
+	mov.b32 	%f1780, %r99;
+	mov.b32 	%f1781, %r100;
+	mov.b32 	%f1782, %r101;
+	add.s64 	%rd105, %rd96, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd104, %rd105;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r102,%r103,%r104,%r105}, [%rd104];
+	// end inline asm
+	mov.b32 	%f1783, %r102;
+	mov.b32 	%f1784, %r103;
+	mov.b32 	%f1785, %r104;
+	mov.b32 	%f1786, %r105;
+	setp.leu.f32 	%p8, %f29, 0f00000000;
+	@%p8 bra 	$L__BB10_14;
+
+	mov.f32 	%f740, 0f3F800000;
+	sub.f32 	%f741, %f740, %f29;
+	add.s64 	%rd109, %rd96, 64;
+	// begin inline asm
+	cvta.to.global.u64 %rd108, %rd109;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r109,%r110,%r111,%r112}, [%rd108];
+	// end inline asm
+	mov.b32 	%f742, %r109;
+	mov.b32 	%f743, %r110;
+	mov.b32 	%f744, %r111;
+	mov.b32 	%f745, %r112;
+	mul.f32 	%f746, %f29, %f742;
+	mul.f32 	%f747, %f29, %f743;
+	mul.f32 	%f748, %f29, %f744;
+	mul.f32 	%f749, %f29, %f745;
+	fma.rn.f32 	%f1771, %f741, %f1771, %f746;
+	fma.rn.f32 	%f1772, %f741, %f1772, %f747;
+	fma.rn.f32 	%f1773, %f741, %f1773, %f748;
+	fma.rn.f32 	%f1774, %f741, %f1774, %f749;
+	add.s64 	%rd112, %rd96, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd111, %rd112;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r113,%r114,%r115,%r116}, [%rd111];
+	// end inline asm
+	mov.b32 	%f750, %r113;
+	mov.b32 	%f751, %r114;
+	mov.b32 	%f752, %r115;
+	mov.b32 	%f753, %r116;
+	mul.f32 	%f754, %f29, %f750;
+	mul.f32 	%f755, %f29, %f751;
+	mul.f32 	%f756, %f29, %f752;
+	mul.f32 	%f757, %f29, %f753;
+	fma.rn.f32 	%f1775, %f741, %f1775, %f754;
+	fma.rn.f32 	%f1776, %f741, %f1776, %f755;
+	fma.rn.f32 	%f1777, %f741, %f1777, %f756;
+	fma.rn.f32 	%f1778, %f741, %f1778, %f757;
+	add.s64 	%rd115, %rd96, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd114, %rd115;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r117,%r118,%r119,%r120}, [%rd114];
+	// end inline asm
+	mov.b32 	%f758, %r117;
+	mov.b32 	%f759, %r118;
+	mov.b32 	%f760, %r119;
+	mov.b32 	%f761, %r120;
+	mul.f32 	%f762, %f29, %f758;
+	mul.f32 	%f763, %f29, %f759;
+	mul.f32 	%f764, %f29, %f760;
+	mul.f32 	%f765, %f29, %f761;
+	fma.rn.f32 	%f1779, %f741, %f1779, %f762;
+	fma.rn.f32 	%f766, %f741, %f1780, %f763;
+	fma.rn.f32 	%f767, %f741, %f1781, %f764;
+	fma.rn.f32 	%f768, %f741, %f1782, %f765;
+	add.s64 	%rd118, %rd96, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd117, %rd118;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r121,%r122,%r123,%r124}, [%rd117];
+	// end inline asm
+	mov.b32 	%f769, %r121;
+	mov.b32 	%f770, %r122;
+	mov.b32 	%f771, %r123;
+	mov.b32 	%f772, %r124;
+	mul.f32 	%f773, %f29, %f769;
+	mul.f32 	%f774, %f29, %f770;
+	mul.f32 	%f775, %f29, %f771;
+	mul.f32 	%f776, %f29, %f772;
+	fma.rn.f32 	%f777, %f741, %f1783, %f773;
+	fma.rn.f32 	%f1784, %f741, %f1784, %f774;
+	fma.rn.f32 	%f1785, %f741, %f1785, %f775;
+	fma.rn.f32 	%f1786, %f741, %f1786, %f776;
+	mul.f32 	%f778, %f767, %f767;
+	fma.rn.f32 	%f779, %f766, %f766, %f778;
+	fma.rn.f32 	%f780, %f768, %f768, %f779;
+	fma.rn.f32 	%f781, %f777, %f777, %f780;
+	sqrt.rn.f32 	%f782, %f781;
+	rcp.rn.f32 	%f783, %f782;
+	mul.f32 	%f1780, %f766, %f783;
+	mul.f32 	%f1781, %f767, %f783;
+	mul.f32 	%f1782, %f768, %f783;
+	mul.f32 	%f1783, %f783, %f777;
+
+$L__BB10_14:
+	mul.f32 	%f784, %f1781, %f1781;
+	fma.rn.f32 	%f785, %f1780, %f1780, %f784;
+	fma.rn.f32 	%f786, %f1782, %f1782, %f785;
+	fma.rn.f32 	%f787, %f1783, %f1783, %f786;
+	rcp.rn.f32 	%f788, %f787;
+	mul.f32 	%f789, %f1780, %f788;
+	mul.f32 	%f790, %f1781, %f788;
+	mul.f32 	%f791, %f1782, %f788;
+	mul.f32 	%f792, %f1783, %f788;
+	mul.f32 	%f793, %f1780, %f789;
+	mul.f32 	%f794, %f1781, %f790;
+	mul.f32 	%f795, %f1782, %f791;
+	mul.f32 	%f796, %f1780, %f790;
+	mul.f32 	%f797, %f1782, %f792;
+	mul.f32 	%f798, %f1780, %f791;
+	mul.f32 	%f799, %f1781, %f792;
+	mul.f32 	%f800, %f1781, %f791;
+	mul.f32 	%f801, %f1780, %f792;
+	sub.f32 	%f802, %f793, %f794;
+	sub.f32 	%f803, %f802, %f795;
+	fma.rn.f32 	%f804, %f1783, %f792, %f803;
+	sub.f32 	%f805, %f796, %f797;
+	add.f32 	%f806, %f805, %f805;
+	add.f32 	%f807, %f798, %f799;
+	add.f32 	%f808, %f807, %f807;
+	add.f32 	%f809, %f796, %f797;
+	add.f32 	%f810, %f809, %f809;
+	sub.f32 	%f811, %f794, %f793;
+	sub.f32 	%f812, %f811, %f795;
+	fma.rn.f32 	%f813, %f1783, %f792, %f812;
+	sub.f32 	%f814, %f800, %f801;
+	add.f32 	%f815, %f814, %f814;
+	sub.f32 	%f816, %f798, %f799;
+	add.f32 	%f817, %f816, %f816;
+	add.f32 	%f818, %f800, %f801;
+	add.f32 	%f819, %f818, %f818;
+	neg.f32 	%f820, %f793;
+	sub.f32 	%f821, %f820, %f794;
+	add.f32 	%f822, %f795, %f821;
+	fma.rn.f32 	%f823, %f1783, %f792, %f822;
+	mul.f32 	%f824, %f1774, %f804;
+	fma.rn.f32 	%f825, %f1777, %f806, %f824;
+	fma.rn.f32 	%f826, %f1779, %f808, %f825;
+	sub.f32 	%f1787, %f1784, %f826;
+	mul.f32 	%f827, %f1777, %f813;
+	fma.rn.f32 	%f828, %f1774, %f810, %f827;
+	fma.rn.f32 	%f829, %f1779, %f815, %f828;
+	sub.f32 	%f1791, %f1785, %f829;
+	mul.f32 	%f830, %f1777, %f819;
+	fma.rn.f32 	%f831, %f1774, %f817, %f830;
+	fma.rn.f32 	%f832, %f1779, %f823, %f831;
+	sub.f32 	%f1795, %f1786, %f832;
+	mul.f32 	%f833, %f1773, %f804;
+	fma.rn.f32 	%f834, %f1776, %f806, %f833;
+	fma.rn.f32 	%f1788, %f1778, %f808, %f834;
+	mul.f32 	%f835, %f1776, %f813;
+	fma.rn.f32 	%f836, %f1773, %f810, %f835;
+	fma.rn.f32 	%f1792, %f1778, %f815, %f836;
+	mul.f32 	%f837, %f1776, %f819;
+	fma.rn.f32 	%f838, %f1773, %f817, %f837;
+	fma.rn.f32 	%f1796, %f1778, %f823, %f838;
+	mul.f32 	%f839, %f1772, %f804;
+	fma.rn.f32 	%f1789, %f1775, %f806, %f839;
+	mul.f32 	%f840, %f1775, %f813;
+	fma.rn.f32 	%f1793, %f1772, %f810, %f840;
+	mul.f32 	%f841, %f1775, %f819;
+	fma.rn.f32 	%f1797, %f1772, %f817, %f841;
+	mul.f32 	%f1790, %f1771, %f804;
+	mul.f32 	%f1794, %f1771, %f810;
+	mul.f32 	%f1798, %f1771, %f817;
+
+$L__BB10_17:
+	mul.f32 	%f879, %f1792, %f1797;
+	mul.f32 	%f880, %f1793, %f1796;
+	sub.f32 	%f881, %f880, %f879;
+	mul.f32 	%f882, %f1790, %f881;
+	mul.f32 	%f883, %f1792, %f1798;
+	mul.f32 	%f884, %f1794, %f1796;
 	sub.f32 	%f885, %f884, %f883;
-	mul.f32 	%f1764, %f885, %f866;
-	mul.f32 	%f886, %f1743, %f1758;
-	neg.f32 	%f887, %f886;
-	mul.f32 	%f888, %f1747, %f1757;
-	sub.f32 	%f889, %f887, %f888;
-	mul.f32 	%f890, %f1751, %f1756;
-	sub.f32 	%f1755, %f889, %f890;
-	mul.f32 	%f891, %f1743, %f1762;
-	neg.f32 	%f892, %f891;
-	mul.f32 	%f893, %f1747, %f1761;
-	sub.f32 	%f894, %f892, %f893;
-	mul.f32 	%f895, %f1751, %f1760;
-	sub.f32 	%f1759, %f894, %f895;
-	mul.f32 	%f896, %f1743, %f1766;
-	neg.f32 	%f897, %f896;
-	mul.f32 	%f898, %f1747, %f1765;
-	sub.f32 	%f899, %f897, %f898;
-	mul.f32 	%f900, %f1751, %f1764;
-	sub.f32 	%f1763, %f899, %f900;
-	bra.uni 	BB10_18;
-
-BB10_7:
-	setp.ne.s32	%p8, %r36, 1;
-	mov.f32 	%f1756, %f1755;
-	mov.f32 	%f1757, %f1755;
-	mov.f32 	%f1759, %f1755;
-	mov.f32 	%f1760, %f1755;
-	mov.f32 	%f1761, %f1758;
-	mov.f32 	%f1762, %f1755;
-	mov.f32 	%f1763, %f1755;
-	mov.f32 	%f1764, %f1758;
-	mov.f32 	%f1765, %f1755;
-	mov.f32 	%f1766, %f1755;
-	@%p8 bra 	BB10_18;
-
-	// inline asm
-	call (%rd55), _optix_get_static_transform_from_handle, (%rd53);
-	// inline asm
-	add.s64 	%rd663, %rd55, 64;
-
-BB10_10:
-	// inline asm
-	cvta.to.global.u64 %rd59, %rd663;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r38,%r39,%r40,%r41}, [%rd59];
-	// inline asm
-	mov.b32 	 %f1758, %r38;
-	mov.b32 	 %f1757, %r39;
-	mov.b32 	 %f1756, %r40;
-	mov.b32 	 %f1755, %r41;
-	add.s64 	%rd63, %rd663, 16;
-	// inline asm
-	cvta.to.global.u64 %rd62, %rd63;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r42,%r43,%r44,%r45}, [%rd62];
-	// inline asm
-	mov.b32 	 %f1762, %r42;
-	mov.b32 	 %f1761, %r43;
-	mov.b32 	 %f1760, %r44;
-	mov.b32 	 %f1759, %r45;
-	add.s64 	%rd66, %rd663, 32;
-	// inline asm
-	cvta.to.global.u64 %rd65, %rd66;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r46,%r47,%r48,%r49}, [%rd65];
-	// inline asm
-	mov.b32 	 %f1766, %r46;
-	mov.b32 	 %f1765, %r47;
-	mov.b32 	 %f1764, %r48;
-	mov.b32 	 %f1763, %r49;
-
-BB10_18:
-	setp.eq.s32	%p12, %r734, 0;
-	@%p12 bra 	BB10_19;
-	bra.uni 	BB10_20;
-
-BB10_19:
-	mov.f32 	%f1726, %f1755;
-	mov.f32 	%f1725, %f1756;
-	mov.f32 	%f1724, %f1757;
-	mov.f32 	%f1723, %f1758;
-	mov.f32 	%f1722, %f1759;
-	mov.f32 	%f1721, %f1760;
-	mov.f32 	%f1720, %f1761;
-	mov.f32 	%f1719, %f1762;
-	mov.f32 	%f1718, %f1763;
-	mov.f32 	%f1717, %f1764;
-	mov.f32 	%f1716, %f1765;
-	mov.f32 	%f1715, %f1766;
-	bra.uni 	BB10_21;
-
-BB10_20:
-	mul.f32 	%f901, %f1723, %f1758;
-	fma.rn.f32 	%f902, %f1719, %f1757, %f901;
-	fma.rn.f32 	%f151, %f1715, %f1756, %f902;
-	mul.f32 	%f903, %f1724, %f1758;
-	fma.rn.f32 	%f904, %f1720, %f1757, %f903;
-	fma.rn.f32 	%f152, %f1716, %f1756, %f904;
-	mul.f32 	%f905, %f1725, %f1758;
-	fma.rn.f32 	%f906, %f1721, %f1757, %f905;
-	fma.rn.f32 	%f153, %f1717, %f1756, %f906;
-	mul.f32 	%f907, %f1726, %f1758;
-	fma.rn.f32 	%f908, %f1722, %f1757, %f907;
-	fma.rn.f32 	%f909, %f1718, %f1756, %f908;
-	add.f32 	%f154, %f1755, %f909;
-	mul.f32 	%f910, %f1723, %f1762;
-	fma.rn.f32 	%f911, %f1719, %f1761, %f910;
-	fma.rn.f32 	%f155, %f1715, %f1760, %f911;
-	mul.f32 	%f912, %f1724, %f1762;
-	fma.rn.f32 	%f913, %f1720, %f1761, %f912;
-	fma.rn.f32 	%f156, %f1716, %f1760, %f913;
-	mul.f32 	%f914, %f1725, %f1762;
-	fma.rn.f32 	%f915, %f1721, %f1761, %f914;
-	fma.rn.f32 	%f157, %f1717, %f1760, %f915;
-	mul.f32 	%f916, %f1726, %f1762;
-	fma.rn.f32 	%f917, %f1722, %f1761, %f916;
-	fma.rn.f32 	%f918, %f1718, %f1760, %f917;
-	add.f32 	%f158, %f1759, %f918;
-	mul.f32 	%f919, %f1723, %f1766;
-	fma.rn.f32 	%f920, %f1719, %f1765, %f919;
-	fma.rn.f32 	%f1715, %f1715, %f1764, %f920;
-	mul.f32 	%f921, %f1724, %f1766;
-	fma.rn.f32 	%f922, %f1720, %f1765, %f921;
-	fma.rn.f32 	%f1716, %f1716, %f1764, %f922;
-	mul.f32 	%f923, %f1725, %f1766;
-	fma.rn.f32 	%f924, %f1721, %f1765, %f923;
-	fma.rn.f32 	%f1717, %f1717, %f1764, %f924;
-	mul.f32 	%f925, %f1726, %f1766;
-	fma.rn.f32 	%f926, %f1722, %f1765, %f925;
-	fma.rn.f32 	%f927, %f1718, %f1764, %f926;
-	add.f32 	%f1718, %f1763, %f927;
-	mov.f32 	%f1726, %f154;
-	mov.f32 	%f1725, %f153;
-	mov.f32 	%f1724, %f152;
-	mov.f32 	%f1723, %f151;
-	mov.f32 	%f1722, %f158;
-	mov.f32 	%f1721, %f157;
-	mov.f32 	%f1720, %f156;
-	mov.f32 	%f1719, %f155;
-
-BB10_21:
-	add.s32 	%r734, %r734, 1;
-	setp.lt.u32	%p13, %r734, %r33;
-	@%p13 bra 	BB10_5;
-
-	mul.f32 	%f928, %f686, %f1723;
-	fma.rn.f32 	%f929, %f687, %f1724, %f928;
-	fma.rn.f32 	%f930, %f1779, %f1725, %f929;
-	add.f32 	%f1781, %f1726, %f930;
-	mul.f32 	%f931, %f686, %f1719;
-	fma.rn.f32 	%f932, %f687, %f1720, %f931;
-	fma.rn.f32 	%f933, %f1779, %f1721, %f932;
-	add.f32 	%f1780, %f1722, %f933;
-	mul.f32 	%f934, %f686, %f1715;
-	fma.rn.f32 	%f935, %f687, %f1716, %f934;
-	fma.rn.f32 	%f936, %f1779, %f1717, %f935;
-	add.f32 	%f1779, %f1718, %f936;
-	bra.uni 	BB10_23;
-
-BB10_3:
-	mov.f32 	%f1780, %f687;
-	mov.f32 	%f1781, %f686;
-
-BB10_23:
-	// inline asm
-	call (%f937), _optix_get_world_ray_direction_x, ();
-	// inline asm
-	// inline asm
-	call (%f938), _optix_get_world_ray_direction_y, ();
-	// inline asm
-	// inline asm
-	call (%f1830), _optix_get_world_ray_direction_z, ();
-	// inline asm
-	// inline asm
-	call (%f940), _optix_get_ray_time, ();
-	// inline asm
-	mov.u32 	%r735, 0;
-	@%p5 bra 	BB10_24;
-
-BB10_25:
+	mul.f32 	%f886, %f1789, %f885;
+	sub.f32 	%f887, %f882, %f886;
+	mul.f32 	%f888, %f1793, %f1798;
+	mul.f32 	%f889, %f1794, %f1797;
+	sub.f32 	%f890, %f889, %f888;
+	fma.rn.f32 	%f891, %f1788, %f890, %f887;
+	rcp.rn.f32 	%f892, %f891;
+	mul.f32 	%f1802, %f881, %f892;
+	mul.f32 	%f893, %f1789, %f1796;
+	mul.f32 	%f894, %f1788, %f1797;
+	sub.f32 	%f895, %f894, %f893;
+	mul.f32 	%f1801, %f895, %f892;
+	mul.f32 	%f896, %f1788, %f1793;
+	mul.f32 	%f897, %f1789, %f1792;
+	sub.f32 	%f898, %f897, %f896;
+	mul.f32 	%f1800, %f898, %f892;
+	sub.f32 	%f899, %f883, %f884;
+	mul.f32 	%f1806, %f899, %f892;
+	mul.f32 	%f900, %f1788, %f1798;
+	mul.f32 	%f901, %f1790, %f1796;
+	sub.f32 	%f902, %f901, %f900;
+	mul.f32 	%f1805, %f902, %f892;
+	mul.f32 	%f903, %f1790, %f1792;
+	mul.f32 	%f904, %f1788, %f1794;
+	sub.f32 	%f905, %f904, %f903;
+	mul.f32 	%f1804, %f905, %f892;
+	mul.f32 	%f1810, %f890, %f892;
+	mul.f32 	%f906, %f1790, %f1797;
+	mul.f32 	%f907, %f1789, %f1798;
+	sub.f32 	%f908, %f907, %f906;
+	mul.f32 	%f1809, %f908, %f892;
+	mul.f32 	%f909, %f1789, %f1794;
+	mul.f32 	%f910, %f1790, %f1793;
+	sub.f32 	%f911, %f910, %f909;
+	mul.f32 	%f1808, %f911, %f892;
+	mul.f32 	%f912, %f1787, %f1802;
+	neg.f32 	%f913, %f912;
+	mul.f32 	%f914, %f1791, %f1801;
+	sub.f32 	%f915, %f913, %f914;
+	mul.f32 	%f916, %f1795, %f1800;
+	sub.f32 	%f1799, %f915, %f916;
+	mul.f32 	%f917, %f1787, %f1806;
+	neg.f32 	%f918, %f917;
+	mul.f32 	%f919, %f1791, %f1805;
+	sub.f32 	%f920, %f918, %f919;
+	mul.f32 	%f921, %f1795, %f1804;
+	sub.f32 	%f1803, %f920, %f921;
+	mul.f32 	%f922, %f1787, %f1810;
+	neg.f32 	%f923, %f922;
+	mul.f32 	%f924, %f1791, %f1809;
+	sub.f32 	%f925, %f923, %f924;
+	mul.f32 	%f926, %f1795, %f1808;
+	sub.f32 	%f1807, %f925, %f926;
+	bra.uni 	$L__BB10_18;
+
+$L__BB10_9:
+	// begin inline asm
+	call (%rd648), _optix_get_instance_inverse_transform_from_handle, (%rd48);
+	// end inline asm
+
+$L__BB10_10:
+	// begin inline asm
+	cvta.to.global.u64 %rd54, %rd648;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r38,%r39,%r40,%r41}, [%rd54];
+	// end inline asm
+	mov.b32 	%f1802, %r38;
+	mov.b32 	%f1801, %r39;
+	mov.b32 	%f1800, %r40;
+	mov.b32 	%f1799, %r41;
+	add.s64 	%rd58, %rd648, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd57, %rd58;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r42,%r43,%r44,%r45}, [%rd57];
+	// end inline asm
+	mov.b32 	%f1806, %r42;
+	mov.b32 	%f1805, %r43;
+	mov.b32 	%f1804, %r44;
+	mov.b32 	%f1803, %r45;
+	add.s64 	%rd61, %rd648, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd60, %rd61;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r46,%r47,%r48,%r49}, [%rd60];
+	// end inline asm
+	mov.b32 	%f1810, %r46;
+	mov.b32 	%f1809, %r47;
+	mov.b32 	%f1808, %r48;
+	mov.b32 	%f1807, %r49;
+
+$L__BB10_18:
+	setp.eq.s32 	%p10, %r645, 0;
+	@%p10 bra 	$L__BB10_20;
+
+	mul.f32 	%f927, %f1767, %f1802;
+	fma.rn.f32 	%f928, %f1763, %f1801, %f927;
+	fma.rn.f32 	%f151, %f1759, %f1800, %f928;
+	mul.f32 	%f929, %f1768, %f1802;
+	fma.rn.f32 	%f930, %f1764, %f1801, %f929;
+	fma.rn.f32 	%f152, %f1760, %f1800, %f930;
+	mul.f32 	%f931, %f1769, %f1802;
+	fma.rn.f32 	%f932, %f1765, %f1801, %f931;
+	fma.rn.f32 	%f153, %f1761, %f1800, %f932;
+	mul.f32 	%f933, %f1770, %f1802;
+	fma.rn.f32 	%f934, %f1766, %f1801, %f933;
+	fma.rn.f32 	%f935, %f1762, %f1800, %f934;
+	add.f32 	%f1799, %f1799, %f935;
+	mul.f32 	%f936, %f1767, %f1806;
+	fma.rn.f32 	%f937, %f1763, %f1805, %f936;
+	fma.rn.f32 	%f155, %f1759, %f1804, %f937;
+	mul.f32 	%f938, %f1768, %f1806;
+	fma.rn.f32 	%f939, %f1764, %f1805, %f938;
+	fma.rn.f32 	%f156, %f1760, %f1804, %f939;
+	mul.f32 	%f940, %f1769, %f1806;
+	fma.rn.f32 	%f941, %f1765, %f1805, %f940;
+	fma.rn.f32 	%f157, %f1761, %f1804, %f941;
+	mul.f32 	%f942, %f1770, %f1806;
+	fma.rn.f32 	%f943, %f1766, %f1805, %f942;
+	fma.rn.f32 	%f944, %f1762, %f1804, %f943;
+	add.f32 	%f1803, %f1803, %f944;
+	mul.f32 	%f945, %f1767, %f1810;
+	fma.rn.f32 	%f946, %f1763, %f1809, %f945;
+	fma.rn.f32 	%f159, %f1759, %f1808, %f946;
+	mul.f32 	%f947, %f1768, %f1810;
+	fma.rn.f32 	%f948, %f1764, %f1809, %f947;
+	fma.rn.f32 	%f160, %f1760, %f1808, %f948;
+	mul.f32 	%f949, %f1769, %f1810;
+	fma.rn.f32 	%f950, %f1765, %f1809, %f949;
+	fma.rn.f32 	%f161, %f1761, %f1808, %f950;
+	mul.f32 	%f951, %f1770, %f1810;
+	fma.rn.f32 	%f952, %f1766, %f1809, %f951;
+	fma.rn.f32 	%f953, %f1762, %f1808, %f952;
+	add.f32 	%f1807, %f1807, %f953;
+	mov.f32 	%f1800, %f153;
+	mov.f32 	%f1801, %f152;
+	mov.f32 	%f1802, %f151;
+	mov.f32 	%f1804, %f157;
+	mov.f32 	%f1805, %f156;
+	mov.f32 	%f1806, %f155;
+	mov.f32 	%f1808, %f161;
+	mov.f32 	%f1809, %f160;
+	mov.f32 	%f1810, %f159;
+
+$L__BB10_20:
+	add.s32 	%r645, %r645, 1;
+	setp.lt.u32 	%p11, %r645, %r33;
+	mov.f32 	%f1759, %f1810;
+	mov.f32 	%f1760, %f1809;
+	mov.f32 	%f1761, %f1808;
+	mov.f32 	%f1762, %f1807;
+	mov.f32 	%f1763, %f1806;
+	mov.f32 	%f1764, %f1805;
+	mov.f32 	%f1765, %f1804;
+	mov.f32 	%f1766, %f1803;
+	mov.f32 	%f1767, %f1802;
+	mov.f32 	%f1768, %f1801;
+	mov.f32 	%f1769, %f1800;
+	mov.f32 	%f1770, %f1799;
+	@%p11 bra 	$L__BB10_5;
+
+$L__BB10_21:
+	mul.f32 	%f954, %f1835, %f1802;
+	fma.rn.f32 	%f955, %f1836, %f1801, %f954;
+	fma.rn.f32 	%f956, %f1837, %f1800, %f955;
+	mul.f32 	%f957, %f1835, %f1806;
+	fma.rn.f32 	%f958, %f1836, %f1805, %f957;
+	fma.rn.f32 	%f959, %f1837, %f1804, %f958;
+	mul.f32 	%f960, %f1835, %f1810;
+	fma.rn.f32 	%f961, %f1836, %f1809, %f960;
+	fma.rn.f32 	%f962, %f1837, %f1808, %f961;
+	add.f32 	%f1837, %f1807, %f962;
+	add.f32 	%f1836, %f1803, %f959;
+	add.f32 	%f1835, %f1799, %f956;
+
+$L__BB10_23:
+	// begin inline asm
+	call (%f1893), _optix_get_world_ray_direction_x, ();
+	// end inline asm
+	// begin inline asm
+	call (%f1894), _optix_get_world_ray_direction_y, ();
+	// end inline asm
+	// begin inline asm
+	call (%f965), _optix_get_world_ray_direction_z, ();
+	// end inline asm
+	// begin inline asm
+	call (%r184), _optix_get_transform_list_size, ();
+	// end inline asm
+	setp.eq.s32 	%p12, %r184, 0;
+	@%p12 bra 	$L__BB10_43;
+
+	// begin inline asm
+	call (%r185), _optix_get_transform_list_size, ();
+	// end inline asm
+	// begin inline asm
+	call (%f966), _optix_get_ray_time, ();
+	// end inline asm
+	setp.eq.s32 	%p13, %r185, 0;
+	@%p13 bra 	$L__BB10_42;
+
+	mov.u32 	%r646, 0;
+
+$L__BB10_26:
 	.pragma "nounroll";
-	// inline asm
-	call (%rd174), _optix_get_transform_list_handle, (%r735);
-	// inline asm
-	// inline asm
-	call (%r186), _optix_get_transform_type_from_handle, (%rd174);
-	// inline asm
-	and.b32  	%r187, %r186, -2;
-	setp.eq.s32	%p15, %r187, 2;
-	@%p15 bra 	BB10_31;
-	bra.uni 	BB10_26;
-
-BB10_31:
-	setp.eq.s32	%p18, %r186, 2;
-	@%p18 bra 	BB10_35;
-	bra.uni 	BB10_32;
-
-BB10_35:
-	// inline asm
-	call (%rd248), _optix_get_matrix_motion_transform_from_handle, (%rd174);
-	// inline asm
-	// inline asm
-	cvta.to.global.u64 %rd250, %rd248;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r275,%r276,%r277,%r278}, [%rd250];
-	// inline asm
-	mov.b32	{%rs9, %rs10}, %r277;
-	add.s64 	%rd254, %rd248, 16;
-	// inline asm
+	// begin inline asm
+	call (%rd167), _optix_get_transform_list_handle, (%r646);
+	// end inline asm
+	// begin inline asm
+	call (%r188), _optix_get_transform_type_from_handle, (%rd167);
+	// end inline asm
+	or.b32  	%r189, %r188, 1;
+	setp.eq.s32 	%p14, %r189, 3;
+	@%p14 bra 	$L__BB10_32;
+	bra.uni 	$L__BB10_27;
+
+$L__BB10_32:
+	setp.eq.s32 	%p17, %r188, 2;
+	@%p17 bra 	$L__BB10_36;
+	bra.uni 	$L__BB10_33;
+
+$L__BB10_36:
+	// begin inline asm
+	call (%rd239), _optix_get_matrix_motion_transform_from_handle, (%rd167);
+	// end inline asm
+	// begin inline asm
+	cvta.to.global.u64 %rd241, %rd239;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r277,%r278,%r279,%r280}, [%rd241];
+	// end inline asm
+	add.s64 	%rd245, %rd239, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd244, %rd245;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r281,%r282,%r283,%r284}, [%rd244];
+	// end inline asm
+	add.s64 	%rd248, %rd239, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd247, %rd248;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r285,%r286,%r287,%r288}, [%rd247];
+	// end inline asm
+	add.s64 	%rd251, %rd239, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd250, %rd251;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r289,%r290,%r291,%r292}, [%rd250];
+	// end inline asm
+	add.s64 	%rd254, %rd239, 64;
+	// begin inline asm
 	cvta.to.global.u64 %rd253, %rd254;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r279,%r280,%r281,%r282}, [%rd253];
-	// inline asm
-	add.s64 	%rd257, %rd248, 32;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r293,%r294,%r295,%r296}, [%rd253];
+	// end inline asm
+	add.s64 	%rd257, %rd239, 80;
+	// begin inline asm
 	cvta.to.global.u64 %rd256, %rd257;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r283,%r284,%r285,%r286}, [%rd256];
-	// inline asm
-	add.s64 	%rd260, %rd248, 48;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r297,%r298,%r299,%r300}, [%rd256];
+	// end inline asm
+	add.s64 	%rd260, %rd239, 96;
+	// begin inline asm
 	cvta.to.global.u64 %rd259, %rd260;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r287,%r288,%r289,%r290}, [%rd259];
-	// inline asm
-	add.s64 	%rd263, %rd248, 64;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r301,%r302,%r303,%r304}, [%rd259];
+	// end inline asm
+	add.s64 	%rd263, %rd239, 112;
+	// begin inline asm
 	cvta.to.global.u64 %rd262, %rd263;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r291,%r292,%r293,%r294}, [%rd262];
-	// inline asm
-	add.s64 	%rd266, %rd248, 80;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r305,%r306,%r307,%r308}, [%rd262];
+	// end inline asm
+	mov.b32 	%f1070, %r280;
+	mov.b32 	%f1071, %r281;
+	and.b32  	%r321, %r279, 65535;
+	add.s32 	%r322, %r321, -1;
+	cvt.rn.f32.s32 	%f1072, %r322;
+	sub.f32 	%f1073, %f966, %f1070;
+	mul.f32 	%f1074, %f1073, %f1072;
+	sub.f32 	%f1075, %f1071, %f1070;
+	div.rn.f32 	%f1076, %f1074, %f1075;
+	min.f32 	%f1077, %f1072, %f1076;
+	mov.f32 	%f1078, 0f00000000;
+	max.f32 	%f1079, %f1078, %f1077;
+	cvt.rmi.f32.f32 	%f1080, %f1079;
+	sub.f32 	%f258, %f1079, %f1080;
+	cvt.rzi.s32.f32 	%r323, %f1080;
+	cvt.s64.s32 	%rd17, %r323;
+	mul.wide.s32 	%rd274, %r323, 48;
+	add.s64 	%rd266, %rd248, %rd274;
+	// begin inline asm
 	cvta.to.global.u64 %rd265, %rd266;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r295,%r296,%r297,%r298}, [%rd265];
-	// inline asm
-	add.s64 	%rd269, %rd248, 96;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r309,%r310,%r311,%r312}, [%rd265];
+	// end inline asm
+	mov.b32 	%f1863, %r309;
+	mov.b32 	%f1864, %r310;
+	mov.b32 	%f1865, %r311;
+	add.s64 	%rd269, %rd266, 16;
+	// begin inline asm
 	cvta.to.global.u64 %rd268, %rd269;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r299,%r300,%r301,%r302}, [%rd268];
-	// inline asm
-	add.s64 	%rd272, %rd248, 112;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r313,%r314,%r315,%r316}, [%rd268];
+	// end inline asm
+	mov.b32 	%f1860, %r313;
+	mov.b32 	%f1861, %r314;
+	mov.b32 	%f1862, %r315;
+	add.s64 	%rd272, %rd266, 32;
+	// begin inline asm
 	cvta.to.global.u64 %rd271, %rd272;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r303,%r304,%r305,%r306}, [%rd271];
-	// inline asm
-	mov.b32 	 %f1043, %r278;
-	mov.b32 	 %f1044, %r279;
-	cvt.u32.u16	%r319, %rs9;
-	add.s32 	%r320, %r319, -1;
-	cvt.rn.f32.s32	%f1045, %r320;
-	sub.f32 	%f1046, %f940, %f1043;
-	mul.f32 	%f1047, %f1046, %f1045;
-	sub.f32 	%f1048, %f1044, %f1043;
-	div.rn.f32 	%f1049, %f1047, %f1048;
-	min.f32 	%f1050, %f1045, %f1049;
-	mov.f32 	%f1051, 0f00000000;
-	max.f32 	%f1052, %f1051, %f1050;
-	cvt.rmi.f32.f32	%f1053, %f1052;
-	cvt.rzi.s32.f32	%r321, %f1053;
-	cvt.s64.s32	%rd19, %r321;
-	mul.wide.s32 	%rd283, %r321, 48;
-	add.s64 	%rd275, %rd257, %rd283;
-	// inline asm
-	cvta.to.global.u64 %rd274, %rd275;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r307,%r308,%r309,%r310}, [%rd274];
-	// inline asm
-	mov.b32 	 %f1807, %r307;
-	mov.b32 	 %f1808, %r308;
-	mov.b32 	 %f1809, %r309;
-	add.s64 	%rd278, %rd275, 16;
-	// inline asm
-	cvta.to.global.u64 %rd277, %rd278;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r311,%r312,%r313,%r314}, [%rd277];
-	// inline asm
-	mov.b32 	 %f1804, %r311;
-	mov.b32 	 %f1805, %r312;
-	mov.b32 	 %f1806, %r313;
-	add.s64 	%rd281, %rd275, 32;
-	// inline asm
-	cvta.to.global.u64 %rd280, %rd281;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r315,%r316,%r317,%r318}, [%rd280];
-	// inline asm
-	sub.f32 	%f249, %f1052, %f1053;
-	mov.b32 	 %f1801, %r315;
-	mov.b32 	 %f1802, %r316;
-	mov.b32 	 %f1803, %r317;
-	setp.leu.f32	%p20, %f249, 0f00000000;
-	@%p20 bra 	BB10_37;
-
-	mul.lo.s64 	%rd293, %rd19, 48;
-	add.s64 	%rd294, %rd248, %rd293;
-	add.s64 	%rd285, %rd294, 80;
-	// inline asm
-	cvta.to.global.u64 %rd284, %rd285;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r322,%r323,%r324,%r325}, [%rd284];
-	// inline asm
-	mov.b32 	 %f1054, %r322;
-	mov.b32 	 %f1055, %r323;
-	mov.b32 	 %f1056, %r324;
-	add.s64 	%rd288, %rd294, 96;
-	// inline asm
-	cvta.to.global.u64 %rd287, %rd288;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r326,%r327,%r328,%r329}, [%rd287];
-	// inline asm
-	mov.b32 	 %f1057, %r326;
-	mov.b32 	 %f1058, %r327;
-	mov.b32 	 %f1059, %r328;
-	add.s64 	%rd291, %rd294, 112;
-	// inline asm
-	cvta.to.global.u64 %rd290, %rd291;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r330,%r331,%r332,%r333}, [%rd290];
-	// inline asm
-	mov.f32 	%f1060, 0f3F800000;
-	sub.f32 	%f1061, %f1060, %f249;
-	mul.f32 	%f1062, %f249, %f1054;
-	mul.f32 	%f1063, %f249, %f1055;
-	mul.f32 	%f1064, %f249, %f1056;
-	fma.rn.f32 	%f1807, %f1061, %f1807, %f1062;
-	fma.rn.f32 	%f1808, %f1061, %f1808, %f1063;
-	fma.rn.f32 	%f1809, %f1061, %f1809, %f1064;
-	mul.f32 	%f1065, %f249, %f1057;
-	mul.f32 	%f1066, %f249, %f1058;
-	mul.f32 	%f1067, %f249, %f1059;
-	fma.rn.f32 	%f1804, %f1061, %f1804, %f1065;
-	fma.rn.f32 	%f1805, %f1061, %f1805, %f1066;
-	fma.rn.f32 	%f1806, %f1061, %f1806, %f1067;
-	mov.b32 	 %f1068, %r330;
-	mov.b32 	 %f1069, %r331;
-	mov.b32 	 %f1070, %r332;
-	mul.f32 	%f1071, %f249, %f1068;
-	mul.f32 	%f1072, %f249, %f1069;
-	mul.f32 	%f1073, %f249, %f1070;
-	fma.rn.f32 	%f1801, %f1061, %f1801, %f1071;
-	fma.rn.f32 	%f1802, %f1061, %f1802, %f1072;
-	fma.rn.f32 	%f1803, %f1061, %f1803, %f1073;
-	bra.uni 	BB10_37;
-
-BB10_26:
-	mov.f32 	%f1810, 0f00000000;
-	mov.f32 	%f1812, 0f3F800000;
-	setp.eq.s32	%p16, %r186, 4;
-	@%p16 bra 	BB10_29;
-	bra.uni 	BB10_27;
-
-BB10_29:
-	// inline asm
-	call (%rd664), _optix_get_instance_inverse_transform_from_handle, (%rd174);
-	// inline asm
-	bra.uni 	BB10_30;
-
-BB10_32:
-	// inline asm
-	call (%rd189), _optix_get_srt_motion_transform_from_handle, (%rd174);
-	// inline asm
-	// inline asm
-	cvta.to.global.u64 %rd191, %rd189;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r200,%r201,%r202,%r203}, [%rd191];
-	// inline asm
-	mov.b32	{%rs7, %rs8}, %r202;
-	add.s64 	%rd195, %rd189, 16;
-	// inline asm
-	cvta.to.global.u64 %rd194, %rd195;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r204,%r205,%r206,%r207}, [%rd194];
-	// inline asm
-	add.s64 	%rd198, %rd189, 32;
-	// inline asm
-	cvta.to.global.u64 %rd197, %rd198;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r208,%r209,%r210,%r211}, [%rd197];
-	// inline asm
-	add.s64 	%rd201, %rd189, 48;
-	// inline asm
-	cvta.to.global.u64 %rd200, %rd201;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r212,%r213,%r214,%r215}, [%rd200];
-	// inline asm
-	add.s64 	%rd204, %rd189, 64;
-	// inline asm
-	cvta.to.global.u64 %rd203, %rd204;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r216,%r217,%r218,%r219}, [%rd203];
-	// inline asm
-	add.s64 	%rd207, %rd189, 80;
-	// inline asm
-	cvta.to.global.u64 %rd206, %rd207;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r220,%r221,%r222,%r223}, [%rd206];
-	// inline asm
-	add.s64 	%rd210, %rd189, 96;
-	// inline asm
-	cvta.to.global.u64 %rd209, %rd210;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r224,%r225,%r226,%r227}, [%rd209];
-	// inline asm
-	add.s64 	%rd213, %rd189, 112;
-	// inline asm
-	cvta.to.global.u64 %rd212, %rd213;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r228,%r229,%r230,%r231}, [%rd212];
-	// inline asm
-	add.s64 	%rd216, %rd189, 128;
-	// inline asm
-	cvta.to.global.u64 %rd215, %rd216;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r232,%r233,%r234,%r235}, [%rd215];
-	// inline asm
-	add.s64 	%rd219, %rd189, 144;
-	// inline asm
-	cvta.to.global.u64 %rd218, %rd219;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r236,%r237,%r238,%r239}, [%rd218];
-	// inline asm
-	mov.b32 	 %f951, %r203;
-	mov.b32 	 %f952, %r204;
-	cvt.u32.u16	%r256, %rs7;
-	add.s32 	%r257, %r256, -1;
-	cvt.rn.f32.s32	%f953, %r257;
-	sub.f32 	%f954, %f940, %f951;
-	mul.f32 	%f955, %f954, %f953;
-	sub.f32 	%f956, %f952, %f951;
-	div.rn.f32 	%f957, %f955, %f956;
-	min.f32 	%f958, %f953, %f957;
-	mov.f32 	%f959, 0f00000000;
-	max.f32 	%f960, %f959, %f958;
-	cvt.rmi.f32.f32	%f961, %f960;
-	cvt.rzi.s32.f32	%r258, %f961;
-	cvt.s64.s32	%rd17, %r258;
-	mul.wide.s32 	%rd233, %r258, 64;
-	add.s64 	%rd222, %rd198, %rd233;
-	// inline asm
-	cvta.to.global.u64 %rd221, %rd222;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r240,%r241,%r242,%r243}, [%rd221];
-	// inline asm
-	mov.b32 	 %f1791, %r240;
-	mov.b32 	 %f1792, %r241;
-	mov.b32 	 %f1793, %r242;
-	add.s64 	%rd225, %rd222, 16;
-	// inline asm
-	cvta.to.global.u64 %rd224, %rd225;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r244,%r245,%r246,%r247}, [%rd224];
-	// inline asm
-	mov.b32 	 %f1794, %r244;
-	mov.b32 	 %f1795, %r245;
-	mov.b32 	 %f1796, %r247;
-	add.s64 	%rd228, %rd222, 32;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r317,%r318,%r319,%r320}, [%rd271];
+	// end inline asm
+	mov.b32 	%f1857, %r317;
+	mov.b32 	%f1858, %r318;
+	mov.b32 	%f1859, %r319;
+	setp.leu.f32 	%p19, %f258, 0f00000000;
+	@%p19 bra 	$L__BB10_38;
+
+	mov.f32 	%f1081, 0f3F800000;
+	sub.f32 	%f1082, %f1081, %f258;
+	mul.lo.s64 	%rd284, %rd17, 48;
+	add.s64 	%rd285, %rd239, %rd284;
+	add.s64 	%rd276, %rd285, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd275, %rd276;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r324,%r325,%r326,%r327}, [%rd275];
+	// end inline asm
+	mov.b32 	%f1083, %r324;
+	mov.b32 	%f1084, %r325;
+	mov.b32 	%f1085, %r326;
+	mul.f32 	%f1086, %f258, %f1083;
+	mul.f32 	%f1087, %f258, %f1084;
+	mul.f32 	%f1088, %f258, %f1085;
+	fma.rn.f32 	%f1863, %f1082, %f1863, %f1086;
+	fma.rn.f32 	%f1864, %f1082, %f1864, %f1087;
+	fma.rn.f32 	%f1865, %f1082, %f1865, %f1088;
+	add.s64 	%rd279, %rd285, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd278, %rd279;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r328,%r329,%r330,%r331}, [%rd278];
+	// end inline asm
+	mov.b32 	%f1089, %r328;
+	mov.b32 	%f1090, %r329;
+	mov.b32 	%f1091, %r330;
+	mul.f32 	%f1092, %f258, %f1089;
+	mul.f32 	%f1093, %f258, %f1090;
+	mul.f32 	%f1094, %f258, %f1091;
+	fma.rn.f32 	%f1860, %f1082, %f1860, %f1092;
+	fma.rn.f32 	%f1861, %f1082, %f1861, %f1093;
+	fma.rn.f32 	%f1862, %f1082, %f1862, %f1094;
+	add.s64 	%rd282, %rd285, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd281, %rd282;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r332,%r333,%r334,%r335}, [%rd281];
+	// end inline asm
+	mov.b32 	%f1095, %r332;
+	mov.b32 	%f1096, %r333;
+	mov.b32 	%f1097, %r334;
+	mul.f32 	%f1098, %f258, %f1095;
+	mul.f32 	%f1099, %f258, %f1096;
+	mul.f32 	%f1100, %f258, %f1097;
+	fma.rn.f32 	%f1857, %f1082, %f1857, %f1098;
+	fma.rn.f32 	%f1858, %f1082, %f1858, %f1099;
+	fma.rn.f32 	%f1859, %f1082, %f1859, %f1100;
+	bra.uni 	$L__BB10_38;
+
+$L__BB10_27:
+	mov.f32 	%f1866, 0f00000000;
+	mov.f32 	%f1868, 0f3F800000;
+	setp.eq.s32 	%p15, %r188, 4;
+	@%p15 bra 	$L__BB10_30;
+
+	setp.ne.s32 	%p16, %r188, 1;
+	mov.f32 	%f1867, %f1866;
+	mov.f32 	%f1869, %f1866;
+	mov.f32 	%f1870, %f1868;
+	mov.f32 	%f1871, %f1866;
+	mov.f32 	%f1872, %f1868;
+	mov.f32 	%f1873, %f1866;
+	mov.f32 	%f1874, %f1866;
+	@%p16 bra 	$L__BB10_39;
+
+	// begin inline asm
+	call (%rd169), _optix_get_static_transform_from_handle, (%rd167);
+	// end inline asm
+	add.s64 	%rd649, %rd169, 64;
+	bra.uni 	$L__BB10_31;
+
+$L__BB10_33:
+	// begin inline asm
+	call (%rd182), _optix_get_srt_motion_transform_from_handle, (%rd167);
+	// end inline asm
+	// begin inline asm
+	cvta.to.global.u64 %rd184, %rd182;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r202,%r203,%r204,%r205}, [%rd184];
+	// end inline asm
+	add.s64 	%rd188, %rd182, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd187, %rd188;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r206,%r207,%r208,%r209}, [%rd187];
+	// end inline asm
+	add.s64 	%rd191, %rd182, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd190, %rd191;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r210,%r211,%r212,%r213}, [%rd190];
+	// end inline asm
+	add.s64 	%rd194, %rd182, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd193, %rd194;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r214,%r215,%r216,%r217}, [%rd193];
+	// end inline asm
+	add.s64 	%rd197, %rd182, 64;
+	// begin inline asm
+	cvta.to.global.u64 %rd196, %rd197;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r218,%r219,%r220,%r221}, [%rd196];
+	// end inline asm
+	add.s64 	%rd200, %rd182, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd199, %rd200;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r222,%r223,%r224,%r225}, [%rd199];
+	// end inline asm
+	add.s64 	%rd203, %rd182, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd202, %rd203;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r226,%r227,%r228,%r229}, [%rd202];
+	// end inline asm
+	add.s64 	%rd206, %rd182, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd205, %rd206;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r230,%r231,%r232,%r233}, [%rd205];
+	// end inline asm
+	add.s64 	%rd209, %rd182, 128;
+	// begin inline asm
+	cvta.to.global.u64 %rd208, %rd209;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r234,%r235,%r236,%r237}, [%rd208];
+	// end inline asm
+	add.s64 	%rd212, %rd182, 144;
+	// begin inline asm
+	cvta.to.global.u64 %rd211, %rd212;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r238,%r239,%r240,%r241}, [%rd211];
+	// end inline asm
+	mov.b32 	%f978, %r205;
+	mov.b32 	%f979, %r206;
+	and.b32  	%r258, %r204, 65535;
+	add.s32 	%r259, %r258, -1;
+	cvt.rn.f32.s32 	%f980, %r259;
+	sub.f32 	%f981, %f966, %f978;
+	mul.f32 	%f982, %f981, %f980;
+	sub.f32 	%f983, %f979, %f978;
+	div.rn.f32 	%f984, %f982, %f983;
+	min.f32 	%f985, %f980, %f984;
+	mov.f32 	%f986, 0f00000000;
+	max.f32 	%f987, %f986, %f985;
+	cvt.rmi.f32.f32 	%f988, %f987;
+	sub.f32 	%f218, %f987, %f988;
+	cvt.rzi.s32.f32 	%r260, %f988;
+	mul.wide.s32 	%rd226, %r260, 64;
+	add.s64 	%rd215, %rd191, %rd226;
+	// begin inline asm
+	cvta.to.global.u64 %rd214, %rd215;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r242,%r243,%r244,%r245}, [%rd214];
+	// end inline asm
+	mov.b32 	%f1847, %r242;
+	mov.b32 	%f1848, %r243;
+	mov.b32 	%f1849, %r244;
+	add.s64 	%rd218, %rd215, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd217, %rd218;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r246,%r247,%r248,%r249}, [%rd217];
+	// end inline asm
+	mov.b32 	%f1850, %r246;
+	mov.b32 	%f1851, %r247;
+	mov.b32 	%f1852, %r249;
+	add.s64 	%rd221, %rd215, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd220, %rd221;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r250,%r251,%r252,%r253}, [%rd220];
+	// end inline asm
+	mov.b32 	%f1853, %r251;
+	mov.b32 	%f1854, %r252;
+	mov.b32 	%f1855, %r253;
+	add.s64 	%rd224, %rd215, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd223, %rd224;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r254,%r255,%r256,%r257}, [%rd223];
+	// end inline asm
+	mov.b32 	%f1856, %r254;
+	setp.leu.f32 	%p18, %f218, 0f00000000;
+	@%p18 bra 	$L__BB10_35;
+
+	mov.f32 	%f989, 0f3F800000;
+	sub.f32 	%f990, %f989, %f218;
+	add.s64 	%rd228, %rd215, 64;
+	// begin inline asm
 	cvta.to.global.u64 %rd227, %rd228;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r248,%r249,%r250,%r251}, [%rd227];
-	// inline asm
-	sub.f32 	%f209, %f960, %f961;
-	mov.b32 	 %f1797, %r249;
-	mov.b32 	 %f1798, %r250;
-	mov.b32 	 %f1799, %r251;
-	add.s64 	%rd231, %rd222, 48;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r261,%r262,%r263,%r264}, [%rd227];
+	// end inline asm
+	mov.b32 	%f991, %r261;
+	mov.b32 	%f992, %r262;
+	mov.b32 	%f993, %r263;
+	mul.f32 	%f994, %f218, %f991;
+	mul.f32 	%f995, %f218, %f992;
+	mul.f32 	%f996, %f218, %f993;
+	fma.rn.f32 	%f1847, %f990, %f1847, %f994;
+	fma.rn.f32 	%f1848, %f990, %f1848, %f995;
+	fma.rn.f32 	%f1849, %f990, %f1849, %f996;
+	add.s64 	%rd231, %rd215, 80;
+	// begin inline asm
 	cvta.to.global.u64 %rd230, %rd231;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r252,%r253,%r254,%r255}, [%rd230];
-	// inline asm
-	mov.b32 	 %f1800, %r252;
-	setp.leu.f32	%p19, %f209, 0f00000000;
-	@%p19 bra 	BB10_34;
-
-	shl.b64 	%rd246, %rd17, 6;
-	add.s64 	%rd247, %rd246, %rd189;
-	add.s64 	%rd235, %rd247, 96;
-	// inline asm
-	cvta.to.global.u64 %rd234, %rd235;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r259,%r260,%r261,%r262}, [%rd234];
-	// inline asm
-	mov.b32 	 %f962, %r259;
-	mov.b32 	 %f963, %r260;
-	mov.b32 	 %f964, %r261;
-	add.s64 	%rd238, %rd247, 112;
-	// inline asm
-	cvta.to.global.u64 %rd237, %rd238;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r263,%r264,%r265,%r266}, [%rd237];
-	// inline asm
-	mov.b32 	 %f965, %r263;
-	mov.b32 	 %f966, %r264;
-	mov.b32 	 %f967, %r266;
-	add.s64 	%rd241, %rd247, 128;
-	// inline asm
-	cvta.to.global.u64 %rd240, %rd241;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r267,%r268,%r269,%r270}, [%rd240];
-	// inline asm
-	mov.b32 	 %f968, %r268;
-	mov.b32 	 %f969, %r269;
-	mov.b32 	 %f970, %r270;
-	add.s64 	%rd244, %rd247, 144;
-	// inline asm
-	cvta.to.global.u64 %rd243, %rd244;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r271,%r272,%r273,%r274}, [%rd243];
-	// inline asm
-	mov.f32 	%f971, 0f3F800000;
-	sub.f32 	%f972, %f971, %f209;
-	mul.f32 	%f973, %f209, %f962;
-	mul.f32 	%f974, %f209, %f963;
-	mul.f32 	%f975, %f209, %f964;
-	fma.rn.f32 	%f1791, %f972, %f1791, %f973;
-	fma.rn.f32 	%f1792, %f972, %f1792, %f974;
-	fma.rn.f32 	%f1793, %f972, %f1793, %f975;
-	mul.f32 	%f976, %f209, %f965;
-	mul.f32 	%f977, %f209, %f966;
-	mul.f32 	%f978, %f209, %f967;
-	fma.rn.f32 	%f1794, %f972, %f1794, %f976;
-	fma.rn.f32 	%f1795, %f972, %f1795, %f977;
-	fma.rn.f32 	%f1796, %f972, %f1796, %f978;
-	mul.f32 	%f979, %f209, %f968;
-	mul.f32 	%f980, %f209, %f969;
-	mul.f32 	%f981, %f209, %f970;
-	fma.rn.f32 	%f982, %f972, %f1797, %f979;
-	fma.rn.f32 	%f983, %f972, %f1798, %f980;
-	fma.rn.f32 	%f984, %f972, %f1799, %f981;
-	mov.b32 	 %f985, %r271;
-	mul.f32 	%f986, %f209, %f985;
-	fma.rn.f32 	%f987, %f972, %f1800, %f986;
-	mul.f32 	%f988, %f983, %f983;
-	fma.rn.f32 	%f989, %f982, %f982, %f988;
-	fma.rn.f32 	%f990, %f984, %f984, %f989;
-	fma.rn.f32 	%f991, %f987, %f987, %f990;
-	sqrt.rn.f32 	%f992, %f991;
-	rcp.rn.f32 	%f993, %f992;
-	mul.f32 	%f1797, %f982, %f993;
-	mul.f32 	%f1798, %f983, %f993;
-	mul.f32 	%f1799, %f984, %f993;
-	mul.f32 	%f1800, %f987, %f993;
-
-BB10_34:
-	mul.f32 	%f994, %f1798, %f1798;
-	fma.rn.f32 	%f995, %f1797, %f1797, %f994;
-	fma.rn.f32 	%f996, %f1799, %f1799, %f995;
-	fma.rn.f32 	%f997, %f1800, %f1800, %f996;
-	rcp.rn.f32 	%f998, %f997;
-	mul.f32 	%f999, %f1797, %f998;
-	mul.f32 	%f1000, %f1798, %f998;
-	mul.f32 	%f1001, %f1799, %f998;
-	mul.f32 	%f1002, %f1800, %f998;
-	mul.f32 	%f1003, %f1797, %f999;
-	mul.f32 	%f1004, %f1798, %f1000;
-	mul.f32 	%f1005, %f1799, %f1001;
-	mul.f32 	%f1006, %f1797, %f1000;
-	mul.f32 	%f1007, %f1799, %f1002;
-	mul.f32 	%f1008, %f1797, %f1001;
-	mul.f32 	%f1009, %f1798, %f1002;
-	mul.f32 	%f1010, %f1798, %f1001;
-	mul.f32 	%f1011, %f1797, %f1002;
-	sub.f32 	%f1012, %f1003, %f1004;
-	sub.f32 	%f1013, %f1012, %f1005;
-	fma.rn.f32 	%f1014, %f1800, %f1002, %f1013;
-	sub.f32 	%f1015, %f1006, %f1007;
-	add.f32 	%f1016, %f1015, %f1015;
-	add.f32 	%f1017, %f1008, %f1009;
-	add.f32 	%f1018, %f1017, %f1017;
-	add.f32 	%f1019, %f1006, %f1007;
-	add.f32 	%f1020, %f1019, %f1019;
-	sub.f32 	%f1021, %f1004, %f1003;
-	sub.f32 	%f1022, %f1021, %f1005;
-	fma.rn.f32 	%f1023, %f1800, %f1002, %f1022;
-	sub.f32 	%f1024, %f1010, %f1011;
-	add.f32 	%f1025, %f1024, %f1024;
-	sub.f32 	%f1026, %f1008, %f1009;
-	add.f32 	%f1027, %f1026, %f1026;
-	add.f32 	%f1028, %f1010, %f1011;
-	add.f32 	%f1029, %f1028, %f1028;
-	neg.f32 	%f1030, %f1003;
-	sub.f32 	%f1031, %f1030, %f1004;
-	add.f32 	%f1032, %f1005, %f1031;
-	fma.rn.f32 	%f1033, %f1800, %f1002, %f1032;
-	mul.f32 	%f1034, %f1793, %f1014;
-	fma.rn.f32 	%f1035, %f1795, %f1016, %f1034;
-	fma.rn.f32 	%f1809, %f1796, %f1018, %f1035;
-	mul.f32 	%f1036, %f1795, %f1023;
-	fma.rn.f32 	%f1037, %f1793, %f1020, %f1036;
-	fma.rn.f32 	%f1806, %f1796, %f1025, %f1037;
-	mul.f32 	%f1038, %f1795, %f1029;
-	fma.rn.f32 	%f1039, %f1793, %f1027, %f1038;
-	fma.rn.f32 	%f1803, %f1796, %f1033, %f1039;
-	mul.f32 	%f1040, %f1792, %f1014;
-	fma.rn.f32 	%f1808, %f1794, %f1016, %f1040;
-	mul.f32 	%f1041, %f1794, %f1023;
-	fma.rn.f32 	%f1805, %f1792, %f1020, %f1041;
-	mul.f32 	%f1042, %f1794, %f1029;
-	fma.rn.f32 	%f1802, %f1792, %f1027, %f1042;
-	mul.f32 	%f1807, %f1791, %f1014;
-	mul.f32 	%f1804, %f1791, %f1020;
-	mul.f32 	%f1801, %f1791, %f1027;
-
-BB10_37:
-	mul.f32 	%f1074, %f1802, %f1806;
-	mul.f32 	%f1075, %f1803, %f1805;
-	sub.f32 	%f1076, %f1075, %f1074;
-	mul.f32 	%f1077, %f1807, %f1076;
-	mul.f32 	%f1078, %f1801, %f1806;
-	mul.f32 	%f1079, %f1803, %f1804;
-	sub.f32 	%f1080, %f1079, %f1078;
-	mul.f32 	%f1081, %f1080, %f1808;
-	sub.f32 	%f1082, %f1077, %f1081;
-	mul.f32 	%f1083, %f1801, %f1805;
-	mul.f32 	%f1084, %f1802, %f1804;
-	sub.f32 	%f1085, %f1084, %f1083;
-	fma.rn.f32 	%f1086, %f1085, %f1809, %f1082;
-	rcp.rn.f32 	%f1087, %f1086;
-	mul.f32 	%f1816, %f1076, %f1087;
-	mul.f32 	%f1088, %f1803, %f1808;
-	mul.f32 	%f1089, %f1802, %f1809;
-	sub.f32 	%f1090, %f1089, %f1088;
-	mul.f32 	%f1817, %f1087, %f1090;
-	mul.f32 	%f1091, %f1805, %f1809;
-	mul.f32 	%f1092, %f1806, %f1808;
-	sub.f32 	%f1093, %f1092, %f1091;
-	mul.f32 	%f1818, %f1087, %f1093;
-	sub.f32 	%f1094, %f1078, %f1079;
-	mul.f32 	%f1813, %f1094, %f1087;
-	mul.f32 	%f1095, %f1801, %f1809;
-	mul.f32 	%f1096, %f1803, %f1807;
-	sub.f32 	%f1097, %f1096, %f1095;
-	mul.f32 	%f1814, %f1087, %f1097;
-	mul.f32 	%f1098, %f1806, %f1807;
-	mul.f32 	%f1099, %f1804, %f1809;
-	sub.f32 	%f1100, %f1099, %f1098;
-	mul.f32 	%f1815, %f1087, %f1100;
-	mul.f32 	%f1810, %f1085, %f1087;
-	mul.f32 	%f1101, %f1802, %f1807;
-	mul.f32 	%f1102, %f1801, %f1808;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r265,%r266,%r267,%r268}, [%rd230];
+	// end inline asm
+	mov.b32 	%f997, %r265;
+	mov.b32 	%f998, %r266;
+	mov.b32 	%f999, %r268;
+	mul.f32 	%f1000, %f218, %f997;
+	mul.f32 	%f1001, %f218, %f998;
+	mul.f32 	%f1002, %f218, %f999;
+	fma.rn.f32 	%f1850, %f990, %f1850, %f1000;
+	fma.rn.f32 	%f1851, %f990, %f1851, %f1001;
+	fma.rn.f32 	%f1852, %f990, %f1852, %f1002;
+	add.s64 	%rd234, %rd215, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd233, %rd234;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r269,%r270,%r271,%r272}, [%rd233];
+	// end inline asm
+	mov.b32 	%f1003, %r270;
+	mov.b32 	%f1004, %r271;
+	mov.b32 	%f1005, %r272;
+	mul.f32 	%f1006, %f218, %f1003;
+	mul.f32 	%f1007, %f218, %f1004;
+	mul.f32 	%f1008, %f218, %f1005;
+	fma.rn.f32 	%f1009, %f990, %f1853, %f1006;
+	fma.rn.f32 	%f1010, %f990, %f1854, %f1007;
+	fma.rn.f32 	%f1011, %f990, %f1855, %f1008;
+	add.s64 	%rd237, %rd215, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd236, %rd237;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r273,%r274,%r275,%r276}, [%rd236];
+	// end inline asm
+	mov.b32 	%f1012, %r273;
+	mul.f32 	%f1013, %f218, %f1012;
+	fma.rn.f32 	%f1014, %f990, %f1856, %f1013;
+	mul.f32 	%f1015, %f1010, %f1010;
+	fma.rn.f32 	%f1016, %f1009, %f1009, %f1015;
+	fma.rn.f32 	%f1017, %f1011, %f1011, %f1016;
+	fma.rn.f32 	%f1018, %f1014, %f1014, %f1017;
+	sqrt.rn.f32 	%f1019, %f1018;
+	rcp.rn.f32 	%f1020, %f1019;
+	mul.f32 	%f1853, %f1009, %f1020;
+	mul.f32 	%f1854, %f1010, %f1020;
+	mul.f32 	%f1855, %f1011, %f1020;
+	mul.f32 	%f1856, %f1020, %f1014;
+
+$L__BB10_35:
+	mul.f32 	%f1021, %f1854, %f1854;
+	fma.rn.f32 	%f1022, %f1853, %f1853, %f1021;
+	fma.rn.f32 	%f1023, %f1855, %f1855, %f1022;
+	fma.rn.f32 	%f1024, %f1856, %f1856, %f1023;
+	rcp.rn.f32 	%f1025, %f1024;
+	mul.f32 	%f1026, %f1853, %f1025;
+	mul.f32 	%f1027, %f1854, %f1025;
+	mul.f32 	%f1028, %f1855, %f1025;
+	mul.f32 	%f1029, %f1856, %f1025;
+	mul.f32 	%f1030, %f1853, %f1026;
+	mul.f32 	%f1031, %f1854, %f1027;
+	mul.f32 	%f1032, %f1855, %f1028;
+	mul.f32 	%f1033, %f1853, %f1027;
+	mul.f32 	%f1034, %f1855, %f1029;
+	mul.f32 	%f1035, %f1853, %f1028;
+	mul.f32 	%f1036, %f1854, %f1029;
+	mul.f32 	%f1037, %f1854, %f1028;
+	mul.f32 	%f1038, %f1853, %f1029;
+	sub.f32 	%f1039, %f1030, %f1031;
+	sub.f32 	%f1040, %f1039, %f1032;
+	fma.rn.f32 	%f1041, %f1856, %f1029, %f1040;
+	sub.f32 	%f1042, %f1033, %f1034;
+	add.f32 	%f1043, %f1042, %f1042;
+	add.f32 	%f1044, %f1035, %f1036;
+	add.f32 	%f1045, %f1044, %f1044;
+	add.f32 	%f1046, %f1033, %f1034;
+	add.f32 	%f1047, %f1046, %f1046;
+	sub.f32 	%f1048, %f1031, %f1030;
+	sub.f32 	%f1049, %f1048, %f1032;
+	fma.rn.f32 	%f1050, %f1856, %f1029, %f1049;
+	sub.f32 	%f1051, %f1037, %f1038;
+	add.f32 	%f1052, %f1051, %f1051;
+	sub.f32 	%f1053, %f1035, %f1036;
+	add.f32 	%f1054, %f1053, %f1053;
+	add.f32 	%f1055, %f1037, %f1038;
+	add.f32 	%f1056, %f1055, %f1055;
+	neg.f32 	%f1057, %f1030;
+	sub.f32 	%f1058, %f1057, %f1031;
+	add.f32 	%f1059, %f1032, %f1058;
+	fma.rn.f32 	%f1060, %f1856, %f1029, %f1059;
+	mul.f32 	%f1061, %f1849, %f1041;
+	fma.rn.f32 	%f1062, %f1851, %f1043, %f1061;
+	fma.rn.f32 	%f1865, %f1852, %f1045, %f1062;
+	mul.f32 	%f1063, %f1851, %f1050;
+	fma.rn.f32 	%f1064, %f1849, %f1047, %f1063;
+	fma.rn.f32 	%f1862, %f1852, %f1052, %f1064;
+	mul.f32 	%f1065, %f1851, %f1056;
+	fma.rn.f32 	%f1066, %f1849, %f1054, %f1065;
+	fma.rn.f32 	%f1859, %f1852, %f1060, %f1066;
+	mul.f32 	%f1067, %f1848, %f1041;
+	fma.rn.f32 	%f1864, %f1850, %f1043, %f1067;
+	mul.f32 	%f1068, %f1850, %f1050;
+	fma.rn.f32 	%f1861, %f1848, %f1047, %f1068;
+	mul.f32 	%f1069, %f1850, %f1056;
+	fma.rn.f32 	%f1858, %f1848, %f1054, %f1069;
+	mul.f32 	%f1863, %f1847, %f1041;
+	mul.f32 	%f1860, %f1847, %f1047;
+	mul.f32 	%f1857, %f1847, %f1054;
+
+$L__BB10_38:
+	mul.f32 	%f1101, %f1858, %f1862;
+	mul.f32 	%f1102, %f1859, %f1861;
 	sub.f32 	%f1103, %f1102, %f1101;
-	mul.f32 	%f1811, %f1103, %f1087;
-	mul.f32 	%f1104, %f1804, %f1808;
-	mul.f32 	%f1105, %f1805, %f1807;
-	sub.f32 	%f1106, %f1105, %f1104;
-	mul.f32 	%f1812, %f1106, %f1087;
-	bra.uni 	BB10_38;
-
-BB10_27:
-	setp.ne.s32	%p17, %r186, 1;
-	mov.f32 	%f1811, %f1810;
-	mov.f32 	%f1813, %f1810;
-	mov.f32 	%f1814, %f1812;
-	mov.f32 	%f1815, %f1810;
-	mov.f32 	%f1816, %f1812;
-	mov.f32 	%f1817, %f1810;
-	mov.f32 	%f1818, %f1810;
-	@%p17 bra 	BB10_38;
-
-	// inline asm
-	call (%rd176), _optix_get_static_transform_from_handle, (%rd174);
-	// inline asm
-	add.s64 	%rd664, %rd176, 64;
-
-BB10_30:
-	// inline asm
-	cvta.to.global.u64 %rd180, %rd664;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r188,%r189,%r190,%r191}, [%rd180];
-	// inline asm
-	mov.b32 	 %f1816, %r188;
-	mov.b32 	 %f1817, %r189;
-	mov.b32 	 %f1818, %r190;
-	add.s64 	%rd184, %rd664, 16;
-	// inline asm
-	cvta.to.global.u64 %rd183, %rd184;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r192,%r193,%r194,%r195}, [%rd183];
-	// inline asm
-	mov.b32 	 %f1813, %r192;
-	mov.b32 	 %f1814, %r193;
-	mov.b32 	 %f1815, %r194;
-	add.s64 	%rd187, %rd664, 32;
-	// inline asm
-	cvta.to.global.u64 %rd186, %rd187;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r196,%r197,%r198,%r199}, [%rd186];
-	// inline asm
-	mov.b32 	 %f1810, %r196;
-	mov.b32 	 %f1811, %r197;
-	mov.b32 	 %f1812, %r198;
-
-BB10_38:
-	setp.eq.s32	%p21, %r735, 0;
-	@%p21 bra 	BB10_39;
-	bra.uni 	BB10_40;
-
-BB10_39:
-	mov.f32 	%f1790, %f1810;
-	mov.f32 	%f1789, %f1811;
-	mov.f32 	%f1788, %f1812;
-	mov.f32 	%f1787, %f1813;
-	mov.f32 	%f1786, %f1814;
-	mov.f32 	%f1785, %f1815;
-	mov.f32 	%f1784, %f1816;
-	mov.f32 	%f1783, %f1817;
-	mov.f32 	%f1782, %f1818;
-	bra.uni 	BB10_41;
-
-BB10_40:
-	mul.f32 	%f1107, %f1787, %f1817;
-	fma.rn.f32 	%f1108, %f1784, %f1816, %f1107;
-	fma.rn.f32 	%f289, %f1790, %f1818, %f1108;
-	mul.f32 	%f1109, %f1786, %f1817;
-	fma.rn.f32 	%f1110, %f1783, %f1816, %f1109;
-	fma.rn.f32 	%f290, %f1789, %f1818, %f1110;
-	mul.f32 	%f1111, %f1785, %f1817;
-	fma.rn.f32 	%f1112, %f1782, %f1816, %f1111;
-	fma.rn.f32 	%f291, %f1788, %f1818, %f1112;
-	mul.f32 	%f1113, %f1787, %f1814;
-	fma.rn.f32 	%f1114, %f1784, %f1813, %f1113;
-	fma.rn.f32 	%f292, %f1790, %f1815, %f1114;
-	mul.f32 	%f1115, %f1786, %f1814;
-	fma.rn.f32 	%f1116, %f1783, %f1813, %f1115;
-	fma.rn.f32 	%f293, %f1789, %f1815, %f1116;
-	mul.f32 	%f1117, %f1785, %f1814;
-	fma.rn.f32 	%f1118, %f1782, %f1813, %f1117;
-	fma.rn.f32 	%f294, %f1788, %f1815, %f1118;
-	mul.f32 	%f1119, %f1787, %f1811;
-	fma.rn.f32 	%f1120, %f1784, %f1810, %f1119;
-	fma.rn.f32 	%f1790, %f1790, %f1812, %f1120;
-	mul.f32 	%f1121, %f1786, %f1811;
-	fma.rn.f32 	%f1122, %f1783, %f1810, %f1121;
-	fma.rn.f32 	%f1789, %f1789, %f1812, %f1122;
-	mul.f32 	%f1123, %f1785, %f1811;
-	fma.rn.f32 	%f1124, %f1782, %f1810, %f1123;
-	fma.rn.f32 	%f1788, %f1788, %f1812, %f1124;
-	mov.f32 	%f1787, %f292;
-	mov.f32 	%f1786, %f293;
-	mov.f32 	%f1785, %f294;
-	mov.f32 	%f1784, %f289;
-	mov.f32 	%f1783, %f290;
-	mov.f32 	%f1782, %f291;
-
-BB10_41:
-	add.s32 	%r735, %r735, 1;
-	setp.lt.u32	%p22, %r735, %r33;
-	@%p22 bra 	BB10_25;
-
-	mul.f32 	%f1125, %f938, %f1783;
-	fma.rn.f32 	%f1126, %f937, %f1784, %f1125;
-	fma.rn.f32 	%f1828, %f1830, %f1782, %f1126;
-	mul.f32 	%f1127, %f938, %f1786;
-	fma.rn.f32 	%f1128, %f937, %f1787, %f1127;
-	fma.rn.f32 	%f1829, %f1830, %f1785, %f1128;
-	mul.f32 	%f1129, %f938, %f1789;
-	fma.rn.f32 	%f1130, %f937, %f1790, %f1129;
-	fma.rn.f32 	%f1830, %f1830, %f1788, %f1130;
-	bra.uni 	BB10_43;
-
-BB10_24:
-	mov.f32 	%f1828, %f937;
-	mov.f32 	%f1829, %f938;
-
-BB10_43:
-	// inline asm
-	call (%f1132), _optix_get_ray_tmax, ();
-	// inline asm
-	ld.const.u64 	%rd295, [params+80];
-	setp.eq.s64	%p23, %rd295, 0;
-	@%p23 bra 	BB10_48;
-
-	ld.u64 	%rd296, [%rd52];
-	ld.const.u64 	%rd297, [params+328];
-	cvta.to.global.u64 	%rd298, %rd297;
-	cvt.u64.u32	%rd20, %r1;
-	mul.wide.u32 	%rd299, %r1, 8;
-	add.s64 	%rd300, %rd298, %rd299;
-	st.global.u64 	[%rd300], %rd296;
-	ld.const.u64 	%rd301, [params+336];
-	cvta.to.global.u64 	%rd302, %rd301;
-	mul.wide.u32 	%rd303, %r1, 4;
-	add.s64 	%rd304, %rd302, %rd303;
-	mov.u32 	%r334, 0;
-	st.global.u32 	[%rd304], %r334;
-	ld.const.u64 	%rd305, [params+344];
-	cvta.to.global.u64 	%rd306, %rd305;
-	add.s64 	%rd21, %rd306, %rd303;
-	ld.global.u32 	%r9, [%rd21];
-	setp.eq.s32	%p24, %r9, 0;
-	@%p24 bra 	BB10_47;
-
-	// inline asm
-	call (%r335), _optix_read_instance_id, ();
-	// inline asm
-	setp.ge.u32	%p25, %r335, %r9;
-	@%p25 bra 	BB10_47;
-
-	st.global.u32 	[%rd21], %r335;
-
-BB10_47:
-	ld.const.u64 	%rd307, [params+72];
-	cvta.to.global.u64 	%rd308, %rd307;
-	shl.b64 	%rd309, %rd20, 2;
-	add.s64 	%rd310, %rd308, %rd309;
-	st.global.f32 	[%rd310], %f1132;
-	bra.uni 	BB10_178;
-
-BB10_48:
-	fma.rn.f32 	%f1991, %f1132, %f1828, %f1781;
-	fma.rn.f32 	%f1993, %f1132, %f1830, %f1779;
-	ld.v2.f32 	{%f1133, %f1134}, [%rd3+288];
-	sub.f32 	%f316, %f1991, %f1133;
-	fma.rn.f32 	%f1992, %f1132, %f1829, %f1780;
-	sub.f32 	%f318, %f1992, %f1134;
-	ld.u8 	%rs1, [%rd3+324];
-	ld.f32 	%f319, [%rd3+312];
-	ld.f32 	%f320, [%rd3+308];
-	cvt.f64.f32	%fd1, %f316;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r11}, %fd1;
-	}
-	mov.f64 	%fd48, 0d4000000000000000;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r12}, %fd48;
-	}
-	bfe.u32 	%r336, %r12, 20, 11;
-	add.s32 	%r337, %r336, -1012;
-	mov.u64 	%rd311, 4611686018427387904;
-	shl.b64 	%rd23, %rd311, %r337;
-	setp.ne.s64	%p26, %rd23, -9223372036854775808;
-	setp.eq.s64	%p27, %rd23, -9223372036854775808;
-	abs.f64 	%fd2, %fd1;
-	// Callseq Start 22
-	{
-	.reg .b32 temp_param_reg;
-	// <end>}
-	.param .b64 param0;
-	st.param.f64	[param0+0], %fd2;
-	.param .b64 retval0;
-	call.uni (retval0), 
-	__internal_accurate_pow, 
-	(
-	param0
-	);
-	ld.param.f64	%fd78, [retval0+0];
-	
-	//{
-	}// Callseq End 22
-	setp.gt.s32	%p28, %r11, -1;
-	setp.lt.s32	%p29, %r11, 0;
-	and.pred  	%p1, %p29, %p27;
-	or.pred  	%p30, %p28, %p26;
-	@%p30 bra 	BB10_50;
-
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r338}, %fd78;
-	}
-	xor.b32  	%r339, %r338, -2147483648;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r340, %temp}, %fd78;
-	}
-	mov.b64 	%fd78, {%r340, %r339};
-
-BB10_50:
-	setp.eq.f32	%p31, %f316, 0f00000000;
-	@%p31 bra 	BB10_53;
-	bra.uni 	BB10_51;
-
-BB10_53:
-	selp.b32	%r341, %r11, 0, %p27;
-	mov.u32 	%r342, 0;
-	or.b32  	%r343, %r341, 2146435072;
-	setp.lt.s32	%p35, %r12, 0;
-	selp.b32	%r344, %r343, %r341, %p35;
-	mov.b64 	%fd78, {%r342, %r344};
-	bra.uni 	BB10_54;
-
-BB10_51:
-	@%p28 bra 	BB10_54;
-
-	cvt.rzi.f64.f64	%fd50, %fd48;
-	setp.neu.f64	%p33, %fd50, 0d4000000000000000;
-	selp.f64	%fd78, 0dFFF8000000000000, %fd78, %p33;
-
-BB10_54:
-	add.f64 	%fd79, %fd1, 0d4000000000000000;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r345}, %fd79;
-	}
-	and.b32  	%r346, %r345, 2146435072;
-	setp.ne.s32	%p36, %r346, 2146435072;
-	@%p36 bra 	BB10_55;
-
-	setp.gtu.f64	%p37, %fd2, 0d7FF0000000000000;
-	@%p37 bra 	BB10_64;
-
-	and.b32  	%r347, %r12, 2147483647;
-	setp.ne.s32	%p38, %r347, 2146435072;
-	@%p38 bra 	BB10_59;
-
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r348, %temp}, %fd48;
-	}
-	setp.eq.s32	%p39, %r348, 0;
-	@%p39 bra 	BB10_63;
-
-BB10_59:
-	and.b32  	%r349, %r11, 2147483647;
-	setp.ne.s32	%p40, %r349, 2146435072;
-	@%p40 bra 	BB10_60;
-
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r350, %temp}, %fd1;
-	}
-	setp.ne.s32	%p41, %r350, 0;
-	mov.f64 	%fd79, %fd78;
-	@%p41 bra 	BB10_64;
-
-	shr.s32 	%r351, %r12, 31;
-	and.b32  	%r352, %r351, -2146435072;
-	add.s32 	%r353, %r352, 2146435072;
-	or.b32  	%r354, %r353, -2147483648;
-	selp.b32	%r355, %r354, %r353, %p1;
-	mov.u32 	%r356, 0;
-	mov.b64 	%fd79, {%r356, %r355};
-	bra.uni 	BB10_64;
-
-BB10_55:
-	mov.f64 	%fd79, %fd78;
-	bra.uni 	BB10_64;
-
-BB10_60:
-	mov.f64 	%fd79, %fd78;
-	bra.uni 	BB10_64;
-
-BB10_63:
-	setp.gt.f64	%p42, %fd2, 0d3FF0000000000000;
-	selp.b32	%r357, 2146435072, 0, %p42;
-	mov.u32 	%r358, 0;
-	xor.b32  	%r359, %r357, 2146435072;
-	setp.lt.s32	%p43, %r12, 0;
-	selp.b32	%r360, %r359, %r357, %p43;
-	setp.eq.f32	%p44, %f316, 0fBF800000;
-	selp.b32	%r361, 1072693248, %r360, %p44;
-	mov.b64 	%fd79, {%r358, %r361};
-
-BB10_64:
-	cvt.f64.f32	%fd13, %f318;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r13}, %fd13;
-	}
-	abs.f64 	%fd14, %fd13;
-	// Callseq Start 23
-	{
-	.reg .b32 temp_param_reg;
-	// <end>}
-	.param .b64 param0;
-	st.param.f64	[param0+0], %fd14;
-	.param .b64 retval0;
-	call.uni (retval0), 
-	__internal_accurate_pow, 
-	(
-	param0
-	);
-	ld.param.f64	%fd81, [retval0+0];
-	
-	//{
-	}// Callseq End 23
-	setp.gt.s32	%p45, %r13, -1;
-	setp.lt.s32	%p46, %r13, 0;
-	and.pred  	%p2, %p46, %p27;
-	or.pred  	%p49, %p45, %p26;
-	@%p49 bra 	BB10_66;
-
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r362}, %fd81;
-	}
-	xor.b32  	%r363, %r362, -2147483648;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r364, %temp}, %fd81;
-	}
-	mov.b64 	%fd81, {%r364, %r363};
-
-BB10_66:
-	setp.eq.f32	%p50, %f318, 0f00000000;
-	setp.eq.f32	%p51, %f316, 0f3F800000;
-	selp.f64	%fd18, 0d3FF0000000000000, %fd79, %p51;
-	@%p50 bra 	BB10_69;
-	bra.uni 	BB10_67;
-
-BB10_69:
-	selp.b32	%r365, %r13, 0, %p27;
-	mov.u32 	%r366, 0;
-	or.b32  	%r367, %r365, 2146435072;
-	setp.lt.s32	%p55, %r12, 0;
-	selp.b32	%r368, %r367, %r365, %p55;
-	mov.b64 	%fd81, {%r366, %r368};
-	bra.uni 	BB10_70;
-
-BB10_67:
-	@%p45 bra 	BB10_70;
-
-	cvt.rzi.f64.f64	%fd53, %fd48;
-	setp.neu.f64	%p53, %fd53, 0d4000000000000000;
-	selp.f64	%fd81, 0dFFF8000000000000, %fd81, %p53;
-
-BB10_70:
-	add.f64 	%fd82, %fd13, 0d4000000000000000;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r369}, %fd82;
-	}
-	and.b32  	%r370, %r369, 2146435072;
-	setp.ne.s32	%p56, %r370, 2146435072;
-	@%p56 bra 	BB10_71;
-
-	setp.gtu.f64	%p57, %fd14, 0d7FF0000000000000;
-	@%p57 bra 	BB10_80;
-
-	and.b32  	%r371, %r12, 2147483647;
-	setp.ne.s32	%p58, %r371, 2146435072;
-	@%p58 bra 	BB10_75;
-
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r372, %temp}, %fd48;
-	}
-	setp.eq.s32	%p59, %r372, 0;
-	@%p59 bra 	BB10_79;
-
-BB10_75:
-	and.b32  	%r373, %r13, 2147483647;
-	setp.ne.s32	%p60, %r373, 2146435072;
-	@%p60 bra 	BB10_76;
-
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r374, %temp}, %fd13;
-	}
-	setp.ne.s32	%p61, %r374, 0;
-	mov.f64 	%fd82, %fd81;
-	@%p61 bra 	BB10_80;
-
-	shr.s32 	%r375, %r12, 31;
-	and.b32  	%r376, %r375, -2146435072;
-	add.s32 	%r377, %r376, 2146435072;
-	or.b32  	%r378, %r377, -2147483648;
-	selp.b32	%r379, %r378, %r377, %p2;
-	mov.u32 	%r380, 0;
-	mov.b64 	%fd82, {%r380, %r379};
-	bra.uni 	BB10_80;
-
-BB10_71:
-	mov.f64 	%fd82, %fd81;
-	bra.uni 	BB10_80;
-
-BB10_76:
-	mov.f64 	%fd82, %fd81;
-	bra.uni 	BB10_80;
-
-BB10_79:
-	setp.gt.f64	%p62, %fd14, 0d3FF0000000000000;
-	selp.b32	%r381, 2146435072, 0, %p62;
-	mov.u32 	%r382, 0;
-	xor.b32  	%r383, %r381, 2146435072;
-	setp.lt.s32	%p63, %r12, 0;
-	selp.b32	%r384, %r383, %r381, %p63;
-	setp.eq.f32	%p64, %f318, 0fBF800000;
-	selp.b32	%r385, 1072693248, %r384, %p64;
-	mov.b64 	%fd82, {%r382, %r385};
-
-BB10_80:
-	setp.eq.f32	%p65, %f318, 0f3F800000;
-	selp.f64	%fd55, 0d3FF0000000000000, %fd82, %p65;
-	add.f64 	%fd56, %fd18, %fd55;
-	add.f32 	%f1137, %f320, 0f3F800000;
-	cvt.f64.f32	%fd57, %f1137;
-	mul.f64 	%fd26, %fd57, %fd56;
-	cvt.f64.f32	%fd27, %f319;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r14}, %fd27;
-	}
-	abs.f64 	%fd28, %fd27;
-	// Callseq Start 24
-	{
-	.reg .b32 temp_param_reg;
-	// <end>}
-	.param .b64 param0;
-	st.param.f64	[param0+0], %fd28;
-	.param .b64 retval0;
-	call.uni (retval0), 
-	__internal_accurate_pow, 
-	(
-	param0
-	);
-	ld.param.f64	%fd87, [retval0+0];
-	
-	//{
-	}// Callseq End 24
-	setp.gt.s32	%p66, %r14, -1;
-	setp.lt.s32	%p67, %r14, 0;
-	and.pred  	%p3, %p67, %p27;
-	or.pred  	%p70, %p66, %p26;
-	mov.f64 	%fd84, %fd87;
-	@%p70 bra 	BB10_82;
-
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r386}, %fd87;
-	}
-	xor.b32  	%r387, %r386, -2147483648;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r388, %temp}, %fd87;
-	}
-	mov.b64 	%fd84, {%r388, %r387};
-
-BB10_82:
-	setp.eq.f32	%p71, %f319, 0f00000000;
-	@%p71 bra 	BB10_85;
-	bra.uni 	BB10_83;
-
-BB10_85:
-	selp.b32	%r389, %r14, 0, %p27;
-	mov.u32 	%r390, 0;
-	or.b32  	%r391, %r389, 2146435072;
-	setp.lt.s32	%p75, %r12, 0;
-	selp.b32	%r392, %r391, %r389, %p75;
-	mov.b64 	%fd84, {%r390, %r392};
-	bra.uni 	BB10_86;
-
-BB10_83:
-	@%p66 bra 	BB10_86;
-
-	cvt.rzi.f64.f64	%fd59, %fd48;
-	setp.neu.f64	%p73, %fd59, 0d4000000000000000;
-	selp.f64	%fd84, 0dFFF8000000000000, %fd84, %p73;
-
-BB10_86:
-	add.f64 	%fd88, %fd27, 0d4000000000000000;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r393}, %fd88;
-	}
-	and.b32  	%r15, %r393, 2146435072;
-	setp.ne.s32	%p76, %r15, 2146435072;
-	@%p76 bra 	BB10_87;
-
-	setp.gtu.f64	%p77, %fd28, 0d7FF0000000000000;
-	mov.f64 	%fd85, %fd88;
-	@%p77 bra 	BB10_96;
-
-	and.b32  	%r394, %r12, 2147483647;
-	setp.ne.s32	%p78, %r394, 2146435072;
-	@%p78 bra 	BB10_91;
-
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r395, %temp}, %fd48;
-	}
-	setp.eq.s32	%p79, %r395, 0;
-	@%p79 bra 	BB10_95;
-
-BB10_91:
-	and.b32  	%r396, %r14, 2147483647;
-	setp.ne.s32	%p80, %r396, 2146435072;
-	@%p80 bra 	BB10_92;
-
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r397, %temp}, %fd27;
-	}
-	setp.ne.s32	%p81, %r397, 0;
-	mov.f64 	%fd85, %fd84;
-	@%p81 bra 	BB10_96;
-
-	shr.s32 	%r398, %r12, 31;
-	and.b32  	%r399, %r398, -2146435072;
-	add.s32 	%r400, %r399, 2146435072;
-	or.b32  	%r401, %r400, -2147483648;
-	selp.b32	%r402, %r401, %r400, %p3;
-	mov.u32 	%r403, 0;
-	mov.b64 	%fd85, {%r403, %r402};
-	bra.uni 	BB10_96;
-
-BB10_87:
-	mov.f64 	%fd85, %fd84;
-	bra.uni 	BB10_96;
-
-BB10_92:
-	mov.f64 	%fd85, %fd84;
-	bra.uni 	BB10_96;
-
-BB10_95:
-	setp.gt.f64	%p82, %fd28, 0d3FF0000000000000;
-	selp.b32	%r404, 2146435072, 0, %p82;
-	mov.u32 	%r405, 0;
-	xor.b32  	%r406, %r404, 2146435072;
-	setp.lt.s32	%p83, %r12, 0;
-	selp.b32	%r407, %r406, %r404, %p83;
-	setp.eq.f32	%p84, %f319, 0fBF800000;
-	selp.b32	%r408, 1072693248, %r407, %p84;
-	mov.b64 	%fd85, {%r405, %r408};
-
-BB10_96:
-	setp.eq.f32	%p85, %f319, 0f3F800000;
-	selp.f64	%fd61, 0d3FF0000000000000, %fd85, %p85;
-	mul.f64 	%fd62, %fd26, %fd61;
-	mov.f64 	%fd63, 0d3FF0000000000000;
-	sub.f64 	%fd64, %fd63, %fd62;
-	sqrt.rn.f64 	%fd65, %fd64;
-	mul.f32 	%f1138, %f316, %f319;
-	cvt.f64.f32	%fd66, %f1138;
-	div.rn.f64 	%fd39, %fd66, %fd65;
-	@!%p3 bra 	BB10_98;
-	bra.uni 	BB10_97;
-
-BB10_97:
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r409}, %fd87;
-	}
-	xor.b32  	%r410, %r409, -2147483648;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r411, %temp}, %fd87;
-	}
-	mov.b64 	%fd87, {%r411, %r410};
-
-BB10_98:
-	@%p71 bra 	BB10_101;
-	bra.uni 	BB10_99;
-
-BB10_101:
-	selp.b32	%r412, %r14, 0, %p27;
-	mov.u32 	%r413, 0;
-	or.b32  	%r414, %r412, 2146435072;
-	setp.lt.s32	%p90, %r12, 0;
-	selp.b32	%r415, %r414, %r412, %p90;
-	mov.b64 	%fd87, {%r413, %r415};
-	bra.uni 	BB10_102;
-
-BB10_99:
-	@%p66 bra 	BB10_102;
-
-	cvt.rzi.f64.f64	%fd68, %fd48;
-	setp.neu.f64	%p88, %fd68, 0d4000000000000000;
-	selp.f64	%fd87, 0dFFF8000000000000, %fd87, %p88;
-
-BB10_102:
-	@%p76 bra 	BB10_103;
-
-	setp.gtu.f64	%p92, %fd28, 0d7FF0000000000000;
-	@%p92 bra 	BB10_112;
-
-	and.b32  	%r416, %r12, 2147483647;
-	setp.ne.s32	%p93, %r416, 2146435072;
-	@%p93 bra 	BB10_107;
-
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r417, %temp}, %fd48;
-	}
-	setp.eq.s32	%p94, %r417, 0;
-	@%p94 bra 	BB10_111;
-
-BB10_107:
-	and.b32  	%r418, %r14, 2147483647;
-	setp.ne.s32	%p95, %r418, 2146435072;
-	@%p95 bra 	BB10_108;
-
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r419, %temp}, %fd27;
-	}
-	setp.ne.s32	%p96, %r419, 0;
-	mov.f64 	%fd88, %fd87;
-	@%p96 bra 	BB10_112;
-
-	shr.s32 	%r420, %r12, 31;
-	and.b32  	%r421, %r420, -2146435072;
-	add.s32 	%r422, %r421, 2146435072;
-	or.b32  	%r423, %r422, -2147483648;
-	selp.b32	%r424, %r423, %r422, %p3;
-	mov.u32 	%r425, 0;
-	mov.b64 	%fd88, {%r425, %r424};
-	bra.uni 	BB10_112;
-
-BB10_103:
-	mov.f64 	%fd88, %fd87;
-	bra.uni 	BB10_112;
-
-BB10_108:
-	mov.f64 	%fd88, %fd87;
-	bra.uni 	BB10_112;
-
-BB10_111:
-	setp.gt.f64	%p97, %fd28, 0d3FF0000000000000;
-	selp.b32	%r426, 2146435072, 0, %p97;
-	mov.u32 	%r427, 0;
-	xor.b32  	%r428, %r426, 2146435072;
-	setp.lt.s32	%p98, %r12, 0;
-	selp.b32	%r429, %r428, %r426, %p98;
-	setp.eq.f32	%p99, %f319, 0fBF800000;
-	selp.b32	%r430, 1072693248, %r429, %p99;
-	mov.b64 	%fd88, {%r427, %r430};
-
-BB10_112:
-	selp.f64	%fd70, 0d3FF0000000000000, %fd88, %p85;
-	mul.f64 	%fd71, %fd26, %fd70;
-	sub.f64 	%fd73, %fd63, %fd71;
-	sqrt.rn.f64 	%fd74, %fd73;
-	mul.f32 	%f1139, %f318, %f319;
-	cvt.f64.f32	%fd75, %f1139;
-	div.rn.f64 	%fd76, %fd75, %fd74;
-	setp.eq.s16	%p101, %rs1, 0;
-	selp.f32	%f321, 0fBF800000, 0f3F800000, %p101;
-	cvt.rn.f32.f64	%f1973, %fd76;
-	cvt.rn.f32.f64	%f1976, %fd39;
-	ld.u8 	%rs12, [%rd3+332];
-	setp.eq.s16	%p102, %rs12, 0;
-	@%p102 bra 	BB10_114;
-
-	neg.f32 	%f1140, %f1976;
-	mul.f32 	%f1141, %f1976, %f1976;
-	neg.f32 	%f1142, %f1973;
-	fma.rn.f32 	%f1143, %f1142, %f1142, %f1141;
-	neg.f32 	%f1144, %f321;
-	fma.rn.f32 	%f1145, %f1144, %f1144, %f1143;
-	sqrt.rn.f32 	%f1146, %f1145;
-	div.rn.f32 	%f1988, %f1140, %f1146;
-	div.rn.f32 	%f1989, %f1142, %f1146;
-	div.rn.f32 	%f1990, %f1144, %f1146;
-	bra.uni 	BB10_115;
-
-BB10_114:
-	mul.f32 	%f1147, %f1976, %f1976;
-	fma.rn.f32 	%f1148, %f1973, %f1973, %f1147;
-	fma.rn.f32 	%f1149, %f321, %f321, %f1148;
-	sqrt.rn.f32 	%f1150, %f1149;
-	div.rn.f32 	%f1988, %f1976, %f1150;
-	div.rn.f32 	%f1989, %f1973, %f1150;
-	div.rn.f32 	%f1990, %f321, %f1150;
-
-BB10_115:
-	ld.const.u64 	%rd24, [params+96];
-	setp.eq.s64	%p103, %rd24, 0;
-	@%p103 bra 	BB10_117;
-
-	ld.v4.f32 	{%f1152, %f1153, %f1154, %f1155}, [%rd3+208];
-	ld.v2.f32 	{%f1158, %f1159}, [%rd3+160];
-	fma.rn.f32 	%f1161, %f1991, %f1158, %f1152;
-	fma.rn.f32 	%f1163, %f1991, %f1159, %f1153;
-	ld.v2.f32 	{%f1164, %f1165}, [%rd3+176];
-	fma.rn.f32 	%f1167, %f1992, %f1164, %f1161;
-	fma.rn.f32 	%f1169, %f1992, %f1165, %f1163;
-	ld.v2.f32 	{%f1170, %f1171}, [%rd3+192];
-	fma.rn.f32 	%f1173, %f1993, %f1170, %f1167;
-	fma.rn.f32 	%f1175, %f1993, %f1171, %f1169;
-	ld.f32 	%f1176, [%rd3+316];
-	div.rn.f32 	%f1834, %f1173, %f1176;
-	div.rn.f32 	%f1835, %f1175, %f1176;
-
-BB10_117:
-	ld.u64 	%rd25, [%rd52];
-	ld.const.u64 	%rd312, [params+344];
-	cvta.to.global.u64 	%rd313, %rd312;
-	cvt.u64.u32	%rd26, %r1;
-	mul.wide.u32 	%rd314, %r1, 4;
-	add.s64 	%rd27, %rd313, %rd314;
-	ld.global.u32 	%r16, [%rd27];
-	setp.eq.s32	%p104, %r16, 0;
-	mov.f32 	%f1974, 0f00000000;
-	mov.f32 	%f1975, 0f3F800000;
-	@%p104 bra 	BB10_118;
-
-	// inline asm
-	call (%r431), _optix_read_instance_id, ();
-	// inline asm
-	setp.ge.u32	%p105, %r431, %r16;
-	@%p105 bra 	BB10_118;
-
-	mov.f32 	%f1901, 0f00000000;
-	mov.f32 	%f1902, 0f3F800000;
-	mov.f32 	%f1839, %f1902;
-	mov.f32 	%f1838, %f1901;
-	mov.f32 	%f1837, %f1901;
-	mov.f32 	%f1836, %f1901;
-	mov.f32 	%f1843, %f1901;
-	mov.f32 	%f1842, %f1902;
-	mov.f32 	%f1841, %f1901;
-	mov.f32 	%f1840, %f1901;
-	mov.f32 	%f1847, %f1901;
-	mov.f32 	%f1846, %f1901;
-	mov.f32 	%f1845, %f1902;
-	mov.f32 	%f1844, %f1901;
-	@%p5 bra 	BB10_138;
-
-	add.s32 	%r736, %r33, -1;
-	setp.lt.s32	%p107, %r736, 0;
-	@%p107 bra 	BB10_138;
-
-BB10_122:
+	mul.f32 	%f1104, %f1863, %f1103;
+	mul.f32 	%f1105, %f1857, %f1862;
+	mul.f32 	%f1106, %f1859, %f1860;
+	sub.f32 	%f1107, %f1106, %f1105;
+	mul.f32 	%f1108, %f1107, %f1864;
+	sub.f32 	%f1109, %f1104, %f1108;
+	mul.f32 	%f1110, %f1857, %f1861;
+	mul.f32 	%f1111, %f1858, %f1860;
+	sub.f32 	%f1112, %f1111, %f1110;
+	fma.rn.f32 	%f1113, %f1112, %f1865, %f1109;
+	rcp.rn.f32 	%f1114, %f1113;
+	mul.f32 	%f1872, %f1103, %f1114;
+	mul.f32 	%f1115, %f1859, %f1864;
+	mul.f32 	%f1116, %f1858, %f1865;
+	sub.f32 	%f1117, %f1116, %f1115;
+	mul.f32 	%f1873, %f1117, %f1114;
+	mul.f32 	%f1118, %f1861, %f1865;
+	mul.f32 	%f1119, %f1862, %f1864;
+	sub.f32 	%f1120, %f1119, %f1118;
+	mul.f32 	%f1874, %f1120, %f1114;
+	sub.f32 	%f1121, %f1105, %f1106;
+	mul.f32 	%f1869, %f1121, %f1114;
+	mul.f32 	%f1122, %f1857, %f1865;
+	mul.f32 	%f1123, %f1859, %f1863;
+	sub.f32 	%f1124, %f1123, %f1122;
+	mul.f32 	%f1870, %f1124, %f1114;
+	mul.f32 	%f1125, %f1862, %f1863;
+	mul.f32 	%f1126, %f1860, %f1865;
+	sub.f32 	%f1127, %f1126, %f1125;
+	mul.f32 	%f1871, %f1127, %f1114;
+	mul.f32 	%f1866, %f1112, %f1114;
+	mul.f32 	%f1128, %f1858, %f1863;
+	mul.f32 	%f1129, %f1857, %f1864;
+	sub.f32 	%f1130, %f1129, %f1128;
+	mul.f32 	%f1867, %f1130, %f1114;
+	mul.f32 	%f1131, %f1860, %f1864;
+	mul.f32 	%f1132, %f1861, %f1863;
+	sub.f32 	%f1133, %f1132, %f1131;
+	mul.f32 	%f1868, %f1133, %f1114;
+	bra.uni 	$L__BB10_39;
+
+$L__BB10_30:
+	// begin inline asm
+	call (%rd649), _optix_get_instance_inverse_transform_from_handle, (%rd167);
+	// end inline asm
+
+$L__BB10_31:
+	// begin inline asm
+	cvta.to.global.u64 %rd173, %rd649;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r190,%r191,%r192,%r193}, [%rd173];
+	// end inline asm
+	mov.b32 	%f1872, %r190;
+	mov.b32 	%f1873, %r191;
+	mov.b32 	%f1874, %r192;
+	add.s64 	%rd177, %rd649, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd176, %rd177;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r194,%r195,%r196,%r197}, [%rd176];
+	// end inline asm
+	mov.b32 	%f1869, %r194;
+	mov.b32 	%f1870, %r195;
+	mov.b32 	%f1871, %r196;
+	add.s64 	%rd180, %rd649, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd179, %rd180;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r198,%r199,%r200,%r201}, [%rd179];
+	// end inline asm
+	mov.b32 	%f1866, %r198;
+	mov.b32 	%f1867, %r199;
+	mov.b32 	%f1868, %r200;
+
+$L__BB10_39:
+	setp.eq.s32 	%p20, %r646, 0;
+	@%p20 bra 	$L__BB10_41;
+
+	mul.f32 	%f1134, %f1843, %f1873;
+	fma.rn.f32 	%f1135, %f1840, %f1872, %f1134;
+	fma.rn.f32 	%f304, %f1846, %f1874, %f1135;
+	mul.f32 	%f1136, %f1842, %f1873;
+	fma.rn.f32 	%f1137, %f1839, %f1872, %f1136;
+	fma.rn.f32 	%f305, %f1845, %f1874, %f1137;
+	mul.f32 	%f1138, %f1841, %f1873;
+	fma.rn.f32 	%f1139, %f1838, %f1872, %f1138;
+	fma.rn.f32 	%f1874, %f1844, %f1874, %f1139;
+	mul.f32 	%f1140, %f1843, %f1870;
+	fma.rn.f32 	%f1141, %f1840, %f1869, %f1140;
+	fma.rn.f32 	%f307, %f1846, %f1871, %f1141;
+	mul.f32 	%f1142, %f1842, %f1870;
+	fma.rn.f32 	%f1143, %f1839, %f1869, %f1142;
+	fma.rn.f32 	%f308, %f1845, %f1871, %f1143;
+	mul.f32 	%f1144, %f1841, %f1870;
+	fma.rn.f32 	%f1145, %f1838, %f1869, %f1144;
+	fma.rn.f32 	%f1871, %f1844, %f1871, %f1145;
+	mul.f32 	%f1146, %f1843, %f1867;
+	fma.rn.f32 	%f1147, %f1840, %f1866, %f1146;
+	fma.rn.f32 	%f310, %f1846, %f1868, %f1147;
+	mul.f32 	%f1148, %f1842, %f1867;
+	fma.rn.f32 	%f1149, %f1839, %f1866, %f1148;
+	fma.rn.f32 	%f311, %f1845, %f1868, %f1149;
+	mul.f32 	%f1150, %f1841, %f1867;
+	fma.rn.f32 	%f1151, %f1838, %f1866, %f1150;
+	fma.rn.f32 	%f1868, %f1844, %f1868, %f1151;
+	mov.f32 	%f1866, %f310;
+	mov.f32 	%f1867, %f311;
+	mov.f32 	%f1869, %f307;
+	mov.f32 	%f1870, %f308;
+	mov.f32 	%f1872, %f304;
+	mov.f32 	%f1873, %f305;
+
+$L__BB10_41:
+	add.s32 	%r646, %r646, 1;
+	setp.lt.u32 	%p21, %r646, %r185;
+	mov.f32 	%f1838, %f1874;
+	mov.f32 	%f1839, %f1873;
+	mov.f32 	%f1840, %f1872;
+	mov.f32 	%f1841, %f1871;
+	mov.f32 	%f1842, %f1870;
+	mov.f32 	%f1843, %f1869;
+	mov.f32 	%f1844, %f1868;
+	mov.f32 	%f1845, %f1867;
+	mov.f32 	%f1846, %f1866;
+	@%p21 bra 	$L__BB10_26;
+
+$L__BB10_42:
+	mul.f32 	%f1152, %f1894, %f1873;
+	fma.rn.f32 	%f1153, %f1893, %f1872, %f1152;
+	mul.f32 	%f1154, %f1894, %f1870;
+	fma.rn.f32 	%f1155, %f1893, %f1869, %f1154;
+	mul.f32 	%f1156, %f1894, %f1867;
+	fma.rn.f32 	%f1157, %f1893, %f1866, %f1156;
+	fma.rn.f32 	%f1895, %f965, %f1868, %f1157;
+	fma.rn.f32 	%f1894, %f965, %f1871, %f1155;
+	fma.rn.f32 	%f1893, %f965, %f1874, %f1153;
+	bra.uni 	$L__BB10_44;
+
+$L__BB10_43:
+	mov.f32 	%f1895, %f965;
+
+$L__BB10_44:
+	// begin inline asm
+	call (%f1159), _optix_get_ray_tmax, ();
+	// end inline asm
+	ld.const.u64 	%rd286, [params+80];
+	setp.eq.s64 	%p22, %rd286, 0;
+	@%p22 bra 	$L__BB10_49;
+
+	ld.u64 	%rd287, [%rd47];
+	ld.const.u64 	%rd288, [params+328];
+	cvta.to.global.u64 	%rd289, %rd288;
+	cvt.u64.u32 	%rd18, %r1;
+	mul.wide.u32 	%rd290, %r1, 8;
+	add.s64 	%rd291, %rd289, %rd290;
+	st.global.u64 	[%rd291], %rd287;
+	ld.const.u64 	%rd292, [params+336];
+	cvta.to.global.u64 	%rd293, %rd292;
+	mul.wide.u32 	%rd294, %r1, 4;
+	add.s64 	%rd295, %rd293, %rd294;
+	mov.u32 	%r336, 0;
+	st.global.u32 	[%rd295], %r336;
+	ld.const.u64 	%rd296, [params+344];
+	cvta.to.global.u64 	%rd297, %rd296;
+	add.s64 	%rd19, %rd297, %rd294;
+	ld.global.u32 	%r10, [%rd19];
+	setp.eq.s32 	%p23, %r10, 0;
+	@%p23 bra 	$L__BB10_48;
+
+	// begin inline asm
+	call (%r337), _optix_read_instance_id, ();
+	// end inline asm
+	setp.ge.u32 	%p24, %r337, %r10;
+	@%p24 bra 	$L__BB10_48;
+
+	st.global.u32 	[%rd19], %r337;
+
+$L__BB10_48:
+	ld.const.u64 	%rd298, [params+72];
+	cvta.to.global.u64 	%rd299, %rd298;
+	shl.b64 	%rd300, %rd18, 2;
+	add.s64 	%rd301, %rd299, %rd300;
+	st.global.f32 	[%rd301], %f1159;
+	bra.uni 	$L__BB10_114;
+
+$L__BB10_49:
+	fma.rn.f32 	%f2056, %f1159, %f1893, %f1835;
+	fma.rn.f32 	%f2057, %f1159, %f1894, %f1836;
+	fma.rn.f32 	%f2058, %f1159, %f1895, %f1837;
+	add.s64 	%rd20, %rd3, 288;
+	ld.f32 	%f1161, [%rd3+288];
+	sub.f32 	%f1162, %f2056, %f1161;
+	ld.f32 	%f1163, [%rd3+292];
+	sub.f32 	%f1164, %f2057, %f1163;
+	ld.v2.f32 	{%f1165, %f1166}, [%rd3+304];
+	mul.f32 	%f1169, %f1162, %f1166;
+	add.f32 	%f1170, %f1165, 0f3F800000;
+	mov.f32 	%f2040, 0f3F800000;
+	mul.f32 	%f1172, %f1164, %f1164;
+	fma.rn.f32 	%f1173, %f1162, %f1162, %f1172;
+	mul.f32 	%f1174, %f1173, %f1170;
+	mul.f32 	%f1175, %f1166, %f1166;
+	mul.f32 	%f1176, %f1174, %f1175;
+	sub.f32 	%f1177, %f2040, %f1176;
+	sqrt.rn.f32 	%f1178, %f1177;
+	div.rn.f32 	%f2035, %f1169, %f1178;
+	mul.f32 	%f1179, %f1164, %f1166;
+	div.rn.f32 	%f2032, %f1179, %f1178;
+	ld.u8 	%rs2, [%rd3+324];
+	setp.eq.s16 	%p25, %rs2, 0;
+	mul.f32 	%f346, %f2035, %f2035;
+	@%p25 bra 	$L__BB10_51;
+
+	neg.f32 	%f1180, %f2035;
+	neg.f32 	%f1181, %f2032;
+	fma.rn.f32 	%f1182, %f1181, %f1181, %f346;
+	fma.rn.f32 	%f1184, %f2040, %f2040, %f1182;
+	sqrt.rn.f32 	%f1185, %f1184;
+	div.rn.f32 	%f2050, %f1180, %f1185;
+	div.rn.f32 	%f2051, %f1181, %f1185;
+	rcp.rn.f32 	%f2052, %f1185;
+	bra.uni 	$L__BB10_52;
+
+$L__BB10_51:
+	fma.rn.f32 	%f1186, %f2032, %f2032, %f346;
+	mov.f32 	%f1187, 0fBF800000;
+	fma.rn.f32 	%f1188, %f1187, %f1187, %f1186;
+	sqrt.rn.f32 	%f1189, %f1188;
+	div.rn.f32 	%f2050, %f2035, %f1189;
+	div.rn.f32 	%f2051, %f2032, %f1189;
+	div.rn.f32 	%f2052, %f1187, %f1189;
+
+$L__BB10_52:
+	ld.const.u64 	%rd21, [params+96];
+	setp.eq.s64 	%p26, %rd21, 0;
+	@%p26 bra 	$L__BB10_54;
+
+	ld.v4.f32 	{%f1191, %f1192, %f1193, %f1194}, [%rd20+-80];
+	ld.f32 	%f1197, [%rd20+-128];
+	fma.rn.f32 	%f1198, %f2056, %f1197, %f1191;
+	ld.f32 	%f1199, [%rd20+-124];
+	fma.rn.f32 	%f1200, %f2056, %f1199, %f1192;
+	ld.f32 	%f1201, [%rd20+-112];
+	fma.rn.f32 	%f1202, %f2057, %f1201, %f1198;
+	ld.f32 	%f1203, [%rd20+-108];
+	fma.rn.f32 	%f1204, %f2057, %f1203, %f1200;
+	ld.f32 	%f1205, [%rd20+-96];
+	fma.rn.f32 	%f1206, %f2058, %f1205, %f1202;
+	ld.f32 	%f1207, [%rd20+-92];
+	fma.rn.f32 	%f1208, %f2058, %f1207, %f1204;
+	ld.f32 	%f1209, [%rd20+24];
+	div.rn.f32 	%f1900, %f1206, %f1209;
+	div.rn.f32 	%f1899, %f1208, %f1209;
+
+$L__BB10_54:
+	ld.u64 	%rd22, [%rd47];
+	ld.const.u64 	%rd302, [params+344];
+	cvta.to.global.u64 	%rd303, %rd302;
+	cvt.u64.u32 	%rd23, %r1;
+	mul.wide.u32 	%rd304, %r1, 4;
+	add.s64 	%rd24, %rd303, %rd304;
+	ld.global.u32 	%r12, [%rd24];
+	setp.eq.s32 	%p27, %r12, 0;
+	mov.f32 	%f2039, 0f00000000;
+	mov.f32 	%f2042, 0f3F800000;
+	mov.f32 	%f2043, 0f00000000;
+	mov.f32 	%f2045, 0f00000000;
+	mov.f32 	%f2046, 0f3F800000;
+	mov.f32 	%f2048, 0f3F800000;
+	mov.f32 	%f2049, 0f00000000;
+	mov.f32 	%f2044, %f2032;
+	mov.f32 	%f2047, %f2035;
+	mov.f32 	%f2053, %f2050;
+	mov.f32 	%f2054, %f2051;
+	mov.f32 	%f2055, %f2052;
+	@%p27 bra 	$L__BB10_102;
+
+	// begin inline asm
+	call (%r338), _optix_read_instance_id, ();
+	// end inline asm
+	setp.ge.u32 	%p28, %r338, %r12;
+	mov.f32 	%f2044, %f2032;
+	mov.f32 	%f2047, %f2035;
+	mov.f32 	%f2053, %f2050;
+	mov.f32 	%f2054, %f2051;
+	mov.f32 	%f2055, %f2052;
+	@%p28 bra 	$L__BB10_102;
+
+	// begin inline asm
+	call (%r339), _optix_get_transform_list_size, ();
+	// end inline asm
+	setp.eq.s32 	%p29, %r339, 0;
+	mov.f32 	%f2000, 0f00000000;
+	mov.f32 	%f1999, 0f3F800000;
+	mov.f32 	%f1937, %f1999;
+	mov.f32 	%f1938, %f2000;
+	mov.f32 	%f1939, %f2000;
+	mov.f32 	%f1940, %f2000;
+	mov.f32 	%f1933, %f2000;
+	mov.f32 	%f1934, %f1999;
+	mov.f32 	%f1935, %f2000;
+	mov.f32 	%f1936, %f2000;
+	mov.f32 	%f1929, %f2000;
+	mov.f32 	%f1930, %f2000;
+	mov.f32 	%f1931, %f1999;
+	mov.f32 	%f1932, %f2000;
+	@%p29 bra 	$L__BB10_74;
+
+	// begin inline asm
+	call (%r340), _optix_get_transform_list_size, ();
+	// end inline asm
+	// begin inline asm
+	call (%f1238), _optix_get_ray_time, ();
+	// end inline asm
+	setp.lt.s32 	%p30, %r340, 1;
+	@%p30 bra 	$L__BB10_74;
+
+	add.s32 	%r647, %r340, 1;
+	mov.u32 	%r648, 1;
+
+$L__BB10_59:
 	.pragma "nounroll";
-	// inline asm
-	call (%rd315), _optix_get_transform_list_handle, (%r736);
-	// inline asm
-	// inline asm
-	call (%r433), _optix_get_transform_type_from_handle, (%rd315);
-	// inline asm
-	and.b32  	%r434, %r433, -2;
-	setp.eq.s32	%p108, %r434, 2;
-	@%p108 bra 	BB10_128;
-	bra.uni 	BB10_123;
-
-BB10_128:
-	setp.eq.s32	%p111, %r433, 2;
-	@%p111 bra 	BB10_132;
-	bra.uni 	BB10_129;
-
-BB10_132:
-	// inline asm
-	call (%rd389), _optix_get_matrix_motion_transform_from_handle, (%rd315);
-	// inline asm
-	// inline asm
-	cvta.to.global.u64 %rd391, %rd389;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r522,%r523,%r524,%r525}, [%rd391];
-	// inline asm
-	mov.b32	{%rs15, %rs16}, %r524;
-	add.s64 	%rd395, %rd389, 16;
-	// inline asm
+	add.s32 	%r342, %r647, -2;
+	// begin inline asm
+	call (%rd305), _optix_get_transform_list_handle, (%r342);
+	// end inline asm
+	// begin inline asm
+	call (%r343), _optix_get_transform_type_from_handle, (%rd305);
+	// end inline asm
+	or.b32  	%r344, %r343, 1;
+	setp.eq.s32 	%p31, %r344, 3;
+	@%p31 bra 	$L__BB10_65;
+	bra.uni 	$L__BB10_60;
+
+$L__BB10_65:
+	setp.eq.s32 	%p34, %r343, 2;
+	@%p34 bra 	$L__BB10_69;
+	bra.uni 	$L__BB10_66;
+
+$L__BB10_69:
+	// begin inline asm
+	call (%rd377), _optix_get_matrix_motion_transform_from_handle, (%rd305);
+	// end inline asm
+	// begin inline asm
+	cvta.to.global.u64 %rd379, %rd377;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r432,%r433,%r434,%r435}, [%rd379];
+	// end inline asm
+	add.s64 	%rd383, %rd377, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd382, %rd383;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r436,%r437,%r438,%r439}, [%rd382];
+	// end inline asm
+	add.s64 	%rd386, %rd377, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd385, %rd386;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r440,%r441,%r442,%r443}, [%rd385];
+	// end inline asm
+	add.s64 	%rd389, %rd377, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd388, %rd389;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r444,%r445,%r446,%r447}, [%rd388];
+	// end inline asm
+	add.s64 	%rd392, %rd377, 64;
+	// begin inline asm
+	cvta.to.global.u64 %rd391, %rd392;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r448,%r449,%r450,%r451}, [%rd391];
+	// end inline asm
+	add.s64 	%rd395, %rd377, 80;
+	// begin inline asm
 	cvta.to.global.u64 %rd394, %rd395;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r526,%r527,%r528,%r529}, [%rd394];
-	// inline asm
-	add.s64 	%rd398, %rd389, 32;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r452,%r453,%r454,%r455}, [%rd394];
+	// end inline asm
+	add.s64 	%rd398, %rd377, 96;
+	// begin inline asm
 	cvta.to.global.u64 %rd397, %rd398;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r530,%r531,%r532,%r533}, [%rd397];
-	// inline asm
-	add.s64 	%rd401, %rd389, 48;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r456,%r457,%r458,%r459}, [%rd397];
+	// end inline asm
+	add.s64 	%rd401, %rd377, 112;
+	// begin inline asm
 	cvta.to.global.u64 %rd400, %rd401;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r534,%r535,%r536,%r537}, [%rd400];
-	// inline asm
-	add.s64 	%rd404, %rd389, 64;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r460,%r461,%r462,%r463}, [%rd400];
+	// end inline asm
+	mov.b32 	%f1366, %r435;
+	mov.b32 	%f1367, %r436;
+	and.b32  	%r476, %r434, 65535;
+	add.s32 	%r477, %r476, -1;
+	cvt.rn.f32.s32 	%f1368, %r477;
+	sub.f32 	%f1369, %f1238, %f1366;
+	mul.f32 	%f1370, %f1369, %f1368;
+	sub.f32 	%f1371, %f1367, %f1366;
+	div.rn.f32 	%f1372, %f1370, %f1371;
+	min.f32 	%f1373, %f1368, %f1372;
+	mov.f32 	%f1374, 0f00000000;
+	max.f32 	%f1375, %f1374, %f1373;
+	cvt.rmi.f32.f32 	%f1376, %f1375;
+	sub.f32 	%f446, %f1375, %f1376;
+	cvt.rzi.s32.f32 	%r478, %f1376;
+	cvt.s64.s32 	%rd31, %r478;
+	mul.wide.s32 	%rd412, %r478, 48;
+	add.s64 	%rd404, %rd386, %rd412;
+	// begin inline asm
 	cvta.to.global.u64 %rd403, %rd404;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r538,%r539,%r540,%r541}, [%rd403];
-	// inline asm
-	add.s64 	%rd407, %rd389, 80;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r464,%r465,%r466,%r467}, [%rd403];
+	// end inline asm
+	mov.b32 	%f1937, %r464;
+	mov.b32 	%f1938, %r465;
+	mov.b32 	%f1939, %r466;
+	mov.b32 	%f1940, %r467;
+	add.s64 	%rd407, %rd404, 16;
+	// begin inline asm
 	cvta.to.global.u64 %rd406, %rd407;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r542,%r543,%r544,%r545}, [%rd406];
-	// inline asm
-	add.s64 	%rd410, %rd389, 96;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r468,%r469,%r470,%r471}, [%rd406];
+	// end inline asm
+	mov.b32 	%f1933, %r468;
+	mov.b32 	%f1934, %r469;
+	mov.b32 	%f1935, %r470;
+	mov.b32 	%f1936, %r471;
+	add.s64 	%rd410, %rd404, 32;
+	// begin inline asm
 	cvta.to.global.u64 %rd409, %rd410;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r546,%r547,%r548,%r549}, [%rd409];
-	// inline asm
-	add.s64 	%rd413, %rd389, 112;
-	// inline asm
-	cvta.to.global.u64 %rd412, %rd413;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r550,%r551,%r552,%r553}, [%rd412];
-	// inline asm
-	mov.b32 	 %f1331, %r525;
-	mov.b32 	 %f1332, %r526;
-	cvt.u32.u16	%r566, %rs15;
-	add.s32 	%r567, %r566, -1;
-	cvt.rn.f32.s32	%f1333, %r567;
-	sub.f32 	%f1334, %f940, %f1331;
-	mul.f32 	%f1335, %f1334, %f1333;
-	sub.f32 	%f1336, %f1332, %f1331;
-	div.rn.f32 	%f1337, %f1335, %f1336;
-	min.f32 	%f1338, %f1333, %f1337;
-	mov.f32 	%f1339, 0f00000000;
-	max.f32 	%f1340, %f1339, %f1338;
-	cvt.rmi.f32.f32	%f1341, %f1340;
-	cvt.rzi.s32.f32	%r568, %f1341;
-	cvt.s64.s32	%rd35, %r568;
-	mul.wide.s32 	%rd424, %r568, 48;
-	add.s64 	%rd416, %rd398, %rd424;
-	// inline asm
-	cvta.to.global.u64 %rd415, %rd416;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r554,%r555,%r556,%r557}, [%rd415];
-	// inline asm
-	mov.b32 	 %f1872, %r554;
-	mov.b32 	 %f1873, %r555;
-	mov.b32 	 %f1874, %r556;
-	mov.b32 	 %f1875, %r557;
-	add.s64 	%rd419, %rd416, 16;
-	// inline asm
-	cvta.to.global.u64 %rd418, %rd419;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r558,%r559,%r560,%r561}, [%rd418];
-	// inline asm
-	mov.b32 	 %f1868, %r558;
-	mov.b32 	 %f1869, %r559;
-	mov.b32 	 %f1870, %r560;
-	mov.b32 	 %f1871, %r561;
-	add.s64 	%rd422, %rd416, 32;
-	// inline asm
-	cvta.to.global.u64 %rd421, %rd422;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r562,%r563,%r564,%r565}, [%rd421];
-	// inline asm
-	sub.f32 	%f430, %f1340, %f1341;
-	mov.b32 	 %f1864, %r562;
-	mov.b32 	 %f1865, %r563;
-	mov.b32 	 %f1866, %r564;
-	mov.b32 	 %f1867, %r565;
-	setp.leu.f32	%p113, %f430, 0f00000000;
-	@%p113 bra 	BB10_134;
-
-	mul.lo.s64 	%rd434, %rd35, 48;
-	add.s64 	%rd435, %rd389, %rd434;
-	add.s64 	%rd426, %rd435, 80;
-	// inline asm
-	cvta.to.global.u64 %rd425, %rd426;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r569,%r570,%r571,%r572}, [%rd425];
-	// inline asm
-	mov.b32 	 %f1342, %r569;
-	mov.b32 	 %f1343, %r570;
-	mov.b32 	 %f1344, %r571;
-	mov.b32 	 %f1345, %r572;
-	add.s64 	%rd429, %rd435, 96;
-	// inline asm
-	cvta.to.global.u64 %rd428, %rd429;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r573,%r574,%r575,%r576}, [%rd428];
-	// inline asm
-	mov.b32 	 %f1346, %r573;
-	mov.b32 	 %f1347, %r574;
-	mov.b32 	 %f1348, %r575;
-	mov.b32 	 %f1349, %r576;
-	add.s64 	%rd432, %rd435, 112;
-	// inline asm
-	cvta.to.global.u64 %rd431, %rd432;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r577,%r578,%r579,%r580}, [%rd431];
-	// inline asm
-	mov.f32 	%f1350, 0f3F800000;
-	sub.f32 	%f1351, %f1350, %f430;
-	mul.f32 	%f1352, %f430, %f1342;
-	mul.f32 	%f1353, %f430, %f1343;
-	mul.f32 	%f1354, %f430, %f1344;
-	mul.f32 	%f1355, %f430, %f1345;
-	fma.rn.f32 	%f1872, %f1351, %f1872, %f1352;
-	fma.rn.f32 	%f1873, %f1351, %f1873, %f1353;
-	fma.rn.f32 	%f1874, %f1351, %f1874, %f1354;
-	fma.rn.f32 	%f1875, %f1351, %f1875, %f1355;
-	mul.f32 	%f1356, %f430, %f1346;
-	mul.f32 	%f1357, %f430, %f1347;
-	mul.f32 	%f1358, %f430, %f1348;
-	mul.f32 	%f1359, %f430, %f1349;
-	fma.rn.f32 	%f1868, %f1351, %f1868, %f1356;
-	fma.rn.f32 	%f1869, %f1351, %f1869, %f1357;
-	fma.rn.f32 	%f1870, %f1351, %f1870, %f1358;
-	fma.rn.f32 	%f1871, %f1351, %f1871, %f1359;
-	mov.b32 	 %f1360, %r577;
-	mov.b32 	 %f1361, %r578;
-	mov.b32 	 %f1362, %r579;
-	mov.b32 	 %f1363, %r580;
-	mul.f32 	%f1364, %f430, %f1360;
-	mul.f32 	%f1365, %f430, %f1361;
-	mul.f32 	%f1366, %f430, %f1362;
-	mul.f32 	%f1367, %f430, %f1363;
-	fma.rn.f32 	%f1864, %f1351, %f1864, %f1364;
-	fma.rn.f32 	%f1865, %f1351, %f1865, %f1365;
-	fma.rn.f32 	%f1866, %f1351, %f1866, %f1366;
-	fma.rn.f32 	%f1867, %f1351, %f1867, %f1367;
-	bra.uni 	BB10_134;
-
-BB10_123:
-	mov.f32 	%f1864, 0f00000000;
-	mov.f32 	%f1866, 0f3F800000;
-	setp.eq.s32	%p109, %r433, 4;
-	@%p109 bra 	BB10_126;
-	bra.uni 	BB10_124;
-
-BB10_126:
-	// inline asm
-	call (%rd665), _optix_get_instance_transform_from_handle, (%rd315);
-	// inline asm
-	bra.uni 	BB10_127;
-
-BB10_129:
-	// inline asm
-	call (%rd330), _optix_get_srt_motion_transform_from_handle, (%rd315);
-	// inline asm
-	// inline asm
-	cvta.to.global.u64 %rd332, %rd330;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r447,%r448,%r449,%r450}, [%rd332];
-	// inline asm
-	mov.b32	{%rs13, %rs14}, %r449;
-	add.s64 	%rd336, %rd330, 16;
-	// inline asm
-	cvta.to.global.u64 %rd335, %rd336;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r451,%r452,%r453,%r454}, [%rd335];
-	// inline asm
-	add.s64 	%rd339, %rd330, 32;
-	// inline asm
-	cvta.to.global.u64 %rd338, %rd339;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r455,%r456,%r457,%r458}, [%rd338];
-	// inline asm
-	add.s64 	%rd342, %rd330, 48;
-	// inline asm
-	cvta.to.global.u64 %rd341, %rd342;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r459,%r460,%r461,%r462}, [%rd341];
-	// inline asm
-	add.s64 	%rd345, %rd330, 64;
-	// inline asm
-	cvta.to.global.u64 %rd344, %rd345;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r463,%r464,%r465,%r466}, [%rd344];
-	// inline asm
-	add.s64 	%rd348, %rd330, 80;
-	// inline asm
-	cvta.to.global.u64 %rd347, %rd348;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r467,%r468,%r469,%r470}, [%rd347];
-	// inline asm
-	add.s64 	%rd351, %rd330, 96;
-	// inline asm
-	cvta.to.global.u64 %rd350, %rd351;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r471,%r472,%r473,%r474}, [%rd350];
-	// inline asm
-	add.s64 	%rd354, %rd330, 112;
-	// inline asm
-	cvta.to.global.u64 %rd353, %rd354;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r475,%r476,%r477,%r478}, [%rd353];
-	// inline asm
-	add.s64 	%rd357, %rd330, 128;
-	// inline asm
-	cvta.to.global.u64 %rd356, %rd357;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r479,%r480,%r481,%r482}, [%rd356];
-	// inline asm
-	add.s64 	%rd360, %rd330, 144;
-	// inline asm
-	cvta.to.global.u64 %rd359, %rd360;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r483,%r484,%r485,%r486}, [%rd359];
-	// inline asm
-	mov.b32 	 %f1218, %r450;
-	mov.b32 	 %f1219, %r451;
-	cvt.u32.u16	%r503, %rs13;
-	add.s32 	%r504, %r503, -1;
-	cvt.rn.f32.s32	%f1220, %r504;
-	sub.f32 	%f1221, %f940, %f1218;
-	mul.f32 	%f1222, %f1221, %f1220;
-	sub.f32 	%f1223, %f1219, %f1218;
-	div.rn.f32 	%f1224, %f1222, %f1223;
-	min.f32 	%f1225, %f1220, %f1224;
-	mov.f32 	%f1226, 0f00000000;
-	max.f32 	%f1227, %f1226, %f1225;
-	cvt.rmi.f32.f32	%f1228, %f1227;
-	cvt.rzi.s32.f32	%r505, %f1228;
-	cvt.s64.s32	%rd33, %r505;
-	mul.wide.s32 	%rd374, %r505, 64;
-	add.s64 	%rd363, %rd339, %rd374;
-	// inline asm
-	cvta.to.global.u64 %rd362, %rd363;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r487,%r488,%r489,%r490}, [%rd362];
-	// inline asm
-	mov.b32 	 %f1848, %r487;
-	mov.b32 	 %f1849, %r488;
-	mov.b32 	 %f1850, %r489;
-	mov.b32 	 %f1851, %r490;
-	add.s64 	%rd366, %rd363, 16;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r472,%r473,%r474,%r475}, [%rd409];
+	// end inline asm
+	mov.b32 	%f1929, %r472;
+	mov.b32 	%f1930, %r473;
+	mov.b32 	%f1931, %r474;
+	mov.b32 	%f1932, %r475;
+	setp.leu.f32 	%p36, %f446, 0f00000000;
+	@%p36 bra 	$L__BB10_71;
+
+	mov.f32 	%f1377, 0f3F800000;
+	sub.f32 	%f1378, %f1377, %f446;
+	mul.lo.s64 	%rd422, %rd31, 48;
+	add.s64 	%rd423, %rd377, %rd422;
+	add.s64 	%rd414, %rd423, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd413, %rd414;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r479,%r480,%r481,%r482}, [%rd413];
+	// end inline asm
+	mov.b32 	%f1379, %r479;
+	mov.b32 	%f1380, %r480;
+	mov.b32 	%f1381, %r481;
+	mov.b32 	%f1382, %r482;
+	mul.f32 	%f1383, %f446, %f1379;
+	mul.f32 	%f1384, %f446, %f1380;
+	mul.f32 	%f1385, %f446, %f1381;
+	mul.f32 	%f1386, %f446, %f1382;
+	fma.rn.f32 	%f1937, %f1378, %f1937, %f1383;
+	fma.rn.f32 	%f1938, %f1378, %f1938, %f1384;
+	fma.rn.f32 	%f1939, %f1378, %f1939, %f1385;
+	fma.rn.f32 	%f1940, %f1378, %f1940, %f1386;
+	add.s64 	%rd417, %rd423, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd416, %rd417;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r483,%r484,%r485,%r486}, [%rd416];
+	// end inline asm
+	mov.b32 	%f1387, %r483;
+	mov.b32 	%f1388, %r484;
+	mov.b32 	%f1389, %r485;
+	mov.b32 	%f1390, %r486;
+	mul.f32 	%f1391, %f446, %f1387;
+	mul.f32 	%f1392, %f446, %f1388;
+	mul.f32 	%f1393, %f446, %f1389;
+	mul.f32 	%f1394, %f446, %f1390;
+	fma.rn.f32 	%f1933, %f1378, %f1933, %f1391;
+	fma.rn.f32 	%f1934, %f1378, %f1934, %f1392;
+	fma.rn.f32 	%f1935, %f1378, %f1935, %f1393;
+	fma.rn.f32 	%f1936, %f1378, %f1936, %f1394;
+	add.s64 	%rd420, %rd423, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd419, %rd420;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r487,%r488,%r489,%r490}, [%rd419];
+	// end inline asm
+	mov.b32 	%f1395, %r487;
+	mov.b32 	%f1396, %r488;
+	mov.b32 	%f1397, %r489;
+	mov.b32 	%f1398, %r490;
+	mul.f32 	%f1399, %f446, %f1395;
+	mul.f32 	%f1400, %f446, %f1396;
+	mul.f32 	%f1401, %f446, %f1397;
+	mul.f32 	%f1402, %f446, %f1398;
+	fma.rn.f32 	%f1929, %f1378, %f1929, %f1399;
+	fma.rn.f32 	%f1930, %f1378, %f1930, %f1400;
+	fma.rn.f32 	%f1931, %f1378, %f1931, %f1401;
+	fma.rn.f32 	%f1932, %f1378, %f1932, %f1402;
+	bra.uni 	$L__BB10_71;
+
+$L__BB10_60:
+	mov.f32 	%f1929, 0f00000000;
+	mov.f32 	%f1931, 0f3F800000;
+	setp.eq.s32 	%p32, %r343, 4;
+	@%p32 bra 	$L__BB10_63;
+
+	setp.ne.s32 	%p33, %r343, 1;
+	mov.f32 	%f1930, %f1929;
+	mov.f32 	%f1932, %f1929;
+	mov.f32 	%f1933, %f1929;
+	mov.f32 	%f1934, %f1931;
+	mov.f32 	%f1935, %f1929;
+	mov.f32 	%f1936, %f1929;
+	mov.f32 	%f1937, %f1931;
+	mov.f32 	%f1938, %f1929;
+	mov.f32 	%f1939, %f1929;
+	mov.f32 	%f1940, %f1929;
+	@%p33 bra 	$L__BB10_71;
+
+	// begin inline asm
+	call (%rd307), _optix_get_static_transform_from_handle, (%rd305);
+	// end inline asm
+	add.s64 	%rd650, %rd307, 16;
+	bra.uni 	$L__BB10_64;
+
+$L__BB10_66:
+	// begin inline asm
+	call (%rd320), _optix_get_srt_motion_transform_from_handle, (%rd305);
+	// end inline asm
+	// begin inline asm
+	cvta.to.global.u64 %rd322, %rd320;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r357,%r358,%r359,%r360}, [%rd322];
+	// end inline asm
+	add.s64 	%rd326, %rd320, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd325, %rd326;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r361,%r362,%r363,%r364}, [%rd325];
+	// end inline asm
+	add.s64 	%rd329, %rd320, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd328, %rd329;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r365,%r366,%r367,%r368}, [%rd328];
+	// end inline asm
+	add.s64 	%rd332, %rd320, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd331, %rd332;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r369,%r370,%r371,%r372}, [%rd331];
+	// end inline asm
+	add.s64 	%rd335, %rd320, 64;
+	// begin inline asm
+	cvta.to.global.u64 %rd334, %rd335;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r373,%r374,%r375,%r376}, [%rd334];
+	// end inline asm
+	add.s64 	%rd338, %rd320, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd337, %rd338;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r377,%r378,%r379,%r380}, [%rd337];
+	// end inline asm
+	add.s64 	%rd341, %rd320, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd340, %rd341;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r381,%r382,%r383,%r384}, [%rd340];
+	// end inline asm
+	add.s64 	%rd344, %rd320, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd343, %rd344;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r385,%r386,%r387,%r388}, [%rd343];
+	// end inline asm
+	add.s64 	%rd347, %rd320, 128;
+	// begin inline asm
+	cvta.to.global.u64 %rd346, %rd347;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r389,%r390,%r391,%r392}, [%rd346];
+	// end inline asm
+	add.s64 	%rd350, %rd320, 144;
+	// begin inline asm
+	cvta.to.global.u64 %rd349, %rd350;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r393,%r394,%r395,%r396}, [%rd349];
+	// end inline asm
+	mov.b32 	%f1253, %r360;
+	mov.b32 	%f1254, %r361;
+	and.b32  	%r413, %r359, 65535;
+	add.s32 	%r414, %r413, -1;
+	cvt.rn.f32.s32 	%f1255, %r414;
+	sub.f32 	%f1256, %f1238, %f1253;
+	mul.f32 	%f1257, %f1256, %f1255;
+	sub.f32 	%f1258, %f1254, %f1253;
+	div.rn.f32 	%f1259, %f1257, %f1258;
+	min.f32 	%f1260, %f1255, %f1259;
+	mov.f32 	%f1261, 0f00000000;
+	max.f32 	%f1262, %f1261, %f1260;
+	cvt.rmi.f32.f32 	%f1263, %f1262;
+	sub.f32 	%f385, %f1262, %f1263;
+	cvt.rzi.s32.f32 	%r415, %f1263;
+	mul.wide.s32 	%rd364, %r415, 64;
+	add.s64 	%rd353, %rd329, %rd364;
+	// begin inline asm
+	cvta.to.global.u64 %rd352, %rd353;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r397,%r398,%r399,%r400}, [%rd352];
+	// end inline asm
+	mov.b32 	%f1913, %r397;
+	mov.b32 	%f1914, %r398;
+	mov.b32 	%f1915, %r399;
+	mov.b32 	%f1916, %r400;
+	add.s64 	%rd356, %rd353, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd355, %rd356;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r401,%r402,%r403,%r404}, [%rd355];
+	// end inline asm
+	mov.b32 	%f1917, %r401;
+	mov.b32 	%f1918, %r402;
+	mov.b32 	%f1919, %r403;
+	mov.b32 	%f1920, %r404;
+	add.s64 	%rd359, %rd353, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd358, %rd359;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r405,%r406,%r407,%r408}, [%rd358];
+	// end inline asm
+	mov.b32 	%f1921, %r405;
+	mov.b32 	%f1922, %r406;
+	mov.b32 	%f1923, %r407;
+	mov.b32 	%f1924, %r408;
+	add.s64 	%rd362, %rd353, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd361, %rd362;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r409,%r410,%r411,%r412}, [%rd361];
+	// end inline asm
+	mov.b32 	%f1925, %r409;
+	mov.b32 	%f1926, %r410;
+	mov.b32 	%f1927, %r411;
+	mov.b32 	%f1928, %r412;
+	setp.leu.f32 	%p35, %f385, 0f00000000;
+	@%p35 bra 	$L__BB10_68;
+
+	mov.f32 	%f1264, 0f3F800000;
+	sub.f32 	%f1265, %f1264, %f385;
+	add.s64 	%rd366, %rd353, 64;
+	// begin inline asm
 	cvta.to.global.u64 %rd365, %rd366;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r491,%r492,%r493,%r494}, [%rd365];
-	// inline asm
-	mov.b32 	 %f1852, %r491;
-	mov.b32 	 %f1853, %r492;
-	mov.b32 	 %f1854, %r493;
-	mov.b32 	 %f1855, %r494;
-	add.s64 	%rd369, %rd363, 32;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r416,%r417,%r418,%r419}, [%rd365];
+	// end inline asm
+	mov.b32 	%f1266, %r416;
+	mov.b32 	%f1267, %r417;
+	mov.b32 	%f1268, %r418;
+	mov.b32 	%f1269, %r419;
+	mul.f32 	%f1270, %f385, %f1266;
+	mul.f32 	%f1271, %f385, %f1267;
+	mul.f32 	%f1272, %f385, %f1268;
+	mul.f32 	%f1273, %f385, %f1269;
+	fma.rn.f32 	%f1913, %f1265, %f1913, %f1270;
+	fma.rn.f32 	%f1914, %f1265, %f1914, %f1271;
+	fma.rn.f32 	%f1915, %f1265, %f1915, %f1272;
+	fma.rn.f32 	%f1916, %f1265, %f1916, %f1273;
+	add.s64 	%rd369, %rd353, 80;
+	// begin inline asm
 	cvta.to.global.u64 %rd368, %rd369;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r495,%r496,%r497,%r498}, [%rd368];
-	// inline asm
-	sub.f32 	%f369, %f1227, %f1228;
-	mov.b32 	 %f1856, %r495;
-	mov.b32 	 %f1857, %r496;
-	mov.b32 	 %f1858, %r497;
-	mov.b32 	 %f1859, %r498;
-	add.s64 	%rd372, %rd363, 48;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r420,%r421,%r422,%r423}, [%rd368];
+	// end inline asm
+	mov.b32 	%f1274, %r420;
+	mov.b32 	%f1275, %r421;
+	mov.b32 	%f1276, %r422;
+	mov.b32 	%f1277, %r423;
+	mul.f32 	%f1278, %f385, %f1274;
+	mul.f32 	%f1279, %f385, %f1275;
+	mul.f32 	%f1280, %f385, %f1276;
+	mul.f32 	%f1281, %f385, %f1277;
+	fma.rn.f32 	%f1917, %f1265, %f1917, %f1278;
+	fma.rn.f32 	%f1918, %f1265, %f1918, %f1279;
+	fma.rn.f32 	%f1919, %f1265, %f1919, %f1280;
+	fma.rn.f32 	%f1920, %f1265, %f1920, %f1281;
+	add.s64 	%rd372, %rd353, 96;
+	// begin inline asm
 	cvta.to.global.u64 %rd371, %rd372;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r499,%r500,%r501,%r502}, [%rd371];
-	// inline asm
-	mov.b32 	 %f1860, %r499;
-	mov.b32 	 %f1861, %r500;
-	mov.b32 	 %f1862, %r501;
-	mov.b32 	 %f1863, %r502;
-	setp.leu.f32	%p112, %f369, 0f00000000;
-	@%p112 bra 	BB10_131;
-
-	shl.b64 	%rd387, %rd33, 6;
-	add.s64 	%rd388, %rd387, %rd330;
-	add.s64 	%rd376, %rd388, 96;
-	// inline asm
-	cvta.to.global.u64 %rd375, %rd376;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r506,%r507,%r508,%r509}, [%rd375];
-	// inline asm
-	mov.b32 	 %f1229, %r506;
-	mov.b32 	 %f1230, %r507;
-	mov.b32 	 %f1231, %r508;
-	mov.b32 	 %f1232, %r509;
-	add.s64 	%rd379, %rd388, 112;
-	// inline asm
-	cvta.to.global.u64 %rd378, %rd379;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r510,%r511,%r512,%r513}, [%rd378];
-	// inline asm
-	mov.b32 	 %f1233, %r510;
-	mov.b32 	 %f1234, %r511;
-	mov.b32 	 %f1235, %r512;
-	mov.b32 	 %f1236, %r513;
-	add.s64 	%rd382, %rd388, 128;
-	// inline asm
-	cvta.to.global.u64 %rd381, %rd382;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r514,%r515,%r516,%r517}, [%rd381];
-	// inline asm
-	mov.b32 	 %f1237, %r514;
-	mov.b32 	 %f1238, %r515;
-	mov.b32 	 %f1239, %r516;
-	mov.b32 	 %f1240, %r517;
-	add.s64 	%rd385, %rd388, 144;
-	// inline asm
-	cvta.to.global.u64 %rd384, %rd385;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r518,%r519,%r520,%r521}, [%rd384];
-	// inline asm
-	mov.f32 	%f1241, 0f3F800000;
-	sub.f32 	%f1242, %f1241, %f369;
-	mul.f32 	%f1243, %f369, %f1229;
-	mul.f32 	%f1244, %f369, %f1230;
-	mul.f32 	%f1245, %f369, %f1231;
-	mul.f32 	%f1246, %f369, %f1232;
-	fma.rn.f32 	%f1848, %f1242, %f1848, %f1243;
-	fma.rn.f32 	%f1849, %f1242, %f1849, %f1244;
-	fma.rn.f32 	%f1850, %f1242, %f1850, %f1245;
-	fma.rn.f32 	%f1851, %f1242, %f1851, %f1246;
-	mul.f32 	%f1247, %f369, %f1233;
-	mul.f32 	%f1248, %f369, %f1234;
-	mul.f32 	%f1249, %f369, %f1235;
-	mul.f32 	%f1250, %f369, %f1236;
-	fma.rn.f32 	%f1852, %f1242, %f1852, %f1247;
-	fma.rn.f32 	%f1853, %f1242, %f1853, %f1248;
-	fma.rn.f32 	%f1854, %f1242, %f1854, %f1249;
-	fma.rn.f32 	%f1855, %f1242, %f1855, %f1250;
-	mul.f32 	%f1251, %f369, %f1237;
-	mul.f32 	%f1252, %f369, %f1238;
-	mul.f32 	%f1253, %f369, %f1239;
-	mul.f32 	%f1254, %f369, %f1240;
-	fma.rn.f32 	%f1856, %f1242, %f1856, %f1251;
-	fma.rn.f32 	%f1255, %f1242, %f1857, %f1252;
-	fma.rn.f32 	%f1256, %f1242, %f1858, %f1253;
-	fma.rn.f32 	%f1257, %f1242, %f1859, %f1254;
-	mov.b32 	 %f1258, %r518;
-	mov.b32 	 %f1259, %r519;
-	mov.b32 	 %f1260, %r520;
-	mov.b32 	 %f1261, %r521;
-	mul.f32 	%f1262, %f369, %f1258;
-	mul.f32 	%f1263, %f369, %f1259;
-	mul.f32 	%f1264, %f369, %f1260;
-	mul.f32 	%f1265, %f369, %f1261;
-	fma.rn.f32 	%f1266, %f1242, %f1860, %f1262;
-	fma.rn.f32 	%f1861, %f1242, %f1861, %f1263;
-	fma.rn.f32 	%f1862, %f1242, %f1862, %f1264;
-	fma.rn.f32 	%f1863, %f1242, %f1863, %f1265;
-	mul.f32 	%f1267, %f1256, %f1256;
-	fma.rn.f32 	%f1268, %f1255, %f1255, %f1267;
-	fma.rn.f32 	%f1269, %f1257, %f1257, %f1268;
-	fma.rn.f32 	%f1270, %f1266, %f1266, %f1269;
-	sqrt.rn.f32 	%f1271, %f1270;
-	rcp.rn.f32 	%f1272, %f1271;
-	mul.f32 	%f1857, %f1255, %f1272;
-	mul.f32 	%f1858, %f1256, %f1272;
-	mul.f32 	%f1859, %f1257, %f1272;
-	mul.f32 	%f1860, %f1266, %f1272;
-
-BB10_131:
-	mul.f32 	%f1273, %f1858, %f1858;
-	fma.rn.f32 	%f1274, %f1857, %f1857, %f1273;
-	fma.rn.f32 	%f1275, %f1859, %f1859, %f1274;
-	fma.rn.f32 	%f1276, %f1860, %f1860, %f1275;
-	rcp.rn.f32 	%f1277, %f1276;
-	mul.f32 	%f1278, %f1857, %f1277;
-	mul.f32 	%f1279, %f1858, %f1277;
-	mul.f32 	%f1280, %f1859, %f1277;
-	mul.f32 	%f1281, %f1860, %f1277;
-	mul.f32 	%f1282, %f1857, %f1278;
-	mul.f32 	%f1283, %f1858, %f1279;
-	mul.f32 	%f1284, %f1859, %f1280;
-	mul.f32 	%f1285, %f1857, %f1279;
-	mul.f32 	%f1286, %f1859, %f1281;
-	mul.f32 	%f1287, %f1857, %f1280;
-	mul.f32 	%f1288, %f1858, %f1281;
-	mul.f32 	%f1289, %f1858, %f1280;
-	mul.f32 	%f1290, %f1857, %f1281;
-	sub.f32 	%f1291, %f1282, %f1283;
-	sub.f32 	%f1292, %f1291, %f1284;
-	fma.rn.f32 	%f1293, %f1860, %f1281, %f1292;
-	sub.f32 	%f1294, %f1285, %f1286;
-	add.f32 	%f1295, %f1294, %f1294;
-	add.f32 	%f1296, %f1287, %f1288;
-	add.f32 	%f1297, %f1296, %f1296;
-	add.f32 	%f1298, %f1285, %f1286;
-	add.f32 	%f1299, %f1298, %f1298;
-	sub.f32 	%f1300, %f1283, %f1282;
-	sub.f32 	%f1301, %f1300, %f1284;
-	fma.rn.f32 	%f1302, %f1860, %f1281, %f1301;
-	sub.f32 	%f1303, %f1289, %f1290;
-	add.f32 	%f1304, %f1303, %f1303;
-	sub.f32 	%f1305, %f1287, %f1288;
-	add.f32 	%f1306, %f1305, %f1305;
-	add.f32 	%f1307, %f1289, %f1290;
-	add.f32 	%f1308, %f1307, %f1307;
-	neg.f32 	%f1309, %f1282;
-	sub.f32 	%f1310, %f1309, %f1283;
-	add.f32 	%f1311, %f1284, %f1310;
-	fma.rn.f32 	%f1312, %f1860, %f1281, %f1311;
-	mul.f32 	%f1313, %f1851, %f1293;
-	fma.rn.f32 	%f1314, %f1854, %f1295, %f1313;
-	fma.rn.f32 	%f1315, %f1856, %f1297, %f1314;
-	sub.f32 	%f1875, %f1861, %f1315;
-	mul.f32 	%f1316, %f1854, %f1302;
-	fma.rn.f32 	%f1317, %f1851, %f1299, %f1316;
-	fma.rn.f32 	%f1318, %f1856, %f1304, %f1317;
-	sub.f32 	%f1871, %f1862, %f1318;
-	mul.f32 	%f1319, %f1854, %f1308;
-	fma.rn.f32 	%f1320, %f1851, %f1306, %f1319;
-	fma.rn.f32 	%f1321, %f1856, %f1312, %f1320;
-	sub.f32 	%f1867, %f1863, %f1321;
-	mul.f32 	%f1322, %f1850, %f1293;
-	fma.rn.f32 	%f1323, %f1853, %f1295, %f1322;
-	fma.rn.f32 	%f1874, %f1855, %f1297, %f1323;
-	mul.f32 	%f1324, %f1853, %f1302;
-	fma.rn.f32 	%f1325, %f1850, %f1299, %f1324;
-	fma.rn.f32 	%f1870, %f1855, %f1304, %f1325;
-	mul.f32 	%f1326, %f1853, %f1308;
-	fma.rn.f32 	%f1327, %f1850, %f1306, %f1326;
-	fma.rn.f32 	%f1866, %f1855, %f1312, %f1327;
-	mul.f32 	%f1328, %f1849, %f1293;
-	fma.rn.f32 	%f1873, %f1852, %f1295, %f1328;
-	mul.f32 	%f1329, %f1852, %f1302;
-	fma.rn.f32 	%f1869, %f1849, %f1299, %f1329;
-	mul.f32 	%f1330, %f1852, %f1308;
-	fma.rn.f32 	%f1865, %f1849, %f1306, %f1330;
-	mul.f32 	%f1872, %f1848, %f1293;
-	mul.f32 	%f1868, %f1848, %f1299;
-	mul.f32 	%f1864, %f1848, %f1306;
-	bra.uni 	BB10_134;
-
-BB10_124:
-	setp.ne.s32	%p110, %r433, 1;
-	mov.f32 	%f1865, %f1864;
-	mov.f32 	%f1867, %f1864;
-	mov.f32 	%f1868, %f1864;
-	mov.f32 	%f1869, %f1866;
-	mov.f32 	%f1870, %f1864;
-	mov.f32 	%f1871, %f1864;
-	mov.f32 	%f1872, %f1866;
-	mov.f32 	%f1873, %f1864;
-	mov.f32 	%f1874, %f1864;
-	mov.f32 	%f1875, %f1864;
-	@%p110 bra 	BB10_134;
-
-	// inline asm
-	call (%rd317), _optix_get_static_transform_from_handle, (%rd315);
-	// inline asm
-	add.s64 	%rd665, %rd317, 16;
-
-BB10_127:
-	// inline asm
-	cvta.to.global.u64 %rd321, %rd665;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r435,%r436,%r437,%r438}, [%rd321];
-	// inline asm
-	mov.b32 	 %f1872, %r435;
-	mov.b32 	 %f1873, %r436;
-	mov.b32 	 %f1874, %r437;
-	mov.b32 	 %f1875, %r438;
-	add.s64 	%rd325, %rd665, 16;
-	// inline asm
-	cvta.to.global.u64 %rd324, %rd325;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r439,%r440,%r441,%r442}, [%rd324];
-	// inline asm
-	mov.b32 	 %f1868, %r439;
-	mov.b32 	 %f1869, %r440;
-	mov.b32 	 %f1870, %r441;
-	mov.b32 	 %f1871, %r442;
-	add.s64 	%rd328, %rd665, 32;
-	// inline asm
-	cvta.to.global.u64 %rd327, %rd328;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r443,%r444,%r445,%r446}, [%rd327];
-	// inline asm
-	mov.b32 	 %f1864, %r443;
-	mov.b32 	 %f1865, %r444;
-	mov.b32 	 %f1866, %r445;
-	mov.b32 	 %f1867, %r446;
-
-BB10_134:
-	add.s32 	%r21, %r736, 1;
-	setp.eq.s32	%p114, %r21, %r33;
-	@%p114 bra 	BB10_135;
-	bra.uni 	BB10_136;
-
-BB10_135:
-	mov.f32 	%f1847, %f1864;
-	mov.f32 	%f1846, %f1865;
-	mov.f32 	%f1845, %f1866;
-	mov.f32 	%f1844, %f1867;
-	mov.f32 	%f1843, %f1868;
-	mov.f32 	%f1842, %f1869;
-	mov.f32 	%f1841, %f1870;
-	mov.f32 	%f1840, %f1871;
-	mov.f32 	%f1839, %f1872;
-	mov.f32 	%f1838, %f1873;
-	mov.f32 	%f1837, %f1874;
-	mov.f32 	%f1836, %f1875;
-	bra.uni 	BB10_137;
-
-BB10_136:
-	mul.f32 	%f1368, %f1843, %f1873;
-	fma.rn.f32 	%f1369, %f1839, %f1872, %f1368;
-	fma.rn.f32 	%f459, %f1847, %f1874, %f1369;
-	mul.f32 	%f1370, %f1842, %f1873;
-	fma.rn.f32 	%f1371, %f1838, %f1872, %f1370;
-	fma.rn.f32 	%f460, %f1846, %f1874, %f1371;
-	mul.f32 	%f1372, %f1841, %f1873;
-	fma.rn.f32 	%f1373, %f1837, %f1872, %f1372;
-	fma.rn.f32 	%f461, %f1845, %f1874, %f1373;
-	mul.f32 	%f1374, %f1840, %f1873;
-	fma.rn.f32 	%f1375, %f1836, %f1872, %f1374;
-	fma.rn.f32 	%f1376, %f1844, %f1874, %f1375;
-	add.f32 	%f462, %f1875, %f1376;
-	mul.f32 	%f1377, %f1843, %f1869;
-	fma.rn.f32 	%f1378, %f1839, %f1868, %f1377;
-	fma.rn.f32 	%f463, %f1847, %f1870, %f1378;
-	mul.f32 	%f1379, %f1842, %f1869;
-	fma.rn.f32 	%f1380, %f1838, %f1868, %f1379;
-	fma.rn.f32 	%f464, %f1846, %f1870, %f1380;
-	mul.f32 	%f1381, %f1841, %f1869;
-	fma.rn.f32 	%f1382, %f1837, %f1868, %f1381;
-	fma.rn.f32 	%f465, %f1845, %f1870, %f1382;
-	mul.f32 	%f1383, %f1840, %f1869;
-	fma.rn.f32 	%f1384, %f1836, %f1868, %f1383;
-	fma.rn.f32 	%f1385, %f1844, %f1870, %f1384;
-	add.f32 	%f466, %f1871, %f1385;
-	mul.f32 	%f1386, %f1843, %f1865;
-	fma.rn.f32 	%f1387, %f1839, %f1864, %f1386;
-	fma.rn.f32 	%f1847, %f1847, %f1866, %f1387;
-	mul.f32 	%f1388, %f1842, %f1865;
-	fma.rn.f32 	%f1389, %f1838, %f1864, %f1388;
-	fma.rn.f32 	%f1846, %f1846, %f1866, %f1389;
-	mul.f32 	%f1390, %f1841, %f1865;
-	fma.rn.f32 	%f1391, %f1837, %f1864, %f1390;
-	fma.rn.f32 	%f1845, %f1845, %f1866, %f1391;
-	mul.f32 	%f1392, %f1840, %f1865;
-	fma.rn.f32 	%f1393, %f1836, %f1864, %f1392;
-	fma.rn.f32 	%f1394, %f1844, %f1866, %f1393;
-	add.f32 	%f1844, %f1867, %f1394;
-	mov.f32 	%f1843, %f463;
-	mov.f32 	%f1842, %f464;
-	mov.f32 	%f1841, %f465;
-	mov.f32 	%f1840, %f466;
-	mov.f32 	%f1839, %f459;
-	mov.f32 	%f1838, %f460;
-	mov.f32 	%f1837, %f461;
-	mov.f32 	%f1836, %f462;
-
-BB10_137:
-	add.s32 	%r736, %r21, -2;
-	setp.gt.s32	%p115, %r736, -1;
-	@%p115 bra 	BB10_122;
-
-BB10_138:
-	mov.u32 	%r737, 0;
-	mov.f32 	%f1900, %f1901;
-	mov.f32 	%f1905, %f1901;
-	mov.f32 	%f1904, %f1902;
-	mov.f32 	%f1903, %f1901;
-	mov.f32 	%f1908, %f1901;
-	mov.f32 	%f1907, %f1901;
-	mov.f32 	%f1906, %f1902;
-	@%p5 bra 	BB10_156;
-
-BB10_139:
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r424,%r425,%r426,%r427}, [%rd371];
+	// end inline asm
+	mov.b32 	%f1282, %r424;
+	mov.b32 	%f1283, %r425;
+	mov.b32 	%f1284, %r426;
+	mov.b32 	%f1285, %r427;
+	mul.f32 	%f1286, %f385, %f1282;
+	mul.f32 	%f1287, %f385, %f1283;
+	mul.f32 	%f1288, %f385, %f1284;
+	mul.f32 	%f1289, %f385, %f1285;
+	fma.rn.f32 	%f1921, %f1265, %f1921, %f1286;
+	fma.rn.f32 	%f1290, %f1265, %f1922, %f1287;
+	fma.rn.f32 	%f1291, %f1265, %f1923, %f1288;
+	fma.rn.f32 	%f1292, %f1265, %f1924, %f1289;
+	add.s64 	%rd375, %rd353, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd374, %rd375;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r428,%r429,%r430,%r431}, [%rd374];
+	// end inline asm
+	mov.b32 	%f1293, %r428;
+	mov.b32 	%f1294, %r429;
+	mov.b32 	%f1295, %r430;
+	mov.b32 	%f1296, %r431;
+	mul.f32 	%f1297, %f385, %f1293;
+	mul.f32 	%f1298, %f385, %f1294;
+	mul.f32 	%f1299, %f385, %f1295;
+	mul.f32 	%f1300, %f385, %f1296;
+	fma.rn.f32 	%f1301, %f1265, %f1925, %f1297;
+	fma.rn.f32 	%f1926, %f1265, %f1926, %f1298;
+	fma.rn.f32 	%f1927, %f1265, %f1927, %f1299;
+	fma.rn.f32 	%f1928, %f1265, %f1928, %f1300;
+	mul.f32 	%f1302, %f1291, %f1291;
+	fma.rn.f32 	%f1303, %f1290, %f1290, %f1302;
+	fma.rn.f32 	%f1304, %f1292, %f1292, %f1303;
+	fma.rn.f32 	%f1305, %f1301, %f1301, %f1304;
+	sqrt.rn.f32 	%f1306, %f1305;
+	rcp.rn.f32 	%f1307, %f1306;
+	mul.f32 	%f1922, %f1290, %f1307;
+	mul.f32 	%f1923, %f1291, %f1307;
+	mul.f32 	%f1924, %f1292, %f1307;
+	mul.f32 	%f1925, %f1307, %f1301;
+
+$L__BB10_68:
+	mul.f32 	%f1308, %f1923, %f1923;
+	fma.rn.f32 	%f1309, %f1922, %f1922, %f1308;
+	fma.rn.f32 	%f1310, %f1924, %f1924, %f1309;
+	fma.rn.f32 	%f1311, %f1925, %f1925, %f1310;
+	rcp.rn.f32 	%f1312, %f1311;
+	mul.f32 	%f1313, %f1922, %f1312;
+	mul.f32 	%f1314, %f1923, %f1312;
+	mul.f32 	%f1315, %f1924, %f1312;
+	mul.f32 	%f1316, %f1925, %f1312;
+	mul.f32 	%f1317, %f1922, %f1313;
+	mul.f32 	%f1318, %f1923, %f1314;
+	mul.f32 	%f1319, %f1924, %f1315;
+	mul.f32 	%f1320, %f1922, %f1314;
+	mul.f32 	%f1321, %f1924, %f1316;
+	mul.f32 	%f1322, %f1922, %f1315;
+	mul.f32 	%f1323, %f1923, %f1316;
+	mul.f32 	%f1324, %f1923, %f1315;
+	mul.f32 	%f1325, %f1922, %f1316;
+	sub.f32 	%f1326, %f1317, %f1318;
+	sub.f32 	%f1327, %f1326, %f1319;
+	fma.rn.f32 	%f1328, %f1925, %f1316, %f1327;
+	sub.f32 	%f1329, %f1320, %f1321;
+	add.f32 	%f1330, %f1329, %f1329;
+	add.f32 	%f1331, %f1322, %f1323;
+	add.f32 	%f1332, %f1331, %f1331;
+	add.f32 	%f1333, %f1320, %f1321;
+	add.f32 	%f1334, %f1333, %f1333;
+	sub.f32 	%f1335, %f1318, %f1317;
+	sub.f32 	%f1336, %f1335, %f1319;
+	fma.rn.f32 	%f1337, %f1925, %f1316, %f1336;
+	sub.f32 	%f1338, %f1324, %f1325;
+	add.f32 	%f1339, %f1338, %f1338;
+	sub.f32 	%f1340, %f1322, %f1323;
+	add.f32 	%f1341, %f1340, %f1340;
+	add.f32 	%f1342, %f1324, %f1325;
+	add.f32 	%f1343, %f1342, %f1342;
+	neg.f32 	%f1344, %f1317;
+	sub.f32 	%f1345, %f1344, %f1318;
+	add.f32 	%f1346, %f1319, %f1345;
+	fma.rn.f32 	%f1347, %f1925, %f1316, %f1346;
+	mul.f32 	%f1348, %f1916, %f1328;
+	fma.rn.f32 	%f1349, %f1919, %f1330, %f1348;
+	fma.rn.f32 	%f1350, %f1921, %f1332, %f1349;
+	sub.f32 	%f1940, %f1926, %f1350;
+	mul.f32 	%f1351, %f1919, %f1337;
+	fma.rn.f32 	%f1352, %f1916, %f1334, %f1351;
+	fma.rn.f32 	%f1353, %f1921, %f1339, %f1352;
+	sub.f32 	%f1936, %f1927, %f1353;
+	mul.f32 	%f1354, %f1919, %f1343;
+	fma.rn.f32 	%f1355, %f1916, %f1341, %f1354;
+	fma.rn.f32 	%f1356, %f1921, %f1347, %f1355;
+	sub.f32 	%f1932, %f1928, %f1356;
+	mul.f32 	%f1357, %f1915, %f1328;
+	fma.rn.f32 	%f1358, %f1918, %f1330, %f1357;
+	fma.rn.f32 	%f1939, %f1920, %f1332, %f1358;
+	mul.f32 	%f1359, %f1918, %f1337;
+	fma.rn.f32 	%f1360, %f1915, %f1334, %f1359;
+	fma.rn.f32 	%f1935, %f1920, %f1339, %f1360;
+	mul.f32 	%f1361, %f1918, %f1343;
+	fma.rn.f32 	%f1362, %f1915, %f1341, %f1361;
+	fma.rn.f32 	%f1931, %f1920, %f1347, %f1362;
+	mul.f32 	%f1363, %f1914, %f1328;
+	fma.rn.f32 	%f1938, %f1917, %f1330, %f1363;
+	mul.f32 	%f1364, %f1917, %f1337;
+	fma.rn.f32 	%f1934, %f1914, %f1334, %f1364;
+	mul.f32 	%f1365, %f1917, %f1343;
+	fma.rn.f32 	%f1930, %f1914, %f1341, %f1365;
+	mul.f32 	%f1937, %f1913, %f1328;
+	mul.f32 	%f1933, %f1913, %f1334;
+	mul.f32 	%f1929, %f1913, %f1341;
+	bra.uni 	$L__BB10_71;
+
+$L__BB10_63:
+	// begin inline asm
+	call (%rd650), _optix_get_instance_transform_from_handle, (%rd305);
+	// end inline asm
+
+$L__BB10_64:
+	// begin inline asm
+	cvta.to.global.u64 %rd311, %rd650;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r345,%r346,%r347,%r348}, [%rd311];
+	// end inline asm
+	mov.b32 	%f1937, %r345;
+	mov.b32 	%f1938, %r346;
+	mov.b32 	%f1939, %r347;
+	mov.b32 	%f1940, %r348;
+	add.s64 	%rd315, %rd650, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd314, %rd315;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r349,%r350,%r351,%r352}, [%rd314];
+	// end inline asm
+	mov.b32 	%f1933, %r349;
+	mov.b32 	%f1934, %r350;
+	mov.b32 	%f1935, %r351;
+	mov.b32 	%f1936, %r352;
+	add.s64 	%rd318, %rd650, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd317, %rd318;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r353,%r354,%r355,%r356}, [%rd317];
+	// end inline asm
+	mov.b32 	%f1929, %r353;
+	mov.b32 	%f1930, %r354;
+	mov.b32 	%f1931, %r355;
+	mov.b32 	%f1932, %r356;
+
+$L__BB10_71:
+	setp.eq.s32 	%p37, %r648, 1;
+	@%p37 bra 	$L__BB10_73;
+
+	mul.f32 	%f1403, %f1908, %f1938;
+	fma.rn.f32 	%f1404, %f1904, %f1937, %f1403;
+	fma.rn.f32 	%f483, %f1912, %f1939, %f1404;
+	mul.f32 	%f1405, %f1907, %f1938;
+	fma.rn.f32 	%f1406, %f1903, %f1937, %f1405;
+	fma.rn.f32 	%f484, %f1911, %f1939, %f1406;
+	mul.f32 	%f1407, %f1906, %f1938;
+	fma.rn.f32 	%f1408, %f1902, %f1937, %f1407;
+	fma.rn.f32 	%f485, %f1910, %f1939, %f1408;
+	mul.f32 	%f1409, %f1905, %f1938;
+	fma.rn.f32 	%f1410, %f1901, %f1937, %f1409;
+	fma.rn.f32 	%f1411, %f1909, %f1939, %f1410;
+	add.f32 	%f1940, %f1940, %f1411;
+	mul.f32 	%f1412, %f1908, %f1934;
+	fma.rn.f32 	%f1413, %f1904, %f1933, %f1412;
+	fma.rn.f32 	%f487, %f1912, %f1935, %f1413;
+	mul.f32 	%f1414, %f1907, %f1934;
+	fma.rn.f32 	%f1415, %f1903, %f1933, %f1414;
+	fma.rn.f32 	%f488, %f1911, %f1935, %f1415;
+	mul.f32 	%f1416, %f1906, %f1934;
+	fma.rn.f32 	%f1417, %f1902, %f1933, %f1416;
+	fma.rn.f32 	%f489, %f1910, %f1935, %f1417;
+	mul.f32 	%f1418, %f1905, %f1934;
+	fma.rn.f32 	%f1419, %f1901, %f1933, %f1418;
+	fma.rn.f32 	%f1420, %f1909, %f1935, %f1419;
+	add.f32 	%f1936, %f1936, %f1420;
+	mul.f32 	%f1421, %f1908, %f1930;
+	fma.rn.f32 	%f1422, %f1904, %f1929, %f1421;
+	fma.rn.f32 	%f491, %f1912, %f1931, %f1422;
+	mul.f32 	%f1423, %f1907, %f1930;
+	fma.rn.f32 	%f1424, %f1903, %f1929, %f1423;
+	fma.rn.f32 	%f492, %f1911, %f1931, %f1424;
+	mul.f32 	%f1425, %f1906, %f1930;
+	fma.rn.f32 	%f1426, %f1902, %f1929, %f1425;
+	fma.rn.f32 	%f493, %f1910, %f1931, %f1426;
+	mul.f32 	%f1427, %f1905, %f1930;
+	fma.rn.f32 	%f1428, %f1901, %f1929, %f1427;
+	fma.rn.f32 	%f1429, %f1909, %f1931, %f1428;
+	add.f32 	%f1932, %f1932, %f1429;
+	mov.f32 	%f1929, %f491;
+	mov.f32 	%f1930, %f492;
+	mov.f32 	%f1931, %f493;
+	mov.f32 	%f1933, %f487;
+	mov.f32 	%f1934, %f488;
+	mov.f32 	%f1935, %f489;
+	mov.f32 	%f1937, %f483;
+	mov.f32 	%f1938, %f484;
+	mov.f32 	%f1939, %f485;
+
+$L__BB10_73:
+	add.s32 	%r648, %r648, -1;
+	add.s32 	%r647, %r647, -1;
+	setp.gt.s32 	%p38, %r647, 1;
+	mov.f32 	%f1901, %f1940;
+	mov.f32 	%f1902, %f1939;
+	mov.f32 	%f1903, %f1938;
+	mov.f32 	%f1904, %f1937;
+	mov.f32 	%f1905, %f1936;
+	mov.f32 	%f1906, %f1935;
+	mov.f32 	%f1907, %f1934;
+	mov.f32 	%f1908, %f1933;
+	mov.f32 	%f1909, %f1932;
+	mov.f32 	%f1910, %f1931;
+	mov.f32 	%f1911, %f1930;
+	mov.f32 	%f1912, %f1929;
+	@%p38 bra 	$L__BB10_59;
+
+$L__BB10_74:
+	// begin inline asm
+	call (%r491), _optix_get_transform_list_size, ();
+	// end inline asm
+	setp.eq.s32 	%p39, %r491, 0;
+	mov.f32 	%f2001, %f2000;
+	mov.f32 	%f1996, %f2000;
+	mov.f32 	%f1997, %f1999;
+	mov.f32 	%f1998, %f2000;
+	mov.f32 	%f1993, %f2000;
+	mov.f32 	%f1994, %f2000;
+	mov.f32 	%f1995, %f1999;
+	@%p39 bra 	$L__BB10_93;
+
+	// begin inline asm
+	call (%r492), _optix_get_transform_list_size, ();
+	// end inline asm
+	// begin inline asm
+	call (%f1439), _optix_get_ray_time, ();
+	// end inline asm
+	setp.eq.s32 	%p40, %r492, 0;
+	@%p40 bra 	$L__BB10_93;
+
+	mov.u32 	%r649, 0;
+
+$L__BB10_77:
 	.pragma "nounroll";
-	// inline asm
-	call (%rd436), _optix_get_transform_list_handle, (%r737);
-	// inline asm
-	// inline asm
-	call (%r583), _optix_get_transform_type_from_handle, (%rd436);
-	// inline asm
-	and.b32  	%r584, %r583, -2;
-	setp.eq.s32	%p117, %r584, 2;
-	@%p117 bra 	BB10_145;
-	bra.uni 	BB10_140;
-
-BB10_145:
-	setp.eq.s32	%p120, %r583, 2;
-	@%p120 bra 	BB10_149;
-	bra.uni 	BB10_146;
-
-BB10_149:
-	// inline asm
-	call (%rd510), _optix_get_matrix_motion_transform_from_handle, (%rd436);
-	// inline asm
-	// inline asm
-	cvta.to.global.u64 %rd512, %rd510;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r672,%r673,%r674,%r675}, [%rd512];
-	// inline asm
-	mov.b32	{%rs19, %rs20}, %r674;
-	add.s64 	%rd516, %rd510, 16;
-	// inline asm
-	cvta.to.global.u64 %rd515, %rd516;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r676,%r677,%r678,%r679}, [%rd515];
-	// inline asm
-	add.s64 	%rd519, %rd510, 32;
-	// inline asm
-	cvta.to.global.u64 %rd518, %rd519;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r680,%r681,%r682,%r683}, [%rd518];
-	// inline asm
-	add.s64 	%rd522, %rd510, 48;
-	// inline asm
-	cvta.to.global.u64 %rd521, %rd522;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r684,%r685,%r686,%r687}, [%rd521];
-	// inline asm
-	add.s64 	%rd525, %rd510, 64;
-	// inline asm
-	cvta.to.global.u64 %rd524, %rd525;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r688,%r689,%r690,%r691}, [%rd524];
-	// inline asm
-	add.s64 	%rd528, %rd510, 80;
-	// inline asm
-	cvta.to.global.u64 %rd527, %rd528;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r692,%r693,%r694,%r695}, [%rd527];
-	// inline asm
-	add.s64 	%rd531, %rd510, 96;
-	// inline asm
-	cvta.to.global.u64 %rd530, %rd531;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r696,%r697,%r698,%r699}, [%rd530];
-	// inline asm
-	add.s64 	%rd534, %rd510, 112;
-	// inline asm
-	cvta.to.global.u64 %rd533, %rd534;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r700,%r701,%r702,%r703}, [%rd533];
-	// inline asm
-	mov.b32 	 %f1506, %r675;
-	mov.b32 	 %f1507, %r676;
-	cvt.u32.u16	%r716, %rs19;
-	add.s32 	%r717, %r716, -1;
-	cvt.rn.f32.s32	%f1508, %r717;
-	sub.f32 	%f1509, %f940, %f1506;
-	mul.f32 	%f1510, %f1509, %f1508;
-	sub.f32 	%f1511, %f1507, %f1506;
-	div.rn.f32 	%f1512, %f1510, %f1511;
-	min.f32 	%f1513, %f1508, %f1512;
-	mov.f32 	%f1514, 0f00000000;
-	max.f32 	%f1515, %f1514, %f1513;
-	cvt.rmi.f32.f32	%f1516, %f1515;
-	cvt.rzi.s32.f32	%r718, %f1516;
-	cvt.s64.s32	%rd43, %r718;
-	mul.wide.s32 	%rd545, %r718, 48;
-	add.s64 	%rd537, %rd519, %rd545;
-	// inline asm
-	cvta.to.global.u64 %rd536, %rd537;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r704,%r705,%r706,%r707}, [%rd536];
-	// inline asm
-	mov.b32 	 %f1925, %r704;
-	mov.b32 	 %f1926, %r705;
-	mov.b32 	 %f1927, %r706;
-	add.s64 	%rd540, %rd537, 16;
-	// inline asm
-	cvta.to.global.u64 %rd539, %rd540;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r708,%r709,%r710,%r711}, [%rd539];
-	// inline asm
-	mov.b32 	 %f1922, %r708;
-	mov.b32 	 %f1923, %r709;
-	mov.b32 	 %f1924, %r710;
-	add.s64 	%rd543, %rd537, 32;
-	// inline asm
-	cvta.to.global.u64 %rd542, %rd543;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r712,%r713,%r714,%r715}, [%rd542];
-	// inline asm
-	sub.f32 	%f559, %f1515, %f1516;
-	mov.b32 	 %f1919, %r712;
-	mov.b32 	 %f1920, %r713;
-	mov.b32 	 %f1921, %r714;
-	setp.leu.f32	%p122, %f559, 0f00000000;
-	@%p122 bra 	BB10_151;
-
-	mul.lo.s64 	%rd555, %rd43, 48;
-	add.s64 	%rd556, %rd510, %rd555;
-	add.s64 	%rd547, %rd556, 80;
-	// inline asm
-	cvta.to.global.u64 %rd546, %rd547;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r719,%r720,%r721,%r722}, [%rd546];
-	// inline asm
-	mov.b32 	 %f1517, %r719;
-	mov.b32 	 %f1518, %r720;
-	mov.b32 	 %f1519, %r721;
-	add.s64 	%rd550, %rd556, 96;
-	// inline asm
-	cvta.to.global.u64 %rd549, %rd550;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r723,%r724,%r725,%r726}, [%rd549];
-	// inline asm
-	mov.b32 	 %f1520, %r723;
-	mov.b32 	 %f1521, %r724;
-	mov.b32 	 %f1522, %r725;
-	add.s64 	%rd553, %rd556, 112;
-	// inline asm
-	cvta.to.global.u64 %rd552, %rd553;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r727,%r728,%r729,%r730}, [%rd552];
-	// inline asm
-	mov.f32 	%f1523, 0f3F800000;
-	sub.f32 	%f1524, %f1523, %f559;
-	mul.f32 	%f1525, %f559, %f1517;
-	mul.f32 	%f1526, %f559, %f1518;
-	mul.f32 	%f1527, %f559, %f1519;
-	fma.rn.f32 	%f1925, %f1524, %f1925, %f1525;
-	fma.rn.f32 	%f1926, %f1524, %f1926, %f1526;
-	fma.rn.f32 	%f1927, %f1524, %f1927, %f1527;
-	mul.f32 	%f1528, %f559, %f1520;
-	mul.f32 	%f1529, %f559, %f1521;
-	mul.f32 	%f1530, %f559, %f1522;
-	fma.rn.f32 	%f1922, %f1524, %f1922, %f1528;
-	fma.rn.f32 	%f1923, %f1524, %f1923, %f1529;
-	fma.rn.f32 	%f1924, %f1524, %f1924, %f1530;
-	mov.b32 	 %f1531, %r727;
-	mov.b32 	 %f1532, %r728;
-	mov.b32 	 %f1533, %r729;
-	mul.f32 	%f1534, %f559, %f1531;
-	mul.f32 	%f1535, %f559, %f1532;
-	mul.f32 	%f1536, %f559, %f1533;
-	fma.rn.f32 	%f1919, %f1524, %f1919, %f1534;
-	fma.rn.f32 	%f1920, %f1524, %f1920, %f1535;
-	fma.rn.f32 	%f1921, %f1524, %f1921, %f1536;
-	bra.uni 	BB10_151;
-
-BB10_140:
-	mov.f32 	%f1928, 0f00000000;
-	mov.f32 	%f1930, 0f3F800000;
-	setp.eq.s32	%p118, %r583, 4;
-	@%p118 bra 	BB10_143;
-	bra.uni 	BB10_141;
-
-BB10_143:
-	// inline asm
-	call (%rd666), _optix_get_instance_inverse_transform_from_handle, (%rd436);
-	// inline asm
-	bra.uni 	BB10_144;
-
-BB10_146:
-	// inline asm
-	call (%rd451), _optix_get_srt_motion_transform_from_handle, (%rd436);
-	// inline asm
-	// inline asm
-	cvta.to.global.u64 %rd453, %rd451;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r597,%r598,%r599,%r600}, [%rd453];
-	// inline asm
-	mov.b32	{%rs17, %rs18}, %r599;
-	add.s64 	%rd457, %rd451, 16;
-	// inline asm
-	cvta.to.global.u64 %rd456, %rd457;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r601,%r602,%r603,%r604}, [%rd456];
-	// inline asm
-	add.s64 	%rd460, %rd451, 32;
-	// inline asm
-	cvta.to.global.u64 %rd459, %rd460;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r605,%r606,%r607,%r608}, [%rd459];
-	// inline asm
-	add.s64 	%rd463, %rd451, 48;
-	// inline asm
-	cvta.to.global.u64 %rd462, %rd463;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r609,%r610,%r611,%r612}, [%rd462];
-	// inline asm
-	add.s64 	%rd466, %rd451, 64;
-	// inline asm
-	cvta.to.global.u64 %rd465, %rd466;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r613,%r614,%r615,%r616}, [%rd465];
-	// inline asm
-	add.s64 	%rd469, %rd451, 80;
-	// inline asm
-	cvta.to.global.u64 %rd468, %rd469;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r617,%r618,%r619,%r620}, [%rd468];
-	// inline asm
-	add.s64 	%rd472, %rd451, 96;
-	// inline asm
-	cvta.to.global.u64 %rd471, %rd472;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r621,%r622,%r623,%r624}, [%rd471];
-	// inline asm
-	add.s64 	%rd475, %rd451, 112;
-	// inline asm
-	cvta.to.global.u64 %rd474, %rd475;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r625,%r626,%r627,%r628}, [%rd474];
-	// inline asm
-	add.s64 	%rd478, %rd451, 128;
-	// inline asm
-	cvta.to.global.u64 %rd477, %rd478;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r629,%r630,%r631,%r632}, [%rd477];
-	// inline asm
-	add.s64 	%rd481, %rd451, 144;
-	// inline asm
-	cvta.to.global.u64 %rd480, %rd481;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r633,%r634,%r635,%r636}, [%rd480];
-	// inline asm
-	mov.b32 	 %f1414, %r600;
-	mov.b32 	 %f1415, %r601;
-	cvt.u32.u16	%r653, %rs17;
-	add.s32 	%r654, %r653, -1;
-	cvt.rn.f32.s32	%f1416, %r654;
-	sub.f32 	%f1417, %f940, %f1414;
-	mul.f32 	%f1418, %f1417, %f1416;
-	sub.f32 	%f1419, %f1415, %f1414;
-	div.rn.f32 	%f1420, %f1418, %f1419;
-	min.f32 	%f1421, %f1416, %f1420;
-	mov.f32 	%f1422, 0f00000000;
-	max.f32 	%f1423, %f1422, %f1421;
-	cvt.rmi.f32.f32	%f1424, %f1423;
-	cvt.rzi.s32.f32	%r655, %f1424;
-	cvt.s64.s32	%rd41, %r655;
-	mul.wide.s32 	%rd495, %r655, 64;
-	add.s64 	%rd484, %rd460, %rd495;
-	// inline asm
-	cvta.to.global.u64 %rd483, %rd484;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r637,%r638,%r639,%r640}, [%rd483];
-	// inline asm
-	mov.b32 	 %f1909, %r637;
-	mov.b32 	 %f1910, %r638;
-	mov.b32 	 %f1911, %r639;
-	add.s64 	%rd487, %rd484, 16;
-	// inline asm
-	cvta.to.global.u64 %rd486, %rd487;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r641,%r642,%r643,%r644}, [%rd486];
-	// inline asm
-	mov.b32 	 %f1912, %r641;
-	mov.b32 	 %f1913, %r642;
-	mov.b32 	 %f1914, %r644;
-	add.s64 	%rd490, %rd484, 32;
-	// inline asm
-	cvta.to.global.u64 %rd489, %rd490;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r645,%r646,%r647,%r648}, [%rd489];
-	// inline asm
-	sub.f32 	%f519, %f1423, %f1424;
-	mov.b32 	 %f1915, %r646;
-	mov.b32 	 %f1916, %r647;
-	mov.b32 	 %f1917, %r648;
-	add.s64 	%rd493, %rd484, 48;
-	// inline asm
-	cvta.to.global.u64 %rd492, %rd493;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r649,%r650,%r651,%r652}, [%rd492];
-	// inline asm
-	mov.b32 	 %f1918, %r649;
-	setp.leu.f32	%p121, %f519, 0f00000000;
-	@%p121 bra 	BB10_148;
-
-	shl.b64 	%rd508, %rd41, 6;
-	add.s64 	%rd509, %rd508, %rd451;
-	add.s64 	%rd497, %rd509, 96;
-	// inline asm
-	cvta.to.global.u64 %rd496, %rd497;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r656,%r657,%r658,%r659}, [%rd496];
-	// inline asm
-	mov.b32 	 %f1425, %r656;
-	mov.b32 	 %f1426, %r657;
-	mov.b32 	 %f1427, %r658;
-	add.s64 	%rd500, %rd509, 112;
-	// inline asm
-	cvta.to.global.u64 %rd499, %rd500;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r660,%r661,%r662,%r663}, [%rd499];
-	// inline asm
-	mov.b32 	 %f1428, %r660;
-	mov.b32 	 %f1429, %r661;
-	mov.b32 	 %f1430, %r663;
-	add.s64 	%rd503, %rd509, 128;
-	// inline asm
-	cvta.to.global.u64 %rd502, %rd503;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r664,%r665,%r666,%r667}, [%rd502];
-	// inline asm
-	mov.b32 	 %f1431, %r665;
-	mov.b32 	 %f1432, %r666;
-	mov.b32 	 %f1433, %r667;
-	add.s64 	%rd506, %rd509, 144;
-	// inline asm
-	cvta.to.global.u64 %rd505, %rd506;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r668,%r669,%r670,%r671}, [%rd505];
-	// inline asm
-	mov.f32 	%f1434, 0f3F800000;
-	sub.f32 	%f1435, %f1434, %f519;
-	mul.f32 	%f1436, %f519, %f1425;
-	mul.f32 	%f1437, %f519, %f1426;
-	mul.f32 	%f1438, %f519, %f1427;
-	fma.rn.f32 	%f1909, %f1435, %f1909, %f1436;
-	fma.rn.f32 	%f1910, %f1435, %f1910, %f1437;
-	fma.rn.f32 	%f1911, %f1435, %f1911, %f1438;
-	mul.f32 	%f1439, %f519, %f1428;
-	mul.f32 	%f1440, %f519, %f1429;
-	mul.f32 	%f1441, %f519, %f1430;
-	fma.rn.f32 	%f1912, %f1435, %f1912, %f1439;
-	fma.rn.f32 	%f1913, %f1435, %f1913, %f1440;
-	fma.rn.f32 	%f1914, %f1435, %f1914, %f1441;
-	mul.f32 	%f1442, %f519, %f1431;
-	mul.f32 	%f1443, %f519, %f1432;
-	mul.f32 	%f1444, %f519, %f1433;
-	fma.rn.f32 	%f1445, %f1435, %f1915, %f1442;
-	fma.rn.f32 	%f1446, %f1435, %f1916, %f1443;
-	fma.rn.f32 	%f1447, %f1435, %f1917, %f1444;
-	mov.b32 	 %f1448, %r668;
-	mul.f32 	%f1449, %f519, %f1448;
-	fma.rn.f32 	%f1450, %f1435, %f1918, %f1449;
-	mul.f32 	%f1451, %f1446, %f1446;
-	fma.rn.f32 	%f1452, %f1445, %f1445, %f1451;
-	fma.rn.f32 	%f1453, %f1447, %f1447, %f1452;
-	fma.rn.f32 	%f1454, %f1450, %f1450, %f1453;
-	sqrt.rn.f32 	%f1455, %f1454;
-	rcp.rn.f32 	%f1456, %f1455;
-	mul.f32 	%f1915, %f1445, %f1456;
-	mul.f32 	%f1916, %f1446, %f1456;
-	mul.f32 	%f1917, %f1447, %f1456;
-	mul.f32 	%f1918, %f1450, %f1456;
-
-BB10_148:
-	mul.f32 	%f1457, %f1916, %f1916;
-	fma.rn.f32 	%f1458, %f1915, %f1915, %f1457;
-	fma.rn.f32 	%f1459, %f1917, %f1917, %f1458;
-	fma.rn.f32 	%f1460, %f1918, %f1918, %f1459;
-	rcp.rn.f32 	%f1461, %f1460;
-	mul.f32 	%f1462, %f1915, %f1461;
-	mul.f32 	%f1463, %f1916, %f1461;
-	mul.f32 	%f1464, %f1917, %f1461;
-	mul.f32 	%f1465, %f1918, %f1461;
-	mul.f32 	%f1466, %f1915, %f1462;
-	mul.f32 	%f1467, %f1916, %f1463;
-	mul.f32 	%f1468, %f1917, %f1464;
-	mul.f32 	%f1469, %f1915, %f1463;
-	mul.f32 	%f1470, %f1917, %f1465;
-	mul.f32 	%f1471, %f1915, %f1464;
-	mul.f32 	%f1472, %f1916, %f1465;
-	mul.f32 	%f1473, %f1916, %f1464;
-	mul.f32 	%f1474, %f1915, %f1465;
-	sub.f32 	%f1475, %f1466, %f1467;
-	sub.f32 	%f1476, %f1475, %f1468;
-	fma.rn.f32 	%f1477, %f1918, %f1465, %f1476;
-	sub.f32 	%f1478, %f1469, %f1470;
-	add.f32 	%f1479, %f1478, %f1478;
-	add.f32 	%f1480, %f1471, %f1472;
-	add.f32 	%f1481, %f1480, %f1480;
-	add.f32 	%f1482, %f1469, %f1470;
-	add.f32 	%f1483, %f1482, %f1482;
-	sub.f32 	%f1484, %f1467, %f1466;
-	sub.f32 	%f1485, %f1484, %f1468;
-	fma.rn.f32 	%f1486, %f1918, %f1465, %f1485;
-	sub.f32 	%f1487, %f1473, %f1474;
-	add.f32 	%f1488, %f1487, %f1487;
-	sub.f32 	%f1489, %f1471, %f1472;
-	add.f32 	%f1490, %f1489, %f1489;
-	add.f32 	%f1491, %f1473, %f1474;
-	add.f32 	%f1492, %f1491, %f1491;
-	neg.f32 	%f1493, %f1466;
-	sub.f32 	%f1494, %f1493, %f1467;
-	add.f32 	%f1495, %f1468, %f1494;
-	fma.rn.f32 	%f1496, %f1918, %f1465, %f1495;
-	mul.f32 	%f1497, %f1911, %f1477;
-	fma.rn.f32 	%f1498, %f1913, %f1479, %f1497;
-	fma.rn.f32 	%f1927, %f1914, %f1481, %f1498;
-	mul.f32 	%f1499, %f1913, %f1486;
-	fma.rn.f32 	%f1500, %f1911, %f1483, %f1499;
-	fma.rn.f32 	%f1924, %f1914, %f1488, %f1500;
-	mul.f32 	%f1501, %f1913, %f1492;
-	fma.rn.f32 	%f1502, %f1911, %f1490, %f1501;
-	fma.rn.f32 	%f1921, %f1914, %f1496, %f1502;
-	mul.f32 	%f1503, %f1910, %f1477;
-	fma.rn.f32 	%f1926, %f1912, %f1479, %f1503;
-	mul.f32 	%f1504, %f1912, %f1486;
-	fma.rn.f32 	%f1923, %f1910, %f1483, %f1504;
-	mul.f32 	%f1505, %f1912, %f1492;
-	fma.rn.f32 	%f1920, %f1910, %f1490, %f1505;
-	mul.f32 	%f1925, %f1909, %f1477;
-	mul.f32 	%f1922, %f1909, %f1483;
-	mul.f32 	%f1919, %f1909, %f1490;
-
-BB10_151:
-	mul.f32 	%f1537, %f1920, %f1924;
-	mul.f32 	%f1538, %f1921, %f1923;
-	sub.f32 	%f1539, %f1538, %f1537;
-	mul.f32 	%f1540, %f1925, %f1539;
-	mul.f32 	%f1541, %f1919, %f1924;
-	mul.f32 	%f1542, %f1921, %f1922;
-	sub.f32 	%f1543, %f1542, %f1541;
-	mul.f32 	%f1544, %f1543, %f1926;
-	sub.f32 	%f1545, %f1540, %f1544;
-	mul.f32 	%f1546, %f1919, %f1923;
-	mul.f32 	%f1547, %f1920, %f1922;
-	sub.f32 	%f1548, %f1547, %f1546;
-	fma.rn.f32 	%f1549, %f1548, %f1927, %f1545;
-	rcp.rn.f32 	%f1550, %f1549;
-	mul.f32 	%f1934, %f1539, %f1550;
-	mul.f32 	%f1551, %f1921, %f1926;
-	mul.f32 	%f1552, %f1920, %f1927;
-	sub.f32 	%f1553, %f1552, %f1551;
-	mul.f32 	%f1935, %f1550, %f1553;
-	mul.f32 	%f1554, %f1923, %f1927;
-	mul.f32 	%f1555, %f1924, %f1926;
-	sub.f32 	%f1556, %f1555, %f1554;
-	mul.f32 	%f1936, %f1550, %f1556;
-	sub.f32 	%f1557, %f1541, %f1542;
-	mul.f32 	%f1931, %f1557, %f1550;
-	mul.f32 	%f1558, %f1919, %f1927;
-	mul.f32 	%f1559, %f1921, %f1925;
-	sub.f32 	%f1560, %f1559, %f1558;
-	mul.f32 	%f1932, %f1550, %f1560;
-	mul.f32 	%f1561, %f1924, %f1925;
-	mul.f32 	%f1562, %f1922, %f1927;
-	sub.f32 	%f1563, %f1562, %f1561;
-	mul.f32 	%f1933, %f1550, %f1563;
-	mul.f32 	%f1928, %f1548, %f1550;
-	mul.f32 	%f1564, %f1920, %f1925;
-	mul.f32 	%f1565, %f1919, %f1926;
-	sub.f32 	%f1566, %f1565, %f1564;
-	mul.f32 	%f1929, %f1566, %f1550;
-	mul.f32 	%f1567, %f1922, %f1926;
-	mul.f32 	%f1568, %f1923, %f1925;
-	sub.f32 	%f1569, %f1568, %f1567;
-	mul.f32 	%f1930, %f1569, %f1550;
-	bra.uni 	BB10_152;
-
-BB10_141:
-	setp.ne.s32	%p119, %r583, 1;
-	mov.f32 	%f1929, %f1928;
-	mov.f32 	%f1931, %f1928;
-	mov.f32 	%f1932, %f1930;
-	mov.f32 	%f1933, %f1928;
-	mov.f32 	%f1934, %f1930;
-	mov.f32 	%f1935, %f1928;
-	mov.f32 	%f1936, %f1928;
-	@%p119 bra 	BB10_152;
-
-	// inline asm
-	call (%rd438), _optix_get_static_transform_from_handle, (%rd436);
-	// inline asm
-	add.s64 	%rd666, %rd438, 64;
-
-BB10_144:
-	// inline asm
-	cvta.to.global.u64 %rd442, %rd666;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r585,%r586,%r587,%r588}, [%rd442];
-	// inline asm
-	mov.b32 	 %f1934, %r585;
-	mov.b32 	 %f1935, %r586;
-	mov.b32 	 %f1936, %r587;
-	add.s64 	%rd446, %rd666, 16;
-	// inline asm
-	cvta.to.global.u64 %rd445, %rd446;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r589,%r590,%r591,%r592}, [%rd445];
-	// inline asm
-	mov.b32 	 %f1931, %r589;
-	mov.b32 	 %f1932, %r590;
-	mov.b32 	 %f1933, %r591;
-	add.s64 	%rd449, %rd666, 32;
-	// inline asm
-	cvta.to.global.u64 %rd448, %rd449;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r593,%r594,%r595,%r596}, [%rd448];
-	// inline asm
-	mov.b32 	 %f1928, %r593;
-	mov.b32 	 %f1929, %r594;
-	mov.b32 	 %f1930, %r595;
-
-BB10_152:
-	setp.eq.s32	%p123, %r737, 0;
-	@%p123 bra 	BB10_153;
-	bra.uni 	BB10_154;
-
-BB10_153:
-	mov.f32 	%f1908, %f1928;
-	mov.f32 	%f1907, %f1929;
-	mov.f32 	%f1906, %f1930;
-	mov.f32 	%f1905, %f1931;
-	mov.f32 	%f1904, %f1932;
-	mov.f32 	%f1903, %f1933;
-	mov.f32 	%f1902, %f1934;
-	mov.f32 	%f1901, %f1935;
-	mov.f32 	%f1900, %f1936;
-	bra.uni 	BB10_155;
-
-BB10_154:
-	mul.f32 	%f1570, %f1905, %f1935;
-	fma.rn.f32 	%f1571, %f1902, %f1934, %f1570;
-	fma.rn.f32 	%f599, %f1908, %f1936, %f1571;
-	mul.f32 	%f1572, %f1904, %f1935;
-	fma.rn.f32 	%f1573, %f1901, %f1934, %f1572;
-	fma.rn.f32 	%f600, %f1907, %f1936, %f1573;
-	mul.f32 	%f1574, %f1903, %f1935;
-	fma.rn.f32 	%f1575, %f1900, %f1934, %f1574;
-	fma.rn.f32 	%f601, %f1906, %f1936, %f1575;
-	mul.f32 	%f1576, %f1905, %f1932;
-	fma.rn.f32 	%f1577, %f1902, %f1931, %f1576;
-	fma.rn.f32 	%f602, %f1908, %f1933, %f1577;
-	mul.f32 	%f1578, %f1904, %f1932;
-	fma.rn.f32 	%f1579, %f1901, %f1931, %f1578;
-	fma.rn.f32 	%f603, %f1907, %f1933, %f1579;
-	mul.f32 	%f1580, %f1903, %f1932;
-	fma.rn.f32 	%f1581, %f1900, %f1931, %f1580;
-	fma.rn.f32 	%f604, %f1906, %f1933, %f1581;
-	mul.f32 	%f1582, %f1905, %f1929;
-	fma.rn.f32 	%f1583, %f1902, %f1928, %f1582;
-	fma.rn.f32 	%f1908, %f1908, %f1930, %f1583;
-	mul.f32 	%f1584, %f1904, %f1929;
-	fma.rn.f32 	%f1585, %f1901, %f1928, %f1584;
-	fma.rn.f32 	%f1907, %f1907, %f1930, %f1585;
-	mul.f32 	%f1586, %f1903, %f1929;
-	fma.rn.f32 	%f1587, %f1900, %f1928, %f1586;
-	fma.rn.f32 	%f1906, %f1906, %f1930, %f1587;
-	mov.f32 	%f1905, %f602;
-	mov.f32 	%f1904, %f603;
-	mov.f32 	%f1903, %f604;
-	mov.f32 	%f1902, %f599;
-	mov.f32 	%f1901, %f600;
-	mov.f32 	%f1900, %f601;
-
-BB10_155:
-	add.s32 	%r737, %r737, 1;
-	setp.lt.u32	%p124, %r737, %r33;
-	@%p124 bra 	BB10_139;
-
-BB10_156:
-	fma.rn.f32 	%f1588, %f1991, %f1839, %f1836;
-	fma.rn.f32 	%f1589, %f1992, %f1838, %f1588;
-	fma.rn.f32 	%f1590, %f1991, %f1843, %f1840;
-	fma.rn.f32 	%f1591, %f1992, %f1842, %f1590;
-	fma.rn.f32 	%f1592, %f1991, %f1847, %f1844;
-	fma.rn.f32 	%f1593, %f1992, %f1846, %f1592;
-	fma.rn.f32 	%f1991, %f1993, %f1837, %f1589;
-	fma.rn.f32 	%f1992, %f1993, %f1841, %f1591;
-	fma.rn.f32 	%f1993, %f1993, %f1845, %f1593;
-	ld.const.u64 	%rd557, [params+112];
-	setp.eq.s64	%p125, %rd557, 0;
-	mov.f32 	%f1985, %f1988;
-	mov.f32 	%f1986, %f1989;
-	mov.f32 	%f1987, %f1990;
-	@%p125 bra 	BB10_158;
-
-	mul.f32 	%f1594, %f1988, %f1902;
-	fma.rn.f32 	%f1595, %f1989, %f1905, %f1594;
-	mul.f32 	%f1596, %f1988, %f1901;
-	fma.rn.f32 	%f1597, %f1989, %f1904, %f1596;
-	mul.f32 	%f1598, %f1988, %f1900;
-	fma.rn.f32 	%f1599, %f1989, %f1903, %f1598;
-	fma.rn.f32 	%f1600, %f1990, %f1908, %f1595;
-	fma.rn.f32 	%f1601, %f1990, %f1907, %f1597;
-	fma.rn.f32 	%f1602, %f1990, %f1906, %f1599;
-	mul.f32 	%f1603, %f1600, %f1600;
-	fma.rn.f32 	%f1604, %f1601, %f1601, %f1603;
-	fma.rn.f32 	%f1605, %f1602, %f1602, %f1604;
-	sqrt.rn.f32 	%f1606, %f1605;
-	div.rn.f32 	%f1985, %f1600, %f1606;
-	div.rn.f32 	%f1986, %f1601, %f1606;
-	div.rn.f32 	%f1987, %f1602, %f1606;
-
-BB10_158:
-	ld.const.u64 	%rd558, [params+136];
-	setp.eq.s64	%p126, %rd558, 0;
-	@%p126 bra 	BB10_160;
-
-	mul.f32 	%f1607, %f1988, %f1902;
-	fma.rn.f32 	%f1608, %f1989, %f1905, %f1607;
-	mul.f32 	%f1609, %f1988, %f1901;
-	fma.rn.f32 	%f1610, %f1989, %f1904, %f1609;
-	mul.f32 	%f1611, %f1988, %f1900;
-	fma.rn.f32 	%f1612, %f1989, %f1903, %f1611;
-	fma.rn.f32 	%f1613, %f1990, %f1908, %f1608;
-	fma.rn.f32 	%f1614, %f1990, %f1907, %f1610;
-	fma.rn.f32 	%f1615, %f1990, %f1906, %f1612;
-	mul.f32 	%f1616, %f1613, %f1613;
-	fma.rn.f32 	%f1617, %f1614, %f1614, %f1616;
-	fma.rn.f32 	%f1618, %f1615, %f1615, %f1617;
-	sqrt.rn.f32 	%f1619, %f1618;
-	div.rn.f32 	%f1988, %f1613, %f1619;
-	div.rn.f32 	%f1989, %f1614, %f1619;
-	div.rn.f32 	%f1990, %f1615, %f1619;
-
-BB10_160:
-	ld.const.u64 	%rd559, [params+184];
-	setp.eq.s64	%p127, %rd559, 0;
-	mov.f32 	%f1974, 0f00000000;
-	mov.f32 	%f1975, 0f3F800000;
-	mov.f32 	%f1979, %f1973;
-	mov.f32 	%f1980, %f1974;
-	mov.f32 	%f1981, %f1975;
-	mov.f32 	%f1982, %f1976;
-	mov.f32 	%f1983, %f1975;
-	mov.f32 	%f1984, %f1974;
-	@%p127 bra 	BB10_162;
-
-	mul.f32 	%f1624, %f1976, %f1839;
-	mov.f32 	%f1625, 0f3F800000;
-	fma.rn.f32 	%f1626, %f1625, %f1838, %f1624;
-	mul.f32 	%f1627, %f1976, %f1843;
-	fma.rn.f32 	%f1628, %f1625, %f1842, %f1627;
-	mul.f32 	%f1629, %f1976, %f1847;
-	fma.rn.f32 	%f1630, %f1625, %f1846, %f1629;
-	mov.f32 	%f1631, 0f00000000;
-	fma.rn.f32 	%f1982, %f1631, %f1837, %f1626;
-	fma.rn.f32 	%f1983, %f1631, %f1841, %f1628;
-	fma.rn.f32 	%f1984, %f1631, %f1845, %f1630;
-	mul.f32 	%f1632, %f1973, %f1839;
-	fma.rn.f32 	%f1633, %f1631, %f1838, %f1632;
-	mul.f32 	%f1634, %f1973, %f1843;
-	fma.rn.f32 	%f1635, %f1631, %f1842, %f1634;
-	mul.f32 	%f1636, %f1973, %f1847;
-	fma.rn.f32 	%f1637, %f1631, %f1846, %f1636;
-	fma.rn.f32 	%f1979, %f1625, %f1837, %f1633;
-	fma.rn.f32 	%f1980, %f1625, %f1841, %f1635;
-	fma.rn.f32 	%f1981, %f1625, %f1845, %f1637;
-
-BB10_162:
-	ld.const.u64 	%rd560, [params+280];
-	ld.const.u64 	%rd561, [params+232];
-	or.b64  	%rd562, %rd560, %rd561;
-	setp.eq.s64	%p128, %rd562, 0;
-	@%p128 bra 	BB10_163;
-
-	mul.f32 	%f1642, %f1988, %f1839;
-	fma.rn.f32 	%f1643, %f1989, %f1843, %f1642;
-	mul.f32 	%f1644, %f1988, %f1838;
-	fma.rn.f32 	%f1645, %f1989, %f1842, %f1644;
-	mul.f32 	%f1646, %f1988, %f1837;
-	fma.rn.f32 	%f1647, %f1989, %f1841, %f1646;
-	fma.rn.f32 	%f1648, %f1990, %f1847, %f1643;
-	fma.rn.f32 	%f1649, %f1990, %f1846, %f1645;
-	fma.rn.f32 	%f1650, %f1990, %f1845, %f1647;
-	mul.f32 	%f1651, %f1648, %f1648;
-	fma.rn.f32 	%f1652, %f1649, %f1649, %f1651;
-	fma.rn.f32 	%f1653, %f1650, %f1650, %f1652;
-	sqrt.rn.f32 	%f1654, %f1653;
-	div.rn.f32 	%f1655, %f1648, %f1654;
-	div.rn.f32 	%f1656, %f1649, %f1654;
-	div.rn.f32 	%f1657, %f1650, %f1654;
-	mul.f32 	%f1658, %f1655, %f1902;
-	mul.f32 	%f1659, %f1655, %f1901;
-	mul.f32 	%f1660, %f1655, %f1900;
-	fma.rn.f32 	%f1661, %f1656, %f1905, %f1658;
-	fma.rn.f32 	%f1662, %f1656, %f1904, %f1659;
-	fma.rn.f32 	%f1663, %f1656, %f1903, %f1660;
-	fma.rn.f32 	%f1664, %f1657, %f1908, %f1661;
-	fma.rn.f32 	%f1665, %f1657, %f1907, %f1662;
-	fma.rn.f32 	%f1666, %f1657, %f1906, %f1663;
-	mul.f32 	%f1667, %f1664, %f1664;
-	fma.rn.f32 	%f1668, %f1665, %f1665, %f1667;
-	fma.rn.f32 	%f1669, %f1666, %f1666, %f1668;
-	sqrt.rn.f32 	%f1670, %f1669;
-	rcp.rn.f32 	%f1671, %f1670;
-	mul.f32 	%f1672, %f1671, %f1664;
-	mul.f32 	%f1673, %f1671, %f1665;
-	mul.f32 	%f1674, %f1671, %f1666;
-	mul.f32 	%f1675, %f1976, %f1902;
-	mov.f32 	%f1676, 0f3F800000;
-	fma.rn.f32 	%f1677, %f1676, %f1905, %f1675;
-	mul.f32 	%f1678, %f1976, %f1901;
-	fma.rn.f32 	%f1679, %f1676, %f1904, %f1678;
-	mul.f32 	%f1680, %f1976, %f1900;
-	fma.rn.f32 	%f1681, %f1676, %f1903, %f1680;
-	mov.f32 	%f1682, 0f00000000;
-	fma.rn.f32 	%f1683, %f1682, %f1908, %f1677;
-	fma.rn.f32 	%f1684, %f1682, %f1907, %f1679;
-	fma.rn.f32 	%f1685, %f1682, %f1906, %f1681;
-	mul.f32 	%f1686, %f1683, %f1671;
-	mul.f32 	%f1687, %f1684, %f1671;
-	mul.f32 	%f1688, %f1685, %f1671;
-	mul.f32 	%f1689, %f1973, %f1902;
-	fma.rn.f32 	%f1690, %f1682, %f1905, %f1689;
-	mul.f32 	%f1691, %f1973, %f1901;
-	fma.rn.f32 	%f1692, %f1682, %f1904, %f1691;
-	mul.f32 	%f1693, %f1973, %f1900;
-	fma.rn.f32 	%f1694, %f1682, %f1903, %f1693;
-	fma.rn.f32 	%f1695, %f1676, %f1908, %f1690;
-	fma.rn.f32 	%f1696, %f1676, %f1907, %f1692;
-	fma.rn.f32 	%f1697, %f1676, %f1906, %f1694;
-	mul.f32 	%f1698, %f1695, %f1671;
-	mul.f32 	%f1699, %f1696, %f1671;
-	mul.f32 	%f1700, %f1697, %f1671;
-	mul.f32 	%f1701, %f1672, %f1686;
-	fma.rn.f32 	%f1702, %f1673, %f1687, %f1701;
-	fma.rn.f32 	%f1703, %f1674, %f1688, %f1702;
-	mul.f32 	%f1704, %f1672, %f1703;
-	mul.f32 	%f1705, %f1673, %f1703;
-	mul.f32 	%f1706, %f1674, %f1703;
-	sub.f32 	%f1976, %f1686, %f1704;
-	sub.f32 	%f1977, %f1687, %f1705;
-	sub.f32 	%f1978, %f1688, %f1706;
-	mul.f32 	%f1707, %f1672, %f1698;
-	fma.rn.f32 	%f1708, %f1673, %f1699, %f1707;
-	fma.rn.f32 	%f1709, %f1674, %f1700, %f1708;
-	mul.f32 	%f1710, %f1672, %f1709;
-	mul.f32 	%f1711, %f1673, %f1709;
-	mul.f32 	%f1712, %f1674, %f1709;
-	sub.f32 	%f1973, %f1698, %f1710;
-	sub.f32 	%f1974, %f1699, %f1711;
-	sub.f32 	%f1975, %f1700, %f1712;
-	bra.uni 	BB10_165;
-
-BB10_118:
-	mov.f32 	%f1977, %f1975;
-	mov.f32 	%f1978, %f1974;
-	mov.f32 	%f1979, %f1973;
-	mov.f32 	%f1980, %f1974;
-	mov.f32 	%f1981, %f1975;
-	mov.f32 	%f1982, %f1976;
-	mov.f32 	%f1983, %f1975;
-	mov.f32 	%f1984, %f1974;
-	mov.f32 	%f1985, %f1988;
-	mov.f32 	%f1986, %f1989;
-	mov.f32 	%f1987, %f1990;
-	bra.uni 	BB10_166;
-
-BB10_163:
-	mov.f32 	%f1977, %f1975;
-	mov.f32 	%f1978, %f1974;
-
-BB10_165:
-	st.global.u32 	[%rd27], %r431;
-
-BB10_166:
-	ld.const.u64 	%rd563, [params+328];
+	// begin inline asm
+	call (%rd424), _optix_get_transform_list_handle, (%r649);
+	// end inline asm
+	// begin inline asm
+	call (%r495), _optix_get_transform_type_from_handle, (%rd424);
+	// end inline asm
+	or.b32  	%r496, %r495, 1;
+	setp.eq.s32 	%p41, %r496, 3;
+	@%p41 bra 	$L__BB10_83;
+	bra.uni 	$L__BB10_78;
+
+$L__BB10_83:
+	setp.eq.s32 	%p44, %r495, 2;
+	@%p44 bra 	$L__BB10_87;
+	bra.uni 	$L__BB10_84;
+
+$L__BB10_87:
+	// begin inline asm
+	call (%rd496), _optix_get_matrix_motion_transform_from_handle, (%rd424);
+	// end inline asm
+	// begin inline asm
+	cvta.to.global.u64 %rd498, %rd496;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r584,%r585,%r586,%r587}, [%rd498];
+	// end inline asm
+	add.s64 	%rd502, %rd496, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd501, %rd502;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r588,%r589,%r590,%r591}, [%rd501];
+	// end inline asm
+	add.s64 	%rd505, %rd496, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd504, %rd505;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r592,%r593,%r594,%r595}, [%rd504];
+	// end inline asm
+	add.s64 	%rd508, %rd496, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd507, %rd508;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r596,%r597,%r598,%r599}, [%rd507];
+	// end inline asm
+	add.s64 	%rd511, %rd496, 64;
+	// begin inline asm
+	cvta.to.global.u64 %rd510, %rd511;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r600,%r601,%r602,%r603}, [%rd510];
+	// end inline asm
+	add.s64 	%rd514, %rd496, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd513, %rd514;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r604,%r605,%r606,%r607}, [%rd513];
+	// end inline asm
+	add.s64 	%rd517, %rd496, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd516, %rd517;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r608,%r609,%r610,%r611}, [%rd516];
+	// end inline asm
+	add.s64 	%rd520, %rd496, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd519, %rd520;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r612,%r613,%r614,%r615}, [%rd519];
+	// end inline asm
+	mov.b32 	%f1543, %r587;
+	mov.b32 	%f1544, %r588;
+	and.b32  	%r628, %r586, 65535;
+	add.s32 	%r629, %r628, -1;
+	cvt.rn.f32.s32 	%f1545, %r629;
+	sub.f32 	%f1546, %f1439, %f1543;
+	mul.f32 	%f1547, %f1546, %f1545;
+	sub.f32 	%f1548, %f1544, %f1543;
+	div.rn.f32 	%f1549, %f1547, %f1548;
+	min.f32 	%f1550, %f1545, %f1549;
+	mov.f32 	%f1551, 0f00000000;
+	max.f32 	%f1552, %f1551, %f1550;
+	cvt.rmi.f32.f32 	%f1553, %f1552;
+	sub.f32 	%f578, %f1552, %f1553;
+	cvt.rzi.s32.f32 	%r630, %f1553;
+	cvt.s64.s32 	%rd38, %r630;
+	mul.wide.s32 	%rd531, %r630, 48;
+	add.s64 	%rd523, %rd505, %rd531;
+	// begin inline asm
+	cvta.to.global.u64 %rd522, %rd523;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r616,%r617,%r618,%r619}, [%rd522];
+	// end inline asm
+	mov.b32 	%f1990, %r616;
+	mov.b32 	%f1991, %r617;
+	mov.b32 	%f1992, %r618;
+	add.s64 	%rd526, %rd523, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd525, %rd526;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r620,%r621,%r622,%r623}, [%rd525];
+	// end inline asm
+	mov.b32 	%f1987, %r620;
+	mov.b32 	%f1988, %r621;
+	mov.b32 	%f1989, %r622;
+	add.s64 	%rd529, %rd523, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd528, %rd529;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r624,%r625,%r626,%r627}, [%rd528];
+	// end inline asm
+	mov.b32 	%f1984, %r624;
+	mov.b32 	%f1985, %r625;
+	mov.b32 	%f1986, %r626;
+	setp.leu.f32 	%p46, %f578, 0f00000000;
+	@%p46 bra 	$L__BB10_89;
+
+	mov.f32 	%f1554, 0f3F800000;
+	sub.f32 	%f1555, %f1554, %f578;
+	mul.lo.s64 	%rd541, %rd38, 48;
+	add.s64 	%rd542, %rd496, %rd541;
+	add.s64 	%rd533, %rd542, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd532, %rd533;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r631,%r632,%r633,%r634}, [%rd532];
+	// end inline asm
+	mov.b32 	%f1556, %r631;
+	mov.b32 	%f1557, %r632;
+	mov.b32 	%f1558, %r633;
+	mul.f32 	%f1559, %f578, %f1556;
+	mul.f32 	%f1560, %f578, %f1557;
+	mul.f32 	%f1561, %f578, %f1558;
+	fma.rn.f32 	%f1990, %f1555, %f1990, %f1559;
+	fma.rn.f32 	%f1991, %f1555, %f1991, %f1560;
+	fma.rn.f32 	%f1992, %f1555, %f1992, %f1561;
+	add.s64 	%rd536, %rd542, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd535, %rd536;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r635,%r636,%r637,%r638}, [%rd535];
+	// end inline asm
+	mov.b32 	%f1562, %r635;
+	mov.b32 	%f1563, %r636;
+	mov.b32 	%f1564, %r637;
+	mul.f32 	%f1565, %f578, %f1562;
+	mul.f32 	%f1566, %f578, %f1563;
+	mul.f32 	%f1567, %f578, %f1564;
+	fma.rn.f32 	%f1987, %f1555, %f1987, %f1565;
+	fma.rn.f32 	%f1988, %f1555, %f1988, %f1566;
+	fma.rn.f32 	%f1989, %f1555, %f1989, %f1567;
+	add.s64 	%rd539, %rd542, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd538, %rd539;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r639,%r640,%r641,%r642}, [%rd538];
+	// end inline asm
+	mov.b32 	%f1568, %r639;
+	mov.b32 	%f1569, %r640;
+	mov.b32 	%f1570, %r641;
+	mul.f32 	%f1571, %f578, %f1568;
+	mul.f32 	%f1572, %f578, %f1569;
+	mul.f32 	%f1573, %f578, %f1570;
+	fma.rn.f32 	%f1984, %f1555, %f1984, %f1571;
+	fma.rn.f32 	%f1985, %f1555, %f1985, %f1572;
+	fma.rn.f32 	%f1986, %f1555, %f1986, %f1573;
+	bra.uni 	$L__BB10_89;
+
+$L__BB10_78:
+	mov.f32 	%f1993, 0f00000000;
+	mov.f32 	%f1995, 0f3F800000;
+	setp.eq.s32 	%p42, %r495, 4;
+	@%p42 bra 	$L__BB10_81;
+
+	setp.ne.s32 	%p43, %r495, 1;
+	mov.f32 	%f1994, %f1993;
+	mov.f32 	%f1996, %f1993;
+	mov.f32 	%f1997, %f1995;
+	mov.f32 	%f1998, %f1993;
+	mov.f32 	%f1999, %f1995;
+	mov.f32 	%f2000, %f1993;
+	mov.f32 	%f2001, %f1993;
+	@%p43 bra 	$L__BB10_90;
+
+	// begin inline asm
+	call (%rd426), _optix_get_static_transform_from_handle, (%rd424);
+	// end inline asm
+	add.s64 	%rd651, %rd426, 64;
+	bra.uni 	$L__BB10_82;
+
+$L__BB10_84:
+	// begin inline asm
+	call (%rd439), _optix_get_srt_motion_transform_from_handle, (%rd424);
+	// end inline asm
+	// begin inline asm
+	cvta.to.global.u64 %rd441, %rd439;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r509,%r510,%r511,%r512}, [%rd441];
+	// end inline asm
+	add.s64 	%rd445, %rd439, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd444, %rd445;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r513,%r514,%r515,%r516}, [%rd444];
+	// end inline asm
+	add.s64 	%rd448, %rd439, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd447, %rd448;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r517,%r518,%r519,%r520}, [%rd447];
+	// end inline asm
+	add.s64 	%rd451, %rd439, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd450, %rd451;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r521,%r522,%r523,%r524}, [%rd450];
+	// end inline asm
+	add.s64 	%rd454, %rd439, 64;
+	// begin inline asm
+	cvta.to.global.u64 %rd453, %rd454;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r525,%r526,%r527,%r528}, [%rd453];
+	// end inline asm
+	add.s64 	%rd457, %rd439, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd456, %rd457;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r529,%r530,%r531,%r532}, [%rd456];
+	// end inline asm
+	add.s64 	%rd460, %rd439, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd459, %rd460;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r533,%r534,%r535,%r536}, [%rd459];
+	// end inline asm
+	add.s64 	%rd463, %rd439, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd462, %rd463;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r537,%r538,%r539,%r540}, [%rd462];
+	// end inline asm
+	add.s64 	%rd466, %rd439, 128;
+	// begin inline asm
+	cvta.to.global.u64 %rd465, %rd466;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r541,%r542,%r543,%r544}, [%rd465];
+	// end inline asm
+	add.s64 	%rd469, %rd439, 144;
+	// begin inline asm
+	cvta.to.global.u64 %rd468, %rd469;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r545,%r546,%r547,%r548}, [%rd468];
+	// end inline asm
+	mov.b32 	%f1451, %r512;
+	mov.b32 	%f1452, %r513;
+	and.b32  	%r565, %r511, 65535;
+	add.s32 	%r566, %r565, -1;
+	cvt.rn.f32.s32 	%f1453, %r566;
+	sub.f32 	%f1454, %f1439, %f1451;
+	mul.f32 	%f1455, %f1454, %f1453;
+	sub.f32 	%f1456, %f1452, %f1451;
+	div.rn.f32 	%f1457, %f1455, %f1456;
+	min.f32 	%f1458, %f1453, %f1457;
+	mov.f32 	%f1459, 0f00000000;
+	max.f32 	%f1460, %f1459, %f1458;
+	cvt.rmi.f32.f32 	%f1461, %f1460;
+	sub.f32 	%f538, %f1460, %f1461;
+	cvt.rzi.s32.f32 	%r567, %f1461;
+	mul.wide.s32 	%rd483, %r567, 64;
+	add.s64 	%rd472, %rd448, %rd483;
+	// begin inline asm
+	cvta.to.global.u64 %rd471, %rd472;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r549,%r550,%r551,%r552}, [%rd471];
+	// end inline asm
+	mov.b32 	%f1974, %r549;
+	mov.b32 	%f1975, %r550;
+	mov.b32 	%f1976, %r551;
+	add.s64 	%rd475, %rd472, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd474, %rd475;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r553,%r554,%r555,%r556}, [%rd474];
+	// end inline asm
+	mov.b32 	%f1977, %r553;
+	mov.b32 	%f1978, %r554;
+	mov.b32 	%f1979, %r556;
+	add.s64 	%rd478, %rd472, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd477, %rd478;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r557,%r558,%r559,%r560}, [%rd477];
+	// end inline asm
+	mov.b32 	%f1980, %r558;
+	mov.b32 	%f1981, %r559;
+	mov.b32 	%f1982, %r560;
+	add.s64 	%rd481, %rd472, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd480, %rd481;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r561,%r562,%r563,%r564}, [%rd480];
+	// end inline asm
+	mov.b32 	%f1983, %r561;
+	setp.leu.f32 	%p45, %f538, 0f00000000;
+	@%p45 bra 	$L__BB10_86;
+
+	mov.f32 	%f1462, 0f3F800000;
+	sub.f32 	%f1463, %f1462, %f538;
+	add.s64 	%rd485, %rd472, 64;
+	// begin inline asm
+	cvta.to.global.u64 %rd484, %rd485;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r568,%r569,%r570,%r571}, [%rd484];
+	// end inline asm
+	mov.b32 	%f1464, %r568;
+	mov.b32 	%f1465, %r569;
+	mov.b32 	%f1466, %r570;
+	mul.f32 	%f1467, %f538, %f1464;
+	mul.f32 	%f1468, %f538, %f1465;
+	mul.f32 	%f1469, %f538, %f1466;
+	fma.rn.f32 	%f1974, %f1463, %f1974, %f1467;
+	fma.rn.f32 	%f1975, %f1463, %f1975, %f1468;
+	fma.rn.f32 	%f1976, %f1463, %f1976, %f1469;
+	add.s64 	%rd488, %rd472, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd487, %rd488;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r572,%r573,%r574,%r575}, [%rd487];
+	// end inline asm
+	mov.b32 	%f1470, %r572;
+	mov.b32 	%f1471, %r573;
+	mov.b32 	%f1472, %r575;
+	mul.f32 	%f1473, %f538, %f1470;
+	mul.f32 	%f1474, %f538, %f1471;
+	mul.f32 	%f1475, %f538, %f1472;
+	fma.rn.f32 	%f1977, %f1463, %f1977, %f1473;
+	fma.rn.f32 	%f1978, %f1463, %f1978, %f1474;
+	fma.rn.f32 	%f1979, %f1463, %f1979, %f1475;
+	add.s64 	%rd491, %rd472, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd490, %rd491;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r576,%r577,%r578,%r579}, [%rd490];
+	// end inline asm
+	mov.b32 	%f1476, %r577;
+	mov.b32 	%f1477, %r578;
+	mov.b32 	%f1478, %r579;
+	mul.f32 	%f1479, %f538, %f1476;
+	mul.f32 	%f1480, %f538, %f1477;
+	mul.f32 	%f1481, %f538, %f1478;
+	fma.rn.f32 	%f1482, %f1463, %f1980, %f1479;
+	fma.rn.f32 	%f1483, %f1463, %f1981, %f1480;
+	fma.rn.f32 	%f1484, %f1463, %f1982, %f1481;
+	add.s64 	%rd494, %rd472, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd493, %rd494;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r580,%r581,%r582,%r583}, [%rd493];
+	// end inline asm
+	mov.b32 	%f1485, %r580;
+	mul.f32 	%f1486, %f538, %f1485;
+	fma.rn.f32 	%f1487, %f1463, %f1983, %f1486;
+	mul.f32 	%f1488, %f1483, %f1483;
+	fma.rn.f32 	%f1489, %f1482, %f1482, %f1488;
+	fma.rn.f32 	%f1490, %f1484, %f1484, %f1489;
+	fma.rn.f32 	%f1491, %f1487, %f1487, %f1490;
+	sqrt.rn.f32 	%f1492, %f1491;
+	rcp.rn.f32 	%f1493, %f1492;
+	mul.f32 	%f1980, %f1482, %f1493;
+	mul.f32 	%f1981, %f1483, %f1493;
+	mul.f32 	%f1982, %f1484, %f1493;
+	mul.f32 	%f1983, %f1493, %f1487;
+
+$L__BB10_86:
+	mul.f32 	%f1494, %f1981, %f1981;
+	fma.rn.f32 	%f1495, %f1980, %f1980, %f1494;
+	fma.rn.f32 	%f1496, %f1982, %f1982, %f1495;
+	fma.rn.f32 	%f1497, %f1983, %f1983, %f1496;
+	rcp.rn.f32 	%f1498, %f1497;
+	mul.f32 	%f1499, %f1980, %f1498;
+	mul.f32 	%f1500, %f1981, %f1498;
+	mul.f32 	%f1501, %f1982, %f1498;
+	mul.f32 	%f1502, %f1983, %f1498;
+	mul.f32 	%f1503, %f1980, %f1499;
+	mul.f32 	%f1504, %f1981, %f1500;
+	mul.f32 	%f1505, %f1982, %f1501;
+	mul.f32 	%f1506, %f1980, %f1500;
+	mul.f32 	%f1507, %f1982, %f1502;
+	mul.f32 	%f1508, %f1980, %f1501;
+	mul.f32 	%f1509, %f1981, %f1502;
+	mul.f32 	%f1510, %f1981, %f1501;
+	mul.f32 	%f1511, %f1980, %f1502;
+	sub.f32 	%f1512, %f1503, %f1504;
+	sub.f32 	%f1513, %f1512, %f1505;
+	fma.rn.f32 	%f1514, %f1983, %f1502, %f1513;
+	sub.f32 	%f1515, %f1506, %f1507;
+	add.f32 	%f1516, %f1515, %f1515;
+	add.f32 	%f1517, %f1508, %f1509;
+	add.f32 	%f1518, %f1517, %f1517;
+	add.f32 	%f1519, %f1506, %f1507;
+	add.f32 	%f1520, %f1519, %f1519;
+	sub.f32 	%f1521, %f1504, %f1503;
+	sub.f32 	%f1522, %f1521, %f1505;
+	fma.rn.f32 	%f1523, %f1983, %f1502, %f1522;
+	sub.f32 	%f1524, %f1510, %f1511;
+	add.f32 	%f1525, %f1524, %f1524;
+	sub.f32 	%f1526, %f1508, %f1509;
+	add.f32 	%f1527, %f1526, %f1526;
+	add.f32 	%f1528, %f1510, %f1511;
+	add.f32 	%f1529, %f1528, %f1528;
+	neg.f32 	%f1530, %f1503;
+	sub.f32 	%f1531, %f1530, %f1504;
+	add.f32 	%f1532, %f1505, %f1531;
+	fma.rn.f32 	%f1533, %f1983, %f1502, %f1532;
+	mul.f32 	%f1534, %f1976, %f1514;
+	fma.rn.f32 	%f1535, %f1978, %f1516, %f1534;
+	fma.rn.f32 	%f1992, %f1979, %f1518, %f1535;
+	mul.f32 	%f1536, %f1978, %f1523;
+	fma.rn.f32 	%f1537, %f1976, %f1520, %f1536;
+	fma.rn.f32 	%f1989, %f1979, %f1525, %f1537;
+	mul.f32 	%f1538, %f1978, %f1529;
+	fma.rn.f32 	%f1539, %f1976, %f1527, %f1538;
+	fma.rn.f32 	%f1986, %f1979, %f1533, %f1539;
+	mul.f32 	%f1540, %f1975, %f1514;
+	fma.rn.f32 	%f1991, %f1977, %f1516, %f1540;
+	mul.f32 	%f1541, %f1977, %f1523;
+	fma.rn.f32 	%f1988, %f1975, %f1520, %f1541;
+	mul.f32 	%f1542, %f1977, %f1529;
+	fma.rn.f32 	%f1985, %f1975, %f1527, %f1542;
+	mul.f32 	%f1990, %f1974, %f1514;
+	mul.f32 	%f1987, %f1974, %f1520;
+	mul.f32 	%f1984, %f1974, %f1527;
+
+$L__BB10_89:
+	mul.f32 	%f1574, %f1985, %f1989;
+	mul.f32 	%f1575, %f1986, %f1988;
+	sub.f32 	%f1576, %f1575, %f1574;
+	mul.f32 	%f1577, %f1990, %f1576;
+	mul.f32 	%f1578, %f1984, %f1989;
+	mul.f32 	%f1579, %f1986, %f1987;
+	sub.f32 	%f1580, %f1579, %f1578;
+	mul.f32 	%f1581, %f1580, %f1991;
+	sub.f32 	%f1582, %f1577, %f1581;
+	mul.f32 	%f1583, %f1984, %f1988;
+	mul.f32 	%f1584, %f1985, %f1987;
+	sub.f32 	%f1585, %f1584, %f1583;
+	fma.rn.f32 	%f1586, %f1585, %f1992, %f1582;
+	rcp.rn.f32 	%f1587, %f1586;
+	mul.f32 	%f1999, %f1576, %f1587;
+	mul.f32 	%f1588, %f1986, %f1991;
+	mul.f32 	%f1589, %f1985, %f1992;
+	sub.f32 	%f1590, %f1589, %f1588;
+	mul.f32 	%f2000, %f1590, %f1587;
+	mul.f32 	%f1591, %f1988, %f1992;
+	mul.f32 	%f1592, %f1989, %f1991;
+	sub.f32 	%f1593, %f1592, %f1591;
+	mul.f32 	%f2001, %f1593, %f1587;
+	sub.f32 	%f1594, %f1578, %f1579;
+	mul.f32 	%f1996, %f1594, %f1587;
+	mul.f32 	%f1595, %f1984, %f1992;
+	mul.f32 	%f1596, %f1986, %f1990;
+	sub.f32 	%f1597, %f1596, %f1595;
+	mul.f32 	%f1997, %f1597, %f1587;
+	mul.f32 	%f1598, %f1989, %f1990;
+	mul.f32 	%f1599, %f1987, %f1992;
+	sub.f32 	%f1600, %f1599, %f1598;
+	mul.f32 	%f1998, %f1600, %f1587;
+	mul.f32 	%f1993, %f1585, %f1587;
+	mul.f32 	%f1601, %f1985, %f1990;
+	mul.f32 	%f1602, %f1984, %f1991;
+	sub.f32 	%f1603, %f1602, %f1601;
+	mul.f32 	%f1994, %f1603, %f1587;
+	mul.f32 	%f1604, %f1987, %f1991;
+	mul.f32 	%f1605, %f1988, %f1990;
+	sub.f32 	%f1606, %f1605, %f1604;
+	mul.f32 	%f1995, %f1606, %f1587;
+	bra.uni 	$L__BB10_90;
+
+$L__BB10_81:
+	// begin inline asm
+	call (%rd651), _optix_get_instance_inverse_transform_from_handle, (%rd424);
+	// end inline asm
+
+$L__BB10_82:
+	// begin inline asm
+	cvta.to.global.u64 %rd430, %rd651;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r497,%r498,%r499,%r500}, [%rd430];
+	// end inline asm
+	mov.b32 	%f1999, %r497;
+	mov.b32 	%f2000, %r498;
+	mov.b32 	%f2001, %r499;
+	add.s64 	%rd434, %rd651, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd433, %rd434;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r501,%r502,%r503,%r504}, [%rd433];
+	// end inline asm
+	mov.b32 	%f1996, %r501;
+	mov.b32 	%f1997, %r502;
+	mov.b32 	%f1998, %r503;
+	add.s64 	%rd437, %rd651, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd436, %rd437;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r505,%r506,%r507,%r508}, [%rd436];
+	// end inline asm
+	mov.b32 	%f1993, %r505;
+	mov.b32 	%f1994, %r506;
+	mov.b32 	%f1995, %r507;
+
+$L__BB10_90:
+	setp.eq.s32 	%p47, %r649, 0;
+	@%p47 bra 	$L__BB10_92;
+
+	mul.f32 	%f1607, %f1970, %f2000;
+	fma.rn.f32 	%f1608, %f1967, %f1999, %f1607;
+	fma.rn.f32 	%f624, %f1973, %f2001, %f1608;
+	mul.f32 	%f1609, %f1969, %f2000;
+	fma.rn.f32 	%f1610, %f1966, %f1999, %f1609;
+	fma.rn.f32 	%f625, %f1972, %f2001, %f1610;
+	mul.f32 	%f1611, %f1968, %f2000;
+	fma.rn.f32 	%f1612, %f1965, %f1999, %f1611;
+	fma.rn.f32 	%f2001, %f1971, %f2001, %f1612;
+	mul.f32 	%f1613, %f1970, %f1997;
+	fma.rn.f32 	%f1614, %f1967, %f1996, %f1613;
+	fma.rn.f32 	%f627, %f1973, %f1998, %f1614;
+	mul.f32 	%f1615, %f1969, %f1997;
+	fma.rn.f32 	%f1616, %f1966, %f1996, %f1615;
+	fma.rn.f32 	%f628, %f1972, %f1998, %f1616;
+	mul.f32 	%f1617, %f1968, %f1997;
+	fma.rn.f32 	%f1618, %f1965, %f1996, %f1617;
+	fma.rn.f32 	%f1998, %f1971, %f1998, %f1618;
+	mul.f32 	%f1619, %f1970, %f1994;
+	fma.rn.f32 	%f1620, %f1967, %f1993, %f1619;
+	fma.rn.f32 	%f630, %f1973, %f1995, %f1620;
+	mul.f32 	%f1621, %f1969, %f1994;
+	fma.rn.f32 	%f1622, %f1966, %f1993, %f1621;
+	fma.rn.f32 	%f631, %f1972, %f1995, %f1622;
+	mul.f32 	%f1623, %f1968, %f1994;
+	fma.rn.f32 	%f1624, %f1965, %f1993, %f1623;
+	fma.rn.f32 	%f1995, %f1971, %f1995, %f1624;
+	mov.f32 	%f1993, %f630;
+	mov.f32 	%f1994, %f631;
+	mov.f32 	%f1996, %f627;
+	mov.f32 	%f1997, %f628;
+	mov.f32 	%f1999, %f624;
+	mov.f32 	%f2000, %f625;
+
+$L__BB10_92:
+	add.s32 	%r649, %r649, 1;
+	setp.lt.u32 	%p48, %r649, %r492;
+	mov.f32 	%f1965, %f2001;
+	mov.f32 	%f1966, %f2000;
+	mov.f32 	%f1967, %f1999;
+	mov.f32 	%f1968, %f1998;
+	mov.f32 	%f1969, %f1997;
+	mov.f32 	%f1970, %f1996;
+	mov.f32 	%f1971, %f1995;
+	mov.f32 	%f1972, %f1994;
+	mov.f32 	%f1973, %f1993;
+	@%p48 bra 	$L__BB10_77;
+
+$L__BB10_93:
+	fma.rn.f32 	%f1625, %f2056, %f1937, %f1940;
+	fma.rn.f32 	%f1626, %f2057, %f1938, %f1625;
+	fma.rn.f32 	%f1627, %f2056, %f1933, %f1936;
+	fma.rn.f32 	%f1628, %f2057, %f1934, %f1627;
+	fma.rn.f32 	%f1629, %f2056, %f1929, %f1932;
+	fma.rn.f32 	%f1630, %f2057, %f1930, %f1629;
+	fma.rn.f32 	%f2056, %f2058, %f1939, %f1626;
+	fma.rn.f32 	%f2057, %f2058, %f1935, %f1628;
+	fma.rn.f32 	%f2058, %f2058, %f1931, %f1630;
+	ld.const.u64 	%rd543, [params+112];
+	setp.eq.s64 	%p49, %rd543, 0;
+	mov.f32 	%f2020, %f2050;
+	mov.f32 	%f2021, %f2051;
+	mov.f32 	%f2022, %f2052;
+	@%p49 bra 	$L__BB10_95;
+
+	mul.f32 	%f1631, %f2050, %f1999;
+	fma.rn.f32 	%f1632, %f2051, %f1996, %f1631;
+	mul.f32 	%f1633, %f2050, %f2000;
+	fma.rn.f32 	%f1634, %f2051, %f1997, %f1633;
+	mul.f32 	%f1635, %f2050, %f2001;
+	fma.rn.f32 	%f1636, %f2051, %f1998, %f1635;
+	fma.rn.f32 	%f1637, %f2052, %f1993, %f1632;
+	fma.rn.f32 	%f1638, %f2052, %f1994, %f1634;
+	fma.rn.f32 	%f1639, %f2052, %f1995, %f1636;
+	mul.f32 	%f1640, %f1637, %f1637;
+	fma.rn.f32 	%f1641, %f1638, %f1638, %f1640;
+	fma.rn.f32 	%f1642, %f1639, %f1639, %f1641;
+	sqrt.rn.f32 	%f1643, %f1642;
+	div.rn.f32 	%f2020, %f1637, %f1643;
+	div.rn.f32 	%f2021, %f1638, %f1643;
+	div.rn.f32 	%f2022, %f1639, %f1643;
+
+$L__BB10_95:
+	ld.const.u64 	%rd544, [params+136];
+	setp.eq.s64 	%p50, %rd544, 0;
+	@%p50 bra 	$L__BB10_97;
+
+	mul.f32 	%f1644, %f2050, %f1999;
+	fma.rn.f32 	%f1645, %f2051, %f1996, %f1644;
+	mul.f32 	%f1646, %f2050, %f2000;
+	fma.rn.f32 	%f1647, %f2051, %f1997, %f1646;
+	mul.f32 	%f1648, %f2050, %f2001;
+	fma.rn.f32 	%f1649, %f2051, %f1998, %f1648;
+	fma.rn.f32 	%f1650, %f2052, %f1993, %f1645;
+	fma.rn.f32 	%f1651, %f2052, %f1994, %f1647;
+	fma.rn.f32 	%f1652, %f2052, %f1995, %f1649;
+	mul.f32 	%f1653, %f1650, %f1650;
+	fma.rn.f32 	%f1654, %f1651, %f1651, %f1653;
+	fma.rn.f32 	%f1655, %f1652, %f1652, %f1654;
+	sqrt.rn.f32 	%f1656, %f1655;
+	div.rn.f32 	%f2050, %f1650, %f1656;
+	div.rn.f32 	%f2051, %f1651, %f1656;
+	div.rn.f32 	%f2052, %f1652, %f1656;
+
+$L__BB10_97:
+	mov.f32 	%f2055, %f2052;
+	mov.f32 	%f2054, %f2051;
+	mov.f32 	%f2053, %f2050;
+	ld.const.u64 	%rd545, [params+184];
+	setp.eq.s64 	%p51, %rd545, 0;
+	mov.f32 	%f2039, 0f00000000;
+	mov.f32 	%f2040, 0f3F800000;
+	mov.f32 	%f2044, %f2032;
+	mov.f32 	%f2045, %f2039;
+	mov.f32 	%f2046, %f2040;
+	mov.f32 	%f2047, %f2035;
+	mov.f32 	%f2048, %f2040;
+	mov.f32 	%f2049, %f2039;
+	@%p51 bra 	$L__BB10_99;
+
+	mul.f32 	%f1661, %f2035, %f1937;
+	mov.f32 	%f1662, 0f3F800000;
+	fma.rn.f32 	%f1663, %f1662, %f1938, %f1661;
+	mul.f32 	%f1664, %f2035, %f1933;
+	fma.rn.f32 	%f1665, %f1662, %f1934, %f1664;
+	mul.f32 	%f1666, %f2035, %f1929;
+	fma.rn.f32 	%f1667, %f1662, %f1930, %f1666;
+	mov.f32 	%f1668, 0f00000000;
+	fma.rn.f32 	%f2047, %f1668, %f1939, %f1663;
+	fma.rn.f32 	%f2048, %f1668, %f1935, %f1665;
+	fma.rn.f32 	%f2049, %f1668, %f1931, %f1667;
+	mul.f32 	%f1669, %f2032, %f1937;
+	fma.rn.f32 	%f1670, %f1668, %f1938, %f1669;
+	mul.f32 	%f1671, %f2032, %f1933;
+	fma.rn.f32 	%f1672, %f1668, %f1934, %f1671;
+	mul.f32 	%f1673, %f2032, %f1929;
+	fma.rn.f32 	%f1674, %f1668, %f1930, %f1673;
+	fma.rn.f32 	%f2044, %f1662, %f1939, %f1670;
+	fma.rn.f32 	%f2045, %f1662, %f1935, %f1672;
+	fma.rn.f32 	%f2046, %f1662, %f1931, %f1674;
+
+$L__BB10_99:
+	ld.const.u64 	%rd546, [params+232];
+	ld.const.u64 	%rd547, [params+280];
+	or.b64  	%rd548, %rd546, %rd547;
+	setp.eq.s64 	%p52, %rd548, 0;
+	mov.f32 	%f2042, %f2040;
+	mov.f32 	%f2043, %f2039;
+	@%p52 bra 	$L__BB10_101;
+
+	mul.f32 	%f1679, %f2053, %f1937;
+	fma.rn.f32 	%f1680, %f2054, %f1933, %f1679;
+	mul.f32 	%f1681, %f2053, %f1938;
+	fma.rn.f32 	%f1682, %f2054, %f1934, %f1681;
+	mul.f32 	%f1683, %f2053, %f1939;
+	fma.rn.f32 	%f1684, %f2054, %f1935, %f1683;
+	fma.rn.f32 	%f1685, %f2055, %f1929, %f1680;
+	fma.rn.f32 	%f1686, %f2055, %f1930, %f1682;
+	fma.rn.f32 	%f1687, %f2055, %f1931, %f1684;
+	mul.f32 	%f1688, %f1685, %f1685;
+	fma.rn.f32 	%f1689, %f1686, %f1686, %f1688;
+	fma.rn.f32 	%f1690, %f1687, %f1687, %f1689;
+	sqrt.rn.f32 	%f1691, %f1690;
+	div.rn.f32 	%f1692, %f1685, %f1691;
+	div.rn.f32 	%f1693, %f1686, %f1691;
+	div.rn.f32 	%f1694, %f1687, %f1691;
+	mul.f32 	%f1695, %f1692, %f1999;
+	mul.f32 	%f1696, %f1692, %f2000;
+	mul.f32 	%f1697, %f1692, %f2001;
+	fma.rn.f32 	%f1698, %f1693, %f1996, %f1695;
+	fma.rn.f32 	%f1699, %f1693, %f1997, %f1696;
+	fma.rn.f32 	%f1700, %f1693, %f1998, %f1697;
+	fma.rn.f32 	%f1701, %f1694, %f1993, %f1698;
+	fma.rn.f32 	%f1702, %f1694, %f1994, %f1699;
+	fma.rn.f32 	%f1703, %f1694, %f1995, %f1700;
+	mul.f32 	%f1704, %f1701, %f1701;
+	fma.rn.f32 	%f1705, %f1702, %f1702, %f1704;
+	fma.rn.f32 	%f1706, %f1703, %f1703, %f1705;
+	sqrt.rn.f32 	%f1707, %f1706;
+	rcp.rn.f32 	%f1708, %f1707;
+	mov.f32 	%f1709, 0f3F800000;
+	mul.f32 	%f1710, %f1708, %f1701;
+	mul.f32 	%f1711, %f1708, %f1702;
+	mul.f32 	%f1712, %f1708, %f1703;
+	mul.f32 	%f1713, %f2035, %f1999;
+	fma.rn.f32 	%f1714, %f1709, %f1996, %f1713;
+	mul.f32 	%f1715, %f2035, %f2000;
+	fma.rn.f32 	%f1716, %f1709, %f1997, %f1715;
+	mul.f32 	%f1717, %f2035, %f2001;
+	fma.rn.f32 	%f1718, %f1709, %f1998, %f1717;
+	mov.f32 	%f1719, 0f00000000;
+	fma.rn.f32 	%f1720, %f1719, %f1993, %f1714;
+	fma.rn.f32 	%f1721, %f1719, %f1994, %f1716;
+	fma.rn.f32 	%f1722, %f1719, %f1995, %f1718;
+	mul.f32 	%f1723, %f1720, %f1708;
+	mul.f32 	%f1724, %f1721, %f1708;
+	mul.f32 	%f1725, %f1722, %f1708;
+	mul.f32 	%f1726, %f2032, %f1999;
+	fma.rn.f32 	%f1727, %f1719, %f1996, %f1726;
+	mul.f32 	%f1728, %f2032, %f2000;
+	fma.rn.f32 	%f1729, %f1719, %f1997, %f1728;
+	mul.f32 	%f1730, %f2032, %f2001;
+	fma.rn.f32 	%f1731, %f1719, %f1998, %f1730;
+	fma.rn.f32 	%f1732, %f1709, %f1993, %f1727;
+	fma.rn.f32 	%f1733, %f1709, %f1994, %f1729;
+	fma.rn.f32 	%f1734, %f1709, %f1995, %f1731;
+	mul.f32 	%f1735, %f1732, %f1708;
+	mul.f32 	%f1736, %f1733, %f1708;
+	mul.f32 	%f1737, %f1734, %f1708;
+	mul.f32 	%f1738, %f1710, %f1723;
+	fma.rn.f32 	%f1739, %f1711, %f1724, %f1738;
+	fma.rn.f32 	%f1740, %f1712, %f1725, %f1739;
+	mul.f32 	%f1741, %f1710, %f1740;
+	mul.f32 	%f1742, %f1711, %f1740;
+	mul.f32 	%f1743, %f1712, %f1740;
+	sub.f32 	%f2035, %f1723, %f1741;
+	sub.f32 	%f2042, %f1724, %f1742;
+	sub.f32 	%f2043, %f1725, %f1743;
+	mul.f32 	%f1744, %f1710, %f1735;
+	fma.rn.f32 	%f1745, %f1711, %f1736, %f1744;
+	fma.rn.f32 	%f1746, %f1712, %f1737, %f1745;
+	mul.f32 	%f1747, %f1710, %f1746;
+	mul.f32 	%f1748, %f1711, %f1746;
+	mul.f32 	%f1749, %f1712, %f1746;
+	sub.f32 	%f2032, %f1735, %f1747;
+	sub.f32 	%f2039, %f1736, %f1748;
+	sub.f32 	%f2040, %f1737, %f1749;
+
+$L__BB10_101:
+	st.global.u32 	[%rd24], %r338;
+	mov.f32 	%f2050, %f2020;
+	mov.f32 	%f2051, %f2021;
+	mov.f32 	%f2052, %f2022;
+
+$L__BB10_102:
+	ld.const.u64 	%rd549, [params+328];
+	cvta.to.global.u64 	%rd550, %rd549;
+	shl.b64 	%rd551, %rd23, 3;
+	add.s64 	%rd552, %rd550, %rd551;
+	st.global.u64 	[%rd552], %rd22;
+	ld.const.u64 	%rd553, [params+336];
+	cvta.to.global.u64 	%rd554, %rd553;
+	shl.b64 	%rd555, %rd23, 2;
+	add.s64 	%rd556, %rd554, %rd555;
+	mov.u32 	%r643, 0;
+	st.global.u32 	[%rd556], %r643;
+	ld.const.u64 	%rd557, [params+160];
+	cvta.to.global.u64 	%rd558, %rd557;
+	add.s64 	%rd559, %rd558, %rd555;
+	st.global.f32 	[%rd559], %f2056;
+	ld.const.u64 	%rd560, [params+168];
+	cvta.to.global.u64 	%rd561, %rd560;
+	add.s64 	%rd562, %rd561, %rd555;
+	st.global.f32 	[%rd562], %f2057;
+	ld.const.u64 	%rd563, [params+176];
 	cvta.to.global.u64 	%rd564, %rd563;
-	shl.b64 	%rd565, %rd26, 3;
-	add.s64 	%rd566, %rd564, %rd565;
-	st.global.u64 	[%rd566], %rd25;
-	ld.const.u64 	%rd567, [params+336];
-	cvta.to.global.u64 	%rd568, %rd567;
-	shl.b64 	%rd569, %rd26, 2;
-	add.s64 	%rd570, %rd568, %rd569;
-	mov.u32 	%r731, 0;
-	st.global.u32 	[%rd570], %r731;
-	ld.const.u64 	%rd571, [params+160];
-	cvta.to.global.u64 	%rd572, %rd571;
-	add.s64 	%rd573, %rd572, %rd569;
-	st.global.f32 	[%rd573], %f1991;
-	ld.const.u64 	%rd574, [params+168];
-	cvta.to.global.u64 	%rd575, %rd574;
-	add.s64 	%rd576, %rd575, %rd569;
-	st.global.f32 	[%rd576], %f1992;
-	ld.const.u64 	%rd577, [params+176];
-	cvta.to.global.u64 	%rd578, %rd577;
-	add.s64 	%rd579, %rd578, %rd569;
-	st.global.f32 	[%rd579], %f1993;
-	ld.const.u64 	%rd580, [params+72];
-	cvta.to.global.u64 	%rd581, %rd580;
-	add.s64 	%rd582, %rd581, %rd569;
-	st.global.f32 	[%rd582], %f1132;
-	@%p103 bra 	BB10_168;
-
-	cvta.to.global.u64 	%rd583, %rd24;
-	add.s64 	%rd585, %rd583, %rd569;
-	st.global.f32 	[%rd585], %f1834;
-	ld.const.u64 	%rd586, [params+104];
-	cvta.to.global.u64 	%rd587, %rd586;
-	add.s64 	%rd588, %rd587, %rd569;
-	st.global.f32 	[%rd588], %f1835;
-
-BB10_168:
-	ld.const.u64 	%rd44, [params+112];
-	setp.eq.s64	%p130, %rd44, 0;
-	@%p130 bra 	BB10_170;
-
-	cvta.to.global.u64 	%rd589, %rd44;
-	add.s64 	%rd591, %rd589, %rd569;
-	st.global.f32 	[%rd591], %f1985;
-	ld.const.u64 	%rd592, [params+120];
-	cvta.to.global.u64 	%rd593, %rd592;
-	add.s64 	%rd594, %rd593, %rd569;
-	st.global.f32 	[%rd594], %f1986;
-	ld.const.u64 	%rd595, [params+128];
-	cvta.to.global.u64 	%rd596, %rd595;
-	add.s64 	%rd597, %rd596, %rd569;
-	st.global.f32 	[%rd597], %f1987;
-
-BB10_170:
-	ld.const.u64 	%rd45, [params+136];
-	setp.eq.s64	%p131, %rd45, 0;
-	@%p131 bra 	BB10_172;
-
-	cvta.to.global.u64 	%rd598, %rd45;
-	add.s64 	%rd600, %rd598, %rd569;
-	st.global.f32 	[%rd600], %f1988;
-	ld.const.u64 	%rd601, [params+144];
-	cvta.to.global.u64 	%rd602, %rd601;
-	add.s64 	%rd603, %rd602, %rd569;
-	st.global.f32 	[%rd603], %f1989;
-	ld.const.u64 	%rd604, [params+152];
-	cvta.to.global.u64 	%rd605, %rd604;
-	add.s64 	%rd606, %rd605, %rd569;
-	st.global.f32 	[%rd606], %f1990;
-
-BB10_172:
-	ld.const.u64 	%rd46, [params+184];
-	setp.eq.s64	%p132, %rd46, 0;
-	@%p132 bra 	BB10_174;
-
-	cvta.to.global.u64 	%rd607, %rd46;
-	add.s64 	%rd609, %rd607, %rd569;
-	st.global.f32 	[%rd609], %f1982;
-	ld.const.u64 	%rd610, [params+192];
-	cvta.to.global.u64 	%rd611, %rd610;
-	add.s64 	%rd612, %rd611, %rd569;
-	st.global.f32 	[%rd612], %f1983;
-	ld.const.u64 	%rd613, [params+200];
-	cvta.to.global.u64 	%rd614, %rd613;
-	add.s64 	%rd615, %rd614, %rd569;
-	st.global.f32 	[%rd615], %f1984;
-	ld.const.u64 	%rd616, [params+208];
-	cvta.to.global.u64 	%rd617, %rd616;
-	add.s64 	%rd618, %rd617, %rd569;
-	st.global.f32 	[%rd618], %f1979;
-	ld.const.u64 	%rd619, [params+216];
-	cvta.to.global.u64 	%rd620, %rd619;
-	add.s64 	%rd621, %rd620, %rd569;
-	st.global.f32 	[%rd621], %f1980;
-	ld.const.u64 	%rd622, [params+224];
-	cvta.to.global.u64 	%rd623, %rd622;
-	add.s64 	%rd624, %rd623, %rd569;
-	st.global.f32 	[%rd624], %f1981;
-
-BB10_174:
-	ld.const.u64 	%rd47, [params+232];
-	setp.eq.s64	%p133, %rd47, 0;
-	@%p133 bra 	BB10_176;
-
-	cvta.to.global.u64 	%rd625, %rd47;
-	add.s64 	%rd627, %rd625, %rd569;
-	st.global.f32 	[%rd627], %f1976;
-	ld.const.u64 	%rd628, [params+240];
-	cvta.to.global.u64 	%rd629, %rd628;
-	add.s64 	%rd630, %rd629, %rd569;
-	st.global.f32 	[%rd630], %f1977;
-	ld.const.u64 	%rd631, [params+248];
-	cvta.to.global.u64 	%rd632, %rd631;
-	add.s64 	%rd633, %rd632, %rd569;
-	st.global.f32 	[%rd633], %f1978;
-	ld.const.u64 	%rd634, [params+256];
-	cvta.to.global.u64 	%rd635, %rd634;
-	add.s64 	%rd636, %rd635, %rd569;
-	st.global.f32 	[%rd636], %f1973;
-	ld.const.u64 	%rd637, [params+264];
-	cvta.to.global.u64 	%rd638, %rd637;
-	add.s64 	%rd639, %rd638, %rd569;
-	st.global.f32 	[%rd639], %f1974;
-	ld.const.u64 	%rd640, [params+272];
-	cvta.to.global.u64 	%rd641, %rd640;
-	add.s64 	%rd642, %rd641, %rd569;
-	st.global.f32 	[%rd642], %f1975;
-
-BB10_176:
-	ld.const.u64 	%rd48, [params+280];
-	setp.eq.s64	%p134, %rd48, 0;
-	@%p134 bra 	BB10_178;
-
-	cvta.to.global.u64 	%rd643, %rd48;
-	add.s64 	%rd645, %rd643, %rd569;
-	st.global.f32 	[%rd645], %f1976;
-	ld.const.u64 	%rd646, [params+288];
-	cvta.to.global.u64 	%rd647, %rd646;
-	add.s64 	%rd648, %rd647, %rd569;
-	st.global.f32 	[%rd648], %f1977;
-	ld.const.u64 	%rd649, [params+296];
-	cvta.to.global.u64 	%rd650, %rd649;
-	add.s64 	%rd651, %rd650, %rd569;
-	st.global.f32 	[%rd651], %f1978;
-	ld.const.u64 	%rd652, [params+304];
-	cvta.to.global.u64 	%rd653, %rd652;
-	add.s64 	%rd654, %rd653, %rd569;
-	st.global.f32 	[%rd654], %f1973;
-	ld.const.u64 	%rd655, [params+312];
-	cvta.to.global.u64 	%rd656, %rd655;
-	add.s64 	%rd657, %rd656, %rd569;
-	st.global.f32 	[%rd657], %f1974;
-	ld.const.u64 	%rd658, [params+320];
-	cvta.to.global.u64 	%rd659, %rd658;
-	add.s64 	%rd660, %rd659, %rd569;
-	st.global.f32 	[%rd660], %f1975;
-
-BB10_178:
+	add.s64 	%rd565, %rd564, %rd555;
+	st.global.f32 	[%rd565], %f2058;
+	ld.const.u64 	%rd566, [params+72];
+	cvta.to.global.u64 	%rd567, %rd566;
+	add.s64 	%rd568, %rd567, %rd555;
+	st.global.f32 	[%rd568], %f1159;
+	@%p26 bra 	$L__BB10_104;
+
+	cvta.to.global.u64 	%rd569, %rd21;
+	add.s64 	%rd571, %rd569, %rd555;
+	st.global.f32 	[%rd571], %f1900;
+	ld.const.u64 	%rd572, [params+104];
+	cvta.to.global.u64 	%rd573, %rd572;
+	add.s64 	%rd574, %rd573, %rd555;
+	st.global.f32 	[%rd574], %f1899;
+
+$L__BB10_104:
+	ld.const.u64 	%rd39, [params+112];
+	setp.eq.s64 	%p54, %rd39, 0;
+	@%p54 bra 	$L__BB10_106;
+
+	cvta.to.global.u64 	%rd575, %rd39;
+	add.s64 	%rd577, %rd575, %rd555;
+	st.global.f32 	[%rd577], %f2050;
+	ld.const.u64 	%rd578, [params+120];
+	cvta.to.global.u64 	%rd579, %rd578;
+	add.s64 	%rd580, %rd579, %rd555;
+	st.global.f32 	[%rd580], %f2051;
+	ld.const.u64 	%rd581, [params+128];
+	cvta.to.global.u64 	%rd582, %rd581;
+	add.s64 	%rd583, %rd582, %rd555;
+	st.global.f32 	[%rd583], %f2052;
+
+$L__BB10_106:
+	ld.const.u64 	%rd40, [params+136];
+	setp.eq.s64 	%p55, %rd40, 0;
+	@%p55 bra 	$L__BB10_108;
+
+	cvta.to.global.u64 	%rd584, %rd40;
+	add.s64 	%rd586, %rd584, %rd555;
+	st.global.f32 	[%rd586], %f2053;
+	ld.const.u64 	%rd587, [params+144];
+	cvta.to.global.u64 	%rd588, %rd587;
+	add.s64 	%rd589, %rd588, %rd555;
+	st.global.f32 	[%rd589], %f2054;
+	ld.const.u64 	%rd590, [params+152];
+	cvta.to.global.u64 	%rd591, %rd590;
+	add.s64 	%rd592, %rd591, %rd555;
+	st.global.f32 	[%rd592], %f2055;
+
+$L__BB10_108:
+	ld.const.u64 	%rd41, [params+184];
+	setp.eq.s64 	%p56, %rd41, 0;
+	@%p56 bra 	$L__BB10_110;
+
+	cvta.to.global.u64 	%rd593, %rd41;
+	add.s64 	%rd595, %rd593, %rd555;
+	st.global.f32 	[%rd595], %f2047;
+	ld.const.u64 	%rd596, [params+192];
+	cvta.to.global.u64 	%rd597, %rd596;
+	add.s64 	%rd598, %rd597, %rd555;
+	st.global.f32 	[%rd598], %f2048;
+	ld.const.u64 	%rd599, [params+200];
+	cvta.to.global.u64 	%rd600, %rd599;
+	add.s64 	%rd601, %rd600, %rd555;
+	st.global.f32 	[%rd601], %f2049;
+	ld.const.u64 	%rd602, [params+208];
+	cvta.to.global.u64 	%rd603, %rd602;
+	add.s64 	%rd604, %rd603, %rd555;
+	st.global.f32 	[%rd604], %f2044;
+	ld.const.u64 	%rd605, [params+216];
+	cvta.to.global.u64 	%rd606, %rd605;
+	add.s64 	%rd607, %rd606, %rd555;
+	st.global.f32 	[%rd607], %f2045;
+	ld.const.u64 	%rd608, [params+224];
+	cvta.to.global.u64 	%rd609, %rd608;
+	add.s64 	%rd610, %rd609, %rd555;
+	st.global.f32 	[%rd610], %f2046;
+
+$L__BB10_110:
+	ld.const.u64 	%rd42, [params+232];
+	setp.eq.s64 	%p57, %rd42, 0;
+	@%p57 bra 	$L__BB10_112;
+
+	cvta.to.global.u64 	%rd611, %rd42;
+	add.s64 	%rd613, %rd611, %rd555;
+	st.global.f32 	[%rd613], %f2035;
+	ld.const.u64 	%rd614, [params+240];
+	cvta.to.global.u64 	%rd615, %rd614;
+	add.s64 	%rd616, %rd615, %rd555;
+	st.global.f32 	[%rd616], %f2042;
+	ld.const.u64 	%rd617, [params+248];
+	cvta.to.global.u64 	%rd618, %rd617;
+	add.s64 	%rd619, %rd618, %rd555;
+	st.global.f32 	[%rd619], %f2043;
+	ld.const.u64 	%rd620, [params+256];
+	cvta.to.global.u64 	%rd621, %rd620;
+	add.s64 	%rd622, %rd621, %rd555;
+	st.global.f32 	[%rd622], %f2032;
+	ld.const.u64 	%rd623, [params+264];
+	cvta.to.global.u64 	%rd624, %rd623;
+	add.s64 	%rd625, %rd624, %rd555;
+	st.global.f32 	[%rd625], %f2039;
+	ld.const.u64 	%rd626, [params+272];
+	cvta.to.global.u64 	%rd627, %rd626;
+	add.s64 	%rd628, %rd627, %rd555;
+	st.global.f32 	[%rd628], %f2040;
+
+$L__BB10_112:
+	ld.const.u64 	%rd43, [params+280];
+	setp.eq.s64 	%p58, %rd43, 0;
+	@%p58 bra 	$L__BB10_114;
+
+	cvta.to.global.u64 	%rd629, %rd43;
+	add.s64 	%rd631, %rd629, %rd555;
+	st.global.f32 	[%rd631], %f2035;
+	ld.const.u64 	%rd632, [params+288];
+	cvta.to.global.u64 	%rd633, %rd632;
+	add.s64 	%rd634, %rd633, %rd555;
+	st.global.f32 	[%rd634], %f2042;
+	ld.const.u64 	%rd635, [params+296];
+	cvta.to.global.u64 	%rd636, %rd635;
+	add.s64 	%rd637, %rd636, %rd555;
+	st.global.f32 	[%rd637], %f2043;
+	ld.const.u64 	%rd638, [params+304];
+	cvta.to.global.u64 	%rd639, %rd638;
+	add.s64 	%rd640, %rd639, %rd555;
+	st.global.f32 	[%rd640], %f2032;
+	ld.const.u64 	%rd641, [params+312];
+	cvta.to.global.u64 	%rd642, %rd641;
+	add.s64 	%rd643, %rd642, %rd555;
+	st.global.f32 	[%rd643], %f2039;
+	ld.const.u64 	%rd644, [params+320];
+	cvta.to.global.u64 	%rd645, %rd644;
+	add.s64 	%rd646, %rd645, %rd555;
+	st.global.f32 	[%rd646], %f2040;
+
+$L__BB10_114:
 	ret;
-}
 
+}
 	// .globl	__intersection__cylhollow
-.visible .entry __intersection__cylhollow(
-
-)
+.visible .entry __intersection__cylhollow()
 {
-	.reg .pred 	%p<334>;
-	.reg .b16 	%rs<27>;
-	.reg .f32 	%f<2395>;
-	.reg .b32 	%r<427>;
-	.reg .b64 	%rd<266>;
-
-
-	// inline asm
-	call (%rd19), _optix_get_sbt_data_ptr_64, ();
-	// inline asm
-	ld.u64 	%rd1, [%rd19+8];
-	// inline asm
-	call (%f571), _optix_get_world_ray_origin_x, ();
-	// inline asm
-	// inline asm
-	call (%f572), _optix_get_world_ray_origin_y, ();
-	// inline asm
-	// inline asm
-	call (%f2292), _optix_get_world_ray_origin_z, ();
-	// inline asm
-	// inline asm
-	call (%r14), _optix_get_transform_list_size, ();
-	// inline asm
-	setp.eq.s32	%p11, %r14, 0;
-	@%p11 bra 	BB11_1;
-
-	mov.u32 	%r425, 0;
-	// inline asm
-	call (%f574), _optix_get_ray_time, ();
-	// inline asm
-
-BB11_3:
+	.reg .pred 	%p<319>;
+	.reg .b16 	%rs<11>;
+	.reg .f32 	%f<2109>;
+	.reg .b32 	%r<458>;
+	.reg .b64 	%rd<259>;
+
+
+	// begin inline asm
+	call (%rd17), _optix_get_sbt_data_ptr_64, ();
+	// end inline asm
+	ld.u64 	%rd1, [%rd17+8];
+	// begin inline asm
+	call (%f1999), _optix_get_world_ray_origin_x, ();
+	// end inline asm
+	// begin inline asm
+	call (%f2000), _optix_get_world_ray_origin_y, ();
+	// end inline asm
+	// begin inline asm
+	call (%f2001), _optix_get_world_ray_origin_z, ();
+	// end inline asm
+	// begin inline asm
+	call (%r15), _optix_get_transform_list_size, ();
+	// end inline asm
+	setp.eq.s32 	%p13, %r15, 0;
+	@%p13 bra 	$L__BB11_20;
+
+	// begin inline asm
+	call (%r16), _optix_get_transform_list_size, ();
+	// end inline asm
+	// begin inline asm
+	call (%f541), _optix_get_ray_time, ();
+	// end inline asm
+	setp.eq.s32 	%p14, %r16, 0;
+	@%p14 bra 	$L__BB11_19;
+
+	mov.u32 	%r456, 0;
+
+$L__BB11_3:
 	.pragma "nounroll";
-	// inline asm
-	call (%rd20), _optix_get_transform_list_handle, (%r425);
-	// inline asm
-	// inline asm
-	call (%r17), _optix_get_transform_type_from_handle, (%rd20);
-	// inline asm
-	and.b32  	%r18, %r17, -2;
-	setp.eq.s32	%p12, %r18, 2;
-	@%p12 bra 	BB11_9;
-	bra.uni 	BB11_4;
-
-BB11_9:
-	setp.eq.s32	%p15, %r17, 2;
-	@%p15 bra 	BB11_13;
-	bra.uni 	BB11_10;
-
-BB11_13:
-	// inline asm
-	call (%rd94), _optix_get_matrix_motion_transform_from_handle, (%rd20);
-	// inline asm
-	// inline asm
-	cvta.to.global.u64 %rd96, %rd94;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r106,%r107,%r108,%r109}, [%rd96];
-	// inline asm
-	mov.b32	{%rs9, %rs10}, %r108;
-	add.s64 	%rd100, %rd94, 16;
-	// inline asm
-	cvta.to.global.u64 %rd99, %rd100;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r110,%r111,%r112,%r113}, [%rd99];
-	// inline asm
-	add.s64 	%rd103, %rd94, 32;
-	// inline asm
-	cvta.to.global.u64 %rd102, %rd103;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r114,%r115,%r116,%r117}, [%rd102];
-	// inline asm
-	add.s64 	%rd106, %rd94, 48;
-	// inline asm
-	cvta.to.global.u64 %rd105, %rd106;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r118,%r119,%r120,%r121}, [%rd105];
-	// inline asm
-	add.s64 	%rd109, %rd94, 64;
-	// inline asm
-	cvta.to.global.u64 %rd108, %rd109;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r122,%r123,%r124,%r125}, [%rd108];
-	// inline asm
-	add.s64 	%rd112, %rd94, 80;
-	// inline asm
-	cvta.to.global.u64 %rd111, %rd112;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r126,%r127,%r128,%r129}, [%rd111];
-	// inline asm
-	add.s64 	%rd115, %rd94, 96;
-	// inline asm
-	cvta.to.global.u64 %rd114, %rd115;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r130,%r131,%r132,%r133}, [%rd114];
-	// inline asm
-	add.s64 	%rd118, %rd94, 112;
-	// inline asm
-	cvta.to.global.u64 %rd117, %rd118;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r134,%r135,%r136,%r137}, [%rd117];
-	// inline asm
-	mov.b32 	 %f701, %r109;
-	mov.b32 	 %f702, %r110;
-	cvt.u32.u16	%r150, %rs9;
-	add.s32 	%r151, %r150, -1;
-	cvt.rn.f32.s32	%f703, %r151;
-	sub.f32 	%f704, %f574, %f701;
-	mul.f32 	%f705, %f704, %f703;
-	sub.f32 	%f706, %f702, %f701;
-	div.rn.f32 	%f707, %f705, %f706;
-	min.f32 	%f708, %f703, %f707;
-	mov.f32 	%f709, 0f00000000;
-	max.f32 	%f710, %f709, %f708;
-	cvt.rmi.f32.f32	%f711, %f710;
-	cvt.rzi.s32.f32	%r152, %f711;
-	mul.wide.s32 	%rd129, %r152, 48;
-	add.s64 	%rd121, %rd103, %rd129;
-	// inline asm
-	cvta.to.global.u64 %rd120, %rd121;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r138,%r139,%r140,%r141}, [%rd120];
-	// inline asm
-	mov.b32 	 %f2264, %r138;
-	mov.b32 	 %f2265, %r139;
-	mov.b32 	 %f2266, %r140;
-	mov.b32 	 %f2267, %r141;
-	add.s64 	%rd124, %rd121, 16;
-	// inline asm
-	cvta.to.global.u64 %rd123, %rd124;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r142,%r143,%r144,%r145}, [%rd123];
-	// inline asm
-	mov.b32 	 %f2260, %r142;
-	mov.b32 	 %f2261, %r143;
-	mov.b32 	 %f2262, %r144;
-	mov.b32 	 %f2263, %r145;
-	add.s64 	%rd127, %rd121, 32;
-	// inline asm
+	// begin inline asm
+	call (%rd18), _optix_get_transform_list_handle, (%r456);
+	// end inline asm
+	// begin inline asm
+	call (%r19), _optix_get_transform_type_from_handle, (%rd18);
+	// end inline asm
+	or.b32  	%r20, %r19, 1;
+	setp.eq.s32 	%p15, %r20, 3;
+	@%p15 bra 	$L__BB11_9;
+	bra.uni 	$L__BB11_4;
+
+$L__BB11_9:
+	setp.eq.s32 	%p18, %r19, 2;
+	@%p18 bra 	$L__BB11_13;
+	bra.uni 	$L__BB11_10;
+
+$L__BB11_13:
+	// begin inline asm
+	call (%rd90), _optix_get_matrix_motion_transform_from_handle, (%rd18);
+	// end inline asm
+	// begin inline asm
+	cvta.to.global.u64 %rd92, %rd90;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r108,%r109,%r110,%r111}, [%rd92];
+	// end inline asm
+	add.s64 	%rd96, %rd90, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd95, %rd96;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r112,%r113,%r114,%r115}, [%rd95];
+	// end inline asm
+	add.s64 	%rd99, %rd90, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd98, %rd99;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r116,%r117,%r118,%r119}, [%rd98];
+	// end inline asm
+	add.s64 	%rd102, %rd90, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd101, %rd102;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r120,%r121,%r122,%r123}, [%rd101];
+	// end inline asm
+	add.s64 	%rd105, %rd90, 64;
+	// begin inline asm
+	cvta.to.global.u64 %rd104, %rd105;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r124,%r125,%r126,%r127}, [%rd104];
+	// end inline asm
+	add.s64 	%rd108, %rd90, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd107, %rd108;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r128,%r129,%r130,%r131}, [%rd107];
+	// end inline asm
+	add.s64 	%rd111, %rd90, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd110, %rd111;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r132,%r133,%r134,%r135}, [%rd110];
+	// end inline asm
+	add.s64 	%rd114, %rd90, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd113, %rd114;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r136,%r137,%r138,%r139}, [%rd113];
+	// end inline asm
+	mov.b32 	%f669, %r111;
+	mov.b32 	%f670, %r112;
+	and.b32  	%r152, %r110, 65535;
+	add.s32 	%r153, %r152, -1;
+	cvt.rn.f32.s32 	%f671, %r153;
+	sub.f32 	%f672, %f541, %f669;
+	mul.f32 	%f673, %f672, %f671;
+	sub.f32 	%f674, %f670, %f669;
+	div.rn.f32 	%f675, %f673, %f674;
+	min.f32 	%f676, %f671, %f675;
+	mov.f32 	%f677, 0f00000000;
+	max.f32 	%f678, %f677, %f676;
+	cvt.rmi.f32.f32 	%f679, %f678;
+	sub.f32 	%f90, %f678, %f679;
+	cvt.rzi.s32.f32 	%r154, %f679;
+	mul.wide.s32 	%rd125, %r154, 48;
+	add.s64 	%rd117, %rd99, %rd125;
+	// begin inline asm
+	cvta.to.global.u64 %rd116, %rd117;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r140,%r141,%r142,%r143}, [%rd116];
+	// end inline asm
+	mov.b32 	%f1954, %r140;
+	mov.b32 	%f1953, %r141;
+	mov.b32 	%f1952, %r142;
+	mov.b32 	%f1951, %r143;
+	add.s64 	%rd120, %rd117, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd119, %rd120;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r144,%r145,%r146,%r147}, [%rd119];
+	// end inline asm
+	mov.b32 	%f1958, %r144;
+	mov.b32 	%f1957, %r145;
+	mov.b32 	%f1956, %r146;
+	mov.b32 	%f1955, %r147;
+	add.s64 	%rd123, %rd117, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd122, %rd123;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r148,%r149,%r150,%r151}, [%rd122];
+	// end inline asm
+	mov.b32 	%f1962, %r148;
+	mov.b32 	%f1961, %r149;
+	mov.b32 	%f1960, %r150;
+	mov.b32 	%f1959, %r151;
+	setp.leu.f32 	%p20, %f90, 0f00000000;
+	@%p20 bra 	$L__BB11_15;
+
+	cvt.rmi.f32.f32 	%f1902, %f678;
+	cvt.rzi.s32.f32 	%r455, %f1902;
+	cvt.s64.s32 	%rd256, %r455;
+	mov.f32 	%f680, 0f3F800000;
+	sub.f32 	%f681, %f680, %f90;
+	mul.lo.s64 	%rd135, %rd256, 48;
+	add.s64 	%rd136, %rd90, %rd135;
+	add.s64 	%rd127, %rd136, 80;
+	// begin inline asm
 	cvta.to.global.u64 %rd126, %rd127;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r146,%r147,%r148,%r149}, [%rd126];
-	// inline asm
-	sub.f32 	%f98, %f710, %f711;
-	mov.b32 	 %f2256, %r146;
-	mov.b32 	 %f2257, %r147;
-	mov.b32 	 %f2258, %r148;
-	mov.b32 	 %f2259, %r149;
-	setp.leu.f32	%p17, %f98, 0f00000000;
-	@%p17 bra 	BB11_15;
-
-	cvt.rmi.f32.f32	%f2210, %f710;
-	cvt.rzi.s32.f32	%r424, %f2210;
-	cvt.s64.s32	%rd263, %r424;
-	mul.lo.s64 	%rd139, %rd263, 48;
-	add.s64 	%rd140, %rd94, %rd139;
-	add.s64 	%rd131, %rd140, 80;
-	// inline asm
-	cvta.to.global.u64 %rd130, %rd131;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r153,%r154,%r155,%r156}, [%rd130];
-	// inline asm
-	mov.b32 	 %f712, %r153;
-	mov.b32 	 %f713, %r154;
-	mov.b32 	 %f714, %r155;
-	mov.b32 	 %f715, %r156;
-	add.s64 	%rd134, %rd140, 96;
-	// inline asm
-	cvta.to.global.u64 %rd133, %rd134;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r157,%r158,%r159,%r160}, [%rd133];
-	// inline asm
-	mov.b32 	 %f716, %r157;
-	mov.b32 	 %f717, %r158;
-	mov.b32 	 %f718, %r159;
-	mov.b32 	 %f719, %r160;
-	add.s64 	%rd137, %rd140, 112;
-	// inline asm
-	cvta.to.global.u64 %rd136, %rd137;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r161,%r162,%r163,%r164}, [%rd136];
-	// inline asm
-	mov.f32 	%f720, 0f3F800000;
-	sub.f32 	%f721, %f720, %f98;
-	mul.f32 	%f722, %f98, %f712;
-	mul.f32 	%f723, %f98, %f713;
-	mul.f32 	%f724, %f98, %f714;
-	mul.f32 	%f725, %f98, %f715;
-	fma.rn.f32 	%f2264, %f721, %f2264, %f722;
-	fma.rn.f32 	%f2265, %f721, %f2265, %f723;
-	fma.rn.f32 	%f2266, %f721, %f2266, %f724;
-	fma.rn.f32 	%f2267, %f721, %f2267, %f725;
-	mul.f32 	%f726, %f98, %f716;
-	mul.f32 	%f727, %f98, %f717;
-	mul.f32 	%f728, %f98, %f718;
-	mul.f32 	%f729, %f98, %f719;
-	fma.rn.f32 	%f2260, %f721, %f2260, %f726;
-	fma.rn.f32 	%f2261, %f721, %f2261, %f727;
-	fma.rn.f32 	%f2262, %f721, %f2262, %f728;
-	fma.rn.f32 	%f2263, %f721, %f2263, %f729;
-	mov.b32 	 %f730, %r161;
-	mov.b32 	 %f731, %r162;
-	mov.b32 	 %f732, %r163;
-	mov.b32 	 %f733, %r164;
-	mul.f32 	%f734, %f98, %f730;
-	mul.f32 	%f735, %f98, %f731;
-	mul.f32 	%f736, %f98, %f732;
-	mul.f32 	%f737, %f98, %f733;
-	fma.rn.f32 	%f2256, %f721, %f2256, %f734;
-	fma.rn.f32 	%f2257, %f721, %f2257, %f735;
-	fma.rn.f32 	%f2258, %f721, %f2258, %f736;
-	fma.rn.f32 	%f2259, %f721, %f2259, %f737;
-	bra.uni 	BB11_15;
-
-BB11_4:
-	mov.f32 	%f2268, 0f00000000;
-	mov.f32 	%f2270, 0f3F800000;
-	setp.eq.s32	%p13, %r17, 4;
-	@%p13 bra 	BB11_7;
-	bra.uni 	BB11_5;
-
-BB11_7:
-	// inline asm
-	call (%rd264), _optix_get_instance_inverse_transform_from_handle, (%rd20);
-	// inline asm
-	bra.uni 	BB11_8;
-
-BB11_10:
-	// inline asm
-	call (%rd35), _optix_get_srt_motion_transform_from_handle, (%rd20);
-	// inline asm
-	// inline asm
-	cvta.to.global.u64 %rd37, %rd35;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r31,%r32,%r33,%r34}, [%rd37];
-	// inline asm
-	mov.b32	{%rs7, %rs8}, %r33;
-	add.s64 	%rd41, %rd35, 16;
-	// inline asm
-	cvta.to.global.u64 %rd40, %rd41;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r35,%r36,%r37,%r38}, [%rd40];
-	// inline asm
-	add.s64 	%rd44, %rd35, 32;
-	// inline asm
-	cvta.to.global.u64 %rd43, %rd44;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r39,%r40,%r41,%r42}, [%rd43];
-	// inline asm
-	add.s64 	%rd47, %rd35, 48;
-	// inline asm
-	cvta.to.global.u64 %rd46, %rd47;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r43,%r44,%r45,%r46}, [%rd46];
-	// inline asm
-	add.s64 	%rd50, %rd35, 64;
-	// inline asm
-	cvta.to.global.u64 %rd49, %rd50;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r47,%r48,%r49,%r50}, [%rd49];
-	// inline asm
-	add.s64 	%rd53, %rd35, 80;
-	// inline asm
-	cvta.to.global.u64 %rd52, %rd53;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r51,%r52,%r53,%r54}, [%rd52];
-	// inline asm
-	add.s64 	%rd56, %rd35, 96;
-	// inline asm
-	cvta.to.global.u64 %rd55, %rd56;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r55,%r56,%r57,%r58}, [%rd55];
-	// inline asm
-	add.s64 	%rd59, %rd35, 112;
-	// inline asm
-	cvta.to.global.u64 %rd58, %rd59;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r59,%r60,%r61,%r62}, [%rd58];
-	// inline asm
-	add.s64 	%rd62, %rd35, 128;
-	// inline asm
-	cvta.to.global.u64 %rd61, %rd62;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r63,%r64,%r65,%r66}, [%rd61];
-	// inline asm
-	add.s64 	%rd65, %rd35, 144;
-	// inline asm
-	cvta.to.global.u64 %rd64, %rd65;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r67,%r68,%r69,%r70}, [%rd64];
-	// inline asm
-	mov.b32 	 %f588, %r34;
-	mov.b32 	 %f589, %r35;
-	cvt.u32.u16	%r87, %rs7;
-	add.s32 	%r88, %r87, -1;
-	cvt.rn.f32.s32	%f590, %r88;
-	sub.f32 	%f591, %f574, %f588;
-	mul.f32 	%f592, %f591, %f590;
-	sub.f32 	%f593, %f589, %f588;
-	div.rn.f32 	%f594, %f592, %f593;
-	min.f32 	%f595, %f590, %f594;
-	mov.f32 	%f596, 0f00000000;
-	max.f32 	%f597, %f596, %f595;
-	cvt.rmi.f32.f32	%f598, %f597;
-	cvt.rzi.s32.f32	%r89, %f598;
-	mul.wide.s32 	%rd79, %r89, 64;
-	add.s64 	%rd68, %rd44, %rd79;
-	// inline asm
-	cvta.to.global.u64 %rd67, %rd68;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r71,%r72,%r73,%r74}, [%rd67];
-	// inline asm
-	mov.b32 	 %f2240, %r71;
-	mov.b32 	 %f2241, %r72;
-	mov.b32 	 %f2242, %r73;
-	mov.b32 	 %f2243, %r74;
-	add.s64 	%rd71, %rd68, 16;
-	// inline asm
-	cvta.to.global.u64 %rd70, %rd71;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r75,%r76,%r77,%r78}, [%rd70];
-	// inline asm
-	mov.b32 	 %f2244, %r75;
-	mov.b32 	 %f2245, %r76;
-	mov.b32 	 %f2246, %r77;
-	mov.b32 	 %f2247, %r78;
-	add.s64 	%rd74, %rd68, 32;
-	// inline asm
-	cvta.to.global.u64 %rd73, %rd74;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r79,%r80,%r81,%r82}, [%rd73];
-	// inline asm
-	sub.f32 	%f37, %f597, %f598;
-	mov.b32 	 %f2248, %r79;
-	mov.b32 	 %f2249, %r80;
-	mov.b32 	 %f2250, %r81;
-	mov.b32 	 %f2251, %r82;
-	add.s64 	%rd77, %rd68, 48;
-	// inline asm
-	cvta.to.global.u64 %rd76, %rd77;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r83,%r84,%r85,%r86}, [%rd76];
-	// inline asm
-	mov.b32 	 %f2252, %r83;
-	mov.b32 	 %f2253, %r84;
-	mov.b32 	 %f2254, %r85;
-	mov.b32 	 %f2255, %r86;
-	setp.leu.f32	%p16, %f37, 0f00000000;
-	@%p16 bra 	BB11_12;
-
-	cvt.rmi.f32.f32	%f2209, %f597;
-	cvt.rzi.s32.f32	%r423, %f2209;
-	cvt.s64.s32	%rd262, %r423;
-	shl.b64 	%rd92, %rd262, 6;
-	add.s64 	%rd93, %rd92, %rd35;
-	add.s64 	%rd81, %rd93, 96;
-	// inline asm
-	cvta.to.global.u64 %rd80, %rd81;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r90,%r91,%r92,%r93}, [%rd80];
-	// inline asm
-	mov.b32 	 %f599, %r90;
-	mov.b32 	 %f600, %r91;
-	mov.b32 	 %f601, %r92;
-	mov.b32 	 %f602, %r93;
-	add.s64 	%rd84, %rd93, 112;
-	// inline asm
-	cvta.to.global.u64 %rd83, %rd84;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r94,%r95,%r96,%r97}, [%rd83];
-	// inline asm
-	mov.b32 	 %f603, %r94;
-	mov.b32 	 %f604, %r95;
-	mov.b32 	 %f605, %r96;
-	mov.b32 	 %f606, %r97;
-	add.s64 	%rd87, %rd93, 128;
-	// inline asm
-	cvta.to.global.u64 %rd86, %rd87;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r98,%r99,%r100,%r101}, [%rd86];
-	// inline asm
-	mov.b32 	 %f607, %r98;
-	mov.b32 	 %f608, %r99;
-	mov.b32 	 %f609, %r100;
-	mov.b32 	 %f610, %r101;
-	add.s64 	%rd90, %rd93, 144;
-	// inline asm
-	cvta.to.global.u64 %rd89, %rd90;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r102,%r103,%r104,%r105}, [%rd89];
-	// inline asm
-	mov.f32 	%f611, 0f3F800000;
-	sub.f32 	%f612, %f611, %f37;
-	mul.f32 	%f613, %f37, %f599;
-	mul.f32 	%f614, %f37, %f600;
-	mul.f32 	%f615, %f37, %f601;
-	mul.f32 	%f616, %f37, %f602;
-	fma.rn.f32 	%f2240, %f612, %f2240, %f613;
-	fma.rn.f32 	%f2241, %f612, %f2241, %f614;
-	fma.rn.f32 	%f2242, %f612, %f2242, %f615;
-	fma.rn.f32 	%f2243, %f612, %f2243, %f616;
-	mul.f32 	%f617, %f37, %f603;
-	mul.f32 	%f618, %f37, %f604;
-	mul.f32 	%f619, %f37, %f605;
-	mul.f32 	%f620, %f37, %f606;
-	fma.rn.f32 	%f2244, %f612, %f2244, %f617;
-	fma.rn.f32 	%f2245, %f612, %f2245, %f618;
-	fma.rn.f32 	%f2246, %f612, %f2246, %f619;
-	fma.rn.f32 	%f2247, %f612, %f2247, %f620;
-	mul.f32 	%f621, %f37, %f607;
-	mul.f32 	%f622, %f37, %f608;
-	mul.f32 	%f623, %f37, %f609;
-	mul.f32 	%f624, %f37, %f610;
-	fma.rn.f32 	%f2248, %f612, %f2248, %f621;
-	fma.rn.f32 	%f625, %f612, %f2249, %f622;
-	fma.rn.f32 	%f626, %f612, %f2250, %f623;
-	fma.rn.f32 	%f627, %f612, %f2251, %f624;
-	mov.b32 	 %f628, %r102;
-	mov.b32 	 %f629, %r103;
-	mov.b32 	 %f630, %r104;
-	mov.b32 	 %f631, %r105;
-	mul.f32 	%f632, %f37, %f628;
-	mul.f32 	%f633, %f37, %f629;
-	mul.f32 	%f634, %f37, %f630;
-	mul.f32 	%f635, %f37, %f631;
-	fma.rn.f32 	%f636, %f612, %f2252, %f632;
-	fma.rn.f32 	%f2253, %f612, %f2253, %f633;
-	fma.rn.f32 	%f2254, %f612, %f2254, %f634;
-	fma.rn.f32 	%f2255, %f612, %f2255, %f635;
-	mul.f32 	%f637, %f626, %f626;
-	fma.rn.f32 	%f638, %f625, %f625, %f637;
-	fma.rn.f32 	%f639, %f627, %f627, %f638;
-	fma.rn.f32 	%f640, %f636, %f636, %f639;
-	sqrt.rn.f32 	%f641, %f640;
-	rcp.rn.f32 	%f642, %f641;
-	mul.f32 	%f2249, %f625, %f642;
-	mul.f32 	%f2250, %f626, %f642;
-	mul.f32 	%f2251, %f627, %f642;
-	mul.f32 	%f2252, %f636, %f642;
-
-BB11_12:
-	mul.f32 	%f643, %f2250, %f2250;
-	fma.rn.f32 	%f644, %f2249, %f2249, %f643;
-	fma.rn.f32 	%f645, %f2251, %f2251, %f644;
-	fma.rn.f32 	%f646, %f2252, %f2252, %f645;
-	rcp.rn.f32 	%f647, %f646;
-	mul.f32 	%f648, %f2249, %f647;
-	mul.f32 	%f649, %f2250, %f647;
-	mul.f32 	%f650, %f2251, %f647;
-	mul.f32 	%f651, %f2252, %f647;
-	mul.f32 	%f652, %f2249, %f648;
-	mul.f32 	%f653, %f2250, %f649;
-	mul.f32 	%f654, %f2251, %f650;
-	mul.f32 	%f655, %f2249, %f649;
-	mul.f32 	%f656, %f2251, %f651;
-	mul.f32 	%f657, %f2249, %f650;
-	mul.f32 	%f658, %f2250, %f651;
-	mul.f32 	%f659, %f2250, %f650;
-	mul.f32 	%f660, %f2249, %f651;
-	sub.f32 	%f661, %f652, %f653;
-	sub.f32 	%f662, %f661, %f654;
-	fma.rn.f32 	%f663, %f2252, %f651, %f662;
-	sub.f32 	%f664, %f655, %f656;
-	add.f32 	%f665, %f664, %f664;
-	add.f32 	%f666, %f657, %f658;
-	add.f32 	%f667, %f666, %f666;
-	add.f32 	%f668, %f655, %f656;
-	add.f32 	%f669, %f668, %f668;
-	sub.f32 	%f670, %f653, %f652;
-	sub.f32 	%f671, %f670, %f654;
-	fma.rn.f32 	%f672, %f2252, %f651, %f671;
-	sub.f32 	%f673, %f659, %f660;
-	add.f32 	%f674, %f673, %f673;
-	sub.f32 	%f675, %f657, %f658;
-	add.f32 	%f676, %f675, %f675;
-	add.f32 	%f677, %f659, %f660;
-	add.f32 	%f678, %f677, %f677;
-	neg.f32 	%f679, %f652;
-	sub.f32 	%f680, %f679, %f653;
-	add.f32 	%f681, %f654, %f680;
-	fma.rn.f32 	%f682, %f2252, %f651, %f681;
-	mul.f32 	%f683, %f2243, %f663;
-	fma.rn.f32 	%f684, %f2246, %f665, %f683;
-	fma.rn.f32 	%f685, %f2248, %f667, %f684;
-	sub.f32 	%f2267, %f2253, %f685;
-	mul.f32 	%f686, %f2246, %f672;
-	fma.rn.f32 	%f687, %f2243, %f669, %f686;
-	fma.rn.f32 	%f688, %f2248, %f674, %f687;
-	sub.f32 	%f2263, %f2254, %f688;
-	mul.f32 	%f689, %f2246, %f678;
-	fma.rn.f32 	%f690, %f2243, %f676, %f689;
-	fma.rn.f32 	%f691, %f2248, %f682, %f690;
-	sub.f32 	%f2259, %f2255, %f691;
-	mul.f32 	%f692, %f2242, %f663;
-	fma.rn.f32 	%f693, %f2245, %f665, %f692;
-	fma.rn.f32 	%f2266, %f2247, %f667, %f693;
-	mul.f32 	%f694, %f2245, %f672;
-	fma.rn.f32 	%f695, %f2242, %f669, %f694;
-	fma.rn.f32 	%f2262, %f2247, %f674, %f695;
-	mul.f32 	%f696, %f2245, %f678;
-	fma.rn.f32 	%f697, %f2242, %f676, %f696;
-	fma.rn.f32 	%f2258, %f2247, %f682, %f697;
-	mul.f32 	%f698, %f2241, %f663;
-	fma.rn.f32 	%f2265, %f2244, %f665, %f698;
-	mul.f32 	%f699, %f2244, %f672;
-	fma.rn.f32 	%f2261, %f2241, %f669, %f699;
-	mul.f32 	%f700, %f2244, %f678;
-	fma.rn.f32 	%f2257, %f2241, %f676, %f700;
-	mul.f32 	%f2264, %f2240, %f663;
-	mul.f32 	%f2260, %f2240, %f669;
-	mul.f32 	%f2256, %f2240, %f676;
-
-BB11_15:
-	mul.f32 	%f738, %f2257, %f2262;
-	mul.f32 	%f739, %f2258, %f2261;
-	sub.f32 	%f740, %f739, %f738;
-	mul.f32 	%f741, %f2264, %f740;
-	mul.f32 	%f742, %f2256, %f2262;
-	mul.f32 	%f743, %f2258, %f2260;
-	sub.f32 	%f744, %f743, %f742;
-	mul.f32 	%f745, %f744, %f2265;
-	sub.f32 	%f746, %f741, %f745;
-	mul.f32 	%f747, %f2256, %f2261;
-	mul.f32 	%f748, %f2257, %f2260;
-	sub.f32 	%f749, %f748, %f747;
-	fma.rn.f32 	%f750, %f749, %f2266, %f746;
-	rcp.rn.f32 	%f751, %f750;
-	mul.f32 	%f2276, %f740, %f751;
-	mul.f32 	%f752, %f2258, %f2265;
-	mul.f32 	%f753, %f2257, %f2266;
-	sub.f32 	%f754, %f753, %f752;
-	mul.f32 	%f2277, %f751, %f754;
-	mul.f32 	%f755, %f2261, %f2266;
-	mul.f32 	%f756, %f2262, %f2265;
-	sub.f32 	%f757, %f756, %f755;
-	mul.f32 	%f2278, %f751, %f757;
-	sub.f32 	%f758, %f742, %f743;
-	mul.f32 	%f2272, %f758, %f751;
-	mul.f32 	%f759, %f2256, %f2266;
-	mul.f32 	%f760, %f2258, %f2264;
-	sub.f32 	%f761, %f760, %f759;
-	mul.f32 	%f2273, %f751, %f761;
-	mul.f32 	%f762, %f2262, %f2264;
-	mul.f32 	%f763, %f2260, %f2266;
-	sub.f32 	%f764, %f763, %f762;
-	mul.f32 	%f2274, %f751, %f764;
-	mul.f32 	%f2268, %f749, %f751;
-	mul.f32 	%f765, %f2257, %f2264;
-	mul.f32 	%f766, %f2256, %f2265;
-	sub.f32 	%f767, %f766, %f765;
-	mul.f32 	%f2269, %f767, %f751;
-	mul.f32 	%f768, %f2260, %f2265;
-	mul.f32 	%f769, %f2261, %f2264;
-	sub.f32 	%f770, %f769, %f768;
-	mul.f32 	%f2270, %f770, %f751;
-	mul.f32 	%f771, %f2267, %f2276;
-	neg.f32 	%f772, %f771;
-	mul.f32 	%f773, %f2263, %f2277;
-	sub.f32 	%f774, %f772, %f773;
-	mul.f32 	%f775, %f2259, %f2278;
-	sub.f32 	%f2279, %f774, %f775;
-	mul.f32 	%f776, %f2267, %f2272;
-	neg.f32 	%f777, %f776;
-	mul.f32 	%f778, %f2263, %f2273;
-	sub.f32 	%f779, %f777, %f778;
-	mul.f32 	%f780, %f2259, %f2274;
-	sub.f32 	%f2275, %f779, %f780;
-	mul.f32 	%f781, %f2267, %f2268;
-	neg.f32 	%f782, %f781;
-	mul.f32 	%f783, %f2263, %f2269;
-	sub.f32 	%f784, %f782, %f783;
-	mul.f32 	%f785, %f2259, %f2270;
-	sub.f32 	%f2271, %f784, %f785;
-	bra.uni 	BB11_16;
-
-BB11_5:
-	setp.ne.s32	%p14, %r17, 1;
-	mov.f32 	%f2269, %f2268;
-	mov.f32 	%f2271, %f2268;
-	mov.f32 	%f2272, %f2268;
-	mov.f32 	%f2273, %f2270;
-	mov.f32 	%f2274, %f2268;
-	mov.f32 	%f2275, %f2268;
-	mov.f32 	%f2276, %f2270;
-	mov.f32 	%f2277, %f2268;
-	mov.f32 	%f2278, %f2268;
-	mov.f32 	%f2279, %f2268;
-	@%p14 bra 	BB11_16;
-
-	// inline asm
-	call (%rd22), _optix_get_static_transform_from_handle, (%rd20);
-	// inline asm
-	add.s64 	%rd264, %rd22, 64;
-
-BB11_8:
-	// inline asm
-	cvta.to.global.u64 %rd26, %rd264;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r19,%r20,%r21,%r22}, [%rd26];
-	// inline asm
-	mov.b32 	 %f2276, %r19;
-	mov.b32 	 %f2277, %r20;
-	mov.b32 	 %f2278, %r21;
-	mov.b32 	 %f2279, %r22;
-	add.s64 	%rd30, %rd264, 16;
-	// inline asm
-	cvta.to.global.u64 %rd29, %rd30;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r23,%r24,%r25,%r26}, [%rd29];
-	// inline asm
-	mov.b32 	 %f2272, %r23;
-	mov.b32 	 %f2273, %r24;
-	mov.b32 	 %f2274, %r25;
-	mov.b32 	 %f2275, %r26;
-	add.s64 	%rd33, %rd264, 32;
-	// inline asm
-	cvta.to.global.u64 %rd32, %rd33;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r27,%r28,%r29,%r30}, [%rd32];
-	// inline asm
-	mov.b32 	 %f2268, %r27;
-	mov.b32 	 %f2269, %r28;
-	mov.b32 	 %f2270, %r29;
-	mov.b32 	 %f2271, %r30;
-
-BB11_16:
-	setp.eq.s32	%p18, %r425, 0;
-	@%p18 bra 	BB11_17;
-	bra.uni 	BB11_18;
-
-BB11_17:
-	mov.f32 	%f2239, %f2279;
-	mov.f32 	%f2238, %f2278;
-	mov.f32 	%f2237, %f2277;
-	mov.f32 	%f2236, %f2276;
-	mov.f32 	%f2235, %f2275;
-	mov.f32 	%f2234, %f2274;
-	mov.f32 	%f2233, %f2273;
-	mov.f32 	%f2232, %f2272;
-	mov.f32 	%f2231, %f2271;
-	mov.f32 	%f2230, %f2270;
-	mov.f32 	%f2229, %f2269;
-	mov.f32 	%f2228, %f2268;
-	bra.uni 	BB11_19;
-
-BB11_18:
-	mul.f32 	%f786, %f2232, %f2277;
-	fma.rn.f32 	%f787, %f2236, %f2276, %f786;
-	fma.rn.f32 	%f151, %f2228, %f2278, %f787;
-	mul.f32 	%f788, %f2233, %f2277;
-	fma.rn.f32 	%f789, %f2237, %f2276, %f788;
-	fma.rn.f32 	%f152, %f2229, %f2278, %f789;
-	mul.f32 	%f790, %f2234, %f2277;
-	fma.rn.f32 	%f791, %f2238, %f2276, %f790;
-	fma.rn.f32 	%f153, %f2230, %f2278, %f791;
-	mul.f32 	%f792, %f2235, %f2277;
-	fma.rn.f32 	%f793, %f2239, %f2276, %f792;
-	fma.rn.f32 	%f794, %f2231, %f2278, %f793;
-	add.f32 	%f154, %f2279, %f794;
-	mul.f32 	%f795, %f2232, %f2273;
-	fma.rn.f32 	%f796, %f2236, %f2272, %f795;
-	fma.rn.f32 	%f155, %f2228, %f2274, %f796;
-	mul.f32 	%f797, %f2233, %f2273;
-	fma.rn.f32 	%f798, %f2237, %f2272, %f797;
-	fma.rn.f32 	%f156, %f2229, %f2274, %f798;
-	mul.f32 	%f799, %f2234, %f2273;
-	fma.rn.f32 	%f800, %f2238, %f2272, %f799;
-	fma.rn.f32 	%f157, %f2230, %f2274, %f800;
-	mul.f32 	%f801, %f2235, %f2273;
-	fma.rn.f32 	%f802, %f2239, %f2272, %f801;
-	fma.rn.f32 	%f803, %f2231, %f2274, %f802;
-	add.f32 	%f158, %f2275, %f803;
-	mul.f32 	%f804, %f2232, %f2269;
-	fma.rn.f32 	%f805, %f2236, %f2268, %f804;
-	fma.rn.f32 	%f2228, %f2228, %f2270, %f805;
-	mul.f32 	%f806, %f2233, %f2269;
-	fma.rn.f32 	%f807, %f2237, %f2268, %f806;
-	fma.rn.f32 	%f2229, %f2229, %f2270, %f807;
-	mul.f32 	%f808, %f2234, %f2269;
-	fma.rn.f32 	%f809, %f2238, %f2268, %f808;
-	fma.rn.f32 	%f2230, %f2230, %f2270, %f809;
-	mul.f32 	%f810, %f2235, %f2269;
-	fma.rn.f32 	%f811, %f2239, %f2268, %f810;
-	fma.rn.f32 	%f812, %f2231, %f2270, %f811;
-	add.f32 	%f2231, %f2271, %f812;
-	mov.f32 	%f2239, %f154;
-	mov.f32 	%f2238, %f153;
-	mov.f32 	%f2237, %f152;
-	mov.f32 	%f2236, %f151;
-	mov.f32 	%f2235, %f158;
-	mov.f32 	%f2234, %f157;
-	mov.f32 	%f2233, %f156;
-	mov.f32 	%f2232, %f155;
-
-BB11_19:
-	add.s32 	%r425, %r425, 1;
-	setp.lt.u32	%p19, %r425, %r14;
-	@%p19 bra 	BB11_3;
-
-	mul.f32 	%f813, %f571, %f2236;
-	fma.rn.f32 	%f814, %f572, %f2237, %f813;
-	fma.rn.f32 	%f815, %f2292, %f2238, %f814;
-	add.f32 	%f2294, %f2239, %f815;
-	mul.f32 	%f816, %f571, %f2232;
-	fma.rn.f32 	%f817, %f572, %f2233, %f816;
-	fma.rn.f32 	%f818, %f2292, %f2234, %f817;
-	add.f32 	%f2293, %f2235, %f818;
-	mul.f32 	%f819, %f571, %f2228;
-	fma.rn.f32 	%f820, %f572, %f2229, %f819;
-	fma.rn.f32 	%f821, %f2292, %f2230, %f820;
-	add.f32 	%f2292, %f2231, %f821;
-	bra.uni 	BB11_21;
-
-BB11_1:
-	mov.f32 	%f2293, %f572;
-	mov.f32 	%f2294, %f571;
-
-BB11_21:
-	setp.eq.s32	%p331, %r14, 0;
-	// inline asm
-	call (%f822), _optix_get_world_ray_direction_x, ();
-	// inline asm
-	// inline asm
-	call (%f823), _optix_get_world_ray_direction_y, ();
-	// inline asm
-	// inline asm
-	call (%f2343), _optix_get_world_ray_direction_z, ();
-	// inline asm
-	// inline asm
-	call (%f825), _optix_get_ray_time, ();
-	// inline asm
-	mov.u32 	%r426, 0;
-	@%p331 bra 	BB11_22;
-
-BB11_23:
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r155,%r156,%r157,%r158}, [%rd126];
+	// end inline asm
+	mov.b32 	%f682, %r155;
+	mov.b32 	%f683, %r156;
+	mov.b32 	%f684, %r157;
+	mov.b32 	%f685, %r158;
+	mul.f32 	%f686, %f90, %f682;
+	mul.f32 	%f687, %f90, %f683;
+	mul.f32 	%f688, %f90, %f684;
+	mul.f32 	%f689, %f90, %f685;
+	fma.rn.f32 	%f1954, %f681, %f1954, %f686;
+	fma.rn.f32 	%f1953, %f681, %f1953, %f687;
+	fma.rn.f32 	%f1952, %f681, %f1952, %f688;
+	fma.rn.f32 	%f1951, %f681, %f1951, %f689;
+	add.s64 	%rd130, %rd136, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd129, %rd130;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r159,%r160,%r161,%r162}, [%rd129];
+	// end inline asm
+	mov.b32 	%f690, %r159;
+	mov.b32 	%f691, %r160;
+	mov.b32 	%f692, %r161;
+	mov.b32 	%f693, %r162;
+	mul.f32 	%f694, %f90, %f690;
+	mul.f32 	%f695, %f90, %f691;
+	mul.f32 	%f696, %f90, %f692;
+	mul.f32 	%f697, %f90, %f693;
+	fma.rn.f32 	%f1958, %f681, %f1958, %f694;
+	fma.rn.f32 	%f1957, %f681, %f1957, %f695;
+	fma.rn.f32 	%f1956, %f681, %f1956, %f696;
+	fma.rn.f32 	%f1955, %f681, %f1955, %f697;
+	add.s64 	%rd133, %rd136, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd132, %rd133;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r163,%r164,%r165,%r166}, [%rd132];
+	// end inline asm
+	mov.b32 	%f698, %r163;
+	mov.b32 	%f699, %r164;
+	mov.b32 	%f700, %r165;
+	mov.b32 	%f701, %r166;
+	mul.f32 	%f702, %f90, %f698;
+	mul.f32 	%f703, %f90, %f699;
+	mul.f32 	%f704, %f90, %f700;
+	mul.f32 	%f705, %f90, %f701;
+	fma.rn.f32 	%f1962, %f681, %f1962, %f702;
+	fma.rn.f32 	%f1961, %f681, %f1961, %f703;
+	fma.rn.f32 	%f1960, %f681, %f1960, %f704;
+	fma.rn.f32 	%f1959, %f681, %f1959, %f705;
+	bra.uni 	$L__BB11_15;
+
+$L__BB11_4:
+	mov.f32 	%f1963, 0f00000000;
+	mov.f32 	%f1966, 0f3F800000;
+	setp.eq.s32 	%p16, %r19, 4;
+	@%p16 bra 	$L__BB11_7;
+
+	setp.ne.s32 	%p17, %r19, 1;
+	mov.f32 	%f1964, %f1963;
+	mov.f32 	%f1965, %f1963;
+	mov.f32 	%f1967, %f1963;
+	mov.f32 	%f1968, %f1963;
+	mov.f32 	%f1969, %f1966;
+	mov.f32 	%f1970, %f1963;
+	mov.f32 	%f1971, %f1963;
+	mov.f32 	%f1972, %f1966;
+	mov.f32 	%f1973, %f1963;
+	mov.f32 	%f1974, %f1963;
+	@%p17 bra 	$L__BB11_16;
+
+	// begin inline asm
+	call (%rd20), _optix_get_static_transform_from_handle, (%rd18);
+	// end inline asm
+	add.s64 	%rd257, %rd20, 64;
+	bra.uni 	$L__BB11_8;
+
+$L__BB11_10:
+	// begin inline asm
+	call (%rd33), _optix_get_srt_motion_transform_from_handle, (%rd18);
+	// end inline asm
+	// begin inline asm
+	cvta.to.global.u64 %rd35, %rd33;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r33,%r34,%r35,%r36}, [%rd35];
+	// end inline asm
+	add.s64 	%rd39, %rd33, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd38, %rd39;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r37,%r38,%r39,%r40}, [%rd38];
+	// end inline asm
+	add.s64 	%rd42, %rd33, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd41, %rd42;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r41,%r42,%r43,%r44}, [%rd41];
+	// end inline asm
+	add.s64 	%rd45, %rd33, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd44, %rd45;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r45,%r46,%r47,%r48}, [%rd44];
+	// end inline asm
+	add.s64 	%rd48, %rd33, 64;
+	// begin inline asm
+	cvta.to.global.u64 %rd47, %rd48;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r49,%r50,%r51,%r52}, [%rd47];
+	// end inline asm
+	add.s64 	%rd51, %rd33, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd50, %rd51;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r53,%r54,%r55,%r56}, [%rd50];
+	// end inline asm
+	add.s64 	%rd54, %rd33, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd53, %rd54;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r57,%r58,%r59,%r60}, [%rd53];
+	// end inline asm
+	add.s64 	%rd57, %rd33, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd56, %rd57;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r61,%r62,%r63,%r64}, [%rd56];
+	// end inline asm
+	add.s64 	%rd60, %rd33, 128;
+	// begin inline asm
+	cvta.to.global.u64 %rd59, %rd60;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r65,%r66,%r67,%r68}, [%rd59];
+	// end inline asm
+	add.s64 	%rd63, %rd33, 144;
+	// begin inline asm
+	cvta.to.global.u64 %rd62, %rd63;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r69,%r70,%r71,%r72}, [%rd62];
+	// end inline asm
+	mov.b32 	%f556, %r36;
+	mov.b32 	%f557, %r37;
+	and.b32  	%r89, %r35, 65535;
+	add.s32 	%r90, %r89, -1;
+	cvt.rn.f32.s32 	%f558, %r90;
+	sub.f32 	%f559, %f541, %f556;
+	mul.f32 	%f560, %f559, %f558;
+	sub.f32 	%f561, %f557, %f556;
+	div.rn.f32 	%f562, %f560, %f561;
+	min.f32 	%f563, %f558, %f562;
+	mov.f32 	%f564, 0f00000000;
+	max.f32 	%f565, %f564, %f563;
+	cvt.rmi.f32.f32 	%f566, %f565;
+	sub.f32 	%f29, %f565, %f566;
+	cvt.rzi.s32.f32 	%r91, %f566;
+	mul.wide.s32 	%rd77, %r91, 64;
+	add.s64 	%rd66, %rd42, %rd77;
+	// begin inline asm
+	cvta.to.global.u64 %rd65, %rd66;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r73,%r74,%r75,%r76}, [%rd65];
+	// end inline asm
+	mov.b32 	%f1935, %r73;
+	mov.b32 	%f1936, %r74;
+	mov.b32 	%f1937, %r75;
+	mov.b32 	%f1938, %r76;
+	add.s64 	%rd69, %rd66, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd68, %rd69;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r77,%r78,%r79,%r80}, [%rd68];
+	// end inline asm
+	mov.b32 	%f1939, %r77;
+	mov.b32 	%f1940, %r78;
+	mov.b32 	%f1941, %r79;
+	mov.b32 	%f1942, %r80;
+	add.s64 	%rd72, %rd66, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd71, %rd72;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r81,%r82,%r83,%r84}, [%rd71];
+	// end inline asm
+	mov.b32 	%f1943, %r81;
+	mov.b32 	%f1944, %r82;
+	mov.b32 	%f1945, %r83;
+	mov.b32 	%f1946, %r84;
+	add.s64 	%rd75, %rd66, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd74, %rd75;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r85,%r86,%r87,%r88}, [%rd74];
+	// end inline asm
+	mov.b32 	%f1947, %r85;
+	mov.b32 	%f1948, %r86;
+	mov.b32 	%f1949, %r87;
+	mov.b32 	%f1950, %r88;
+	setp.leu.f32 	%p19, %f29, 0f00000000;
+	@%p19 bra 	$L__BB11_12;
+
+	mov.f32 	%f567, 0f3F800000;
+	sub.f32 	%f568, %f567, %f29;
+	add.s64 	%rd79, %rd66, 64;
+	// begin inline asm
+	cvta.to.global.u64 %rd78, %rd79;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r92,%r93,%r94,%r95}, [%rd78];
+	// end inline asm
+	mov.b32 	%f569, %r92;
+	mov.b32 	%f570, %r93;
+	mov.b32 	%f571, %r94;
+	mov.b32 	%f572, %r95;
+	mul.f32 	%f573, %f29, %f569;
+	mul.f32 	%f574, %f29, %f570;
+	mul.f32 	%f575, %f29, %f571;
+	mul.f32 	%f576, %f29, %f572;
+	fma.rn.f32 	%f1935, %f568, %f1935, %f573;
+	fma.rn.f32 	%f1936, %f568, %f1936, %f574;
+	fma.rn.f32 	%f1937, %f568, %f1937, %f575;
+	fma.rn.f32 	%f1938, %f568, %f1938, %f576;
+	add.s64 	%rd82, %rd66, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd81, %rd82;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r96,%r97,%r98,%r99}, [%rd81];
+	// end inline asm
+	mov.b32 	%f577, %r96;
+	mov.b32 	%f578, %r97;
+	mov.b32 	%f579, %r98;
+	mov.b32 	%f580, %r99;
+	mul.f32 	%f581, %f29, %f577;
+	mul.f32 	%f582, %f29, %f578;
+	mul.f32 	%f583, %f29, %f579;
+	mul.f32 	%f584, %f29, %f580;
+	fma.rn.f32 	%f1939, %f568, %f1939, %f581;
+	fma.rn.f32 	%f1940, %f568, %f1940, %f582;
+	fma.rn.f32 	%f1941, %f568, %f1941, %f583;
+	fma.rn.f32 	%f1942, %f568, %f1942, %f584;
+	add.s64 	%rd85, %rd66, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd84, %rd85;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r100,%r101,%r102,%r103}, [%rd84];
+	// end inline asm
+	mov.b32 	%f585, %r100;
+	mov.b32 	%f586, %r101;
+	mov.b32 	%f587, %r102;
+	mov.b32 	%f588, %r103;
+	mul.f32 	%f589, %f29, %f585;
+	mul.f32 	%f590, %f29, %f586;
+	mul.f32 	%f591, %f29, %f587;
+	mul.f32 	%f592, %f29, %f588;
+	fma.rn.f32 	%f1943, %f568, %f1943, %f589;
+	fma.rn.f32 	%f593, %f568, %f1944, %f590;
+	fma.rn.f32 	%f594, %f568, %f1945, %f591;
+	fma.rn.f32 	%f595, %f568, %f1946, %f592;
+	add.s64 	%rd88, %rd66, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd87, %rd88;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r104,%r105,%r106,%r107}, [%rd87];
+	// end inline asm
+	mov.b32 	%f596, %r104;
+	mov.b32 	%f597, %r105;
+	mov.b32 	%f598, %r106;
+	mov.b32 	%f599, %r107;
+	mul.f32 	%f600, %f29, %f596;
+	mul.f32 	%f601, %f29, %f597;
+	mul.f32 	%f602, %f29, %f598;
+	mul.f32 	%f603, %f29, %f599;
+	fma.rn.f32 	%f604, %f568, %f1947, %f600;
+	fma.rn.f32 	%f1948, %f568, %f1948, %f601;
+	fma.rn.f32 	%f1949, %f568, %f1949, %f602;
+	fma.rn.f32 	%f1950, %f568, %f1950, %f603;
+	mul.f32 	%f605, %f594, %f594;
+	fma.rn.f32 	%f606, %f593, %f593, %f605;
+	fma.rn.f32 	%f607, %f595, %f595, %f606;
+	fma.rn.f32 	%f608, %f604, %f604, %f607;
+	sqrt.rn.f32 	%f609, %f608;
+	rcp.rn.f32 	%f610, %f609;
+	mul.f32 	%f1944, %f593, %f610;
+	mul.f32 	%f1945, %f594, %f610;
+	mul.f32 	%f1946, %f595, %f610;
+	mul.f32 	%f1947, %f610, %f604;
+
+$L__BB11_12:
+	mul.f32 	%f611, %f1945, %f1945;
+	fma.rn.f32 	%f612, %f1944, %f1944, %f611;
+	fma.rn.f32 	%f613, %f1946, %f1946, %f612;
+	fma.rn.f32 	%f614, %f1947, %f1947, %f613;
+	rcp.rn.f32 	%f615, %f614;
+	mul.f32 	%f616, %f1944, %f615;
+	mul.f32 	%f617, %f1945, %f615;
+	mul.f32 	%f618, %f1946, %f615;
+	mul.f32 	%f619, %f1947, %f615;
+	mul.f32 	%f620, %f1944, %f616;
+	mul.f32 	%f621, %f1945, %f617;
+	mul.f32 	%f622, %f1946, %f618;
+	mul.f32 	%f623, %f1944, %f617;
+	mul.f32 	%f624, %f1946, %f619;
+	mul.f32 	%f625, %f1944, %f618;
+	mul.f32 	%f626, %f1945, %f619;
+	mul.f32 	%f627, %f1945, %f618;
+	mul.f32 	%f628, %f1944, %f619;
+	sub.f32 	%f629, %f620, %f621;
+	sub.f32 	%f630, %f629, %f622;
+	fma.rn.f32 	%f631, %f1947, %f619, %f630;
+	sub.f32 	%f632, %f623, %f624;
+	add.f32 	%f633, %f632, %f632;
+	add.f32 	%f634, %f625, %f626;
+	add.f32 	%f635, %f634, %f634;
+	add.f32 	%f636, %f623, %f624;
+	add.f32 	%f637, %f636, %f636;
+	sub.f32 	%f638, %f621, %f620;
+	sub.f32 	%f639, %f638, %f622;
+	fma.rn.f32 	%f640, %f1947, %f619, %f639;
+	sub.f32 	%f641, %f627, %f628;
+	add.f32 	%f642, %f641, %f641;
+	sub.f32 	%f643, %f625, %f626;
+	add.f32 	%f644, %f643, %f643;
+	add.f32 	%f645, %f627, %f628;
+	add.f32 	%f646, %f645, %f645;
+	neg.f32 	%f647, %f620;
+	sub.f32 	%f648, %f647, %f621;
+	add.f32 	%f649, %f622, %f648;
+	fma.rn.f32 	%f650, %f1947, %f619, %f649;
+	mul.f32 	%f651, %f1938, %f631;
+	fma.rn.f32 	%f652, %f1941, %f633, %f651;
+	fma.rn.f32 	%f653, %f1943, %f635, %f652;
+	sub.f32 	%f1951, %f1948, %f653;
+	mul.f32 	%f654, %f1941, %f640;
+	fma.rn.f32 	%f655, %f1938, %f637, %f654;
+	fma.rn.f32 	%f656, %f1943, %f642, %f655;
+	sub.f32 	%f1955, %f1949, %f656;
+	mul.f32 	%f657, %f1941, %f646;
+	fma.rn.f32 	%f658, %f1938, %f644, %f657;
+	fma.rn.f32 	%f659, %f1943, %f650, %f658;
+	sub.f32 	%f1959, %f1950, %f659;
+	mul.f32 	%f660, %f1937, %f631;
+	fma.rn.f32 	%f661, %f1940, %f633, %f660;
+	fma.rn.f32 	%f1952, %f1942, %f635, %f661;
+	mul.f32 	%f662, %f1940, %f640;
+	fma.rn.f32 	%f663, %f1937, %f637, %f662;
+	fma.rn.f32 	%f1956, %f1942, %f642, %f663;
+	mul.f32 	%f664, %f1940, %f646;
+	fma.rn.f32 	%f665, %f1937, %f644, %f664;
+	fma.rn.f32 	%f1960, %f1942, %f650, %f665;
+	mul.f32 	%f666, %f1936, %f631;
+	fma.rn.f32 	%f1953, %f1939, %f633, %f666;
+	mul.f32 	%f667, %f1939, %f640;
+	fma.rn.f32 	%f1957, %f1936, %f637, %f667;
+	mul.f32 	%f668, %f1939, %f646;
+	fma.rn.f32 	%f1961, %f1936, %f644, %f668;
+	mul.f32 	%f1954, %f1935, %f631;
+	mul.f32 	%f1958, %f1935, %f637;
+	mul.f32 	%f1962, %f1935, %f644;
+
+$L__BB11_15:
+	mul.f32 	%f706, %f1956, %f1961;
+	mul.f32 	%f707, %f1957, %f1960;
+	sub.f32 	%f708, %f707, %f706;
+	mul.f32 	%f709, %f1954, %f708;
+	mul.f32 	%f710, %f1956, %f1962;
+	mul.f32 	%f711, %f1958, %f1960;
+	sub.f32 	%f712, %f711, %f710;
+	mul.f32 	%f713, %f1953, %f712;
+	sub.f32 	%f714, %f709, %f713;
+	mul.f32 	%f715, %f1957, %f1962;
+	mul.f32 	%f716, %f1958, %f1961;
+	sub.f32 	%f717, %f716, %f715;
+	fma.rn.f32 	%f718, %f1952, %f717, %f714;
+	rcp.rn.f32 	%f719, %f718;
+	mul.f32 	%f1966, %f708, %f719;
+	mul.f32 	%f720, %f1953, %f1960;
+	mul.f32 	%f721, %f1952, %f1961;
+	sub.f32 	%f722, %f721, %f720;
+	mul.f32 	%f1965, %f722, %f719;
+	mul.f32 	%f723, %f1952, %f1957;
+	mul.f32 	%f724, %f1953, %f1956;
+	sub.f32 	%f725, %f724, %f723;
+	mul.f32 	%f1964, %f725, %f719;
+	sub.f32 	%f726, %f710, %f711;
+	mul.f32 	%f1970, %f726, %f719;
+	mul.f32 	%f727, %f1952, %f1962;
+	mul.f32 	%f728, %f1954, %f1960;
+	sub.f32 	%f729, %f728, %f727;
+	mul.f32 	%f1969, %f729, %f719;
+	mul.f32 	%f730, %f1954, %f1956;
+	mul.f32 	%f731, %f1952, %f1958;
+	sub.f32 	%f732, %f731, %f730;
+	mul.f32 	%f1968, %f732, %f719;
+	mul.f32 	%f1974, %f717, %f719;
+	mul.f32 	%f733, %f1954, %f1961;
+	mul.f32 	%f734, %f1953, %f1962;
+	sub.f32 	%f735, %f734, %f733;
+	mul.f32 	%f1973, %f735, %f719;
+	mul.f32 	%f736, %f1953, %f1958;
+	mul.f32 	%f737, %f1954, %f1957;
+	sub.f32 	%f738, %f737, %f736;
+	mul.f32 	%f1972, %f738, %f719;
+	mul.f32 	%f739, %f1951, %f1966;
+	neg.f32 	%f740, %f739;
+	mul.f32 	%f741, %f1955, %f1965;
+	sub.f32 	%f742, %f740, %f741;
+	mul.f32 	%f743, %f1959, %f1964;
+	sub.f32 	%f1963, %f742, %f743;
+	mul.f32 	%f744, %f1951, %f1970;
+	neg.f32 	%f745, %f744;
+	mul.f32 	%f746, %f1955, %f1969;
+	sub.f32 	%f747, %f745, %f746;
+	mul.f32 	%f748, %f1959, %f1968;
+	sub.f32 	%f1967, %f747, %f748;
+	mul.f32 	%f749, %f1951, %f1974;
+	neg.f32 	%f750, %f749;
+	mul.f32 	%f751, %f1955, %f1973;
+	sub.f32 	%f752, %f750, %f751;
+	mul.f32 	%f753, %f1959, %f1972;
+	sub.f32 	%f1971, %f752, %f753;
+	bra.uni 	$L__BB11_16;
+
+$L__BB11_7:
+	// begin inline asm
+	call (%rd257), _optix_get_instance_inverse_transform_from_handle, (%rd18);
+	// end inline asm
+
+$L__BB11_8:
+	// begin inline asm
+	cvta.to.global.u64 %rd24, %rd257;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r21,%r22,%r23,%r24}, [%rd24];
+	// end inline asm
+	mov.b32 	%f1966, %r21;
+	mov.b32 	%f1965, %r22;
+	mov.b32 	%f1964, %r23;
+	mov.b32 	%f1963, %r24;
+	add.s64 	%rd28, %rd257, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd27, %rd28;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r25,%r26,%r27,%r28}, [%rd27];
+	// end inline asm
+	mov.b32 	%f1970, %r25;
+	mov.b32 	%f1969, %r26;
+	mov.b32 	%f1968, %r27;
+	mov.b32 	%f1967, %r28;
+	add.s64 	%rd31, %rd257, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd30, %rd31;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r29,%r30,%r31,%r32}, [%rd30];
+	// end inline asm
+	mov.b32 	%f1974, %r29;
+	mov.b32 	%f1973, %r30;
+	mov.b32 	%f1972, %r31;
+	mov.b32 	%f1971, %r32;
+
+$L__BB11_16:
+	setp.eq.s32 	%p21, %r456, 0;
+	@%p21 bra 	$L__BB11_18;
+
+	mul.f32 	%f754, %f1931, %f1966;
+	fma.rn.f32 	%f755, %f1927, %f1965, %f754;
+	fma.rn.f32 	%f151, %f1923, %f1964, %f755;
+	mul.f32 	%f756, %f1932, %f1966;
+	fma.rn.f32 	%f757, %f1928, %f1965, %f756;
+	fma.rn.f32 	%f152, %f1924, %f1964, %f757;
+	mul.f32 	%f758, %f1933, %f1966;
+	fma.rn.f32 	%f759, %f1929, %f1965, %f758;
+	fma.rn.f32 	%f153, %f1925, %f1964, %f759;
+	mul.f32 	%f760, %f1934, %f1966;
+	fma.rn.f32 	%f761, %f1930, %f1965, %f760;
+	fma.rn.f32 	%f762, %f1926, %f1964, %f761;
+	add.f32 	%f1963, %f1963, %f762;
+	mul.f32 	%f763, %f1931, %f1970;
+	fma.rn.f32 	%f764, %f1927, %f1969, %f763;
+	fma.rn.f32 	%f155, %f1923, %f1968, %f764;
+	mul.f32 	%f765, %f1932, %f1970;
+	fma.rn.f32 	%f766, %f1928, %f1969, %f765;
+	fma.rn.f32 	%f156, %f1924, %f1968, %f766;
+	mul.f32 	%f767, %f1933, %f1970;
+	fma.rn.f32 	%f768, %f1929, %f1969, %f767;
+	fma.rn.f32 	%f157, %f1925, %f1968, %f768;
+	mul.f32 	%f769, %f1934, %f1970;
+	fma.rn.f32 	%f770, %f1930, %f1969, %f769;
+	fma.rn.f32 	%f771, %f1926, %f1968, %f770;
+	add.f32 	%f1967, %f1967, %f771;
+	mul.f32 	%f772, %f1931, %f1974;
+	fma.rn.f32 	%f773, %f1927, %f1973, %f772;
+	fma.rn.f32 	%f159, %f1923, %f1972, %f773;
+	mul.f32 	%f774, %f1932, %f1974;
+	fma.rn.f32 	%f775, %f1928, %f1973, %f774;
+	fma.rn.f32 	%f160, %f1924, %f1972, %f775;
+	mul.f32 	%f776, %f1933, %f1974;
+	fma.rn.f32 	%f777, %f1929, %f1973, %f776;
+	fma.rn.f32 	%f161, %f1925, %f1972, %f777;
+	mul.f32 	%f778, %f1934, %f1974;
+	fma.rn.f32 	%f779, %f1930, %f1973, %f778;
+	fma.rn.f32 	%f780, %f1926, %f1972, %f779;
+	add.f32 	%f1971, %f1971, %f780;
+	mov.f32 	%f1964, %f153;
+	mov.f32 	%f1965, %f152;
+	mov.f32 	%f1966, %f151;
+	mov.f32 	%f1968, %f157;
+	mov.f32 	%f1969, %f156;
+	mov.f32 	%f1970, %f155;
+	mov.f32 	%f1972, %f161;
+	mov.f32 	%f1973, %f160;
+	mov.f32 	%f1974, %f159;
+
+$L__BB11_18:
+	add.s32 	%r456, %r456, 1;
+	setp.lt.u32 	%p22, %r456, %r16;
+	mov.f32 	%f1923, %f1974;
+	mov.f32 	%f1924, %f1973;
+	mov.f32 	%f1925, %f1972;
+	mov.f32 	%f1926, %f1971;
+	mov.f32 	%f1927, %f1970;
+	mov.f32 	%f1928, %f1969;
+	mov.f32 	%f1929, %f1968;
+	mov.f32 	%f1930, %f1967;
+	mov.f32 	%f1931, %f1966;
+	mov.f32 	%f1932, %f1965;
+	mov.f32 	%f1933, %f1964;
+	mov.f32 	%f1934, %f1963;
+	@%p22 bra 	$L__BB11_3;
+
+$L__BB11_19:
+	mul.f32 	%f781, %f1999, %f1966;
+	fma.rn.f32 	%f782, %f2000, %f1965, %f781;
+	fma.rn.f32 	%f783, %f2001, %f1964, %f782;
+	mul.f32 	%f784, %f1999, %f1970;
+	fma.rn.f32 	%f785, %f2000, %f1969, %f784;
+	fma.rn.f32 	%f786, %f2001, %f1968, %f785;
+	mul.f32 	%f787, %f1999, %f1974;
+	fma.rn.f32 	%f788, %f2000, %f1973, %f787;
+	fma.rn.f32 	%f789, %f2001, %f1972, %f788;
+	add.f32 	%f2001, %f1971, %f789;
+	add.f32 	%f2000, %f1967, %f786;
+	add.f32 	%f1999, %f1963, %f783;
+
+$L__BB11_20:
+	// begin inline asm
+	call (%f2057), _optix_get_world_ray_direction_x, ();
+	// end inline asm
+	// begin inline asm
+	call (%f2058), _optix_get_world_ray_direction_y, ();
+	// end inline asm
+	// begin inline asm
+	call (%f792), _optix_get_world_ray_direction_z, ();
+	// end inline asm
+	// begin inline asm
+	call (%r167), _optix_get_transform_list_size, ();
+	// end inline asm
+	setp.eq.s32 	%p23, %r167, 0;
+	@%p23 bra 	$L__BB11_40;
+
+	// begin inline asm
+	call (%r168), _optix_get_transform_list_size, ();
+	// end inline asm
+	// begin inline asm
+	call (%f793), _optix_get_ray_time, ();
+	// end inline asm
+	setp.eq.s32 	%p24, %r168, 0;
+	@%p24 bra 	$L__BB11_39;
+
+	mov.u32 	%r457, 0;
+
+$L__BB11_23:
 	.pragma "nounroll";
-	// inline asm
-	call (%rd141), _optix_get_transform_list_handle, (%r426);
-	// inline asm
-	// inline asm
-	call (%r167), _optix_get_transform_type_from_handle, (%rd141);
-	// inline asm
-	and.b32  	%r168, %r167, -2;
-	setp.eq.s32	%p21, %r168, 2;
-	@%p21 bra 	BB11_29;
-	bra.uni 	BB11_24;
-
-BB11_29:
-	setp.eq.s32	%p24, %r167, 2;
-	@%p24 bra 	BB11_33;
-	bra.uni 	BB11_30;
-
-BB11_33:
-	// inline asm
-	call (%rd215), _optix_get_matrix_motion_transform_from_handle, (%rd141);
-	// inline asm
-	// inline asm
-	cvta.to.global.u64 %rd217, %rd215;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r256,%r257,%r258,%r259}, [%rd217];
-	// inline asm
-	mov.b32	{%rs13, %rs14}, %r258;
-	add.s64 	%rd221, %rd215, 16;
-	// inline asm
+	// begin inline asm
+	call (%rd137), _optix_get_transform_list_handle, (%r457);
+	// end inline asm
+	// begin inline asm
+	call (%r171), _optix_get_transform_type_from_handle, (%rd137);
+	// end inline asm
+	or.b32  	%r172, %r171, 1;
+	setp.eq.s32 	%p25, %r172, 3;
+	@%p25 bra 	$L__BB11_29;
+	bra.uni 	$L__BB11_24;
+
+$L__BB11_29:
+	setp.eq.s32 	%p28, %r171, 2;
+	@%p28 bra 	$L__BB11_33;
+	bra.uni 	$L__BB11_30;
+
+$L__BB11_33:
+	// begin inline asm
+	call (%rd209), _optix_get_matrix_motion_transform_from_handle, (%rd137);
+	// end inline asm
+	// begin inline asm
+	cvta.to.global.u64 %rd211, %rd209;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r260,%r261,%r262,%r263}, [%rd211];
+	// end inline asm
+	add.s64 	%rd215, %rd209, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd214, %rd215;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r264,%r265,%r266,%r267}, [%rd214];
+	// end inline asm
+	add.s64 	%rd218, %rd209, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd217, %rd218;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r268,%r269,%r270,%r271}, [%rd217];
+	// end inline asm
+	add.s64 	%rd221, %rd209, 48;
+	// begin inline asm
 	cvta.to.global.u64 %rd220, %rd221;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r260,%r261,%r262,%r263}, [%rd220];
-	// inline asm
-	add.s64 	%rd224, %rd215, 32;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r272,%r273,%r274,%r275}, [%rd220];
+	// end inline asm
+	add.s64 	%rd224, %rd209, 64;
+	// begin inline asm
 	cvta.to.global.u64 %rd223, %rd224;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r264,%r265,%r266,%r267}, [%rd223];
-	// inline asm
-	add.s64 	%rd227, %rd215, 48;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r276,%r277,%r278,%r279}, [%rd223];
+	// end inline asm
+	add.s64 	%rd227, %rd209, 80;
+	// begin inline asm
 	cvta.to.global.u64 %rd226, %rd227;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r268,%r269,%r270,%r271}, [%rd226];
-	// inline asm
-	add.s64 	%rd230, %rd215, 64;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r280,%r281,%r282,%r283}, [%rd226];
+	// end inline asm
+	add.s64 	%rd230, %rd209, 96;
+	// begin inline asm
 	cvta.to.global.u64 %rd229, %rd230;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r272,%r273,%r274,%r275}, [%rd229];
-	// inline asm
-	add.s64 	%rd233, %rd215, 80;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r284,%r285,%r286,%r287}, [%rd229];
+	// end inline asm
+	add.s64 	%rd233, %rd209, 112;
+	// begin inline asm
 	cvta.to.global.u64 %rd232, %rd233;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r276,%r277,%r278,%r279}, [%rd232];
-	// inline asm
-	add.s64 	%rd236, %rd215, 96;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r288,%r289,%r290,%r291}, [%rd232];
+	// end inline asm
+	mov.b32 	%f897, %r263;
+	mov.b32 	%f898, %r264;
+	and.b32  	%r304, %r262, 65535;
+	add.s32 	%r305, %r304, -1;
+	cvt.rn.f32.s32 	%f899, %r305;
+	sub.f32 	%f900, %f793, %f897;
+	mul.f32 	%f901, %f900, %f899;
+	sub.f32 	%f902, %f898, %f897;
+	div.rn.f32 	%f903, %f901, %f902;
+	min.f32 	%f904, %f899, %f903;
+	mov.f32 	%f905, 0f00000000;
+	max.f32 	%f906, %f905, %f904;
+	cvt.rmi.f32.f32 	%f907, %f906;
+	sub.f32 	%f260, %f906, %f907;
+	cvt.rzi.s32.f32 	%r306, %f907;
+	cvt.s64.s32 	%rd15, %r306;
+	mul.wide.s32 	%rd244, %r306, 48;
+	add.s64 	%rd236, %rd218, %rd244;
+	// begin inline asm
 	cvta.to.global.u64 %rd235, %rd236;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r280,%r281,%r282,%r283}, [%rd235];
-	// inline asm
-	add.s64 	%rd239, %rd215, 112;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r292,%r293,%r294,%r295}, [%rd235];
+	// end inline asm
+	mov.b32 	%f2027, %r292;
+	mov.b32 	%f2028, %r293;
+	mov.b32 	%f2029, %r294;
+	add.s64 	%rd239, %rd236, 16;
+	// begin inline asm
 	cvta.to.global.u64 %rd238, %rd239;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r284,%r285,%r286,%r287}, [%rd238];
-	// inline asm
-	mov.b32 	 %f928, %r259;
-	mov.b32 	 %f929, %r260;
-	cvt.u32.u16	%r300, %rs13;
-	add.s32 	%r301, %r300, -1;
-	cvt.rn.f32.s32	%f930, %r301;
-	sub.f32 	%f931, %f825, %f928;
-	mul.f32 	%f932, %f931, %f930;
-	sub.f32 	%f933, %f929, %f928;
-	div.rn.f32 	%f934, %f932, %f933;
-	min.f32 	%f935, %f930, %f934;
-	mov.f32 	%f936, 0f00000000;
-	max.f32 	%f937, %f936, %f935;
-	cvt.rmi.f32.f32	%f938, %f937;
-	cvt.rzi.s32.f32	%r302, %f938;
-	cvt.s64.s32	%rd17, %r302;
-	mul.wide.s32 	%rd250, %r302, 48;
-	add.s64 	%rd242, %rd224, %rd250;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r296,%r297,%r298,%r299}, [%rd238];
+	// end inline asm
+	mov.b32 	%f2024, %r296;
+	mov.b32 	%f2025, %r297;
+	mov.b32 	%f2026, %r298;
+	add.s64 	%rd242, %rd236, 32;
+	// begin inline asm
 	cvta.to.global.u64 %rd241, %rd242;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r288,%r289,%r290,%r291}, [%rd241];
-	// inline asm
-	mov.b32 	 %f2320, %r288;
-	mov.b32 	 %f2321, %r289;
-	mov.b32 	 %f2322, %r290;
-	add.s64 	%rd245, %rd242, 16;
-	// inline asm
-	cvta.to.global.u64 %rd244, %rd245;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r292,%r293,%r294,%r295}, [%rd244];
-	// inline asm
-	mov.b32 	 %f2317, %r292;
-	mov.b32 	 %f2318, %r293;
-	mov.b32 	 %f2319, %r294;
-	add.s64 	%rd248, %rd242, 32;
-	// inline asm
-	cvta.to.global.u64 %rd247, %rd248;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r296,%r297,%r298,%r299}, [%rd247];
-	// inline asm
-	sub.f32 	%f249, %f937, %f938;
-	mov.b32 	 %f2314, %r296;
-	mov.b32 	 %f2315, %r297;
-	mov.b32 	 %f2316, %r298;
-	setp.leu.f32	%p26, %f249, 0f00000000;
-	@%p26 bra 	BB11_35;
-
-	mul.lo.s64 	%rd260, %rd17, 48;
-	add.s64 	%rd261, %rd215, %rd260;
-	add.s64 	%rd252, %rd261, 80;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r300,%r301,%r302,%r303}, [%rd241];
+	// end inline asm
+	mov.b32 	%f2021, %r300;
+	mov.b32 	%f2022, %r301;
+	mov.b32 	%f2023, %r302;
+	setp.leu.f32 	%p30, %f260, 0f00000000;
+	@%p30 bra 	$L__BB11_35;
+
+	mov.f32 	%f908, 0f3F800000;
+	sub.f32 	%f909, %f908, %f260;
+	mul.lo.s64 	%rd254, %rd15, 48;
+	add.s64 	%rd255, %rd209, %rd254;
+	add.s64 	%rd246, %rd255, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd245, %rd246;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r307,%r308,%r309,%r310}, [%rd245];
+	// end inline asm
+	mov.b32 	%f910, %r307;
+	mov.b32 	%f911, %r308;
+	mov.b32 	%f912, %r309;
+	mul.f32 	%f913, %f260, %f910;
+	mul.f32 	%f914, %f260, %f911;
+	mul.f32 	%f915, %f260, %f912;
+	fma.rn.f32 	%f2027, %f909, %f2027, %f913;
+	fma.rn.f32 	%f2028, %f909, %f2028, %f914;
+	fma.rn.f32 	%f2029, %f909, %f2029, %f915;
+	add.s64 	%rd249, %rd255, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd248, %rd249;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r311,%r312,%r313,%r314}, [%rd248];
+	// end inline asm
+	mov.b32 	%f916, %r311;
+	mov.b32 	%f917, %r312;
+	mov.b32 	%f918, %r313;
+	mul.f32 	%f919, %f260, %f916;
+	mul.f32 	%f920, %f260, %f917;
+	mul.f32 	%f921, %f260, %f918;
+	fma.rn.f32 	%f2024, %f909, %f2024, %f919;
+	fma.rn.f32 	%f2025, %f909, %f2025, %f920;
+	fma.rn.f32 	%f2026, %f909, %f2026, %f921;
+	add.s64 	%rd252, %rd255, 112;
+	// begin inline asm
 	cvta.to.global.u64 %rd251, %rd252;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r303,%r304,%r305,%r306}, [%rd251];
-	// inline asm
-	mov.b32 	 %f939, %r303;
-	mov.b32 	 %f940, %r304;
-	mov.b32 	 %f941, %r305;
-	add.s64 	%rd255, %rd261, 96;
-	// inline asm
-	cvta.to.global.u64 %rd254, %rd255;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r307,%r308,%r309,%r310}, [%rd254];
-	// inline asm
-	mov.b32 	 %f942, %r307;
-	mov.b32 	 %f943, %r308;
-	mov.b32 	 %f944, %r309;
-	add.s64 	%rd258, %rd261, 112;
-	// inline asm
-	cvta.to.global.u64 %rd257, %rd258;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r311,%r312,%r313,%r314}, [%rd257];
-	// inline asm
-	mov.f32 	%f945, 0f3F800000;
-	sub.f32 	%f946, %f945, %f249;
-	mul.f32 	%f947, %f249, %f939;
-	mul.f32 	%f948, %f249, %f940;
-	mul.f32 	%f949, %f249, %f941;
-	fma.rn.f32 	%f2320, %f946, %f2320, %f947;
-	fma.rn.f32 	%f2321, %f946, %f2321, %f948;
-	fma.rn.f32 	%f2322, %f946, %f2322, %f949;
-	mul.f32 	%f950, %f249, %f942;
-	mul.f32 	%f951, %f249, %f943;
-	mul.f32 	%f952, %f249, %f944;
-	fma.rn.f32 	%f2317, %f946, %f2317, %f950;
-	fma.rn.f32 	%f2318, %f946, %f2318, %f951;
-	fma.rn.f32 	%f2319, %f946, %f2319, %f952;
-	mov.b32 	 %f953, %r311;
-	mov.b32 	 %f954, %r312;
-	mov.b32 	 %f955, %r313;
-	mul.f32 	%f956, %f249, %f953;
-	mul.f32 	%f957, %f249, %f954;
-	mul.f32 	%f958, %f249, %f955;
-	fma.rn.f32 	%f2314, %f946, %f2314, %f956;
-	fma.rn.f32 	%f2315, %f946, %f2315, %f957;
-	fma.rn.f32 	%f2316, %f946, %f2316, %f958;
-	bra.uni 	BB11_35;
-
-BB11_24:
-	mov.f32 	%f2323, 0f00000000;
-	mov.f32 	%f2325, 0f3F800000;
-	setp.eq.s32	%p22, %r167, 4;
-	@%p22 bra 	BB11_27;
-	bra.uni 	BB11_25;
-
-BB11_27:
-	// inline asm
-	call (%rd265), _optix_get_instance_inverse_transform_from_handle, (%rd141);
-	// inline asm
-	bra.uni 	BB11_28;
-
-BB11_30:
-	// inline asm
-	call (%rd156), _optix_get_srt_motion_transform_from_handle, (%rd141);
-	// inline asm
-	// inline asm
-	cvta.to.global.u64 %rd158, %rd156;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r181,%r182,%r183,%r184}, [%rd158];
-	// inline asm
-	mov.b32	{%rs11, %rs12}, %r183;
-	add.s64 	%rd162, %rd156, 16;
-	// inline asm
-	cvta.to.global.u64 %rd161, %rd162;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r185,%r186,%r187,%r188}, [%rd161];
-	// inline asm
-	add.s64 	%rd165, %rd156, 32;
-	// inline asm
-	cvta.to.global.u64 %rd164, %rd165;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r189,%r190,%r191,%r192}, [%rd164];
-	// inline asm
-	add.s64 	%rd168, %rd156, 48;
-	// inline asm
-	cvta.to.global.u64 %rd167, %rd168;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r193,%r194,%r195,%r196}, [%rd167];
-	// inline asm
-	add.s64 	%rd171, %rd156, 64;
-	// inline asm
-	cvta.to.global.u64 %rd170, %rd171;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r197,%r198,%r199,%r200}, [%rd170];
-	// inline asm
-	add.s64 	%rd174, %rd156, 80;
-	// inline asm
-	cvta.to.global.u64 %rd173, %rd174;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r201,%r202,%r203,%r204}, [%rd173];
-	// inline asm
-	add.s64 	%rd177, %rd156, 96;
-	// inline asm
-	cvta.to.global.u64 %rd176, %rd177;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r205,%r206,%r207,%r208}, [%rd176];
-	// inline asm
-	add.s64 	%rd180, %rd156, 112;
-	// inline asm
-	cvta.to.global.u64 %rd179, %rd180;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r209,%r210,%r211,%r212}, [%rd179];
-	// inline asm
-	add.s64 	%rd183, %rd156, 128;
-	// inline asm
-	cvta.to.global.u64 %rd182, %rd183;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r213,%r214,%r215,%r216}, [%rd182];
-	// inline asm
-	add.s64 	%rd186, %rd156, 144;
-	// inline asm
-	cvta.to.global.u64 %rd185, %rd186;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r217,%r218,%r219,%r220}, [%rd185];
-	// inline asm
-	mov.b32 	 %f836, %r184;
-	mov.b32 	 %f837, %r185;
-	cvt.u32.u16	%r237, %rs11;
-	add.s32 	%r238, %r237, -1;
-	cvt.rn.f32.s32	%f838, %r238;
-	sub.f32 	%f839, %f825, %f836;
-	mul.f32 	%f840, %f839, %f838;
-	sub.f32 	%f841, %f837, %f836;
-	div.rn.f32 	%f842, %f840, %f841;
-	min.f32 	%f843, %f838, %f842;
-	mov.f32 	%f844, 0f00000000;
-	max.f32 	%f845, %f844, %f843;
-	cvt.rmi.f32.f32	%f846, %f845;
-	cvt.rzi.s32.f32	%r239, %f846;
-	cvt.s64.s32	%rd15, %r239;
-	mul.wide.s32 	%rd200, %r239, 64;
-	add.s64 	%rd189, %rd165, %rd200;
-	// inline asm
-	cvta.to.global.u64 %rd188, %rd189;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r221,%r222,%r223,%r224}, [%rd188];
-	// inline asm
-	mov.b32 	 %f2304, %r221;
-	mov.b32 	 %f2305, %r222;
-	mov.b32 	 %f2306, %r223;
-	add.s64 	%rd192, %rd189, 16;
-	// inline asm
-	cvta.to.global.u64 %rd191, %rd192;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r225,%r226,%r227,%r228}, [%rd191];
-	// inline asm
-	mov.b32 	 %f2307, %r225;
-	mov.b32 	 %f2308, %r226;
-	mov.b32 	 %f2309, %r228;
-	add.s64 	%rd195, %rd189, 32;
-	// inline asm
-	cvta.to.global.u64 %rd194, %rd195;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r229,%r230,%r231,%r232}, [%rd194];
-	// inline asm
-	sub.f32 	%f209, %f845, %f846;
-	mov.b32 	 %f2310, %r230;
-	mov.b32 	 %f2311, %r231;
-	mov.b32 	 %f2312, %r232;
-	add.s64 	%rd198, %rd189, 48;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r315,%r316,%r317,%r318}, [%rd251];
+	// end inline asm
+	mov.b32 	%f922, %r315;
+	mov.b32 	%f923, %r316;
+	mov.b32 	%f924, %r317;
+	mul.f32 	%f925, %f260, %f922;
+	mul.f32 	%f926, %f260, %f923;
+	mul.f32 	%f927, %f260, %f924;
+	fma.rn.f32 	%f2021, %f909, %f2021, %f925;
+	fma.rn.f32 	%f2022, %f909, %f2022, %f926;
+	fma.rn.f32 	%f2023, %f909, %f2023, %f927;
+	bra.uni 	$L__BB11_35;
+
+$L__BB11_24:
+	mov.f32 	%f2030, 0f00000000;
+	mov.f32 	%f2032, 0f3F800000;
+	setp.eq.s32 	%p26, %r171, 4;
+	@%p26 bra 	$L__BB11_27;
+
+	setp.ne.s32 	%p27, %r171, 1;
+	mov.f32 	%f2031, %f2030;
+	mov.f32 	%f2033, %f2030;
+	mov.f32 	%f2034, %f2032;
+	mov.f32 	%f2035, %f2030;
+	mov.f32 	%f2036, %f2032;
+	mov.f32 	%f2037, %f2030;
+	mov.f32 	%f2038, %f2030;
+	@%p27 bra 	$L__BB11_36;
+
+	// begin inline asm
+	call (%rd139), _optix_get_static_transform_from_handle, (%rd137);
+	// end inline asm
+	add.s64 	%rd258, %rd139, 64;
+	bra.uni 	$L__BB11_28;
+
+$L__BB11_30:
+	// begin inline asm
+	call (%rd152), _optix_get_srt_motion_transform_from_handle, (%rd137);
+	// end inline asm
+	// begin inline asm
+	cvta.to.global.u64 %rd154, %rd152;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r185,%r186,%r187,%r188}, [%rd154];
+	// end inline asm
+	add.s64 	%rd158, %rd152, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd157, %rd158;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r189,%r190,%r191,%r192}, [%rd157];
+	// end inline asm
+	add.s64 	%rd161, %rd152, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd160, %rd161;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r193,%r194,%r195,%r196}, [%rd160];
+	// end inline asm
+	add.s64 	%rd164, %rd152, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd163, %rd164;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r197,%r198,%r199,%r200}, [%rd163];
+	// end inline asm
+	add.s64 	%rd167, %rd152, 64;
+	// begin inline asm
+	cvta.to.global.u64 %rd166, %rd167;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r201,%r202,%r203,%r204}, [%rd166];
+	// end inline asm
+	add.s64 	%rd170, %rd152, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd169, %rd170;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r205,%r206,%r207,%r208}, [%rd169];
+	// end inline asm
+	add.s64 	%rd173, %rd152, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd172, %rd173;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r209,%r210,%r211,%r212}, [%rd172];
+	// end inline asm
+	add.s64 	%rd176, %rd152, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd175, %rd176;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r213,%r214,%r215,%r216}, [%rd175];
+	// end inline asm
+	add.s64 	%rd179, %rd152, 128;
+	// begin inline asm
+	cvta.to.global.u64 %rd178, %rd179;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r217,%r218,%r219,%r220}, [%rd178];
+	// end inline asm
+	add.s64 	%rd182, %rd152, 144;
+	// begin inline asm
+	cvta.to.global.u64 %rd181, %rd182;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r221,%r222,%r223,%r224}, [%rd181];
+	// end inline asm
+	mov.b32 	%f805, %r188;
+	mov.b32 	%f806, %r189;
+	and.b32  	%r241, %r187, 65535;
+	add.s32 	%r242, %r241, -1;
+	cvt.rn.f32.s32 	%f807, %r242;
+	sub.f32 	%f808, %f793, %f805;
+	mul.f32 	%f809, %f808, %f807;
+	sub.f32 	%f810, %f806, %f805;
+	div.rn.f32 	%f811, %f809, %f810;
+	min.f32 	%f812, %f807, %f811;
+	mov.f32 	%f813, 0f00000000;
+	max.f32 	%f814, %f813, %f812;
+	cvt.rmi.f32.f32 	%f815, %f814;
+	sub.f32 	%f220, %f814, %f815;
+	cvt.rzi.s32.f32 	%r243, %f815;
+	mul.wide.s32 	%rd196, %r243, 64;
+	add.s64 	%rd185, %rd161, %rd196;
+	// begin inline asm
+	cvta.to.global.u64 %rd184, %rd185;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r225,%r226,%r227,%r228}, [%rd184];
+	// end inline asm
+	mov.b32 	%f2011, %r225;
+	mov.b32 	%f2012, %r226;
+	mov.b32 	%f2013, %r227;
+	add.s64 	%rd188, %rd185, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd187, %rd188;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r229,%r230,%r231,%r232}, [%rd187];
+	// end inline asm
+	mov.b32 	%f2014, %r229;
+	mov.b32 	%f2015, %r230;
+	mov.b32 	%f2016, %r232;
+	add.s64 	%rd191, %rd185, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd190, %rd191;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r233,%r234,%r235,%r236}, [%rd190];
+	// end inline asm
+	mov.b32 	%f2017, %r234;
+	mov.b32 	%f2018, %r235;
+	mov.b32 	%f2019, %r236;
+	add.s64 	%rd194, %rd185, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd193, %rd194;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r237,%r238,%r239,%r240}, [%rd193];
+	// end inline asm
+	mov.b32 	%f2020, %r237;
+	setp.leu.f32 	%p29, %f220, 0f00000000;
+	@%p29 bra 	$L__BB11_32;
+
+	mov.f32 	%f816, 0f3F800000;
+	sub.f32 	%f817, %f816, %f220;
+	add.s64 	%rd198, %rd185, 64;
+	// begin inline asm
 	cvta.to.global.u64 %rd197, %rd198;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r233,%r234,%r235,%r236}, [%rd197];
-	// inline asm
-	mov.b32 	 %f2313, %r233;
-	setp.leu.f32	%p25, %f209, 0f00000000;
-	@%p25 bra 	BB11_32;
-
-	shl.b64 	%rd213, %rd15, 6;
-	add.s64 	%rd214, %rd213, %rd156;
-	add.s64 	%rd202, %rd214, 96;
-	// inline asm
-	cvta.to.global.u64 %rd201, %rd202;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r240,%r241,%r242,%r243}, [%rd201];
-	// inline asm
-	mov.b32 	 %f847, %r240;
-	mov.b32 	 %f848, %r241;
-	mov.b32 	 %f849, %r242;
-	add.s64 	%rd205, %rd214, 112;
-	// inline asm
-	cvta.to.global.u64 %rd204, %rd205;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r244,%r245,%r246,%r247}, [%rd204];
-	// inline asm
-	mov.b32 	 %f850, %r244;
-	mov.b32 	 %f851, %r245;
-	mov.b32 	 %f852, %r247;
-	add.s64 	%rd208, %rd214, 128;
-	// inline asm
-	cvta.to.global.u64 %rd207, %rd208;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r248,%r249,%r250,%r251}, [%rd207];
-	// inline asm
-	mov.b32 	 %f853, %r249;
-	mov.b32 	 %f854, %r250;
-	mov.b32 	 %f855, %r251;
-	add.s64 	%rd211, %rd214, 144;
-	// inline asm
-	cvta.to.global.u64 %rd210, %rd211;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r252,%r253,%r254,%r255}, [%rd210];
-	// inline asm
-	mov.f32 	%f856, 0f3F800000;
-	sub.f32 	%f857, %f856, %f209;
-	mul.f32 	%f858, %f209, %f847;
-	mul.f32 	%f859, %f209, %f848;
-	mul.f32 	%f860, %f209, %f849;
-	fma.rn.f32 	%f2304, %f857, %f2304, %f858;
-	fma.rn.f32 	%f2305, %f857, %f2305, %f859;
-	fma.rn.f32 	%f2306, %f857, %f2306, %f860;
-	mul.f32 	%f861, %f209, %f850;
-	mul.f32 	%f862, %f209, %f851;
-	mul.f32 	%f863, %f209, %f852;
-	fma.rn.f32 	%f2307, %f857, %f2307, %f861;
-	fma.rn.f32 	%f2308, %f857, %f2308, %f862;
-	fma.rn.f32 	%f2309, %f857, %f2309, %f863;
-	mul.f32 	%f864, %f209, %f853;
-	mul.f32 	%f865, %f209, %f854;
-	mul.f32 	%f866, %f209, %f855;
-	fma.rn.f32 	%f867, %f857, %f2310, %f864;
-	fma.rn.f32 	%f868, %f857, %f2311, %f865;
-	fma.rn.f32 	%f869, %f857, %f2312, %f866;
-	mov.b32 	 %f870, %r252;
-	mul.f32 	%f871, %f209, %f870;
-	fma.rn.f32 	%f872, %f857, %f2313, %f871;
-	mul.f32 	%f873, %f868, %f868;
-	fma.rn.f32 	%f874, %f867, %f867, %f873;
-	fma.rn.f32 	%f875, %f869, %f869, %f874;
-	fma.rn.f32 	%f876, %f872, %f872, %f875;
-	sqrt.rn.f32 	%f877, %f876;
-	rcp.rn.f32 	%f878, %f877;
-	mul.f32 	%f2310, %f867, %f878;
-	mul.f32 	%f2311, %f868, %f878;
-	mul.f32 	%f2312, %f869, %f878;
-	mul.f32 	%f2313, %f872, %f878;
-
-BB11_32:
-	mul.f32 	%f879, %f2311, %f2311;
-	fma.rn.f32 	%f880, %f2310, %f2310, %f879;
-	fma.rn.f32 	%f881, %f2312, %f2312, %f880;
-	fma.rn.f32 	%f882, %f2313, %f2313, %f881;
-	rcp.rn.f32 	%f883, %f882;
-	mul.f32 	%f884, %f2310, %f883;
-	mul.f32 	%f885, %f2311, %f883;
-	mul.f32 	%f886, %f2312, %f883;
-	mul.f32 	%f887, %f2313, %f883;
-	mul.f32 	%f888, %f2310, %f884;
-	mul.f32 	%f889, %f2311, %f885;
-	mul.f32 	%f890, %f2312, %f886;
-	mul.f32 	%f891, %f2310, %f885;
-	mul.f32 	%f892, %f2312, %f887;
-	mul.f32 	%f893, %f2310, %f886;
-	mul.f32 	%f894, %f2311, %f887;
-	mul.f32 	%f895, %f2311, %f886;
-	mul.f32 	%f896, %f2310, %f887;
-	sub.f32 	%f897, %f888, %f889;
-	sub.f32 	%f898, %f897, %f890;
-	fma.rn.f32 	%f899, %f2313, %f887, %f898;
-	sub.f32 	%f900, %f891, %f892;
-	add.f32 	%f901, %f900, %f900;
-	add.f32 	%f902, %f893, %f894;
-	add.f32 	%f903, %f902, %f902;
-	add.f32 	%f904, %f891, %f892;
-	add.f32 	%f905, %f904, %f904;
-	sub.f32 	%f906, %f889, %f888;
-	sub.f32 	%f907, %f906, %f890;
-	fma.rn.f32 	%f908, %f2313, %f887, %f907;
-	sub.f32 	%f909, %f895, %f896;
-	add.f32 	%f910, %f909, %f909;
-	sub.f32 	%f911, %f893, %f894;
-	add.f32 	%f912, %f911, %f911;
-	add.f32 	%f913, %f895, %f896;
-	add.f32 	%f914, %f913, %f913;
-	neg.f32 	%f915, %f888;
-	sub.f32 	%f916, %f915, %f889;
-	add.f32 	%f917, %f890, %f916;
-	fma.rn.f32 	%f918, %f2313, %f887, %f917;
-	mul.f32 	%f919, %f2306, %f899;
-	fma.rn.f32 	%f920, %f2308, %f901, %f919;
-	fma.rn.f32 	%f2322, %f2309, %f903, %f920;
-	mul.f32 	%f921, %f2308, %f908;
-	fma.rn.f32 	%f922, %f2306, %f905, %f921;
-	fma.rn.f32 	%f2319, %f2309, %f910, %f922;
-	mul.f32 	%f923, %f2308, %f914;
-	fma.rn.f32 	%f924, %f2306, %f912, %f923;
-	fma.rn.f32 	%f2316, %f2309, %f918, %f924;
-	mul.f32 	%f925, %f2305, %f899;
-	fma.rn.f32 	%f2321, %f2307, %f901, %f925;
-	mul.f32 	%f926, %f2307, %f908;
-	fma.rn.f32 	%f2318, %f2305, %f905, %f926;
-	mul.f32 	%f927, %f2307, %f914;
-	fma.rn.f32 	%f2315, %f2305, %f912, %f927;
-	mul.f32 	%f2320, %f2304, %f899;
-	mul.f32 	%f2317, %f2304, %f905;
-	mul.f32 	%f2314, %f2304, %f912;
-
-BB11_35:
-	mul.f32 	%f959, %f2315, %f2319;
-	mul.f32 	%f960, %f2316, %f2318;
-	sub.f32 	%f961, %f960, %f959;
-	mul.f32 	%f962, %f2320, %f961;
-	mul.f32 	%f963, %f2314, %f2319;
-	mul.f32 	%f964, %f2316, %f2317;
-	sub.f32 	%f965, %f964, %f963;
-	mul.f32 	%f966, %f965, %f2321;
-	sub.f32 	%f967, %f962, %f966;
-	mul.f32 	%f968, %f2314, %f2318;
-	mul.f32 	%f969, %f2315, %f2317;
-	sub.f32 	%f970, %f969, %f968;
-	fma.rn.f32 	%f971, %f970, %f2322, %f967;
-	rcp.rn.f32 	%f972, %f971;
-	mul.f32 	%f2329, %f961, %f972;
-	mul.f32 	%f973, %f2316, %f2321;
-	mul.f32 	%f974, %f2315, %f2322;
-	sub.f32 	%f975, %f974, %f973;
-	mul.f32 	%f2330, %f972, %f975;
-	mul.f32 	%f976, %f2318, %f2322;
-	mul.f32 	%f977, %f2319, %f2321;
-	sub.f32 	%f978, %f977, %f976;
-	mul.f32 	%f2331, %f972, %f978;
-	sub.f32 	%f979, %f963, %f964;
-	mul.f32 	%f2326, %f979, %f972;
-	mul.f32 	%f980, %f2314, %f2322;
-	mul.f32 	%f981, %f2316, %f2320;
-	sub.f32 	%f982, %f981, %f980;
-	mul.f32 	%f2327, %f972, %f982;
-	mul.f32 	%f983, %f2319, %f2320;
-	mul.f32 	%f984, %f2317, %f2322;
-	sub.f32 	%f985, %f984, %f983;
-	mul.f32 	%f2328, %f972, %f985;
-	mul.f32 	%f2323, %f970, %f972;
-	mul.f32 	%f986, %f2315, %f2320;
-	mul.f32 	%f987, %f2314, %f2321;
-	sub.f32 	%f988, %f987, %f986;
-	mul.f32 	%f2324, %f988, %f972;
-	mul.f32 	%f989, %f2317, %f2321;
-	mul.f32 	%f990, %f2318, %f2320;
-	sub.f32 	%f991, %f990, %f989;
-	mul.f32 	%f2325, %f991, %f972;
-	bra.uni 	BB11_36;
-
-BB11_25:
-	setp.ne.s32	%p23, %r167, 1;
-	mov.f32 	%f2324, %f2323;
-	mov.f32 	%f2326, %f2323;
-	mov.f32 	%f2327, %f2325;
-	mov.f32 	%f2328, %f2323;
-	mov.f32 	%f2329, %f2325;
-	mov.f32 	%f2330, %f2323;
-	mov.f32 	%f2331, %f2323;
-	@%p23 bra 	BB11_36;
-
-	// inline asm
-	call (%rd143), _optix_get_static_transform_from_handle, (%rd141);
-	// inline asm
-	add.s64 	%rd265, %rd143, 64;
-
-BB11_28:
-	// inline asm
-	cvta.to.global.u64 %rd147, %rd265;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r169,%r170,%r171,%r172}, [%rd147];
-	// inline asm
-	mov.b32 	 %f2329, %r169;
-	mov.b32 	 %f2330, %r170;
-	mov.b32 	 %f2331, %r171;
-	add.s64 	%rd151, %rd265, 16;
-	// inline asm
-	cvta.to.global.u64 %rd150, %rd151;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r173,%r174,%r175,%r176}, [%rd150];
-	// inline asm
-	mov.b32 	 %f2326, %r173;
-	mov.b32 	 %f2327, %r174;
-	mov.b32 	 %f2328, %r175;
-	add.s64 	%rd154, %rd265, 32;
-	// inline asm
-	cvta.to.global.u64 %rd153, %rd154;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r177,%r178,%r179,%r180}, [%rd153];
-	// inline asm
-	mov.b32 	 %f2323, %r177;
-	mov.b32 	 %f2324, %r178;
-	mov.b32 	 %f2325, %r179;
-
-BB11_36:
-	setp.eq.s32	%p27, %r426, 0;
-	@%p27 bra 	BB11_37;
-	bra.uni 	BB11_38;
-
-BB11_37:
-	mov.f32 	%f2303, %f2323;
-	mov.f32 	%f2302, %f2324;
-	mov.f32 	%f2301, %f2325;
-	mov.f32 	%f2300, %f2326;
-	mov.f32 	%f2299, %f2327;
-	mov.f32 	%f2298, %f2328;
-	mov.f32 	%f2297, %f2329;
-	mov.f32 	%f2296, %f2330;
-	mov.f32 	%f2295, %f2331;
-	bra.uni 	BB11_39;
-
-BB11_38:
-	mul.f32 	%f992, %f2300, %f2330;
-	fma.rn.f32 	%f993, %f2297, %f2329, %f992;
-	fma.rn.f32 	%f289, %f2303, %f2331, %f993;
-	mul.f32 	%f994, %f2299, %f2330;
-	fma.rn.f32 	%f995, %f2296, %f2329, %f994;
-	fma.rn.f32 	%f290, %f2302, %f2331, %f995;
-	mul.f32 	%f996, %f2298, %f2330;
-	fma.rn.f32 	%f997, %f2295, %f2329, %f996;
-	fma.rn.f32 	%f291, %f2301, %f2331, %f997;
-	mul.f32 	%f998, %f2300, %f2327;
-	fma.rn.f32 	%f999, %f2297, %f2326, %f998;
-	fma.rn.f32 	%f292, %f2303, %f2328, %f999;
-	mul.f32 	%f1000, %f2299, %f2327;
-	fma.rn.f32 	%f1001, %f2296, %f2326, %f1000;
-	fma.rn.f32 	%f293, %f2302, %f2328, %f1001;
-	mul.f32 	%f1002, %f2298, %f2327;
-	fma.rn.f32 	%f1003, %f2295, %f2326, %f1002;
-	fma.rn.f32 	%f294, %f2301, %f2328, %f1003;
-	mul.f32 	%f1004, %f2300, %f2324;
-	fma.rn.f32 	%f1005, %f2297, %f2323, %f1004;
-	fma.rn.f32 	%f2303, %f2303, %f2325, %f1005;
-	mul.f32 	%f1006, %f2299, %f2324;
-	fma.rn.f32 	%f1007, %f2296, %f2323, %f1006;
-	fma.rn.f32 	%f2302, %f2302, %f2325, %f1007;
-	mul.f32 	%f1008, %f2298, %f2324;
-	fma.rn.f32 	%f1009, %f2295, %f2323, %f1008;
-	fma.rn.f32 	%f2301, %f2301, %f2325, %f1009;
-	mov.f32 	%f2300, %f292;
-	mov.f32 	%f2299, %f293;
-	mov.f32 	%f2298, %f294;
-	mov.f32 	%f2297, %f289;
-	mov.f32 	%f2296, %f290;
-	mov.f32 	%f2295, %f291;
-
-BB11_39:
-	add.s32 	%r426, %r426, 1;
-	setp.lt.u32	%p28, %r426, %r14;
-	@%p28 bra 	BB11_23;
-
-	mul.f32 	%f1010, %f823, %f2296;
-	fma.rn.f32 	%f1011, %f822, %f2297, %f1010;
-	fma.rn.f32 	%f2341, %f2343, %f2295, %f1011;
-	mul.f32 	%f1012, %f823, %f2299;
-	fma.rn.f32 	%f1013, %f822, %f2300, %f1012;
-	fma.rn.f32 	%f2342, %f2343, %f2298, %f1013;
-	mul.f32 	%f1014, %f823, %f2302;
-	fma.rn.f32 	%f1015, %f822, %f2303, %f1014;
-	fma.rn.f32 	%f2343, %f2343, %f2301, %f1015;
-	bra.uni 	BB11_41;
-
-BB11_22:
-	mov.f32 	%f2341, %f822;
-	mov.f32 	%f2342, %f823;
-
-BB11_41:
-	// inline asm
-	call (%f1016), _optix_get_ray_tmin, ();
-	// inline asm
-	// inline asm
-	call (%f1017), _optix_get_ray_tmax, ();
-	// inline asm
-	ld.f32 	%f315, [%rd1+304];
-	ld.v2.f32 	{%f1020, %f1021}, [%rd1+288];
-	mov.f32 	%f1025, 0f40000000;
-	abs.f32 	%f319, %f2341;
-	setp.lt.f32	%p29, %f319, 0f00800000;
-	mul.f32 	%f1027, %f319, 0f4B800000;
-	selp.f32	%f1028, 0fC3170000, 0fC2FE0000, %p29;
-	selp.f32	%f1029, %f1027, %f319, %p29;
-	mov.b32 	 %r315, %f1029;
-	and.b32  	%r316, %r315, 8388607;
-	or.b32  	%r317, %r316, 1065353216;
-	mov.b32 	 %f1030, %r317;
-	shr.u32 	%r318, %r315, 23;
-	cvt.rn.f32.u32	%f1031, %r318;
-	add.f32 	%f1032, %f1028, %f1031;
-	setp.gt.f32	%p30, %f1030, 0f3FB504F3;
-	mul.f32 	%f1033, %f1030, 0f3F000000;
-	add.f32 	%f1034, %f1032, 0f3F800000;
-	selp.f32	%f1035, %f1033, %f1030, %p30;
-	selp.f32	%f1036, %f1034, %f1032, %p30;
-	add.f32 	%f320, %f1035, 0fBF800000;
-	add.f32 	%f1019, %f1035, 0f3F800000;
-	// inline asm
-	rcp.approx.ftz.f32 %f1018,%f1019;
-	// inline asm
-	add.f32 	%f322, %f320, %f320;
-	mul.f32 	%f1037, %f1018, %f322;
-	mul.f32 	%f1038, %f1037, %f1037;
-	mov.f32 	%f1039, 0f3C4CAF63;
-	mov.f32 	%f1040, 0f3B18F0FE;
-	fma.rn.f32 	%f1041, %f1040, %f1038, %f1039;
-	mov.f32 	%f1042, 0f3DAAAABD;
-	fma.rn.f32 	%f1043, %f1041, %f1038, %f1042;
-	mul.rn.f32 	%f1044, %f1043, %f1038;
-	mul.rn.f32 	%f1045, %f1044, %f1037;
-	sub.f32 	%f1046, %f320, %f1037;
-	neg.f32 	%f1047, %f1037;
-	add.f32 	%f1048, %f1046, %f1046;
-	fma.rn.f32 	%f1049, %f1047, %f320, %f1048;
-	mul.rn.f32 	%f1050, %f1018, %f1049;
-	add.f32 	%f1051, %f1045, %f1037;
-	sub.f32 	%f1052, %f1037, %f1051;
-	add.f32 	%f1053, %f1045, %f1052;
-	add.f32 	%f1054, %f1050, %f1053;
-	add.f32 	%f1055, %f1051, %f1054;
-	sub.f32 	%f1056, %f1051, %f1055;
-	add.f32 	%f1057, %f1054, %f1056;
-	mov.f32 	%f1058, 0f3F317200;
-	mul.rn.f32 	%f323, %f1036, %f1058;
-	mov.f32 	%f1059, 0f35BFBE8E;
-	mul.rn.f32 	%f324, %f1036, %f1059;
-	add.f32 	%f1060, %f323, %f1055;
-	sub.f32 	%f1061, %f323, %f1060;
-	add.f32 	%f1062, %f1055, %f1061;
-	add.f32 	%f1063, %f1057, %f1062;
-	add.f32 	%f1064, %f324, %f1063;
-	add.f32 	%f1065, %f1060, %f1064;
-	sub.f32 	%f1066, %f1060, %f1065;
-	add.f32 	%f1067, %f1064, %f1066;
-	mul.rn.f32 	%f1068, %f1025, %f1065;
-	neg.f32 	%f1069, %f1068;
-	fma.rn.f32 	%f1070, %f1025, %f1065, %f1069;
-	fma.rn.f32 	%f1071, %f1025, %f1067, %f1070;
-	mov.f32 	%f1072, 0f00000000;
-	fma.rn.f32 	%f1073, %f1072, %f1065, %f1071;
-	add.rn.f32 	%f1074, %f1068, %f1073;
-	neg.f32 	%f1075, %f1074;
-	add.rn.f32 	%f1076, %f1068, %f1075;
-	add.rn.f32 	%f1077, %f1076, %f1073;
-	mov.b32 	 %r319, %f1074;
-	setp.eq.s32	%p31, %r319, 1118925336;
-	add.s32 	%r320, %r319, -1;
-	mov.b32 	 %f1078, %r320;
-	add.f32 	%f1079, %f1077, 0f37000000;
-	selp.f32	%f1080, %f1078, %f1074, %p31;
-	selp.f32	%f325, %f1079, %f1077, %p31;
-	mul.f32 	%f1081, %f1080, 0f3FB8AA3B;
-	cvt.rzi.f32.f32	%f1082, %f1081;
-	mov.f32 	%f1083, 0fBF317200;
-	fma.rn.f32 	%f1084, %f1082, %f1083, %f1080;
-	mov.f32 	%f1085, 0fB5BFBE8E;
-	fma.rn.f32 	%f1086, %f1082, %f1085, %f1084;
-	mul.f32 	%f1087, %f1086, 0f3FB8AA3B;
-	ex2.approx.ftz.f32 	%f1088, %f1087;
-	add.f32 	%f1089, %f1082, 0f00000000;
-	ex2.approx.f32 	%f1090, %f1089;
-	mul.f32 	%f1091, %f1088, %f1090;
-	setp.lt.f32	%p32, %f1080, 0fC2D20000;
-	selp.f32	%f1092, 0f00000000, %f1091, %p32;
-	setp.gt.f32	%p33, %f1080, 0f42D20000;
-	selp.f32	%f2344, 0f7F800000, %f1092, %p33;
-	setp.eq.f32	%p34, %f2344, 0f7F800000;
-	@%p34 bra 	BB11_43;
-
-	fma.rn.f32 	%f2344, %f2344, %f325, %f2344;
-
-BB11_43:
-	mov.f32 	%f2216, 0f3F800000;
-	cvt.rzi.f32.f32	%f2215, %f2216;
-	add.f32 	%f2214, %f2215, %f2215;
-	mov.f32 	%f2213, 0f40000000;
-	sub.f32 	%f2212, %f2213, %f2214;
-	abs.f32 	%f2211, %f2212;
-	setp.lt.f32	%p35, %f2341, 0f00000000;
-	setp.eq.f32	%p36, %f2211, 0f3F800000;
-	and.pred  	%p1, %p35, %p36;
-	mov.b32 	 %r321, %f2344;
-	xor.b32  	%r322, %r321, -2147483648;
-	mov.b32 	 %f1093, %r322;
-	selp.f32	%f2346, %f1093, %f2344, %p1;
-	setp.eq.f32	%p37, %f2341, 0f00000000;
-	@%p37 bra 	BB11_46;
-	bra.uni 	BB11_44;
-
-BB11_46:
-	add.f32 	%f1096, %f2341, %f2341;
-	selp.f32	%f2346, %f1096, 0f00000000, %p36;
-	bra.uni 	BB11_47;
-
-BB11_44:
-	setp.geu.f32	%p38, %f2341, 0f00000000;
-	@%p38 bra 	BB11_47;
-
-	cvt.rzi.f32.f32	%f1095, %f1025;
-	setp.neu.f32	%p39, %f1095, 0f40000000;
-	selp.f32	%f2346, 0f7FFFFFFF, %f2346, %p39;
-
-BB11_47:
-	abs.f32 	%f2217, %f2341;
-	add.f32 	%f1097, %f2217, 0f40000000;
-	mov.b32 	 %r8, %f1097;
-	setp.lt.s32	%p41, %r8, 2139095040;
-	@%p41 bra 	BB11_52;
-
-	abs.f32 	%f2226, %f2341;
-	setp.gtu.f32	%p42, %f2226, 0f7F800000;
-	@%p42 bra 	BB11_51;
-	bra.uni 	BB11_49;
-
-BB11_51:
-	add.f32 	%f2346, %f2341, 0f40000000;
-	bra.uni 	BB11_52;
-
-BB11_49:
-	abs.f32 	%f2227, %f2341;
-	setp.neu.f32	%p43, %f2227, 0f7F800000;
-	@%p43 bra 	BB11_52;
-
-	selp.f32	%f2346, 0fFF800000, 0f7F800000, %p1;
-
-BB11_52:
-	mov.f32 	%f2225, 0fB5BFBE8E;
-	mov.f32 	%f2224, 0fBF317200;
-	mov.f32 	%f2223, 0f00000000;
-	mov.f32 	%f2222, 0f35BFBE8E;
-	mov.f32 	%f2221, 0f3F317200;
-	mov.f32 	%f2220, 0f3DAAAABD;
-	mov.f32 	%f2219, 0f3C4CAF63;
-	mov.f32 	%f2218, 0f3B18F0FE;
-	setp.eq.f32	%p44, %f2341, 0f3F800000;
-	selp.f32	%f336, 0f3F800000, %f2346, %p44;
-	abs.f32 	%f337, %f2342;
-	setp.lt.f32	%p45, %f337, 0f00800000;
-	mul.f32 	%f1100, %f337, 0f4B800000;
-	selp.f32	%f1101, 0fC3170000, 0fC2FE0000, %p45;
-	selp.f32	%f1102, %f1100, %f337, %p45;
-	mov.b32 	 %r323, %f1102;
-	and.b32  	%r324, %r323, 8388607;
-	or.b32  	%r325, %r324, 1065353216;
-	mov.b32 	 %f1103, %r325;
-	shr.u32 	%r326, %r323, 23;
-	cvt.rn.f32.u32	%f1104, %r326;
-	add.f32 	%f1105, %f1101, %f1104;
-	setp.gt.f32	%p46, %f1103, 0f3FB504F3;
-	mul.f32 	%f1106, %f1103, 0f3F000000;
-	add.f32 	%f1107, %f1105, 0f3F800000;
-	selp.f32	%f1108, %f1106, %f1103, %p46;
-	selp.f32	%f1109, %f1107, %f1105, %p46;
-	add.f32 	%f338, %f1108, 0fBF800000;
-	add.f32 	%f1099, %f1108, 0f3F800000;
-	// inline asm
-	rcp.approx.ftz.f32 %f1098,%f1099;
-	// inline asm
-	add.f32 	%f340, %f338, %f338;
-	mul.f32 	%f1110, %f1098, %f340;
-	mul.f32 	%f1111, %f1110, %f1110;
-	fma.rn.f32 	%f1114, %f2218, %f1111, %f2219;
-	fma.rn.f32 	%f1116, %f1114, %f1111, %f2220;
-	mul.rn.f32 	%f1117, %f1116, %f1111;
-	mul.rn.f32 	%f1118, %f1117, %f1110;
-	sub.f32 	%f1119, %f338, %f1110;
-	neg.f32 	%f1120, %f1110;
-	add.f32 	%f1121, %f1119, %f1119;
-	fma.rn.f32 	%f1122, %f1120, %f338, %f1121;
-	mul.rn.f32 	%f1123, %f1098, %f1122;
-	add.f32 	%f1124, %f1118, %f1110;
-	sub.f32 	%f1125, %f1110, %f1124;
-	add.f32 	%f1126, %f1118, %f1125;
-	add.f32 	%f1127, %f1123, %f1126;
-	add.f32 	%f1128, %f1124, %f1127;
-	sub.f32 	%f1129, %f1124, %f1128;
-	add.f32 	%f1130, %f1127, %f1129;
-	mul.rn.f32 	%f341, %f1109, %f2221;
-	mul.rn.f32 	%f342, %f1109, %f2222;
-	add.f32 	%f1133, %f341, %f1128;
-	sub.f32 	%f1134, %f341, %f1133;
-	add.f32 	%f1135, %f1128, %f1134;
-	add.f32 	%f1136, %f1130, %f1135;
-	add.f32 	%f1137, %f342, %f1136;
-	add.f32 	%f1138, %f1133, %f1137;
-	sub.f32 	%f1139, %f1133, %f1138;
-	add.f32 	%f1140, %f1137, %f1139;
-	mul.rn.f32 	%f1142, %f1025, %f1138;
-	neg.f32 	%f1143, %f1142;
-	fma.rn.f32 	%f1144, %f1025, %f1138, %f1143;
-	fma.rn.f32 	%f1145, %f1025, %f1140, %f1144;
-	fma.rn.f32 	%f1147, %f2223, %f1138, %f1145;
-	add.rn.f32 	%f1148, %f1142, %f1147;
-	neg.f32 	%f1149, %f1148;
-	add.rn.f32 	%f1150, %f1142, %f1149;
-	add.rn.f32 	%f1151, %f1150, %f1147;
-	mov.b32 	 %r327, %f1148;
-	setp.eq.s32	%p47, %r327, 1118925336;
-	add.s32 	%r328, %r327, -1;
-	mov.b32 	 %f1152, %r328;
-	add.f32 	%f1153, %f1151, 0f37000000;
-	selp.f32	%f1154, %f1152, %f1148, %p47;
-	selp.f32	%f343, %f1153, %f1151, %p47;
-	mul.f32 	%f1155, %f1154, 0f3FB8AA3B;
-	cvt.rzi.f32.f32	%f1156, %f1155;
-	fma.rn.f32 	%f1158, %f1156, %f2224, %f1154;
-	fma.rn.f32 	%f1160, %f1156, %f2225, %f1158;
-	mul.f32 	%f1161, %f1160, 0f3FB8AA3B;
-	ex2.approx.ftz.f32 	%f1162, %f1161;
-	add.f32 	%f1163, %f1156, 0f00000000;
-	ex2.approx.f32 	%f1164, %f1163;
-	mul.f32 	%f1165, %f1162, %f1164;
-	setp.lt.f32	%p48, %f1154, 0fC2D20000;
-	selp.f32	%f1166, 0f00000000, %f1165, %p48;
-	setp.gt.f32	%p49, %f1154, 0f42D20000;
-	selp.f32	%f2347, 0f7F800000, %f1166, %p49;
-	setp.eq.f32	%p50, %f2347, 0f7F800000;
-	@%p50 bra 	BB11_54;
-
-	fma.rn.f32 	%f2347, %f2347, %f343, %f2347;
-
-BB11_54:
-	setp.lt.f32	%p51, %f2342, 0f00000000;
-	and.pred  	%p2, %p51, %p36;
-	mov.b32 	 %r329, %f2347;
-	xor.b32  	%r330, %r329, -2147483648;
-	mov.b32 	 %f1167, %r330;
-	selp.f32	%f2349, %f1167, %f2347, %p2;
-	setp.eq.f32	%p53, %f2342, 0f00000000;
-	@%p53 bra 	BB11_57;
-	bra.uni 	BB11_55;
-
-BB11_57:
-	add.f32 	%f1170, %f2342, %f2342;
-	selp.f32	%f2349, %f1170, 0f00000000, %p36;
-	bra.uni 	BB11_58;
-
-BB11_55:
-	setp.geu.f32	%p54, %f2342, 0f00000000;
-	@%p54 bra 	BB11_58;
-
-	cvt.rzi.f32.f32	%f1169, %f1025;
-	setp.neu.f32	%p55, %f1169, 0f40000000;
-	selp.f32	%f2349, 0f7FFFFFFF, %f2349, %p55;
-
-BB11_58:
-	abs.f32 	%f2198, %f2342;
-	add.f32 	%f1171, %f2198, 0f40000000;
-	mov.b32 	 %r9, %f1171;
-	setp.lt.s32	%p57, %r9, 2139095040;
-	@%p57 bra 	BB11_63;
-
-	abs.f32 	%f2207, %f2342;
-	setp.gtu.f32	%p58, %f2207, 0f7F800000;
-	@%p58 bra 	BB11_62;
-	bra.uni 	BB11_60;
-
-BB11_62:
-	add.f32 	%f2349, %f2342, 0f40000000;
-	bra.uni 	BB11_63;
-
-BB11_60:
-	abs.f32 	%f2208, %f2342;
-	setp.neu.f32	%p59, %f2208, 0f7F800000;
-	@%p59 bra 	BB11_63;
-
-	selp.f32	%f2349, 0fFF800000, 0f7F800000, %p2;
-
-BB11_63:
-	mov.f32 	%f2206, 0fB5BFBE8E;
-	mov.f32 	%f2205, 0fBF317200;
-	mov.f32 	%f2204, 0f00000000;
-	mov.f32 	%f2203, 0f35BFBE8E;
-	mov.f32 	%f2202, 0f3F317200;
-	mov.f32 	%f2201, 0f3DAAAABD;
-	mov.f32 	%f2200, 0f3C4CAF63;
-	mov.f32 	%f2199, 0f3B18F0FE;
-	setp.eq.f32	%p60, %f2342, 0f3F800000;
-	selp.f32	%f1174, 0f3F800000, %f2349, %p60;
-	add.f32 	%f354, %f336, %f1174;
-	add.f32 	%f355, %f2294, %f2294;
-	mul.f32 	%f1175, %f355, %f2341;
-	add.f32 	%f1176, %f1020, %f1020;
-	mul.f32 	%f1177, %f1176, %f2341;
-	sub.f32 	%f1178, %f1175, %f1177;
-	add.f32 	%f356, %f2293, %f2293;
-	fma.rn.f32 	%f1179, %f356, %f2342, %f1178;
-	add.f32 	%f1180, %f1021, %f1021;
-	mul.f32 	%f1181, %f1180, %f2342;
-	sub.f32 	%f357, %f1179, %f1181;
-	abs.f32 	%f358, %f2294;
-	setp.lt.f32	%p61, %f358, 0f00800000;
-	mul.f32 	%f1182, %f358, 0f4B800000;
-	selp.f32	%f1183, 0fC3170000, 0fC2FE0000, %p61;
-	selp.f32	%f1184, %f1182, %f358, %p61;
-	mov.b32 	 %r331, %f1184;
-	and.b32  	%r332, %r331, 8388607;
-	or.b32  	%r333, %r332, 1065353216;
-	mov.b32 	 %f1185, %r333;
-	shr.u32 	%r334, %r331, 23;
-	cvt.rn.f32.u32	%f1186, %r334;
-	add.f32 	%f1187, %f1183, %f1186;
-	setp.gt.f32	%p62, %f1185, 0f3FB504F3;
-	mul.f32 	%f1188, %f1185, 0f3F000000;
-	add.f32 	%f1189, %f1187, 0f3F800000;
-	selp.f32	%f1190, %f1188, %f1185, %p62;
-	selp.f32	%f1191, %f1189, %f1187, %p62;
-	add.f32 	%f359, %f1190, 0fBF800000;
-	add.f32 	%f1173, %f1190, 0f3F800000;
-	// inline asm
-	rcp.approx.ftz.f32 %f1172,%f1173;
-	// inline asm
-	add.f32 	%f361, %f359, %f359;
-	mul.f32 	%f1192, %f1172, %f361;
-	mul.f32 	%f1193, %f1192, %f1192;
-	fma.rn.f32 	%f1196, %f2199, %f1193, %f2200;
-	fma.rn.f32 	%f1198, %f1196, %f1193, %f2201;
-	mul.rn.f32 	%f1199, %f1198, %f1193;
-	mul.rn.f32 	%f1200, %f1199, %f1192;
-	sub.f32 	%f1201, %f359, %f1192;
-	neg.f32 	%f1202, %f1192;
-	add.f32 	%f1203, %f1201, %f1201;
-	fma.rn.f32 	%f1204, %f1202, %f359, %f1203;
-	mul.rn.f32 	%f1205, %f1172, %f1204;
-	add.f32 	%f1206, %f1200, %f1192;
-	sub.f32 	%f1207, %f1192, %f1206;
-	add.f32 	%f1208, %f1200, %f1207;
-	add.f32 	%f1209, %f1205, %f1208;
-	add.f32 	%f1210, %f1206, %f1209;
-	sub.f32 	%f1211, %f1206, %f1210;
-	add.f32 	%f1212, %f1209, %f1211;
-	mul.rn.f32 	%f362, %f1191, %f2202;
-	mul.rn.f32 	%f363, %f1191, %f2203;
-	add.f32 	%f1215, %f362, %f1210;
-	sub.f32 	%f1216, %f362, %f1215;
-	add.f32 	%f1217, %f1210, %f1216;
-	add.f32 	%f1218, %f1212, %f1217;
-	add.f32 	%f1219, %f363, %f1218;
-	add.f32 	%f1220, %f1215, %f1219;
-	sub.f32 	%f1221, %f1215, %f1220;
-	add.f32 	%f1222, %f1219, %f1221;
-	mul.rn.f32 	%f1224, %f1025, %f1220;
-	neg.f32 	%f1225, %f1224;
-	fma.rn.f32 	%f1226, %f1025, %f1220, %f1225;
-	fma.rn.f32 	%f1227, %f1025, %f1222, %f1226;
-	fma.rn.f32 	%f1229, %f2204, %f1220, %f1227;
-	add.rn.f32 	%f1230, %f1224, %f1229;
-	neg.f32 	%f1231, %f1230;
-	add.rn.f32 	%f1232, %f1224, %f1231;
-	add.rn.f32 	%f1233, %f1232, %f1229;
-	mov.b32 	 %r335, %f1230;
-	setp.eq.s32	%p63, %r335, 1118925336;
-	add.s32 	%r336, %r335, -1;
-	mov.b32 	 %f1234, %r336;
-	add.f32 	%f1235, %f1233, 0f37000000;
-	selp.f32	%f1236, %f1234, %f1230, %p63;
-	selp.f32	%f364, %f1235, %f1233, %p63;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r244,%r245,%r246,%r247}, [%rd197];
+	// end inline asm
+	mov.b32 	%f818, %r244;
+	mov.b32 	%f819, %r245;
+	mov.b32 	%f820, %r246;
+	mul.f32 	%f821, %f220, %f818;
+	mul.f32 	%f822, %f220, %f819;
+	mul.f32 	%f823, %f220, %f820;
+	fma.rn.f32 	%f2011, %f817, %f2011, %f821;
+	fma.rn.f32 	%f2012, %f817, %f2012, %f822;
+	fma.rn.f32 	%f2013, %f817, %f2013, %f823;
+	add.s64 	%rd201, %rd185, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd200, %rd201;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r248,%r249,%r250,%r251}, [%rd200];
+	// end inline asm
+	mov.b32 	%f824, %r248;
+	mov.b32 	%f825, %r249;
+	mov.b32 	%f826, %r251;
+	mul.f32 	%f827, %f220, %f824;
+	mul.f32 	%f828, %f220, %f825;
+	mul.f32 	%f829, %f220, %f826;
+	fma.rn.f32 	%f2014, %f817, %f2014, %f827;
+	fma.rn.f32 	%f2015, %f817, %f2015, %f828;
+	fma.rn.f32 	%f2016, %f817, %f2016, %f829;
+	add.s64 	%rd204, %rd185, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd203, %rd204;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r252,%r253,%r254,%r255}, [%rd203];
+	// end inline asm
+	mov.b32 	%f830, %r253;
+	mov.b32 	%f831, %r254;
+	mov.b32 	%f832, %r255;
+	mul.f32 	%f833, %f220, %f830;
+	mul.f32 	%f834, %f220, %f831;
+	mul.f32 	%f835, %f220, %f832;
+	fma.rn.f32 	%f836, %f817, %f2017, %f833;
+	fma.rn.f32 	%f837, %f817, %f2018, %f834;
+	fma.rn.f32 	%f838, %f817, %f2019, %f835;
+	add.s64 	%rd207, %rd185, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd206, %rd207;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r256,%r257,%r258,%r259}, [%rd206];
+	// end inline asm
+	mov.b32 	%f839, %r256;
+	mul.f32 	%f840, %f220, %f839;
+	fma.rn.f32 	%f841, %f817, %f2020, %f840;
+	mul.f32 	%f842, %f837, %f837;
+	fma.rn.f32 	%f843, %f836, %f836, %f842;
+	fma.rn.f32 	%f844, %f838, %f838, %f843;
+	fma.rn.f32 	%f845, %f841, %f841, %f844;
+	sqrt.rn.f32 	%f846, %f845;
+	rcp.rn.f32 	%f847, %f846;
+	mul.f32 	%f2017, %f836, %f847;
+	mul.f32 	%f2018, %f837, %f847;
+	mul.f32 	%f2019, %f838, %f847;
+	mul.f32 	%f2020, %f847, %f841;
+
+$L__BB11_32:
+	mul.f32 	%f848, %f2018, %f2018;
+	fma.rn.f32 	%f849, %f2017, %f2017, %f848;
+	fma.rn.f32 	%f850, %f2019, %f2019, %f849;
+	fma.rn.f32 	%f851, %f2020, %f2020, %f850;
+	rcp.rn.f32 	%f852, %f851;
+	mul.f32 	%f853, %f2017, %f852;
+	mul.f32 	%f854, %f2018, %f852;
+	mul.f32 	%f855, %f2019, %f852;
+	mul.f32 	%f856, %f2020, %f852;
+	mul.f32 	%f857, %f2017, %f853;
+	mul.f32 	%f858, %f2018, %f854;
+	mul.f32 	%f859, %f2019, %f855;
+	mul.f32 	%f860, %f2017, %f854;
+	mul.f32 	%f861, %f2019, %f856;
+	mul.f32 	%f862, %f2017, %f855;
+	mul.f32 	%f863, %f2018, %f856;
+	mul.f32 	%f864, %f2018, %f855;
+	mul.f32 	%f865, %f2017, %f856;
+	sub.f32 	%f866, %f857, %f858;
+	sub.f32 	%f867, %f866, %f859;
+	fma.rn.f32 	%f868, %f2020, %f856, %f867;
+	sub.f32 	%f869, %f860, %f861;
+	add.f32 	%f870, %f869, %f869;
+	add.f32 	%f871, %f862, %f863;
+	add.f32 	%f872, %f871, %f871;
+	add.f32 	%f873, %f860, %f861;
+	add.f32 	%f874, %f873, %f873;
+	sub.f32 	%f875, %f858, %f857;
+	sub.f32 	%f876, %f875, %f859;
+	fma.rn.f32 	%f877, %f2020, %f856, %f876;
+	sub.f32 	%f878, %f864, %f865;
+	add.f32 	%f879, %f878, %f878;
+	sub.f32 	%f880, %f862, %f863;
+	add.f32 	%f881, %f880, %f880;
+	add.f32 	%f882, %f864, %f865;
+	add.f32 	%f883, %f882, %f882;
+	neg.f32 	%f884, %f857;
+	sub.f32 	%f885, %f884, %f858;
+	add.f32 	%f886, %f859, %f885;
+	fma.rn.f32 	%f887, %f2020, %f856, %f886;
+	mul.f32 	%f888, %f2013, %f868;
+	fma.rn.f32 	%f889, %f2015, %f870, %f888;
+	fma.rn.f32 	%f2029, %f2016, %f872, %f889;
+	mul.f32 	%f890, %f2015, %f877;
+	fma.rn.f32 	%f891, %f2013, %f874, %f890;
+	fma.rn.f32 	%f2026, %f2016, %f879, %f891;
+	mul.f32 	%f892, %f2015, %f883;
+	fma.rn.f32 	%f893, %f2013, %f881, %f892;
+	fma.rn.f32 	%f2023, %f2016, %f887, %f893;
+	mul.f32 	%f894, %f2012, %f868;
+	fma.rn.f32 	%f2028, %f2014, %f870, %f894;
+	mul.f32 	%f895, %f2014, %f877;
+	fma.rn.f32 	%f2025, %f2012, %f874, %f895;
+	mul.f32 	%f896, %f2014, %f883;
+	fma.rn.f32 	%f2022, %f2012, %f881, %f896;
+	mul.f32 	%f2027, %f2011, %f868;
+	mul.f32 	%f2024, %f2011, %f874;
+	mul.f32 	%f2021, %f2011, %f881;
+
+$L__BB11_35:
+	mul.f32 	%f928, %f2022, %f2026;
+	mul.f32 	%f929, %f2023, %f2025;
+	sub.f32 	%f930, %f929, %f928;
+	mul.f32 	%f931, %f2027, %f930;
+	mul.f32 	%f932, %f2021, %f2026;
+	mul.f32 	%f933, %f2023, %f2024;
+	sub.f32 	%f934, %f933, %f932;
+	mul.f32 	%f935, %f934, %f2028;
+	sub.f32 	%f936, %f931, %f935;
+	mul.f32 	%f937, %f2021, %f2025;
+	mul.f32 	%f938, %f2022, %f2024;
+	sub.f32 	%f939, %f938, %f937;
+	fma.rn.f32 	%f940, %f939, %f2029, %f936;
+	rcp.rn.f32 	%f941, %f940;
+	mul.f32 	%f2036, %f930, %f941;
+	mul.f32 	%f942, %f2023, %f2028;
+	mul.f32 	%f943, %f2022, %f2029;
+	sub.f32 	%f944, %f943, %f942;
+	mul.f32 	%f2037, %f944, %f941;
+	mul.f32 	%f945, %f2025, %f2029;
+	mul.f32 	%f946, %f2026, %f2028;
+	sub.f32 	%f947, %f946, %f945;
+	mul.f32 	%f2038, %f947, %f941;
+	sub.f32 	%f948, %f932, %f933;
+	mul.f32 	%f2033, %f948, %f941;
+	mul.f32 	%f949, %f2021, %f2029;
+	mul.f32 	%f950, %f2023, %f2027;
+	sub.f32 	%f951, %f950, %f949;
+	mul.f32 	%f2034, %f951, %f941;
+	mul.f32 	%f952, %f2026, %f2027;
+	mul.f32 	%f953, %f2024, %f2029;
+	sub.f32 	%f954, %f953, %f952;
+	mul.f32 	%f2035, %f954, %f941;
+	mul.f32 	%f2030, %f939, %f941;
+	mul.f32 	%f955, %f2022, %f2027;
+	mul.f32 	%f956, %f2021, %f2028;
+	sub.f32 	%f957, %f956, %f955;
+	mul.f32 	%f2031, %f957, %f941;
+	mul.f32 	%f958, %f2024, %f2028;
+	mul.f32 	%f959, %f2025, %f2027;
+	sub.f32 	%f960, %f959, %f958;
+	mul.f32 	%f2032, %f960, %f941;
+	bra.uni 	$L__BB11_36;
+
+$L__BB11_27:
+	// begin inline asm
+	call (%rd258), _optix_get_instance_inverse_transform_from_handle, (%rd137);
+	// end inline asm
+
+$L__BB11_28:
+	// begin inline asm
+	cvta.to.global.u64 %rd143, %rd258;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r173,%r174,%r175,%r176}, [%rd143];
+	// end inline asm
+	mov.b32 	%f2036, %r173;
+	mov.b32 	%f2037, %r174;
+	mov.b32 	%f2038, %r175;
+	add.s64 	%rd147, %rd258, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd146, %rd147;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r177,%r178,%r179,%r180}, [%rd146];
+	// end inline asm
+	mov.b32 	%f2033, %r177;
+	mov.b32 	%f2034, %r178;
+	mov.b32 	%f2035, %r179;
+	add.s64 	%rd150, %rd258, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd149, %rd150;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r181,%r182,%r183,%r184}, [%rd149];
+	// end inline asm
+	mov.b32 	%f2030, %r181;
+	mov.b32 	%f2031, %r182;
+	mov.b32 	%f2032, %r183;
+
+$L__BB11_36:
+	setp.eq.s32 	%p31, %r457, 0;
+	@%p31 bra 	$L__BB11_38;
+
+	mul.f32 	%f961, %f2007, %f2037;
+	fma.rn.f32 	%f962, %f2004, %f2036, %f961;
+	fma.rn.f32 	%f306, %f2010, %f2038, %f962;
+	mul.f32 	%f963, %f2006, %f2037;
+	fma.rn.f32 	%f964, %f2003, %f2036, %f963;
+	fma.rn.f32 	%f307, %f2009, %f2038, %f964;
+	mul.f32 	%f965, %f2005, %f2037;
+	fma.rn.f32 	%f966, %f2002, %f2036, %f965;
+	fma.rn.f32 	%f2038, %f2008, %f2038, %f966;
+	mul.f32 	%f967, %f2007, %f2034;
+	fma.rn.f32 	%f968, %f2004, %f2033, %f967;
+	fma.rn.f32 	%f309, %f2010, %f2035, %f968;
+	mul.f32 	%f969, %f2006, %f2034;
+	fma.rn.f32 	%f970, %f2003, %f2033, %f969;
+	fma.rn.f32 	%f310, %f2009, %f2035, %f970;
+	mul.f32 	%f971, %f2005, %f2034;
+	fma.rn.f32 	%f972, %f2002, %f2033, %f971;
+	fma.rn.f32 	%f2035, %f2008, %f2035, %f972;
+	mul.f32 	%f973, %f2007, %f2031;
+	fma.rn.f32 	%f974, %f2004, %f2030, %f973;
+	fma.rn.f32 	%f312, %f2010, %f2032, %f974;
+	mul.f32 	%f975, %f2006, %f2031;
+	fma.rn.f32 	%f976, %f2003, %f2030, %f975;
+	fma.rn.f32 	%f313, %f2009, %f2032, %f976;
+	mul.f32 	%f977, %f2005, %f2031;
+	fma.rn.f32 	%f978, %f2002, %f2030, %f977;
+	fma.rn.f32 	%f2032, %f2008, %f2032, %f978;
+	mov.f32 	%f2030, %f312;
+	mov.f32 	%f2031, %f313;
+	mov.f32 	%f2033, %f309;
+	mov.f32 	%f2034, %f310;
+	mov.f32 	%f2036, %f306;
+	mov.f32 	%f2037, %f307;
+
+$L__BB11_38:
+	add.s32 	%r457, %r457, 1;
+	setp.lt.u32 	%p32, %r457, %r168;
+	mov.f32 	%f2002, %f2038;
+	mov.f32 	%f2003, %f2037;
+	mov.f32 	%f2004, %f2036;
+	mov.f32 	%f2005, %f2035;
+	mov.f32 	%f2006, %f2034;
+	mov.f32 	%f2007, %f2033;
+	mov.f32 	%f2008, %f2032;
+	mov.f32 	%f2009, %f2031;
+	mov.f32 	%f2010, %f2030;
+	@%p32 bra 	$L__BB11_23;
+
+$L__BB11_39:
+	mul.f32 	%f979, %f2058, %f2037;
+	fma.rn.f32 	%f980, %f2057, %f2036, %f979;
+	mul.f32 	%f981, %f2058, %f2034;
+	fma.rn.f32 	%f982, %f2057, %f2033, %f981;
+	mul.f32 	%f983, %f2058, %f2031;
+	fma.rn.f32 	%f984, %f2057, %f2030, %f983;
+	fma.rn.f32 	%f2059, %f792, %f2032, %f984;
+	fma.rn.f32 	%f2058, %f792, %f2035, %f982;
+	fma.rn.f32 	%f2057, %f792, %f2038, %f980;
+	bra.uni 	$L__BB11_41;
+
+$L__BB11_40:
+	mov.f32 	%f2059, %f792;
+
+$L__BB11_41:
+	// begin inline asm
+	call (%f985), _optix_get_ray_tmin, ();
+	// end inline asm
+	// begin inline asm
+	call (%f986), _optix_get_ray_tmax, ();
+	// end inline asm
+	add.s64 	%rd16, %rd1, 304;
+	ld.f32 	%f346, [%rd1+304];
+	ld.f32 	%f347, [%rd1+288];
+	ld.f32 	%f348, [%rd1+292];
+	mov.f32 	%f992, 0f40000000;
+	abs.f32 	%f350, %f2057;
+	setp.lt.f32 	%p33, %f350, 0f00800000;
+	mul.f32 	%f994, %f350, 0f4B800000;
+	selp.f32 	%f995, %f994, %f350, %p33;
+	selp.f32 	%f996, 0fC3170000, 0fC2FE0000, %p33;
+	mov.b32 	%r319, %f995;
+	and.b32  	%r320, %r319, 8388607;
+	or.b32  	%r321, %r320, 1065353216;
+	mov.b32 	%f997, %r321;
+	shr.u32 	%r322, %r319, 23;
+	cvt.rn.f32.u32 	%f998, %r322;
+	add.f32 	%f999, %f996, %f998;
+	setp.gt.f32 	%p34, %f997, 0f3FB504F3;
+	mul.f32 	%f1000, %f997, 0f3F000000;
+	add.f32 	%f1001, %f999, 0f3F800000;
+	selp.f32 	%f1002, %f1001, %f999, %p34;
+	selp.f32 	%f1003, %f1000, %f997, %p34;
+	add.f32 	%f1004, %f1003, 0fBF800000;
+	add.f32 	%f1005, %f1003, 0f3F800000;
+	rcp.approx.ftz.f32 	%f1006, %f1005;
+	add.f32 	%f1007, %f1004, %f1004;
+	mul.f32 	%f1008, %f1007, %f1006;
+	mul.f32 	%f1009, %f1008, %f1008;
+	mov.f32 	%f1010, 0f3C4CAF63;
+	mov.f32 	%f1011, 0f3B18F0FE;
+	fma.rn.f32 	%f1012, %f1011, %f1009, %f1010;
+	mov.f32 	%f1013, 0f3DAAAABD;
+	fma.rn.f32 	%f1014, %f1012, %f1009, %f1013;
+	mul.rn.f32 	%f1015, %f1014, %f1009;
+	mul.rn.f32 	%f1016, %f1015, %f1008;
+	sub.f32 	%f1017, %f1004, %f1008;
+	add.f32 	%f1018, %f1017, %f1017;
+	neg.f32 	%f1019, %f1008;
+	fma.rn.f32 	%f1020, %f1019, %f1004, %f1018;
+	mul.rn.f32 	%f1021, %f1006, %f1020;
+	add.f32 	%f1022, %f1016, %f1008;
+	sub.f32 	%f1023, %f1008, %f1022;
+	add.f32 	%f1024, %f1016, %f1023;
+	add.f32 	%f1025, %f1021, %f1024;
+	add.f32 	%f1026, %f1022, %f1025;
+	sub.f32 	%f1027, %f1022, %f1026;
+	add.f32 	%f1028, %f1025, %f1027;
+	mov.f32 	%f1029, 0f3F317200;
+	mul.rn.f32 	%f1030, %f1002, %f1029;
+	mov.f32 	%f1031, 0f35BFBE8E;
+	mul.rn.f32 	%f1032, %f1002, %f1031;
+	add.f32 	%f1033, %f1030, %f1026;
+	sub.f32 	%f1034, %f1030, %f1033;
+	add.f32 	%f1035, %f1026, %f1034;
+	add.f32 	%f1036, %f1028, %f1035;
+	add.f32 	%f1037, %f1032, %f1036;
+	add.f32 	%f1038, %f1033, %f1037;
+	sub.f32 	%f1039, %f1033, %f1038;
+	add.f32 	%f1040, %f1037, %f1039;
+	mul.rn.f32 	%f1041, %f992, %f1038;
+	neg.f32 	%f1042, %f1041;
+	fma.rn.f32 	%f1043, %f992, %f1038, %f1042;
+	fma.rn.f32 	%f1044, %f992, %f1040, %f1043;
+	mov.f32 	%f1045, 0f00000000;
+	fma.rn.f32 	%f1046, %f1045, %f1038, %f1044;
+	add.rn.f32 	%f1047, %f1041, %f1046;
+	neg.f32 	%f1048, %f1047;
+	add.rn.f32 	%f1049, %f1041, %f1048;
+	add.rn.f32 	%f1050, %f1049, %f1046;
+	mov.b32 	%r323, %f1047;
+	setp.eq.s32 	%p35, %r323, 1118925336;
+	add.s32 	%r324, %r323, -1;
+	mov.b32 	%f1051, %r324;
+	add.f32 	%f1052, %f1050, 0f37000000;
+	selp.f32 	%f351, %f1052, %f1050, %p35;
+	selp.f32 	%f1053, %f1051, %f1047, %p35;
+	mov.f32 	%f1054, 0f3FB8AA3B;
+	mul.rn.f32 	%f1055, %f1053, %f1054;
+	cvt.rzi.f32.f32 	%f1056, %f1055;
+	abs.f32 	%f1057, %f1056;
+	setp.gt.f32 	%p36, %f1057, 0f42FC0000;
+	mov.b32 	%r325, %f1056;
+	and.b32  	%r326, %r325, -2147483648;
+	or.b32  	%r327, %r326, 1123811328;
+	mov.b32 	%f1058, %r327;
+	selp.f32 	%f1059, %f1058, %f1056, %p36;
+	mov.f32 	%f1060, 0fBF317218;
+	fma.rn.f32 	%f1061, %f1059, %f1060, %f1053;
+	mov.f32 	%f1062, 0f3102E308;
+	fma.rn.f32 	%f1063, %f1059, %f1062, %f1061;
+	mul.f32 	%f1064, %f1063, 0f3FB8AA3B;
+	add.f32 	%f1065, %f1059, 0f4B40007F;
+	mov.b32 	%r328, %f1065;
+	shl.b32 	%r329, %r328, 23;
+	mov.b32 	%f1066, %r329;
+	ex2.approx.ftz.f32 	%f1067, %f1064;
+	mul.f32 	%f352, %f1067, %f1066;
+	setp.eq.f32 	%p37, %f352, 0f7F800000;
+	mov.f32 	%f2060, 0f7F800000;
+	@%p37 bra 	$L__BB11_43;
+
+	fma.rn.f32 	%f2060, %f352, %f351, %f352;
+
+$L__BB11_43:
+	mov.f32 	%f1908, 0f3F800000;
+	cvt.rzi.f32.f32 	%f1907, %f1908;
+	add.f32 	%f1906, %f1907, %f1907;
+	mov.f32 	%f1905, 0f40000000;
+	sub.f32 	%f1904, %f1905, %f1906;
+	abs.f32 	%f1903, %f1904;
+	setp.lt.f32 	%p38, %f2057, 0f00000000;
+	setp.eq.f32 	%p39, %f1903, 0f3F800000;
+	and.pred  	%p1, %p38, %p39;
+	setp.eq.f32 	%p40, %f2057, 0f00000000;
+	@%p40 bra 	$L__BB11_47;
+	bra.uni 	$L__BB11_44;
+
+$L__BB11_47:
+	add.f32 	%f1072, %f2057, %f2057;
+	selp.f32 	%f2062, %f1072, 0f00000000, %p39;
+	bra.uni 	$L__BB11_48;
+
+$L__BB11_44:
+	mov.b32 	%r330, %f2060;
+	xor.b32  	%r331, %r330, -2147483648;
+	mov.b32 	%f1068, %r331;
+	selp.f32 	%f2062, %f1068, %f2060, %p1;
+	setp.geu.f32 	%p41, %f2057, 0f00000000;
+	@%p41 bra 	$L__BB11_48;
+
+	mov.f32 	%f1922, 0f40000000;
+	cvt.rzi.f32.f32 	%f1070, %f1922;
+	setp.eq.f32 	%p42, %f1070, 0f40000000;
+	@%p42 bra 	$L__BB11_48;
+
+	mov.f32 	%f2062, 0f7FFFFFFF;
+
+$L__BB11_48:
+	abs.f32 	%f1909, %f2057;
+	add.f32 	%f1073, %f1909, 0f40000000;
+	mov.b32 	%r9, %f1073;
+	setp.lt.s32 	%p44, %r9, 2139095040;
+	@%p44 bra 	$L__BB11_53;
+
+	abs.f32 	%f1920, %f2057;
+	setp.gtu.f32 	%p45, %f1920, 0f7F800000;
+	@%p45 bra 	$L__BB11_52;
+	bra.uni 	$L__BB11_50;
+
+$L__BB11_52:
+	add.f32 	%f2062, %f2057, 0f40000000;
+	bra.uni 	$L__BB11_53;
+
+$L__BB11_50:
+	abs.f32 	%f1921, %f2057;
+	setp.neu.f32 	%p46, %f1921, 0f7F800000;
+	@%p46 bra 	$L__BB11_53;
+
+	selp.f32 	%f2062, 0fFF800000, 0f7F800000, %p1;
+
+$L__BB11_53:
+	mov.f32 	%f1919, 0f3102E308;
+	mov.f32 	%f1918, 0fBF317218;
+	mov.f32 	%f1917, 0f3FB8AA3B;
+	mov.f32 	%f1916, 0f00000000;
+	mov.f32 	%f1915, 0f35BFBE8E;
+	mov.f32 	%f1914, 0f3F317200;
+	mov.f32 	%f1913, 0f3DAAAABD;
+	mov.f32 	%f1912, 0f3C4CAF63;
+	mov.f32 	%f1911, 0f3B18F0FE;
+	mov.f32 	%f1910, 0f40000000;
+	abs.f32 	%f361, %f2058;
+	setp.lt.f32 	%p47, %f361, 0f00800000;
+	mul.f32 	%f1075, %f361, 0f4B800000;
+	selp.f32 	%f1076, %f1075, %f361, %p47;
+	selp.f32 	%f1077, 0fC3170000, 0fC2FE0000, %p47;
+	mov.b32 	%r332, %f1076;
+	and.b32  	%r333, %r332, 8388607;
+	or.b32  	%r334, %r333, 1065353216;
+	mov.b32 	%f1078, %r334;
+	shr.u32 	%r335, %r332, 23;
+	cvt.rn.f32.u32 	%f1079, %r335;
+	add.f32 	%f1080, %f1077, %f1079;
+	setp.gt.f32 	%p48, %f1078, 0f3FB504F3;
+	mul.f32 	%f1081, %f1078, 0f3F000000;
+	add.f32 	%f1082, %f1080, 0f3F800000;
+	selp.f32 	%f1083, %f1082, %f1080, %p48;
+	selp.f32 	%f1084, %f1081, %f1078, %p48;
+	add.f32 	%f1085, %f1084, 0fBF800000;
+	add.f32 	%f1086, %f1084, 0f3F800000;
+	rcp.approx.ftz.f32 	%f1087, %f1086;
+	add.f32 	%f1088, %f1085, %f1085;
+	mul.f32 	%f1090, %f1088, %f1087;
+	mul.f32 	%f1091, %f1090, %f1090;
+	fma.rn.f32 	%f1094, %f1911, %f1091, %f1912;
+	fma.rn.f32 	%f1096, %f1094, %f1091, %f1913;
+	mul.rn.f32 	%f1097, %f1096, %f1091;
+	mul.rn.f32 	%f1098, %f1097, %f1090;
+	sub.f32 	%f1099, %f1085, %f1090;
+	add.f32 	%f1100, %f1099, %f1099;
+	neg.f32 	%f1101, %f1090;
+	fma.rn.f32 	%f1102, %f1101, %f1085, %f1100;
+	mul.rn.f32 	%f1103, %f1087, %f1102;
+	add.f32 	%f1104, %f1098, %f1090;
+	sub.f32 	%f1105, %f1090, %f1104;
+	add.f32 	%f1106, %f1098, %f1105;
+	add.f32 	%f1107, %f1103, %f1106;
+	add.f32 	%f1108, %f1104, %f1107;
+	sub.f32 	%f1109, %f1104, %f1108;
+	add.f32 	%f1110, %f1107, %f1109;
+	mul.rn.f32 	%f1112, %f1083, %f1914;
+	mul.rn.f32 	%f1114, %f1083, %f1915;
+	add.f32 	%f1115, %f1112, %f1108;
+	sub.f32 	%f1116, %f1112, %f1115;
+	add.f32 	%f1117, %f1108, %f1116;
+	add.f32 	%f1118, %f1110, %f1117;
+	add.f32 	%f1119, %f1114, %f1118;
+	add.f32 	%f1120, %f1115, %f1119;
+	sub.f32 	%f1121, %f1115, %f1120;
+	add.f32 	%f1122, %f1119, %f1121;
+	mul.rn.f32 	%f1123, %f1910, %f1120;
+	neg.f32 	%f1124, %f1123;
+	fma.rn.f32 	%f1125, %f1910, %f1120, %f1124;
+	fma.rn.f32 	%f1126, %f1910, %f1122, %f1125;
+	fma.rn.f32 	%f1128, %f1916, %f1120, %f1126;
+	add.rn.f32 	%f1129, %f1123, %f1128;
+	neg.f32 	%f1130, %f1129;
+	add.rn.f32 	%f1131, %f1123, %f1130;
+	add.rn.f32 	%f1132, %f1131, %f1128;
+	mov.b32 	%r336, %f1129;
+	setp.eq.s32 	%p49, %r336, 1118925336;
+	add.s32 	%r337, %r336, -1;
+	mov.b32 	%f1133, %r337;
+	add.f32 	%f1134, %f1132, 0f37000000;
+	selp.f32 	%f362, %f1134, %f1132, %p49;
+	selp.f32 	%f1135, %f1133, %f1129, %p49;
+	mul.rn.f32 	%f1137, %f1135, %f1917;
+	cvt.rzi.f32.f32 	%f1138, %f1137;
+	abs.f32 	%f1139, %f1138;
+	setp.gt.f32 	%p50, %f1139, 0f42FC0000;
+	mov.b32 	%r338, %f1138;
+	and.b32  	%r339, %r338, -2147483648;
+	or.b32  	%r340, %r339, 1123811328;
+	mov.b32 	%f1140, %r340;
+	selp.f32 	%f1141, %f1140, %f1138, %p50;
+	fma.rn.f32 	%f1143, %f1141, %f1918, %f1135;
+	fma.rn.f32 	%f1145, %f1141, %f1919, %f1143;
+	mul.f32 	%f1146, %f1145, 0f3FB8AA3B;
+	add.f32 	%f1147, %f1141, 0f4B40007F;
+	mov.b32 	%r341, %f1147;
+	shl.b32 	%r342, %r341, 23;
+	mov.b32 	%f1148, %r342;
+	ex2.approx.ftz.f32 	%f1149, %f1146;
+	mul.f32 	%f363, %f1149, %f1148;
+	setp.eq.f32 	%p51, %f363, 0f7F800000;
+	mov.f32 	%f2063, 0f7F800000;
+	@%p51 bra 	$L__BB11_55;
+
+	fma.rn.f32 	%f2063, %f363, %f362, %f363;
+
+$L__BB11_55:
+	setp.lt.f32 	%p52, %f2058, 0f00000000;
+	and.pred  	%p2, %p52, %p39;
+	setp.eq.f32 	%p54, %f2058, 0f00000000;
+	@%p54 bra 	$L__BB11_59;
+	bra.uni 	$L__BB11_56;
+
+$L__BB11_59:
+	add.f32 	%f1154, %f2058, %f2058;
+	selp.f32 	%f2065, %f1154, 0f00000000, %p39;
+	bra.uni 	$L__BB11_60;
+
+$L__BB11_56:
+	mov.b32 	%r343, %f2063;
+	xor.b32  	%r344, %r343, -2147483648;
+	mov.b32 	%f1150, %r344;
+	selp.f32 	%f2065, %f1150, %f2063, %p2;
+	setp.geu.f32 	%p55, %f2058, 0f00000000;
+	@%p55 bra 	$L__BB11_60;
+
+	mov.f32 	%f1901, 0f40000000;
+	cvt.rzi.f32.f32 	%f1152, %f1901;
+	setp.eq.f32 	%p56, %f1152, 0f40000000;
+	@%p56 bra 	$L__BB11_60;
+
+	mov.f32 	%f2065, 0f7FFFFFFF;
+
+$L__BB11_60:
+	abs.f32 	%f1888, %f2058;
+	add.f32 	%f1155, %f1888, 0f40000000;
+	mov.b32 	%r10, %f1155;
+	setp.lt.s32 	%p58, %r10, 2139095040;
+	@%p58 bra 	$L__BB11_65;
+
+	abs.f32 	%f1899, %f2058;
+	setp.gtu.f32 	%p59, %f1899, 0f7F800000;
+	@%p59 bra 	$L__BB11_64;
+	bra.uni 	$L__BB11_62;
+
+$L__BB11_64:
+	add.f32 	%f2065, %f2058, 0f40000000;
+	bra.uni 	$L__BB11_65;
+
+$L__BB11_62:
+	abs.f32 	%f1900, %f2058;
+	setp.neu.f32 	%p60, %f1900, 0f7F800000;
+	@%p60 bra 	$L__BB11_65;
+
+	selp.f32 	%f2065, 0fFF800000, 0f7F800000, %p2;
+
+$L__BB11_65:
+	mov.f32 	%f1898, 0f3102E308;
+	mov.f32 	%f1897, 0fBF317218;
+	mov.f32 	%f1896, 0f3FB8AA3B;
+	mov.f32 	%f1895, 0f00000000;
+	mov.f32 	%f1894, 0f35BFBE8E;
+	mov.f32 	%f1893, 0f3F317200;
+	mov.f32 	%f1892, 0f3DAAAABD;
+	mov.f32 	%f1891, 0f3C4CAF63;
+	mov.f32 	%f1890, 0f3B18F0FE;
+	mov.f32 	%f1889, 0f40000000;
+	setp.eq.f32 	%p61, %f2058, 0f3F800000;
+	selp.f32 	%f1157, 0f3F800000, %f2065, %p61;
+	setp.eq.f32 	%p62, %f2057, 0f3F800000;
+	selp.f32 	%f1158, 0f3F800000, %f2062, %p62;
+	add.f32 	%f372, %f1158, %f1157;
+	add.f32 	%f373, %f1999, %f1999;
+	mul.f32 	%f1160, %f373, %f2057;
+	add.f32 	%f1161, %f347, %f347;
+	mul.f32 	%f1162, %f1161, %f2057;
+	sub.f32 	%f1163, %f1160, %f1162;
+	add.f32 	%f374, %f2000, %f2000;
+	fma.rn.f32 	%f1164, %f374, %f2058, %f1163;
+	add.f32 	%f1165, %f348, %f348;
+	mul.f32 	%f1166, %f1165, %f2058;
+	sub.f32 	%f375, %f1164, %f1166;
+	abs.f32 	%f376, %f1999;
+	setp.lt.f32 	%p63, %f376, 0f00800000;
+	mul.f32 	%f1167, %f376, 0f4B800000;
+	selp.f32 	%f1168, %f1167, %f376, %p63;
+	selp.f32 	%f1169, 0fC3170000, 0fC2FE0000, %p63;
+	mov.b32 	%r345, %f1168;
+	and.b32  	%r346, %r345, 8388607;
+	or.b32  	%r347, %r346, 1065353216;
+	mov.b32 	%f1170, %r347;
+	shr.u32 	%r348, %r345, 23;
+	cvt.rn.f32.u32 	%f1171, %r348;
+	add.f32 	%f1172, %f1169, %f1171;
+	setp.gt.f32 	%p64, %f1170, 0f3FB504F3;
+	mul.f32 	%f1173, %f1170, 0f3F000000;
+	add.f32 	%f1174, %f1172, 0f3F800000;
+	selp.f32 	%f1175, %f1174, %f1172, %p64;
+	selp.f32 	%f1176, %f1173, %f1170, %p64;
+	add.f32 	%f1177, %f1176, 0fBF800000;
+	add.f32 	%f1178, %f1176, 0f3F800000;
+	rcp.approx.ftz.f32 	%f1179, %f1178;
+	add.f32 	%f1180, %f1177, %f1177;
+	mul.f32 	%f1181, %f1180, %f1179;
+	mul.f32 	%f1182, %f1181, %f1181;
+	fma.rn.f32 	%f1185, %f1890, %f1182, %f1891;
+	fma.rn.f32 	%f1187, %f1185, %f1182, %f1892;
+	mul.rn.f32 	%f1188, %f1187, %f1182;
+	mul.rn.f32 	%f1189, %f1188, %f1181;
+	sub.f32 	%f1190, %f1177, %f1181;
+	add.f32 	%f1191, %f1190, %f1190;
+	neg.f32 	%f1192, %f1181;
+	fma.rn.f32 	%f1193, %f1192, %f1177, %f1191;
+	mul.rn.f32 	%f1194, %f1179, %f1193;
+	add.f32 	%f1195, %f1189, %f1181;
+	sub.f32 	%f1196, %f1181, %f1195;
+	add.f32 	%f1197, %f1189, %f1196;
+	add.f32 	%f1198, %f1194, %f1197;
+	add.f32 	%f1199, %f1195, %f1198;
+	sub.f32 	%f1200, %f1195, %f1199;
+	add.f32 	%f1201, %f1198, %f1200;
+	mul.rn.f32 	%f1203, %f1175, %f1893;
+	mul.rn.f32 	%f1205, %f1175, %f1894;
+	add.f32 	%f1206, %f1203, %f1199;
+	sub.f32 	%f1207, %f1203, %f1206;
+	add.f32 	%f1208, %f1199, %f1207;
+	add.f32 	%f1209, %f1201, %f1208;
+	add.f32 	%f1210, %f1205, %f1209;
+	add.f32 	%f1211, %f1206, %f1210;
+	sub.f32 	%f1212, %f1206, %f1211;
+	add.f32 	%f1213, %f1210, %f1212;
+	mul.rn.f32 	%f1214, %f1889, %f1211;
+	neg.f32 	%f1215, %f1214;
+	fma.rn.f32 	%f1216, %f1889, %f1211, %f1215;
+	fma.rn.f32 	%f1217, %f1889, %f1213, %f1216;
+	fma.rn.f32 	%f1219, %f1895, %f1211, %f1217;
+	add.rn.f32 	%f1220, %f1214, %f1219;
+	neg.f32 	%f1221, %f1220;
+	add.rn.f32 	%f1222, %f1214, %f1221;
+	add.rn.f32 	%f1223, %f1222, %f1219;
+	mov.b32 	%r349, %f1220;
+	setp.eq.s32 	%p65, %r349, 1118925336;
+	add.s32 	%r350, %r349, -1;
+	mov.b32 	%f1224, %r350;
+	add.f32 	%f1225, %f1223, 0f37000000;
+	selp.f32 	%f377, %f1225, %f1223, %p65;
+	selp.f32 	%f1226, %f1224, %f1220, %p65;
+	mul.rn.f32 	%f1228, %f1226, %f1896;
+	cvt.rzi.f32.f32 	%f1229, %f1228;
+	abs.f32 	%f1230, %f1229;
+	setp.gt.f32 	%p66, %f1230, 0f42FC0000;
+	mov.b32 	%r351, %f1229;
+	and.b32  	%r352, %r351, -2147483648;
+	or.b32  	%r353, %r352, 1123811328;
+	mov.b32 	%f1231, %r353;
+	selp.f32 	%f1232, %f1231, %f1229, %p66;
+	fma.rn.f32 	%f1234, %f1232, %f1897, %f1226;
+	fma.rn.f32 	%f1236, %f1232, %f1898, %f1234;
 	mul.f32 	%f1237, %f1236, 0f3FB8AA3B;
-	cvt.rzi.f32.f32	%f1238, %f1237;
-	fma.rn.f32 	%f1240, %f1238, %f2205, %f1236;
-	fma.rn.f32 	%f1242, %f1238, %f2206, %f1240;
-	mul.f32 	%f1243, %f1242, 0f3FB8AA3B;
-	ex2.approx.ftz.f32 	%f1244, %f1243;
-	add.f32 	%f1245, %f1238, 0f00000000;
-	ex2.approx.f32 	%f1246, %f1245;
-	mul.f32 	%f1247, %f1244, %f1246;
-	setp.lt.f32	%p64, %f1236, 0fC2D20000;
-	selp.f32	%f1248, 0f00000000, %f1247, %p64;
-	setp.gt.f32	%p65, %f1236, 0f42D20000;
-	selp.f32	%f2350, 0f7F800000, %f1248, %p65;
-	setp.eq.f32	%p66, %f2350, 0f7F800000;
-	@%p66 bra 	BB11_65;
-
-	fma.rn.f32 	%f2350, %f2350, %f364, %f2350;
-
-BB11_65:
-	setp.lt.f32	%p67, %f2294, 0f00000000;
-	and.pred  	%p3, %p67, %p36;
-	mov.b32 	 %r337, %f2350;
-	xor.b32  	%r338, %r337, -2147483648;
-	mov.b32 	 %f1249, %r338;
-	selp.f32	%f2352, %f1249, %f2350, %p3;
-	setp.eq.f32	%p69, %f2294, 0f00000000;
-	@%p69 bra 	BB11_68;
-	bra.uni 	BB11_66;
-
-BB11_68:
-	add.f32 	%f2197, %f2294, %f2294;
-	selp.f32	%f2352, %f2197, 0f00000000, %p36;
-	bra.uni 	BB11_69;
-
-BB11_66:
-	setp.geu.f32	%p70, %f2294, 0f00000000;
-	@%p70 bra 	BB11_69;
-
-	cvt.rzi.f32.f32	%f1251, %f1025;
-	setp.neu.f32	%p71, %f1251, 0f40000000;
-	selp.f32	%f2352, 0f7FFFFFFF, %f2352, %p71;
-
-BB11_69:
-	abs.f32 	%f2185, %f2294;
-	add.f32 	%f1253, %f2185, 0f40000000;
-	mov.b32 	 %r10, %f1253;
-	setp.lt.s32	%p73, %r10, 2139095040;
-	@%p73 bra 	BB11_74;
-
-	abs.f32 	%f2195, %f2294;
-	setp.gtu.f32	%p74, %f2195, 0f7F800000;
-	@%p74 bra 	BB11_73;
-	bra.uni 	BB11_71;
-
-BB11_73:
-	add.f32 	%f2352, %f2294, 0f40000000;
-	bra.uni 	BB11_74;
-
-BB11_71:
-	abs.f32 	%f2196, %f2294;
-	setp.neu.f32	%p75, %f2196, 0f7F800000;
-	@%p75 bra 	BB11_74;
-
-	selp.f32	%f2352, 0fFF800000, 0f7F800000, %p3;
-
-BB11_74:
-	add.f32 	%f2194, %f2294, %f2294;
-	mov.f32 	%f2193, 0fB5BFBE8E;
-	mov.f32 	%f2192, 0fBF317200;
-	mov.f32 	%f2191, 0f00000000;
-	mov.f32 	%f2190, 0f35BFBE8E;
-	mov.f32 	%f2189, 0f3F317200;
-	mov.f32 	%f2188, 0f3DAAAABD;
-	mov.f32 	%f2187, 0f3C4CAF63;
-	mov.f32 	%f2186, 0f3B18F0FE;
-	setp.eq.f32	%p76, %f2294, 0f3F800000;
-	selp.f32	%f1256, 0f3F800000, %f2352, %p76;
-	mul.f32 	%f375, %f1020, %f2194;
-	sub.f32 	%f376, %f1256, %f375;
-	abs.f32 	%f377, %f1020;
-	setp.lt.f32	%p77, %f377, 0f00800000;
-	mul.f32 	%f1257, %f377, 0f4B800000;
-	selp.f32	%f1258, 0fC3170000, 0fC2FE0000, %p77;
-	selp.f32	%f1259, %f1257, %f377, %p77;
-	mov.b32 	 %r339, %f1259;
-	and.b32  	%r340, %r339, 8388607;
-	or.b32  	%r341, %r340, 1065353216;
-	mov.b32 	 %f1260, %r341;
-	shr.u32 	%r342, %r339, 23;
-	cvt.rn.f32.u32	%f1261, %r342;
-	add.f32 	%f1262, %f1258, %f1261;
-	setp.gt.f32	%p78, %f1260, 0f3FB504F3;
-	mul.f32 	%f1263, %f1260, 0f3F000000;
-	add.f32 	%f1264, %f1262, 0f3F800000;
-	selp.f32	%f1265, %f1263, %f1260, %p78;
-	selp.f32	%f1266, %f1264, %f1262, %p78;
-	add.f32 	%f378, %f1265, 0fBF800000;
-	add.f32 	%f1255, %f1265, 0f3F800000;
-	// inline asm
-	rcp.approx.ftz.f32 %f1254,%f1255;
-	// inline asm
-	add.f32 	%f380, %f378, %f378;
-	mul.f32 	%f1267, %f1254, %f380;
-	mul.f32 	%f1268, %f1267, %f1267;
-	fma.rn.f32 	%f1271, %f2186, %f1268, %f2187;
-	fma.rn.f32 	%f1273, %f1271, %f1268, %f2188;
-	mul.rn.f32 	%f1274, %f1273, %f1268;
-	mul.rn.f32 	%f1275, %f1274, %f1267;
-	sub.f32 	%f1276, %f378, %f1267;
-	neg.f32 	%f1277, %f1267;
-	add.f32 	%f1278, %f1276, %f1276;
-	fma.rn.f32 	%f1279, %f1277, %f378, %f1278;
-	mul.rn.f32 	%f1280, %f1254, %f1279;
-	add.f32 	%f1281, %f1275, %f1267;
-	sub.f32 	%f1282, %f1267, %f1281;
-	add.f32 	%f1283, %f1275, %f1282;
-	add.f32 	%f1284, %f1280, %f1283;
-	add.f32 	%f1285, %f1281, %f1284;
-	sub.f32 	%f1286, %f1281, %f1285;
-	add.f32 	%f1287, %f1284, %f1286;
-	mul.rn.f32 	%f381, %f1266, %f2189;
-	mul.rn.f32 	%f382, %f1266, %f2190;
-	add.f32 	%f1290, %f381, %f1285;
-	sub.f32 	%f1291, %f381, %f1290;
-	add.f32 	%f1292, %f1285, %f1291;
-	add.f32 	%f1293, %f1287, %f1292;
-	add.f32 	%f1294, %f382, %f1293;
-	add.f32 	%f1295, %f1290, %f1294;
-	sub.f32 	%f1296, %f1290, %f1295;
-	add.f32 	%f1297, %f1294, %f1296;
-	mul.rn.f32 	%f1299, %f1025, %f1295;
-	neg.f32 	%f1300, %f1299;
-	fma.rn.f32 	%f1301, %f1025, %f1295, %f1300;
-	fma.rn.f32 	%f1302, %f1025, %f1297, %f1301;
-	fma.rn.f32 	%f1304, %f2191, %f1295, %f1302;
-	add.rn.f32 	%f1305, %f1299, %f1304;
-	neg.f32 	%f1306, %f1305;
-	add.rn.f32 	%f1307, %f1299, %f1306;
-	add.rn.f32 	%f1308, %f1307, %f1304;
-	mov.b32 	 %r343, %f1305;
-	setp.eq.s32	%p79, %r343, 1118925336;
-	add.s32 	%r344, %r343, -1;
-	mov.b32 	 %f1309, %r344;
-	add.f32 	%f1310, %f1308, 0f37000000;
-	selp.f32	%f1311, %f1309, %f1305, %p79;
-	selp.f32	%f383, %f1310, %f1308, %p79;
-	mul.f32 	%f1312, %f1311, 0f3FB8AA3B;
-	cvt.rzi.f32.f32	%f1313, %f1312;
-	fma.rn.f32 	%f1315, %f1313, %f2192, %f1311;
-	fma.rn.f32 	%f1317, %f1313, %f2193, %f1315;
-	mul.f32 	%f1318, %f1317, 0f3FB8AA3B;
-	ex2.approx.ftz.f32 	%f1319, %f1318;
-	add.f32 	%f1320, %f1313, 0f00000000;
-	ex2.approx.f32 	%f1321, %f1320;
-	mul.f32 	%f1322, %f1319, %f1321;
-	setp.lt.f32	%p80, %f1311, 0fC2D20000;
-	selp.f32	%f1323, 0f00000000, %f1322, %p80;
-	setp.gt.f32	%p81, %f1311, 0f42D20000;
-	selp.f32	%f2353, 0f7F800000, %f1323, %p81;
-	setp.eq.f32	%p82, %f2353, 0f7F800000;
-	@%p82 bra 	BB11_76;
-
-	fma.rn.f32 	%f2353, %f2353, %f383, %f2353;
-
-BB11_76:
-	setp.lt.f32	%p83, %f1020, 0f00000000;
-	and.pred  	%p4, %p83, %p36;
-	mov.b32 	 %r345, %f2353;
-	xor.b32  	%r346, %r345, -2147483648;
-	mov.b32 	 %f1324, %r346;
-	selp.f32	%f2355, %f1324, %f2353, %p4;
-	setp.eq.f32	%p85, %f1020, 0f00000000;
-	@%p85 bra 	BB11_79;
-	bra.uni 	BB11_77;
-
-BB11_79:
-	add.f32 	%f2184, %f1020, %f1020;
-	selp.f32	%f2355, %f2184, 0f00000000, %p36;
-	bra.uni 	BB11_80;
-
-BB11_77:
-	setp.geu.f32	%p86, %f1020, 0f00000000;
-	@%p86 bra 	BB11_80;
-
-	cvt.rzi.f32.f32	%f1326, %f1025;
-	setp.neu.f32	%p87, %f1326, 0f40000000;
-	selp.f32	%f2355, 0f7FFFFFFF, %f2355, %p87;
-
-BB11_80:
-	abs.f32 	%f2173, %f1020;
-	add.f32 	%f1328, %f2173, 0f40000000;
-	mov.b32 	 %r11, %f1328;
-	setp.lt.s32	%p89, %r11, 2139095040;
-	@%p89 bra 	BB11_85;
-
-	abs.f32 	%f2182, %f1020;
-	setp.gtu.f32	%p90, %f2182, 0f7F800000;
-	@%p90 bra 	BB11_84;
-	bra.uni 	BB11_82;
-
-BB11_84:
-	add.f32 	%f2355, %f1020, 0f40000000;
-	bra.uni 	BB11_85;
-
-BB11_82:
-	abs.f32 	%f2183, %f1020;
-	setp.neu.f32	%p91, %f2183, 0f7F800000;
-	@%p91 bra 	BB11_85;
-
-	selp.f32	%f2355, 0fFF800000, 0f7F800000, %p4;
-
-BB11_85:
-	mov.f32 	%f2181, 0fB5BFBE8E;
-	mov.f32 	%f2180, 0fBF317200;
-	mov.f32 	%f2179, 0f00000000;
-	mov.f32 	%f2178, 0f35BFBE8E;
-	mov.f32 	%f2177, 0f3F317200;
-	mov.f32 	%f2176, 0f3DAAAABD;
-	mov.f32 	%f2175, 0f3C4CAF63;
-	mov.f32 	%f2174, 0f3B18F0FE;
-	setp.eq.f32	%p92, %f1020, 0f3F800000;
-	selp.f32	%f1331, 0f3F800000, %f2355, %p92;
-	add.f32 	%f394, %f376, %f1331;
-	abs.f32 	%f395, %f2293;
-	setp.lt.f32	%p93, %f395, 0f00800000;
-	mul.f32 	%f1332, %f395, 0f4B800000;
-	selp.f32	%f1333, 0fC3170000, 0fC2FE0000, %p93;
-	selp.f32	%f1334, %f1332, %f395, %p93;
-	mov.b32 	 %r347, %f1334;
-	and.b32  	%r348, %r347, 8388607;
-	or.b32  	%r349, %r348, 1065353216;
-	mov.b32 	 %f1335, %r349;
-	shr.u32 	%r350, %r347, 23;
-	cvt.rn.f32.u32	%f1336, %r350;
-	add.f32 	%f1337, %f1333, %f1336;
-	setp.gt.f32	%p94, %f1335, 0f3FB504F3;
-	mul.f32 	%f1338, %f1335, 0f3F000000;
-	add.f32 	%f1339, %f1337, 0f3F800000;
-	selp.f32	%f1340, %f1338, %f1335, %p94;
-	selp.f32	%f1341, %f1339, %f1337, %p94;
-	add.f32 	%f396, %f1340, 0fBF800000;
-	add.f32 	%f1330, %f1340, 0f3F800000;
-	// inline asm
-	rcp.approx.ftz.f32 %f1329,%f1330;
-	// inline asm
-	add.f32 	%f398, %f396, %f396;
-	mul.f32 	%f1342, %f1329, %f398;
-	mul.f32 	%f1343, %f1342, %f1342;
-	fma.rn.f32 	%f1346, %f2174, %f1343, %f2175;
-	fma.rn.f32 	%f1348, %f1346, %f1343, %f2176;
-	mul.rn.f32 	%f1349, %f1348, %f1343;
-	mul.rn.f32 	%f1350, %f1349, %f1342;
-	sub.f32 	%f1351, %f396, %f1342;
-	neg.f32 	%f1352, %f1342;
-	add.f32 	%f1353, %f1351, %f1351;
-	fma.rn.f32 	%f1354, %f1352, %f396, %f1353;
-	mul.rn.f32 	%f1355, %f1329, %f1354;
-	add.f32 	%f1356, %f1350, %f1342;
-	sub.f32 	%f1357, %f1342, %f1356;
-	add.f32 	%f1358, %f1350, %f1357;
-	add.f32 	%f1359, %f1355, %f1358;
-	add.f32 	%f1360, %f1356, %f1359;
-	sub.f32 	%f1361, %f1356, %f1360;
-	add.f32 	%f1362, %f1359, %f1361;
-	mul.rn.f32 	%f399, %f1341, %f2177;
-	mul.rn.f32 	%f400, %f1341, %f2178;
-	add.f32 	%f1365, %f399, %f1360;
-	sub.f32 	%f1366, %f399, %f1365;
-	add.f32 	%f1367, %f1360, %f1366;
-	add.f32 	%f1368, %f1362, %f1367;
-	add.f32 	%f1369, %f400, %f1368;
-	add.f32 	%f1370, %f1365, %f1369;
-	sub.f32 	%f1371, %f1365, %f1370;
-	add.f32 	%f1372, %f1369, %f1371;
-	mul.rn.f32 	%f1374, %f1025, %f1370;
-	neg.f32 	%f1375, %f1374;
-	fma.rn.f32 	%f1376, %f1025, %f1370, %f1375;
-	fma.rn.f32 	%f1377, %f1025, %f1372, %f1376;
-	fma.rn.f32 	%f1379, %f2179, %f1370, %f1377;
-	add.rn.f32 	%f1380, %f1374, %f1379;
-	neg.f32 	%f1381, %f1380;
-	add.rn.f32 	%f1382, %f1374, %f1381;
-	add.rn.f32 	%f1383, %f1382, %f1379;
-	mov.b32 	 %r351, %f1380;
-	setp.eq.s32	%p95, %r351, 1118925336;
-	add.s32 	%r352, %r351, -1;
-	mov.b32 	 %f1384, %r352;
-	add.f32 	%f1385, %f1383, 0f37000000;
-	selp.f32	%f1386, %f1384, %f1380, %p95;
-	selp.f32	%f401, %f1385, %f1383, %p95;
-	mul.f32 	%f1387, %f1386, 0f3FB8AA3B;
-	cvt.rzi.f32.f32	%f1388, %f1387;
-	fma.rn.f32 	%f1390, %f1388, %f2180, %f1386;
-	fma.rn.f32 	%f1392, %f1388, %f2181, %f1390;
-	mul.f32 	%f1393, %f1392, 0f3FB8AA3B;
-	ex2.approx.ftz.f32 	%f1394, %f1393;
-	add.f32 	%f1395, %f1388, 0f00000000;
-	ex2.approx.f32 	%f1396, %f1395;
-	mul.f32 	%f1397, %f1394, %f1396;
-	setp.lt.f32	%p96, %f1386, 0fC2D20000;
-	selp.f32	%f1398, 0f00000000, %f1397, %p96;
-	setp.gt.f32	%p97, %f1386, 0f42D20000;
-	selp.f32	%f2356, 0f7F800000, %f1398, %p97;
-	setp.eq.f32	%p98, %f2356, 0f7F800000;
-	@%p98 bra 	BB11_87;
-
-	fma.rn.f32 	%f2356, %f2356, %f401, %f2356;
-
-BB11_87:
-	setp.lt.f32	%p99, %f2293, 0f00000000;
-	and.pred  	%p5, %p99, %p36;
-	mov.b32 	 %r353, %f2356;
-	xor.b32  	%r354, %r353, -2147483648;
-	mov.b32 	 %f1399, %r354;
-	selp.f32	%f2358, %f1399, %f2356, %p5;
-	setp.eq.f32	%p101, %f2293, 0f00000000;
-	@%p101 bra 	BB11_90;
-	bra.uni 	BB11_88;
-
-BB11_90:
-	add.f32 	%f2169, %f2293, %f2293;
-	selp.f32	%f2358, %f2169, 0f00000000, %p36;
-	bra.uni 	BB11_91;
-
-BB11_88:
-	setp.geu.f32	%p102, %f2293, 0f00000000;
-	@%p102 bra 	BB11_91;
-
-	cvt.rzi.f32.f32	%f1401, %f1025;
-	setp.neu.f32	%p103, %f1401, 0f40000000;
-	selp.f32	%f2358, 0f7FFFFFFF, %f2358, %p103;
-
-BB11_91:
-	abs.f32 	%f2148, %f2293;
-	add.f32 	%f1403, %f2148, 0f40000000;
-	mov.b32 	 %r12, %f1403;
-	setp.lt.s32	%p105, %r12, 2139095040;
-	@%p105 bra 	BB11_96;
-
-	abs.f32 	%f2167, %f2293;
-	setp.gtu.f32	%p106, %f2167, 0f7F800000;
-	@%p106 bra 	BB11_95;
-	bra.uni 	BB11_93;
-
-BB11_95:
-	add.f32 	%f2358, %f2293, 0f40000000;
-	bra.uni 	BB11_96;
-
-BB11_93:
-	abs.f32 	%f2168, %f2293;
-	setp.neu.f32	%p107, %f2168, 0f7F800000;
-	@%p107 bra 	BB11_96;
-
-	selp.f32	%f2358, 0fFF800000, 0f7F800000, %p5;
-
-BB11_96:
-	add.f32 	%f2157, %f2293, %f2293;
-	mov.f32 	%f2156, 0fB5BFBE8E;
-	mov.f32 	%f2155, 0fBF317200;
-	mov.f32 	%f2154, 0f00000000;
-	mov.f32 	%f2153, 0f35BFBE8E;
-	mov.f32 	%f2152, 0f3F317200;
-	mov.f32 	%f2151, 0f3DAAAABD;
-	mov.f32 	%f2150, 0f3C4CAF63;
-	mov.f32 	%f2149, 0f3B18F0FE;
-	setp.eq.f32	%p108, %f2293, 0f3F800000;
-	selp.f32	%f1406, 0f3F800000, %f2358, %p108;
-	add.f32 	%f1407, %f394, %f1406;
-	mul.f32 	%f412, %f1021, %f2157;
-	sub.f32 	%f413, %f1407, %f412;
-	abs.f32 	%f414, %f1021;
-	setp.lt.f32	%p109, %f414, 0f00800000;
-	mul.f32 	%f1408, %f414, 0f4B800000;
-	selp.f32	%f1409, 0fC3170000, 0fC2FE0000, %p109;
-	selp.f32	%f1410, %f1408, %f414, %p109;
-	mov.b32 	 %r355, %f1410;
-	and.b32  	%r356, %r355, 8388607;
-	or.b32  	%r357, %r356, 1065353216;
-	mov.b32 	 %f1411, %r357;
-	shr.u32 	%r358, %r355, 23;
-	cvt.rn.f32.u32	%f1412, %r358;
-	add.f32 	%f1413, %f1409, %f1412;
-	setp.gt.f32	%p110, %f1411, 0f3FB504F3;
-	mul.f32 	%f1414, %f1411, 0f3F000000;
-	add.f32 	%f1415, %f1413, 0f3F800000;
-	selp.f32	%f1416, %f1414, %f1411, %p110;
-	selp.f32	%f1417, %f1415, %f1413, %p110;
-	add.f32 	%f415, %f1416, 0fBF800000;
-	add.f32 	%f1405, %f1416, 0f3F800000;
-	// inline asm
-	rcp.approx.ftz.f32 %f1404,%f1405;
-	// inline asm
-	add.f32 	%f417, %f415, %f415;
-	mul.f32 	%f1418, %f1404, %f417;
-	mul.f32 	%f1419, %f1418, %f1418;
-	fma.rn.f32 	%f1422, %f2149, %f1419, %f2150;
-	fma.rn.f32 	%f1424, %f1422, %f1419, %f2151;
-	mul.rn.f32 	%f1425, %f1424, %f1419;
-	mul.rn.f32 	%f1426, %f1425, %f1418;
-	sub.f32 	%f1427, %f415, %f1418;
-	neg.f32 	%f1428, %f1418;
-	add.f32 	%f1429, %f1427, %f1427;
-	fma.rn.f32 	%f1430, %f1428, %f415, %f1429;
-	mul.rn.f32 	%f1431, %f1404, %f1430;
-	add.f32 	%f1432, %f1426, %f1418;
-	sub.f32 	%f1433, %f1418, %f1432;
-	add.f32 	%f1434, %f1426, %f1433;
-	add.f32 	%f1435, %f1431, %f1434;
-	add.f32 	%f1436, %f1432, %f1435;
-	sub.f32 	%f1437, %f1432, %f1436;
-	add.f32 	%f1438, %f1435, %f1437;
-	mul.rn.f32 	%f418, %f1417, %f2152;
-	mul.rn.f32 	%f419, %f1417, %f2153;
-	add.f32 	%f1441, %f418, %f1436;
-	sub.f32 	%f1442, %f418, %f1441;
-	add.f32 	%f1443, %f1436, %f1442;
-	add.f32 	%f1444, %f1438, %f1443;
-	add.f32 	%f1445, %f419, %f1444;
-	add.f32 	%f1446, %f1441, %f1445;
-	sub.f32 	%f1447, %f1441, %f1446;
-	add.f32 	%f1448, %f1445, %f1447;
-	mul.rn.f32 	%f1450, %f1025, %f1446;
-	neg.f32 	%f1451, %f1450;
-	fma.rn.f32 	%f1452, %f1025, %f1446, %f1451;
-	fma.rn.f32 	%f1453, %f1025, %f1448, %f1452;
-	fma.rn.f32 	%f1455, %f2154, %f1446, %f1453;
-	add.rn.f32 	%f1456, %f1450, %f1455;
-	neg.f32 	%f1457, %f1456;
-	add.rn.f32 	%f1458, %f1450, %f1457;
-	add.rn.f32 	%f1459, %f1458, %f1455;
-	mov.b32 	 %r359, %f1456;
-	setp.eq.s32	%p111, %r359, 1118925336;
-	add.s32 	%r360, %r359, -1;
-	mov.b32 	 %f1460, %r360;
-	add.f32 	%f1461, %f1459, 0f37000000;
-	selp.f32	%f1462, %f1460, %f1456, %p111;
-	selp.f32	%f420, %f1461, %f1459, %p111;
-	mul.f32 	%f1463, %f1462, 0f3FB8AA3B;
-	cvt.rzi.f32.f32	%f1464, %f1463;
-	fma.rn.f32 	%f1466, %f1464, %f2155, %f1462;
-	fma.rn.f32 	%f1468, %f1464, %f2156, %f1466;
-	mul.f32 	%f1469, %f1468, 0f3FB8AA3B;
-	ex2.approx.ftz.f32 	%f1470, %f1469;
-	add.f32 	%f1471, %f1464, 0f00000000;
-	ex2.approx.f32 	%f1472, %f1471;
-	mul.f32 	%f1473, %f1470, %f1472;
-	setp.lt.f32	%p112, %f1462, 0fC2D20000;
-	selp.f32	%f1474, 0f00000000, %f1473, %p112;
-	setp.gt.f32	%p113, %f1462, 0f42D20000;
-	selp.f32	%f2359, 0f7F800000, %f1474, %p113;
-	setp.eq.f32	%p114, %f2359, 0f7F800000;
-	@%p114 bra 	BB11_98;
-
-	fma.rn.f32 	%f2359, %f2359, %f420, %f2359;
-
-BB11_98:
-	setp.lt.f32	%p115, %f1021, 0f00000000;
-	and.pred  	%p6, %p115, %p36;
-	mov.b32 	 %r361, %f2359;
-	xor.b32  	%r362, %r361, -2147483648;
-	mov.b32 	 %f1475, %r362;
-	selp.f32	%f2361, %f1475, %f2359, %p6;
-	setp.eq.f32	%p117, %f1021, 0f00000000;
-	@%p117 bra 	BB11_101;
-	bra.uni 	BB11_99;
-
-BB11_101:
-	add.f32 	%f2166, %f1021, %f1021;
-	selp.f32	%f2361, %f2166, 0f00000000, %p36;
-	bra.uni 	BB11_102;
-
-BB11_99:
-	setp.geu.f32	%p118, %f1021, 0f00000000;
-	@%p118 bra 	BB11_102;
-
-	cvt.rzi.f32.f32	%f1477, %f1025;
-	setp.neu.f32	%p119, %f1477, 0f40000000;
-	selp.f32	%f2361, 0f7FFFFFFF, %f2361, %p119;
-
-BB11_102:
-	abs.f32 	%f2170, %f1021;
-	add.f32 	%f1479, %f2170, 0f40000000;
-	mov.b32 	 %r13, %f1479;
-	setp.lt.s32	%p121, %r13, 2139095040;
-	@%p121 bra 	BB11_107;
-
-	abs.f32 	%f2171, %f1021;
-	setp.gtu.f32	%p122, %f2171, 0f7F800000;
-	@%p122 bra 	BB11_106;
-	bra.uni 	BB11_104;
-
-BB11_106:
-	add.f32 	%f2361, %f1021, 0f40000000;
-	bra.uni 	BB11_107;
-
-BB11_104:
-	abs.f32 	%f2172, %f1021;
-	setp.neu.f32	%p123, %f2172, 0f7F800000;
-	@%p123 bra 	BB11_107;
-
-	selp.f32	%f2361, 0fFF800000, 0f7F800000, %p6;
-
-BB11_107:
-	mov.f32 	%f2165, 0fB5BFBE8E;
-	mov.f32 	%f2164, 0fBF317200;
-	mov.f32 	%f2163, 0f00000000;
-	mov.f32 	%f2162, 0f35BFBE8E;
-	mov.f32 	%f2161, 0f3F317200;
-	mov.f32 	%f2160, 0f3DAAAABD;
-	mov.f32 	%f2159, 0f3C4CAF63;
-	mov.f32 	%f2158, 0f3B18F0FE;
-	setp.eq.f32	%p124, %f1021, 0f3F800000;
-	selp.f32	%f1482, 0f3F800000, %f2361, %p124;
-	add.f32 	%f431, %f413, %f1482;
-	abs.f32 	%f432, %f315;
-	setp.lt.f32	%p125, %f432, 0f00800000;
-	mul.f32 	%f1483, %f432, 0f4B800000;
-	selp.f32	%f1484, 0fC3170000, 0fC2FE0000, %p125;
-	selp.f32	%f1485, %f1483, %f432, %p125;
-	mov.b32 	 %r363, %f1485;
-	and.b32  	%r364, %r363, 8388607;
-	or.b32  	%r365, %r364, 1065353216;
-	mov.b32 	 %f1486, %r365;
-	shr.u32 	%r366, %r363, 23;
-	cvt.rn.f32.u32	%f1487, %r366;
-	add.f32 	%f1488, %f1484, %f1487;
-	setp.gt.f32	%p126, %f1486, 0f3FB504F3;
-	mul.f32 	%f1489, %f1486, 0f3F000000;
-	add.f32 	%f1490, %f1488, 0f3F800000;
-	selp.f32	%f1491, %f1489, %f1486, %p126;
-	selp.f32	%f1492, %f1490, %f1488, %p126;
-	add.f32 	%f1493, %f1491, 0fBF800000;
-	add.f32 	%f1481, %f1491, 0f3F800000;
-	// inline asm
-	rcp.approx.ftz.f32 %f1480,%f1481;
-	// inline asm
-	add.f32 	%f1494, %f1493, %f1493;
-	mul.f32 	%f1495, %f1480, %f1494;
-	mul.f32 	%f1496, %f1495, %f1495;
-	fma.rn.f32 	%f1499, %f2158, %f1496, %f2159;
-	fma.rn.f32 	%f1501, %f1499, %f1496, %f2160;
-	mul.rn.f32 	%f1502, %f1501, %f1496;
-	mul.rn.f32 	%f1503, %f1502, %f1495;
-	sub.f32 	%f1504, %f1493, %f1495;
-	neg.f32 	%f1505, %f1495;
-	add.f32 	%f1506, %f1504, %f1504;
-	fma.rn.f32 	%f1507, %f1505, %f1493, %f1506;
-	mul.rn.f32 	%f1508, %f1480, %f1507;
-	add.f32 	%f1509, %f1503, %f1495;
-	sub.f32 	%f1510, %f1495, %f1509;
-	add.f32 	%f1511, %f1503, %f1510;
-	add.f32 	%f1512, %f1508, %f1511;
-	add.f32 	%f1513, %f1509, %f1512;
-	sub.f32 	%f1514, %f1509, %f1513;
-	add.f32 	%f1515, %f1512, %f1514;
-	mul.rn.f32 	%f1517, %f1492, %f2161;
-	mul.rn.f32 	%f1519, %f1492, %f2162;
-	add.f32 	%f1520, %f1517, %f1513;
-	sub.f32 	%f1521, %f1517, %f1520;
-	add.f32 	%f1522, %f1513, %f1521;
-	add.f32 	%f1523, %f1515, %f1522;
-	add.f32 	%f1524, %f1519, %f1523;
-	add.f32 	%f1525, %f1520, %f1524;
-	sub.f32 	%f1526, %f1520, %f1525;
-	add.f32 	%f1527, %f1524, %f1526;
-	mul.rn.f32 	%f1529, %f1025, %f1525;
-	neg.f32 	%f1530, %f1529;
-	fma.rn.f32 	%f1531, %f1025, %f1525, %f1530;
-	fma.rn.f32 	%f1532, %f1025, %f1527, %f1531;
-	fma.rn.f32 	%f1534, %f2163, %f1525, %f1532;
-	add.rn.f32 	%f1535, %f1529, %f1534;
-	neg.f32 	%f1536, %f1535;
-	add.rn.f32 	%f1537, %f1529, %f1536;
-	add.rn.f32 	%f1538, %f1537, %f1534;
-	mov.b32 	 %r367, %f1535;
-	setp.eq.s32	%p127, %r367, 1118925336;
-	add.s32 	%r368, %r367, -1;
-	mov.b32 	 %f1539, %r368;
-	add.f32 	%f1540, %f1538, 0f37000000;
-	selp.f32	%f1541, %f1539, %f1535, %p127;
-	selp.f32	%f433, %f1540, %f1538, %p127;
-	mul.f32 	%f1542, %f1541, 0f3FB8AA3B;
-	cvt.rzi.f32.f32	%f1543, %f1542;
-	fma.rn.f32 	%f1545, %f1543, %f2164, %f1541;
-	fma.rn.f32 	%f1547, %f1543, %f2165, %f1545;
-	mul.f32 	%f1548, %f1547, 0f3FB8AA3B;
-	ex2.approx.ftz.f32 	%f1549, %f1548;
-	add.f32 	%f1550, %f1543, 0f00000000;
-	ex2.approx.f32 	%f1551, %f1550;
-	mul.f32 	%f1552, %f1549, %f1551;
-	setp.lt.f32	%p128, %f1541, 0fC2D20000;
-	selp.f32	%f1553, 0f00000000, %f1552, %p128;
-	setp.gt.f32	%p129, %f1541, 0f42D20000;
-	selp.f32	%f2362, 0f7F800000, %f1553, %p129;
-	setp.eq.f32	%p130, %f2362, 0f7F800000;
-	@%p130 bra 	BB11_109;
-
-	fma.rn.f32 	%f2362, %f2362, %f433, %f2362;
-
-BB11_109:
-	setp.lt.f32	%p131, %f315, 0f00000000;
-	and.pred  	%p7, %p131, %p36;
-	mov.b32 	 %r369, %f2362;
+	add.f32 	%f1238, %f1232, 0f4B40007F;
+	mov.b32 	%r354, %f1238;
+	shl.b32 	%r355, %r354, 23;
+	mov.b32 	%f1239, %r355;
+	ex2.approx.ftz.f32 	%f1240, %f1237;
+	mul.f32 	%f378, %f1240, %f1239;
+	setp.eq.f32 	%p67, %f378, 0f7F800000;
+	mov.f32 	%f2066, 0f7F800000;
+	@%p67 bra 	$L__BB11_67;
+
+	fma.rn.f32 	%f2066, %f378, %f377, %f378;
+
+$L__BB11_67:
+	setp.lt.f32 	%p68, %f1999, 0f00000000;
+	and.pred  	%p3, %p68, %p39;
+	setp.eq.f32 	%p70, %f1999, 0f00000000;
+	@%p70 bra 	$L__BB11_71;
+	bra.uni 	$L__BB11_68;
+
+$L__BB11_71:
+	add.f32 	%f1887, %f1999, %f1999;
+	selp.f32 	%f2068, %f1887, 0f00000000, %p39;
+	bra.uni 	$L__BB11_72;
+
+$L__BB11_68:
+	mov.b32 	%r356, %f2066;
+	xor.b32  	%r357, %r356, -2147483648;
+	mov.b32 	%f1241, %r357;
+	selp.f32 	%f2068, %f1241, %f2066, %p3;
+	setp.geu.f32 	%p71, %f1999, 0f00000000;
+	@%p71 bra 	$L__BB11_72;
+
+	mov.f32 	%f1886, 0f40000000;
+	cvt.rzi.f32.f32 	%f1243, %f1886;
+	setp.eq.f32 	%p72, %f1243, 0f40000000;
+	@%p72 bra 	$L__BB11_72;
+
+	mov.f32 	%f2068, 0f7FFFFFFF;
+
+$L__BB11_72:
+	abs.f32 	%f1872, %f1999;
+	add.f32 	%f1246, %f1872, 0f40000000;
+	mov.b32 	%r11, %f1246;
+	setp.lt.s32 	%p74, %r11, 2139095040;
+	@%p74 bra 	$L__BB11_77;
+
+	abs.f32 	%f1884, %f1999;
+	setp.gtu.f32 	%p75, %f1884, 0f7F800000;
+	@%p75 bra 	$L__BB11_76;
+	bra.uni 	$L__BB11_74;
+
+$L__BB11_76:
+	add.f32 	%f2068, %f1999, 0f40000000;
+	bra.uni 	$L__BB11_77;
+
+$L__BB11_74:
+	abs.f32 	%f1885, %f1999;
+	setp.neu.f32 	%p76, %f1885, 0f7F800000;
+	@%p76 bra 	$L__BB11_77;
+
+	selp.f32 	%f2068, 0fFF800000, 0f7F800000, %p3;
+
+$L__BB11_77:
+	mov.f32 	%f1882, 0f3102E308;
+	mov.f32 	%f1881, 0fBF317218;
+	mov.f32 	%f1880, 0f3FB8AA3B;
+	mov.f32 	%f1879, 0f00000000;
+	mov.f32 	%f1878, 0f35BFBE8E;
+	mov.f32 	%f1877, 0f3F317200;
+	mov.f32 	%f1876, 0f3DAAAABD;
+	mov.f32 	%f1875, 0f3C4CAF63;
+	mov.f32 	%f1874, 0f3B18F0FE;
+	mov.f32 	%f1873, 0f40000000;
+	abs.f32 	%f388, %f347;
+	setp.lt.f32 	%p77, %f388, 0f00800000;
+	mul.f32 	%f1248, %f388, 0f4B800000;
+	selp.f32 	%f1249, %f1248, %f388, %p77;
+	selp.f32 	%f1250, 0fC3170000, 0fC2FE0000, %p77;
+	mov.b32 	%r358, %f1249;
+	and.b32  	%r359, %r358, 8388607;
+	or.b32  	%r360, %r359, 1065353216;
+	mov.b32 	%f1251, %r360;
+	shr.u32 	%r361, %r358, 23;
+	cvt.rn.f32.u32 	%f1252, %r361;
+	add.f32 	%f1253, %f1250, %f1252;
+	setp.gt.f32 	%p78, %f1251, 0f3FB504F3;
+	mul.f32 	%f1254, %f1251, 0f3F000000;
+	add.f32 	%f1255, %f1253, 0f3F800000;
+	selp.f32 	%f1256, %f1255, %f1253, %p78;
+	selp.f32 	%f1257, %f1254, %f1251, %p78;
+	add.f32 	%f1258, %f1257, 0fBF800000;
+	add.f32 	%f1259, %f1257, 0f3F800000;
+	rcp.approx.ftz.f32 	%f1260, %f1259;
+	add.f32 	%f1261, %f1258, %f1258;
+	mul.f32 	%f1263, %f1261, %f1260;
+	mul.f32 	%f1264, %f1263, %f1263;
+	fma.rn.f32 	%f1267, %f1874, %f1264, %f1875;
+	fma.rn.f32 	%f1269, %f1267, %f1264, %f1876;
+	mul.rn.f32 	%f1270, %f1269, %f1264;
+	mul.rn.f32 	%f1271, %f1270, %f1263;
+	sub.f32 	%f1272, %f1258, %f1263;
+	add.f32 	%f1273, %f1272, %f1272;
+	neg.f32 	%f1274, %f1263;
+	fma.rn.f32 	%f1275, %f1274, %f1258, %f1273;
+	mul.rn.f32 	%f1276, %f1260, %f1275;
+	add.f32 	%f1277, %f1271, %f1263;
+	sub.f32 	%f1278, %f1263, %f1277;
+	add.f32 	%f1279, %f1271, %f1278;
+	add.f32 	%f1280, %f1276, %f1279;
+	add.f32 	%f1281, %f1277, %f1280;
+	sub.f32 	%f1282, %f1277, %f1281;
+	add.f32 	%f1283, %f1280, %f1282;
+	mul.rn.f32 	%f1285, %f1256, %f1877;
+	mul.rn.f32 	%f1287, %f1256, %f1878;
+	add.f32 	%f1288, %f1285, %f1281;
+	sub.f32 	%f1289, %f1285, %f1288;
+	add.f32 	%f1290, %f1281, %f1289;
+	add.f32 	%f1291, %f1283, %f1290;
+	add.f32 	%f1292, %f1287, %f1291;
+	add.f32 	%f1293, %f1288, %f1292;
+	sub.f32 	%f1294, %f1288, %f1293;
+	add.f32 	%f1295, %f1292, %f1294;
+	mul.rn.f32 	%f1296, %f1873, %f1293;
+	neg.f32 	%f1297, %f1296;
+	fma.rn.f32 	%f1298, %f1873, %f1293, %f1297;
+	fma.rn.f32 	%f1299, %f1873, %f1295, %f1298;
+	fma.rn.f32 	%f1301, %f1879, %f1293, %f1299;
+	add.rn.f32 	%f1302, %f1296, %f1301;
+	neg.f32 	%f1303, %f1302;
+	add.rn.f32 	%f1304, %f1296, %f1303;
+	add.rn.f32 	%f1305, %f1304, %f1301;
+	mov.b32 	%r362, %f1302;
+	setp.eq.s32 	%p79, %r362, 1118925336;
+	add.s32 	%r363, %r362, -1;
+	mov.b32 	%f1306, %r363;
+	add.f32 	%f1307, %f1305, 0f37000000;
+	selp.f32 	%f389, %f1307, %f1305, %p79;
+	selp.f32 	%f1308, %f1306, %f1302, %p79;
+	mul.rn.f32 	%f1310, %f1308, %f1880;
+	cvt.rzi.f32.f32 	%f1311, %f1310;
+	abs.f32 	%f1312, %f1311;
+	setp.gt.f32 	%p80, %f1312, 0f42FC0000;
+	mov.b32 	%r364, %f1311;
+	and.b32  	%r365, %r364, -2147483648;
+	or.b32  	%r366, %r365, 1123811328;
+	mov.b32 	%f1313, %r366;
+	selp.f32 	%f1314, %f1313, %f1311, %p80;
+	fma.rn.f32 	%f1316, %f1314, %f1881, %f1308;
+	fma.rn.f32 	%f1318, %f1314, %f1882, %f1316;
+	mul.f32 	%f1319, %f1318, 0f3FB8AA3B;
+	add.f32 	%f1320, %f1314, 0f4B40007F;
+	mov.b32 	%r367, %f1320;
+	shl.b32 	%r368, %r367, 23;
+	mov.b32 	%f1321, %r368;
+	ex2.approx.ftz.f32 	%f1322, %f1319;
+	mul.f32 	%f390, %f1322, %f1321;
+	setp.eq.f32 	%p81, %f390, 0f7F800000;
+	mov.f32 	%f2069, 0f7F800000;
+	@%p81 bra 	$L__BB11_79;
+
+	fma.rn.f32 	%f2069, %f390, %f389, %f390;
+
+$L__BB11_79:
+	setp.lt.f32 	%p82, %f347, 0f00000000;
+	and.pred  	%p4, %p82, %p39;
+	setp.eq.f32 	%p84, %f347, 0f00000000;
+	@%p84 bra 	$L__BB11_83;
+	bra.uni 	$L__BB11_80;
+
+$L__BB11_83:
+	add.f32 	%f1871, %f347, %f347;
+	selp.f32 	%f2071, %f1871, 0f00000000, %p39;
+	bra.uni 	$L__BB11_84;
+
+$L__BB11_80:
+	mov.b32 	%r369, %f2069;
 	xor.b32  	%r370, %r369, -2147483648;
-	mov.b32 	 %f1554, %r370;
-	selp.f32	%f2364, %f1554, %f2362, %p7;
-	setp.eq.f32	%p133, %f315, 0f00000000;
-	@%p133 bra 	BB11_112;
-	bra.uni 	BB11_110;
-
-BB11_112:
-	add.f32 	%f1557, %f315, %f315;
-	selp.f32	%f2364, %f1557, 0f00000000, %p36;
-	bra.uni 	BB11_113;
-
-BB11_110:
-	setp.geu.f32	%p134, %f315, 0f00000000;
-	@%p134 bra 	BB11_113;
-
-	cvt.rzi.f32.f32	%f1556, %f1025;
-	setp.neu.f32	%p135, %f1556, 0f40000000;
-	selp.f32	%f2364, 0f7FFFFFFF, %f2364, %p135;
-
-BB11_113:
-	abs.f32 	%f2069, %f315;
-	add.f32 	%f1558, %f2069, 0f40000000;
-	mov.b32 	 %r371, %f1558;
-	setp.lt.s32	%p137, %r371, 2139095040;
-	@%p137 bra 	BB11_118;
-
-	abs.f32 	%f2146, %f315;
-	setp.gtu.f32	%p138, %f2146, 0f7F800000;
-	@%p138 bra 	BB11_117;
-	bra.uni 	BB11_115;
-
-BB11_117:
-	add.f32 	%f2364, %f315, 0f40000000;
-	bra.uni 	BB11_118;
-
-BB11_115:
-	abs.f32 	%f2147, %f315;
-	setp.neu.f32	%p139, %f2147, 0f7F800000;
-	@%p139 bra 	BB11_118;
-
-	selp.f32	%f2364, 0fFF800000, 0f7F800000, %p7;
-
-BB11_118:
-	setp.eq.f32	%p141, %f315, 0f3F800000;
-	selp.f32	%f1560, 0f3F800000, %f2364, %p141;
-	sub.f32 	%f444, %f431, %f1560;
-	setp.eq.f32	%p142, %f357, 0f00000000;
-	setp.eq.f32	%p143, %f354, 0f00000000;
-	and.pred  	%p144, %p143, %p142;
-	mov.pred 	%p332, -1;
-	@%p144 bra 	BB11_122;
-
-	neg.f32 	%f1561, %f444;
-	div.rn.f32 	%f2365, %f1561, %f357;
-	mul.f32 	%f1562, %f354, 0fC0800000;
-	mul.f32 	%f1563, %f1562, %f444;
-	fma.rn.f32 	%f446, %f357, %f357, %f1563;
-	setp.lt.f32	%p146, %f446, 0f00000000;
-	setp.neu.f32	%p147, %f354, 0f00000000;
-	and.pred  	%p148, %p146, %p147;
-	@%p148 bra 	BB11_120;
-	bra.uni 	BB11_121;
-
-BB11_120:
-	mov.f32 	%f2366, %f2365;
-	bra.uni 	BB11_122;
-
-BB11_121:
-	mov.b32 	 %r372, %f357;
-	and.b32  	%r373, %r372, -2147483648;
-	sqrt.rn.f32 	%f1564, %f446;
-	mov.b32 	 %r374, %f1564;
-	and.b32  	%r375, %r374, 2147483647;
-	or.b32  	%r376, %r375, %r373;
-	mov.b32 	 %f1565, %r376;
-	add.f32 	%f1566, %f357, %f1565;
-	mul.f32 	%f1567, %f1566, 0fBF000000;
-	div.rn.f32 	%f1568, %f1567, %f354;
-	div.rn.f32 	%f1569, %f444, %f1567;
-	min.f32 	%f1570, %f1568, %f1569;
-	max.f32 	%f1571, %f1568, %f1569;
-	selp.f32	%f2366, %f2365, %f1570, %p143;
-	selp.f32	%f2365, %f2365, %f1571, %p143;
-	mov.pred 	%p332, 0;
-
-BB11_122:
-	ld.v2.f32 	{%f1572, %f1573}, [%rd1+320];
-	mov.u16 	%rs25, 0;
-	mov.u16 	%rs24, %rs25;
-	@%p332 bra 	BB11_126;
-
-	setp.leu.f32	%p151, %f2366, %f1016;
-	setp.geu.f32	%p152, %f2366, %f1017;
-	or.pred  	%p153, %p151, %p152;
-	mov.u16 	%rs24, %rs25;
-	@%p153 bra 	BB11_126;
-
-	fma.rn.f32 	%f453, %f2366, %f2343, %f2292;
-	setp.ltu.f32	%p154, %f453, %f1572;
-	mov.u16 	%rs24, %rs25;
-	@%p154 bra 	BB11_126;
-
-	setp.lt.f32	%p155, %f453, %f1573;
-	selp.u16	%rs24, 1, 0, %p155;
-
-BB11_126:
-	@%p332 bra 	BB11_130;
-
-	setp.leu.f32	%p156, %f2365, %f1016;
-	setp.geu.f32	%p157, %f2365, %f1017;
-	or.pred  	%p158, %p156, %p157;
-	@%p158 bra 	BB11_130;
-
-	fma.rn.f32 	%f454, %f2365, %f2343, %f2292;
-	setp.ltu.f32	%p159, %f454, %f1572;
-	@%p159 bra 	BB11_130;
-
-	setp.lt.f32	%p160, %f454, %f1573;
-	selp.u16	%rs25, 1, 0, %p160;
-
-BB11_130:
-	mov.f32 	%f2075, 0fB5BFBE8E;
-	mov.f32 	%f2074, 0fBF317200;
-	mov.f32 	%f2073, 0f00000000;
-	mov.f32 	%f2072, 0f3DAAAABD;
-	mov.f32 	%f2071, 0f3C4CAF63;
-	mov.f32 	%f2070, 0f3B18F0FE;
-	setp.eq.s16	%p161, %rs25, 0;
-	selp.f32	%f1576, 0f7F800000, %f2365, %p161;
-	setp.eq.s16	%p162, %rs24, 0;
-	selp.f32	%f455, %f1576, %f2366, %p162;
-	ld.f32 	%f456, [%rd1+308];
-	// inline asm
-	rcp.approx.ftz.f32 %f1574,%f1019;
-	// inline asm
-	mul.f32 	%f1577, %f1574, %f322;
-	mul.f32 	%f1578, %f1577, %f1577;
-	fma.rn.f32 	%f1581, %f2070, %f1578, %f2071;
-	fma.rn.f32 	%f1583, %f1581, %f1578, %f2072;
-	mul.rn.f32 	%f1584, %f1583, %f1578;
-	mul.rn.f32 	%f1585, %f1584, %f1577;
-	sub.f32 	%f1586, %f320, %f1577;
-	neg.f32 	%f1587, %f1577;
-	add.f32 	%f1588, %f1586, %f1586;
-	fma.rn.f32 	%f1589, %f1587, %f320, %f1588;
-	mul.rn.f32 	%f1590, %f1574, %f1589;
-	add.f32 	%f1591, %f1585, %f1577;
-	sub.f32 	%f1592, %f1577, %f1591;
-	add.f32 	%f1593, %f1585, %f1592;
-	add.f32 	%f1594, %f1590, %f1593;
-	add.f32 	%f1595, %f1591, %f1594;
-	sub.f32 	%f1596, %f1591, %f1595;
-	add.f32 	%f1597, %f1594, %f1596;
-	add.f32 	%f1598, %f323, %f1595;
-	sub.f32 	%f1599, %f323, %f1598;
-	add.f32 	%f1600, %f1595, %f1599;
-	add.f32 	%f1601, %f1597, %f1600;
-	add.f32 	%f1602, %f324, %f1601;
-	add.f32 	%f1603, %f1598, %f1602;
-	sub.f32 	%f1604, %f1598, %f1603;
-	add.f32 	%f1605, %f1602, %f1604;
-	mul.rn.f32 	%f1607, %f1025, %f1603;
-	neg.f32 	%f1608, %f1607;
-	fma.rn.f32 	%f1609, %f1025, %f1603, %f1608;
-	fma.rn.f32 	%f1610, %f1025, %f1605, %f1609;
-	fma.rn.f32 	%f1612, %f2073, %f1603, %f1610;
-	add.rn.f32 	%f1613, %f1607, %f1612;
-	neg.f32 	%f1614, %f1613;
-	add.rn.f32 	%f1615, %f1607, %f1614;
-	add.rn.f32 	%f1616, %f1615, %f1612;
-	mov.b32 	 %r377, %f1613;
-	setp.eq.s32	%p163, %r377, 1118925336;
-	add.s32 	%r378, %r377, -1;
-	mov.b32 	 %f1617, %r378;
-	add.f32 	%f1618, %f1616, 0f37000000;
-	selp.f32	%f1619, %f1617, %f1613, %p163;
-	selp.f32	%f457, %f1618, %f1616, %p163;
-	mul.f32 	%f1620, %f1619, 0f3FB8AA3B;
-	cvt.rzi.f32.f32	%f1621, %f1620;
-	fma.rn.f32 	%f1623, %f1621, %f2074, %f1619;
-	fma.rn.f32 	%f1625, %f1621, %f2075, %f1623;
-	mul.f32 	%f1626, %f1625, 0f3FB8AA3B;
-	ex2.approx.ftz.f32 	%f1627, %f1626;
-	add.f32 	%f1628, %f1621, 0f00000000;
-	ex2.approx.f32 	%f1629, %f1628;
-	mul.f32 	%f1630, %f1627, %f1629;
-	setp.lt.f32	%p164, %f1619, 0fC2D20000;
-	selp.f32	%f1631, 0f00000000, %f1630, %p164;
-	setp.gt.f32	%p165, %f1619, 0f42D20000;
-	selp.f32	%f2367, 0f7F800000, %f1631, %p165;
-	setp.eq.f32	%p166, %f2367, 0f7F800000;
-	@%p166 bra 	BB11_132;
-
-	fma.rn.f32 	%f2367, %f2367, %f457, %f2367;
-
-BB11_132:
-	setp.lt.f32	%p307, %f2341, 0f00000000;
-	and.pred  	%p306, %p307, %p36;
-	setp.eq.f32	%p288, %f2341, 0f00000000;
-	mov.b32 	 %r379, %f2367;
-	xor.b32  	%r380, %r379, -2147483648;
-	mov.b32 	 %f1632, %r380;
-	selp.f32	%f2369, %f1632, %f2367, %p306;
-	@%p288 bra 	BB11_135;
-	bra.uni 	BB11_133;
-
-BB11_135:
-	add.f32 	%f1635, %f2341, %f2341;
-	selp.f32	%f2369, %f1635, 0f00000000, %p36;
-	bra.uni 	BB11_136;
-
-BB11_133:
-	setp.geu.f32	%p168, %f2341, 0f00000000;
-	@%p168 bra 	BB11_136;
-
-	cvt.rzi.f32.f32	%f1634, %f1025;
-	setp.neu.f32	%p169, %f1634, 0f40000000;
-	selp.f32	%f2369, 0f7FFFFFFF, %f2369, %p169;
-
-BB11_136:
-	abs.f32 	%f2077, %f2341;
-	add.f32 	%f2076, %f2077, 0f40000000;
-	mov.b32 	 %r417, %f2076;
-	setp.lt.s32	%p289, %r417, 2139095040;
-	@%p289 bra 	BB11_141;
-
-	abs.f32 	%f2144, %f2341;
-	setp.gtu.f32	%p172, %f2144, 0f7F800000;
-	@%p172 bra 	BB11_140;
-	bra.uni 	BB11_138;
-
-BB11_140:
-	add.f32 	%f2369, %f2341, 0f40000000;
-	bra.uni 	BB11_141;
-
-BB11_138:
-	abs.f32 	%f2145, %f2341;
-	setp.neu.f32	%p173, %f2145, 0f7F800000;
-	@%p173 bra 	BB11_141;
-
-	setp.lt.f32	%p321, %f2341, 0f00000000;
-	and.pred  	%p320, %p321, %p36;
-	selp.f32	%f2369, 0fFF800000, 0f7F800000, %p320;
-
-BB11_141:
-	setp.eq.f32	%p290, %f2341, 0f3F800000;
-	mov.f32 	%f2083, 0fB5BFBE8E;
-	mov.f32 	%f2082, 0fBF317200;
-	mov.f32 	%f2081, 0f00000000;
-	mov.f32 	%f2080, 0f3DAAAABD;
-	mov.f32 	%f2079, 0f3C4CAF63;
-	mov.f32 	%f2078, 0f3B18F0FE;
-	selp.f32	%f468, 0f3F800000, %f2369, %p290;
-	// inline asm
-	rcp.approx.ftz.f32 %f1636,%f1099;
-	// inline asm
-	mul.f32 	%f1638, %f1636, %f340;
-	mul.f32 	%f1639, %f1638, %f1638;
-	fma.rn.f32 	%f1642, %f2078, %f1639, %f2079;
-	fma.rn.f32 	%f1644, %f1642, %f1639, %f2080;
-	mul.rn.f32 	%f1645, %f1644, %f1639;
-	mul.rn.f32 	%f1646, %f1645, %f1638;
-	sub.f32 	%f1647, %f338, %f1638;
-	neg.f32 	%f1648, %f1638;
-	add.f32 	%f1649, %f1647, %f1647;
-	fma.rn.f32 	%f1650, %f1648, %f338, %f1649;
-	mul.rn.f32 	%f1651, %f1636, %f1650;
-	add.f32 	%f1652, %f1646, %f1638;
-	sub.f32 	%f1653, %f1638, %f1652;
-	add.f32 	%f1654, %f1646, %f1653;
-	add.f32 	%f1655, %f1651, %f1654;
-	add.f32 	%f1656, %f1652, %f1655;
-	sub.f32 	%f1657, %f1652, %f1656;
-	add.f32 	%f1658, %f1655, %f1657;
-	add.f32 	%f1659, %f341, %f1656;
-	sub.f32 	%f1660, %f341, %f1659;
-	add.f32 	%f1661, %f1656, %f1660;
-	add.f32 	%f1662, %f1658, %f1661;
-	add.f32 	%f1663, %f342, %f1662;
-	add.f32 	%f1664, %f1659, %f1663;
-	sub.f32 	%f1665, %f1659, %f1664;
-	add.f32 	%f1666, %f1663, %f1665;
-	mul.rn.f32 	%f1668, %f1025, %f1664;
-	neg.f32 	%f1669, %f1668;
-	fma.rn.f32 	%f1670, %f1025, %f1664, %f1669;
-	fma.rn.f32 	%f1671, %f1025, %f1666, %f1670;
-	fma.rn.f32 	%f1673, %f2081, %f1664, %f1671;
-	add.rn.f32 	%f1674, %f1668, %f1673;
-	neg.f32 	%f1675, %f1674;
-	add.rn.f32 	%f1676, %f1668, %f1675;
-	add.rn.f32 	%f1677, %f1676, %f1673;
-	mov.b32 	 %r381, %f1674;
-	setp.eq.s32	%p175, %r381, 1118925336;
-	add.s32 	%r382, %r381, -1;
-	mov.b32 	 %f1678, %r382;
-	add.f32 	%f1679, %f1677, 0f37000000;
-	selp.f32	%f1680, %f1678, %f1674, %p175;
-	selp.f32	%f469, %f1679, %f1677, %p175;
-	mul.f32 	%f1681, %f1680, 0f3FB8AA3B;
-	cvt.rzi.f32.f32	%f1682, %f1681;
-	fma.rn.f32 	%f1684, %f1682, %f2082, %f1680;
-	fma.rn.f32 	%f1686, %f1682, %f2083, %f1684;
-	mul.f32 	%f1687, %f1686, 0f3FB8AA3B;
-	ex2.approx.ftz.f32 	%f1688, %f1687;
-	add.f32 	%f1689, %f1682, 0f00000000;
-	ex2.approx.f32 	%f1690, %f1689;
-	mul.f32 	%f1691, %f1688, %f1690;
-	setp.lt.f32	%p176, %f1680, 0fC2D20000;
-	selp.f32	%f1692, 0f00000000, %f1691, %p176;
-	setp.gt.f32	%p177, %f1680, 0f42D20000;
-	selp.f32	%f2370, 0f7F800000, %f1692, %p177;
-	setp.eq.f32	%p178, %f2370, 0f7F800000;
-	@%p178 bra 	BB11_143;
-
-	fma.rn.f32 	%f2370, %f2370, %f469, %f2370;
-
-BB11_143:
-	setp.lt.f32	%p309, %f2342, 0f00000000;
-	and.pred  	%p308, %p309, %p36;
-	setp.eq.f32	%p291, %f2342, 0f00000000;
-	mov.b32 	 %r383, %f2370;
-	xor.b32  	%r384, %r383, -2147483648;
-	mov.b32 	 %f1693, %r384;
-	selp.f32	%f2372, %f1693, %f2370, %p308;
-	@%p291 bra 	BB11_146;
-	bra.uni 	BB11_144;
-
-BB11_146:
-	add.f32 	%f1696, %f2342, %f2342;
-	selp.f32	%f2372, %f1696, 0f00000000, %p36;
-	bra.uni 	BB11_147;
-
-BB11_144:
-	setp.geu.f32	%p180, %f2342, 0f00000000;
-	@%p180 bra 	BB11_147;
-
-	cvt.rzi.f32.f32	%f1695, %f1025;
-	setp.neu.f32	%p181, %f1695, 0f40000000;
-	selp.f32	%f2372, 0f7FFFFFFF, %f2372, %p181;
-
-BB11_147:
-	abs.f32 	%f2085, %f2342;
-	add.f32 	%f2084, %f2085, 0f40000000;
-	mov.b32 	 %r418, %f2084;
-	setp.lt.s32	%p292, %r418, 2139095040;
-	@%p292 bra 	BB11_152;
-
-	abs.f32 	%f2142, %f2342;
-	setp.gtu.f32	%p184, %f2142, 0f7F800000;
-	@%p184 bra 	BB11_151;
-	bra.uni 	BB11_149;
-
-BB11_151:
-	add.f32 	%f2372, %f2342, 0f40000000;
-	bra.uni 	BB11_152;
-
-BB11_149:
-	abs.f32 	%f2143, %f2342;
-	setp.neu.f32	%p185, %f2143, 0f7F800000;
-	@%p185 bra 	BB11_152;
-
-	setp.lt.f32	%p319, %f2342, 0f00000000;
-	and.pred  	%p318, %p319, %p36;
-	selp.f32	%f2372, 0fFF800000, 0f7F800000, %p318;
-
-BB11_152:
-	setp.eq.f32	%p293, %f2342, 0f3F800000;
-	mov.f32 	%f2091, 0fB5BFBE8E;
-	mov.f32 	%f2090, 0fBF317200;
-	mov.f32 	%f2089, 0f00000000;
-	mov.f32 	%f2088, 0f3DAAAABD;
-	mov.f32 	%f2087, 0f3C4CAF63;
-	mov.f32 	%f2086, 0f3B18F0FE;
-	selp.f32	%f1699, 0f3F800000, %f2372, %p293;
-	add.f32 	%f480, %f468, %f1699;
-	// inline asm
-	rcp.approx.ftz.f32 %f1697,%f1173;
-	// inline asm
-	mul.f32 	%f1700, %f1697, %f361;
-	mul.f32 	%f1701, %f1700, %f1700;
-	fma.rn.f32 	%f1704, %f2086, %f1701, %f2087;
-	fma.rn.f32 	%f1706, %f1704, %f1701, %f2088;
-	mul.rn.f32 	%f1707, %f1706, %f1701;
-	mul.rn.f32 	%f1708, %f1707, %f1700;
-	sub.f32 	%f1709, %f359, %f1700;
-	neg.f32 	%f1710, %f1700;
-	add.f32 	%f1711, %f1709, %f1709;
-	fma.rn.f32 	%f1712, %f1710, %f359, %f1711;
-	mul.rn.f32 	%f1713, %f1697, %f1712;
-	add.f32 	%f1714, %f1708, %f1700;
-	sub.f32 	%f1715, %f1700, %f1714;
-	add.f32 	%f1716, %f1708, %f1715;
-	add.f32 	%f1717, %f1713, %f1716;
-	add.f32 	%f1718, %f1714, %f1717;
-	sub.f32 	%f1719, %f1714, %f1718;
-	add.f32 	%f1720, %f1717, %f1719;
-	add.f32 	%f1721, %f362, %f1718;
-	sub.f32 	%f1722, %f362, %f1721;
-	add.f32 	%f1723, %f1718, %f1722;
-	add.f32 	%f1724, %f1720, %f1723;
-	add.f32 	%f1725, %f363, %f1724;
-	add.f32 	%f1726, %f1721, %f1725;
-	sub.f32 	%f1727, %f1721, %f1726;
-	add.f32 	%f1728, %f1725, %f1727;
-	mul.rn.f32 	%f1730, %f1025, %f1726;
-	neg.f32 	%f1731, %f1730;
-	fma.rn.f32 	%f1732, %f1025, %f1726, %f1731;
-	fma.rn.f32 	%f1733, %f1025, %f1728, %f1732;
-	fma.rn.f32 	%f1735, %f2089, %f1726, %f1733;
-	add.rn.f32 	%f1736, %f1730, %f1735;
-	neg.f32 	%f1737, %f1736;
-	add.rn.f32 	%f1738, %f1730, %f1737;
-	add.rn.f32 	%f1739, %f1738, %f1735;
-	mov.b32 	 %r385, %f1736;
-	setp.eq.s32	%p187, %r385, 1118925336;
-	add.s32 	%r386, %r385, -1;
-	mov.b32 	 %f1740, %r386;
-	add.f32 	%f1741, %f1739, 0f37000000;
-	selp.f32	%f1742, %f1740, %f1736, %p187;
-	selp.f32	%f481, %f1741, %f1739, %p187;
-	mul.f32 	%f1743, %f1742, 0f3FB8AA3B;
-	cvt.rzi.f32.f32	%f1744, %f1743;
-	fma.rn.f32 	%f1746, %f1744, %f2090, %f1742;
-	fma.rn.f32 	%f1748, %f1744, %f2091, %f1746;
-	mul.f32 	%f1749, %f1748, 0f3FB8AA3B;
-	ex2.approx.ftz.f32 	%f1750, %f1749;
-	add.f32 	%f1751, %f1744, 0f00000000;
-	ex2.approx.f32 	%f1752, %f1751;
-	mul.f32 	%f1753, %f1750, %f1752;
-	setp.lt.f32	%p188, %f1742, 0fC2D20000;
-	selp.f32	%f1754, 0f00000000, %f1753, %p188;
-	setp.gt.f32	%p189, %f1742, 0f42D20000;
-	selp.f32	%f2373, 0f7F800000, %f1754, %p189;
-	setp.eq.f32	%p190, %f2373, 0f7F800000;
-	@%p190 bra 	BB11_154;
-
-	fma.rn.f32 	%f2373, %f2373, %f481, %f2373;
-
-BB11_154:
-	setp.lt.f32	%p311, %f2294, 0f00000000;
-	and.pred  	%p310, %p311, %p36;
-	setp.eq.f32	%p294, %f2294, 0f00000000;
-	mov.b32 	 %r387, %f2373;
-	xor.b32  	%r388, %r387, -2147483648;
-	mov.b32 	 %f1755, %r388;
-	selp.f32	%f2375, %f1755, %f2373, %p310;
-	@%p294 bra 	BB11_157;
-	bra.uni 	BB11_155;
-
-BB11_157:
-	add.f32 	%f2141, %f2294, %f2294;
-	selp.f32	%f2375, %f2141, 0f00000000, %p36;
-	bra.uni 	BB11_158;
-
-BB11_155:
-	setp.geu.f32	%p192, %f2294, 0f00000000;
-	@%p192 bra 	BB11_158;
-
-	cvt.rzi.f32.f32	%f1757, %f1025;
-	setp.neu.f32	%p193, %f1757, 0f40000000;
-	selp.f32	%f2375, 0f7FFFFFFF, %f2375, %p193;
-
-BB11_158:
-	abs.f32 	%f2093, %f2294;
-	add.f32 	%f2092, %f2093, 0f40000000;
-	mov.b32 	 %r419, %f2092;
-	setp.lt.s32	%p295, %r419, 2139095040;
-	@%p295 bra 	BB11_163;
-
-	abs.f32 	%f2139, %f2294;
-	setp.gtu.f32	%p196, %f2139, 0f7F800000;
-	@%p196 bra 	BB11_162;
-	bra.uni 	BB11_160;
-
-BB11_162:
-	add.f32 	%f2375, %f2294, 0f40000000;
-	bra.uni 	BB11_163;
-
-BB11_160:
-	abs.f32 	%f2140, %f2294;
-	setp.neu.f32	%p197, %f2140, 0f7F800000;
-	@%p197 bra 	BB11_163;
-
-	setp.lt.f32	%p317, %f2294, 0f00000000;
-	and.pred  	%p316, %p317, %p36;
-	selp.f32	%f2375, 0fFF800000, 0f7F800000, %p316;
-
-BB11_163:
-	add.f32 	%f2101, %f2294, %f2294;
-	mul.f32 	%f2100, %f1020, %f2101;
-	setp.eq.f32	%p296, %f2294, 0f3F800000;
-	mov.f32 	%f2099, 0fB5BFBE8E;
-	mov.f32 	%f2098, 0fBF317200;
-	mov.f32 	%f2097, 0f00000000;
-	mov.f32 	%f2096, 0f3DAAAABD;
-	mov.f32 	%f2095, 0f3C4CAF63;
-	mov.f32 	%f2094, 0f3B18F0FE;
-	selp.f32	%f1761, 0f3F800000, %f2375, %p296;
-	sub.f32 	%f492, %f1761, %f2100;
-	// inline asm
-	rcp.approx.ftz.f32 %f1759,%f1255;
-	// inline asm
-	mul.f32 	%f1762, %f1759, %f380;
-	mul.f32 	%f1763, %f1762, %f1762;
-	fma.rn.f32 	%f1766, %f2094, %f1763, %f2095;
-	fma.rn.f32 	%f1768, %f1766, %f1763, %f2096;
-	mul.rn.f32 	%f1769, %f1768, %f1763;
-	mul.rn.f32 	%f1770, %f1769, %f1762;
-	sub.f32 	%f1771, %f378, %f1762;
-	neg.f32 	%f1772, %f1762;
-	add.f32 	%f1773, %f1771, %f1771;
-	fma.rn.f32 	%f1774, %f1772, %f378, %f1773;
-	mul.rn.f32 	%f1775, %f1759, %f1774;
-	add.f32 	%f1776, %f1770, %f1762;
-	sub.f32 	%f1777, %f1762, %f1776;
-	add.f32 	%f1778, %f1770, %f1777;
-	add.f32 	%f1779, %f1775, %f1778;
-	add.f32 	%f1780, %f1776, %f1779;
-	sub.f32 	%f1781, %f1776, %f1780;
-	add.f32 	%f1782, %f1779, %f1781;
-	add.f32 	%f1783, %f381, %f1780;
-	sub.f32 	%f1784, %f381, %f1783;
-	add.f32 	%f1785, %f1780, %f1784;
-	add.f32 	%f1786, %f1782, %f1785;
-	add.f32 	%f1787, %f382, %f1786;
-	add.f32 	%f1788, %f1783, %f1787;
-	sub.f32 	%f1789, %f1783, %f1788;
-	add.f32 	%f1790, %f1787, %f1789;
-	mul.rn.f32 	%f1792, %f1025, %f1788;
-	neg.f32 	%f1793, %f1792;
-	fma.rn.f32 	%f1794, %f1025, %f1788, %f1793;
-	fma.rn.f32 	%f1795, %f1025, %f1790, %f1794;
-	fma.rn.f32 	%f1797, %f2097, %f1788, %f1795;
-	add.rn.f32 	%f1798, %f1792, %f1797;
-	neg.f32 	%f1799, %f1798;
-	add.rn.f32 	%f1800, %f1792, %f1799;
-	add.rn.f32 	%f1801, %f1800, %f1797;
-	mov.b32 	 %r389, %f1798;
-	setp.eq.s32	%p199, %r389, 1118925336;
-	add.s32 	%r390, %r389, -1;
-	mov.b32 	 %f1802, %r390;
-	add.f32 	%f1803, %f1801, 0f37000000;
-	selp.f32	%f1804, %f1802, %f1798, %p199;
-	selp.f32	%f493, %f1803, %f1801, %p199;
-	mul.f32 	%f1805, %f1804, 0f3FB8AA3B;
-	cvt.rzi.f32.f32	%f1806, %f1805;
-	fma.rn.f32 	%f1808, %f1806, %f2098, %f1804;
-	fma.rn.f32 	%f1810, %f1806, %f2099, %f1808;
-	mul.f32 	%f1811, %f1810, 0f3FB8AA3B;
-	ex2.approx.ftz.f32 	%f1812, %f1811;
-	add.f32 	%f1813, %f1806, 0f00000000;
-	ex2.approx.f32 	%f1814, %f1813;
-	mul.f32 	%f1815, %f1812, %f1814;
-	setp.lt.f32	%p200, %f1804, 0fC2D20000;
-	selp.f32	%f1816, 0f00000000, %f1815, %p200;
-	setp.gt.f32	%p201, %f1804, 0f42D20000;
-	selp.f32	%f2376, 0f7F800000, %f1816, %p201;
-	setp.eq.f32	%p202, %f2376, 0f7F800000;
-	@%p202 bra 	BB11_165;
-
-	fma.rn.f32 	%f2376, %f2376, %f493, %f2376;
-
-BB11_165:
-	setp.lt.f32	%p313, %f1020, 0f00000000;
-	and.pred  	%p312, %p313, %p36;
-	setp.eq.f32	%p297, %f1020, 0f00000000;
-	mov.b32 	 %r391, %f2376;
-	xor.b32  	%r392, %r391, -2147483648;
-	mov.b32 	 %f1817, %r392;
-	selp.f32	%f2378, %f1817, %f2376, %p312;
-	@%p297 bra 	BB11_168;
-	bra.uni 	BB11_166;
-
-BB11_168:
-	add.f32 	%f2138, %f1020, %f1020;
-	selp.f32	%f2378, %f2138, 0f00000000, %p36;
-	bra.uni 	BB11_169;
-
-BB11_166:
-	setp.geu.f32	%p204, %f1020, 0f00000000;
-	@%p204 bra 	BB11_169;
-
-	cvt.rzi.f32.f32	%f1819, %f1025;
-	setp.neu.f32	%p205, %f1819, 0f40000000;
-	selp.f32	%f2378, 0f7FFFFFFF, %f2378, %p205;
-
-BB11_169:
-	abs.f32 	%f2103, %f1020;
-	add.f32 	%f2102, %f2103, 0f40000000;
-	mov.b32 	 %r420, %f2102;
-	setp.lt.s32	%p298, %r420, 2139095040;
-	@%p298 bra 	BB11_174;
-
-	abs.f32 	%f2136, %f1020;
-	setp.gtu.f32	%p208, %f2136, 0f7F800000;
-	@%p208 bra 	BB11_173;
-	bra.uni 	BB11_171;
-
-BB11_173:
-	add.f32 	%f2378, %f1020, 0f40000000;
-	bra.uni 	BB11_174;
-
-BB11_171:
-	abs.f32 	%f2137, %f1020;
-	setp.neu.f32	%p209, %f2137, 0f7F800000;
-	@%p209 bra 	BB11_174;
-
-	setp.lt.f32	%p315, %f1020, 0f00000000;
-	and.pred  	%p314, %p315, %p36;
-	selp.f32	%f2378, 0fFF800000, 0f7F800000, %p314;
-
-BB11_174:
-	setp.eq.f32	%p299, %f1020, 0f3F800000;
-	mov.f32 	%f2109, 0fB5BFBE8E;
-	mov.f32 	%f2108, 0fBF317200;
-	mov.f32 	%f2107, 0f00000000;
-	mov.f32 	%f2106, 0f3DAAAABD;
-	mov.f32 	%f2105, 0f3C4CAF63;
-	mov.f32 	%f2104, 0f3B18F0FE;
-	selp.f32	%f1823, 0f3F800000, %f2378, %p299;
-	add.f32 	%f504, %f492, %f1823;
-	// inline asm
-	rcp.approx.ftz.f32 %f1821,%f1330;
-	// inline asm
-	mul.f32 	%f1824, %f1821, %f398;
-	mul.f32 	%f1825, %f1824, %f1824;
-	fma.rn.f32 	%f1828, %f2104, %f1825, %f2105;
-	fma.rn.f32 	%f1830, %f1828, %f1825, %f2106;
-	mul.rn.f32 	%f1831, %f1830, %f1825;
-	mul.rn.f32 	%f1832, %f1831, %f1824;
-	sub.f32 	%f1833, %f396, %f1824;
-	neg.f32 	%f1834, %f1824;
-	add.f32 	%f1835, %f1833, %f1833;
-	fma.rn.f32 	%f1836, %f1834, %f396, %f1835;
-	mul.rn.f32 	%f1837, %f1821, %f1836;
-	add.f32 	%f1838, %f1832, %f1824;
-	sub.f32 	%f1839, %f1824, %f1838;
-	add.f32 	%f1840, %f1832, %f1839;
-	add.f32 	%f1841, %f1837, %f1840;
-	add.f32 	%f1842, %f1838, %f1841;
-	sub.f32 	%f1843, %f1838, %f1842;
-	add.f32 	%f1844, %f1841, %f1843;
-	add.f32 	%f1845, %f399, %f1842;
-	sub.f32 	%f1846, %f399, %f1845;
-	add.f32 	%f1847, %f1842, %f1846;
-	add.f32 	%f1848, %f1844, %f1847;
-	add.f32 	%f1849, %f400, %f1848;
-	add.f32 	%f1850, %f1845, %f1849;
-	sub.f32 	%f1851, %f1845, %f1850;
-	add.f32 	%f1852, %f1849, %f1851;
-	mul.rn.f32 	%f1854, %f1025, %f1850;
-	neg.f32 	%f1855, %f1854;
-	fma.rn.f32 	%f1856, %f1025, %f1850, %f1855;
-	fma.rn.f32 	%f1857, %f1025, %f1852, %f1856;
-	fma.rn.f32 	%f1859, %f2107, %f1850, %f1857;
-	add.rn.f32 	%f1860, %f1854, %f1859;
-	neg.f32 	%f1861, %f1860;
-	add.rn.f32 	%f1862, %f1854, %f1861;
-	add.rn.f32 	%f1863, %f1862, %f1859;
-	mov.b32 	 %r393, %f1860;
-	setp.eq.s32	%p211, %r393, 1118925336;
-	add.s32 	%r394, %r393, -1;
-	mov.b32 	 %f1864, %r394;
-	add.f32 	%f1865, %f1863, 0f37000000;
-	selp.f32	%f1866, %f1864, %f1860, %p211;
-	selp.f32	%f505, %f1865, %f1863, %p211;
-	mul.f32 	%f1867, %f1866, 0f3FB8AA3B;
-	cvt.rzi.f32.f32	%f1868, %f1867;
-	fma.rn.f32 	%f1870, %f1868, %f2108, %f1866;
-	fma.rn.f32 	%f1872, %f1868, %f2109, %f1870;
-	mul.f32 	%f1873, %f1872, 0f3FB8AA3B;
-	ex2.approx.ftz.f32 	%f1874, %f1873;
-	add.f32 	%f1875, %f1868, 0f00000000;
-	ex2.approx.f32 	%f1876, %f1875;
-	mul.f32 	%f1877, %f1874, %f1876;
-	setp.lt.f32	%p212, %f1866, 0fC2D20000;
-	selp.f32	%f1878, 0f00000000, %f1877, %p212;
-	setp.gt.f32	%p213, %f1866, 0f42D20000;
-	selp.f32	%f2379, 0f7F800000, %f1878, %p213;
-	setp.eq.f32	%p214, %f2379, 0f7F800000;
-	@%p214 bra 	BB11_176;
-
-	fma.rn.f32 	%f2379, %f2379, %f505, %f2379;
-
-BB11_176:
-	setp.lt.f32	%p323, %f2293, 0f00000000;
-	and.pred  	%p322, %p323, %p36;
-	setp.eq.f32	%p300, %f2293, 0f00000000;
-	mov.b32 	 %r395, %f2379;
+	mov.b32 	%f1323, %r370;
+	selp.f32 	%f2071, %f1323, %f2069, %p4;
+	setp.geu.f32 	%p85, %f347, 0f00000000;
+	@%p85 bra 	$L__BB11_84;
+
+	mov.f32 	%f1870, 0f40000000;
+	cvt.rzi.f32.f32 	%f1325, %f1870;
+	setp.eq.f32 	%p86, %f1325, 0f40000000;
+	@%p86 bra 	$L__BB11_84;
+
+	mov.f32 	%f2071, 0f7FFFFFFF;
+
+$L__BB11_84:
+	abs.f32 	%f1855, %f347;
+	add.f32 	%f1328, %f1855, 0f40000000;
+	mov.b32 	%r12, %f1328;
+	setp.lt.s32 	%p88, %r12, 2139095040;
+	@%p88 bra 	$L__BB11_89;
+
+	abs.f32 	%f1868, %f347;
+	setp.gtu.f32 	%p89, %f1868, 0f7F800000;
+	@%p89 bra 	$L__BB11_88;
+	bra.uni 	$L__BB11_86;
+
+$L__BB11_88:
+	add.f32 	%f2071, %f347, 0f40000000;
+	bra.uni 	$L__BB11_89;
+
+$L__BB11_86:
+	abs.f32 	%f1869, %f347;
+	setp.neu.f32 	%p90, %f1869, 0f7F800000;
+	@%p90 bra 	$L__BB11_89;
+
+	selp.f32 	%f2071, 0fFF800000, 0f7F800000, %p4;
+
+$L__BB11_89:
+	add.f32 	%f1867, %f1999, %f1999;
+	mul.f32 	%f1866, %f347, %f1867;
+	mov.f32 	%f1865, 0f3102E308;
+	mov.f32 	%f1864, 0fBF317218;
+	mov.f32 	%f1863, 0f3FB8AA3B;
+	mov.f32 	%f1862, 0f00000000;
+	mov.f32 	%f1861, 0f35BFBE8E;
+	mov.f32 	%f1860, 0f3F317200;
+	mov.f32 	%f1859, 0f3DAAAABD;
+	mov.f32 	%f1858, 0f3C4CAF63;
+	mov.f32 	%f1857, 0f3B18F0FE;
+	mov.f32 	%f1856, 0f40000000;
+	setp.eq.f32 	%p91, %f347, 0f3F800000;
+	selp.f32 	%f1330, 0f3F800000, %f2071, %p91;
+	setp.eq.f32 	%p92, %f1999, 0f3F800000;
+	selp.f32 	%f1331, 0f3F800000, %f2068, %p92;
+	sub.f32 	%f1332, %f1331, %f1866;
+	add.f32 	%f401, %f1332, %f1330;
+	abs.f32 	%f402, %f2000;
+	setp.lt.f32 	%p93, %f402, 0f00800000;
+	mul.f32 	%f1333, %f402, 0f4B800000;
+	selp.f32 	%f1334, %f1333, %f402, %p93;
+	selp.f32 	%f1335, 0fC3170000, 0fC2FE0000, %p93;
+	mov.b32 	%r371, %f1334;
+	and.b32  	%r372, %r371, 8388607;
+	or.b32  	%r373, %r372, 1065353216;
+	mov.b32 	%f1336, %r373;
+	shr.u32 	%r374, %r371, 23;
+	cvt.rn.f32.u32 	%f1337, %r374;
+	add.f32 	%f1338, %f1335, %f1337;
+	setp.gt.f32 	%p94, %f1336, 0f3FB504F3;
+	mul.f32 	%f1339, %f1336, 0f3F000000;
+	add.f32 	%f1340, %f1338, 0f3F800000;
+	selp.f32 	%f1341, %f1340, %f1338, %p94;
+	selp.f32 	%f1342, %f1339, %f1336, %p94;
+	add.f32 	%f1343, %f1342, 0fBF800000;
+	add.f32 	%f1344, %f1342, 0f3F800000;
+	rcp.approx.ftz.f32 	%f1345, %f1344;
+	add.f32 	%f1346, %f1343, %f1343;
+	mul.f32 	%f1348, %f1346, %f1345;
+	mul.f32 	%f1349, %f1348, %f1348;
+	fma.rn.f32 	%f1352, %f1857, %f1349, %f1858;
+	fma.rn.f32 	%f1354, %f1352, %f1349, %f1859;
+	mul.rn.f32 	%f1355, %f1354, %f1349;
+	mul.rn.f32 	%f1356, %f1355, %f1348;
+	sub.f32 	%f1357, %f1343, %f1348;
+	add.f32 	%f1358, %f1357, %f1357;
+	neg.f32 	%f1359, %f1348;
+	fma.rn.f32 	%f1360, %f1359, %f1343, %f1358;
+	mul.rn.f32 	%f1361, %f1345, %f1360;
+	add.f32 	%f1362, %f1356, %f1348;
+	sub.f32 	%f1363, %f1348, %f1362;
+	add.f32 	%f1364, %f1356, %f1363;
+	add.f32 	%f1365, %f1361, %f1364;
+	add.f32 	%f1366, %f1362, %f1365;
+	sub.f32 	%f1367, %f1362, %f1366;
+	add.f32 	%f1368, %f1365, %f1367;
+	mul.rn.f32 	%f1370, %f1341, %f1860;
+	mul.rn.f32 	%f1372, %f1341, %f1861;
+	add.f32 	%f1373, %f1370, %f1366;
+	sub.f32 	%f1374, %f1370, %f1373;
+	add.f32 	%f1375, %f1366, %f1374;
+	add.f32 	%f1376, %f1368, %f1375;
+	add.f32 	%f1377, %f1372, %f1376;
+	add.f32 	%f1378, %f1373, %f1377;
+	sub.f32 	%f1379, %f1373, %f1378;
+	add.f32 	%f1380, %f1377, %f1379;
+	mul.rn.f32 	%f1381, %f1856, %f1378;
+	neg.f32 	%f1382, %f1381;
+	fma.rn.f32 	%f1383, %f1856, %f1378, %f1382;
+	fma.rn.f32 	%f1384, %f1856, %f1380, %f1383;
+	fma.rn.f32 	%f1386, %f1862, %f1378, %f1384;
+	add.rn.f32 	%f1387, %f1381, %f1386;
+	neg.f32 	%f1388, %f1387;
+	add.rn.f32 	%f1389, %f1381, %f1388;
+	add.rn.f32 	%f1390, %f1389, %f1386;
+	mov.b32 	%r375, %f1387;
+	setp.eq.s32 	%p95, %r375, 1118925336;
+	add.s32 	%r376, %r375, -1;
+	mov.b32 	%f1391, %r376;
+	add.f32 	%f1392, %f1390, 0f37000000;
+	selp.f32 	%f403, %f1392, %f1390, %p95;
+	selp.f32 	%f1393, %f1391, %f1387, %p95;
+	mul.rn.f32 	%f1395, %f1393, %f1863;
+	cvt.rzi.f32.f32 	%f1396, %f1395;
+	abs.f32 	%f1397, %f1396;
+	setp.gt.f32 	%p96, %f1397, 0f42FC0000;
+	mov.b32 	%r377, %f1396;
+	and.b32  	%r378, %r377, -2147483648;
+	or.b32  	%r379, %r378, 1123811328;
+	mov.b32 	%f1398, %r379;
+	selp.f32 	%f1399, %f1398, %f1396, %p96;
+	fma.rn.f32 	%f1401, %f1399, %f1864, %f1393;
+	fma.rn.f32 	%f1403, %f1399, %f1865, %f1401;
+	mul.f32 	%f1404, %f1403, 0f3FB8AA3B;
+	add.f32 	%f1405, %f1399, 0f4B40007F;
+	mov.b32 	%r380, %f1405;
+	shl.b32 	%r381, %r380, 23;
+	mov.b32 	%f1406, %r381;
+	ex2.approx.ftz.f32 	%f1407, %f1404;
+	mul.f32 	%f404, %f1407, %f1406;
+	setp.eq.f32 	%p97, %f404, 0f7F800000;
+	mov.f32 	%f2072, 0f7F800000;
+	@%p97 bra 	$L__BB11_91;
+
+	fma.rn.f32 	%f2072, %f404, %f403, %f404;
+
+$L__BB11_91:
+	setp.lt.f32 	%p98, %f2000, 0f00000000;
+	and.pred  	%p5, %p98, %p39;
+	setp.eq.f32 	%p100, %f2000, 0f00000000;
+	@%p100 bra 	$L__BB11_95;
+	bra.uni 	$L__BB11_92;
+
+$L__BB11_95:
+	add.f32 	%f1849, %f2000, %f2000;
+	selp.f32 	%f2074, %f1849, 0f00000000, %p39;
+	bra.uni 	$L__BB11_96;
+
+$L__BB11_92:
+	mov.b32 	%r382, %f2072;
+	xor.b32  	%r383, %r382, -2147483648;
+	mov.b32 	%f1408, %r383;
+	selp.f32 	%f2074, %f1408, %f2072, %p5;
+	setp.geu.f32 	%p101, %f2000, 0f00000000;
+	@%p101 bra 	$L__BB11_96;
+
+	mov.f32 	%f1848, 0f40000000;
+	cvt.rzi.f32.f32 	%f1410, %f1848;
+	setp.eq.f32 	%p102, %f1410, 0f40000000;
+	@%p102 bra 	$L__BB11_96;
+
+	mov.f32 	%f2074, 0f7FFFFFFF;
+
+$L__BB11_96:
+	abs.f32 	%f1822, %f2000;
+	add.f32 	%f1413, %f1822, 0f40000000;
+	mov.b32 	%r13, %f1413;
+	setp.lt.s32 	%p104, %r13, 2139095040;
+	@%p104 bra 	$L__BB11_101;
+
+	abs.f32 	%f1846, %f2000;
+	setp.gtu.f32 	%p105, %f1846, 0f7F800000;
+	@%p105 bra 	$L__BB11_100;
+	bra.uni 	$L__BB11_98;
+
+$L__BB11_100:
+	add.f32 	%f2074, %f2000, 0f40000000;
+	bra.uni 	$L__BB11_101;
+
+$L__BB11_98:
+	abs.f32 	%f1847, %f2000;
+	setp.neu.f32 	%p106, %f1847, 0f7F800000;
+	@%p106 bra 	$L__BB11_101;
+
+	selp.f32 	%f2074, 0fFF800000, 0f7F800000, %p5;
+
+$L__BB11_101:
+	mov.f32 	%f1832, 0f3102E308;
+	mov.f32 	%f1831, 0fBF317218;
+	mov.f32 	%f1830, 0f3FB8AA3B;
+	mov.f32 	%f1829, 0f00000000;
+	mov.f32 	%f1828, 0f35BFBE8E;
+	mov.f32 	%f1827, 0f3F317200;
+	mov.f32 	%f1826, 0f3DAAAABD;
+	mov.f32 	%f1825, 0f3C4CAF63;
+	mov.f32 	%f1824, 0f3B18F0FE;
+	mov.f32 	%f1823, 0f40000000;
+	setp.eq.f32 	%p107, %f2000, 0f3F800000;
+	selp.f32 	%f1415, 0f3F800000, %f2074, %p107;
+	add.f32 	%f413, %f401, %f1415;
+	abs.f32 	%f415, %f348;
+	setp.lt.f32 	%p108, %f415, 0f00800000;
+	mul.f32 	%f1416, %f415, 0f4B800000;
+	selp.f32 	%f1417, %f1416, %f415, %p108;
+	selp.f32 	%f1418, 0fC3170000, 0fC2FE0000, %p108;
+	mov.b32 	%r384, %f1417;
+	and.b32  	%r385, %r384, 8388607;
+	or.b32  	%r386, %r385, 1065353216;
+	mov.b32 	%f1419, %r386;
+	shr.u32 	%r387, %r384, 23;
+	cvt.rn.f32.u32 	%f1420, %r387;
+	add.f32 	%f1421, %f1418, %f1420;
+	setp.gt.f32 	%p109, %f1419, 0f3FB504F3;
+	mul.f32 	%f1422, %f1419, 0f3F000000;
+	add.f32 	%f1423, %f1421, 0f3F800000;
+	selp.f32 	%f1424, %f1423, %f1421, %p109;
+	selp.f32 	%f1425, %f1422, %f1419, %p109;
+	add.f32 	%f1426, %f1425, 0fBF800000;
+	add.f32 	%f1427, %f1425, 0f3F800000;
+	rcp.approx.ftz.f32 	%f1428, %f1427;
+	add.f32 	%f1429, %f1426, %f1426;
+	mul.f32 	%f1431, %f1429, %f1428;
+	mul.f32 	%f1432, %f1431, %f1431;
+	fma.rn.f32 	%f1435, %f1824, %f1432, %f1825;
+	fma.rn.f32 	%f1437, %f1435, %f1432, %f1826;
+	mul.rn.f32 	%f1438, %f1437, %f1432;
+	mul.rn.f32 	%f1439, %f1438, %f1431;
+	sub.f32 	%f1440, %f1426, %f1431;
+	add.f32 	%f1441, %f1440, %f1440;
+	neg.f32 	%f1442, %f1431;
+	fma.rn.f32 	%f1443, %f1442, %f1426, %f1441;
+	mul.rn.f32 	%f1444, %f1428, %f1443;
+	add.f32 	%f1445, %f1439, %f1431;
+	sub.f32 	%f1446, %f1431, %f1445;
+	add.f32 	%f1447, %f1439, %f1446;
+	add.f32 	%f1448, %f1444, %f1447;
+	add.f32 	%f1449, %f1445, %f1448;
+	sub.f32 	%f1450, %f1445, %f1449;
+	add.f32 	%f1451, %f1448, %f1450;
+	mul.rn.f32 	%f1453, %f1424, %f1827;
+	mul.rn.f32 	%f1455, %f1424, %f1828;
+	add.f32 	%f1456, %f1453, %f1449;
+	sub.f32 	%f1457, %f1453, %f1456;
+	add.f32 	%f1458, %f1449, %f1457;
+	add.f32 	%f1459, %f1451, %f1458;
+	add.f32 	%f1460, %f1455, %f1459;
+	add.f32 	%f1461, %f1456, %f1460;
+	sub.f32 	%f1462, %f1456, %f1461;
+	add.f32 	%f1463, %f1460, %f1462;
+	mul.rn.f32 	%f1464, %f1823, %f1461;
+	neg.f32 	%f1465, %f1464;
+	fma.rn.f32 	%f1466, %f1823, %f1461, %f1465;
+	fma.rn.f32 	%f1467, %f1823, %f1463, %f1466;
+	fma.rn.f32 	%f1469, %f1829, %f1461, %f1467;
+	add.rn.f32 	%f1470, %f1464, %f1469;
+	neg.f32 	%f1471, %f1470;
+	add.rn.f32 	%f1472, %f1464, %f1471;
+	add.rn.f32 	%f1473, %f1472, %f1469;
+	mov.b32 	%r388, %f1470;
+	setp.eq.s32 	%p110, %r388, 1118925336;
+	add.s32 	%r389, %r388, -1;
+	mov.b32 	%f1474, %r389;
+	add.f32 	%f1475, %f1473, 0f37000000;
+	selp.f32 	%f416, %f1475, %f1473, %p110;
+	selp.f32 	%f1476, %f1474, %f1470, %p110;
+	mul.rn.f32 	%f1478, %f1476, %f1830;
+	cvt.rzi.f32.f32 	%f1479, %f1478;
+	abs.f32 	%f1480, %f1479;
+	setp.gt.f32 	%p111, %f1480, 0f42FC0000;
+	mov.b32 	%r390, %f1479;
+	and.b32  	%r391, %r390, -2147483648;
+	or.b32  	%r392, %r391, 1123811328;
+	mov.b32 	%f1481, %r392;
+	selp.f32 	%f1482, %f1481, %f1479, %p111;
+	fma.rn.f32 	%f1484, %f1482, %f1831, %f1476;
+	fma.rn.f32 	%f1486, %f1482, %f1832, %f1484;
+	mul.f32 	%f1487, %f1486, 0f3FB8AA3B;
+	add.f32 	%f1488, %f1482, 0f4B40007F;
+	mov.b32 	%r393, %f1488;
+	shl.b32 	%r394, %r393, 23;
+	mov.b32 	%f1489, %r394;
+	ex2.approx.ftz.f32 	%f1490, %f1487;
+	mul.f32 	%f417, %f1490, %f1489;
+	setp.eq.f32 	%p112, %f417, 0f7F800000;
+	mov.f32 	%f2075, 0f7F800000;
+	@%p112 bra 	$L__BB11_103;
+
+	fma.rn.f32 	%f2075, %f417, %f416, %f417;
+
+$L__BB11_103:
+	setp.lt.f32 	%p113, %f348, 0f00000000;
+	and.pred  	%p6, %p113, %p39;
+	setp.eq.f32 	%p115, %f348, 0f00000000;
+	@%p115 bra 	$L__BB11_107;
+	bra.uni 	$L__BB11_104;
+
+$L__BB11_107:
+	add.f32 	%f1845, %f348, %f348;
+	selp.f32 	%f2077, %f1845, 0f00000000, %p39;
+	bra.uni 	$L__BB11_108;
+
+$L__BB11_104:
+	mov.b32 	%r395, %f2075;
 	xor.b32  	%r396, %r395, -2147483648;
-	mov.b32 	 %f1879, %r396;
-	selp.f32	%f2381, %f1879, %f2379, %p322;
-	@%p300 bra 	BB11_179;
-	bra.uni 	BB11_177;
-
-BB11_179:
-	add.f32 	%f2135, %f2293, %f2293;
-	selp.f32	%f2381, %f2135, 0f00000000, %p36;
-	bra.uni 	BB11_180;
-
-BB11_177:
-	setp.geu.f32	%p216, %f2293, 0f00000000;
-	@%p216 bra 	BB11_180;
-
-	cvt.rzi.f32.f32	%f1881, %f1025;
-	setp.neu.f32	%p217, %f1881, 0f40000000;
-	selp.f32	%f2381, 0f7FFFFFFF, %f2381, %p217;
-
-BB11_180:
-	abs.f32 	%f2111, %f2293;
-	add.f32 	%f2110, %f2111, 0f40000000;
-	mov.b32 	 %r421, %f2110;
-	setp.lt.s32	%p301, %r421, 2139095040;
-	@%p301 bra 	BB11_185;
-
-	abs.f32 	%f2133, %f2293;
-	setp.gtu.f32	%p220, %f2133, 0f7F800000;
-	@%p220 bra 	BB11_184;
-	bra.uni 	BB11_182;
-
-BB11_184:
-	add.f32 	%f2381, %f2293, 0f40000000;
-	bra.uni 	BB11_185;
-
-BB11_182:
-	abs.f32 	%f2134, %f2293;
-	setp.neu.f32	%p221, %f2134, 0f7F800000;
-	@%p221 bra 	BB11_185;
-
-	setp.lt.f32	%p325, %f2293, 0f00000000;
-	and.pred  	%p324, %p325, %p36;
-	selp.f32	%f2381, 0fFF800000, 0f7F800000, %p324;
-
-BB11_185:
-	add.f32 	%f2119, %f2293, %f2293;
-	mul.f32 	%f2118, %f1021, %f2119;
-	setp.eq.f32	%p302, %f2293, 0f3F800000;
-	mov.f32 	%f2117, 0fB5BFBE8E;
-	mov.f32 	%f2116, 0fBF317200;
-	mov.f32 	%f2115, 0f00000000;
-	mov.f32 	%f2114, 0f3DAAAABD;
-	mov.f32 	%f2113, 0f3C4CAF63;
-	mov.f32 	%f2112, 0f3B18F0FE;
-	selp.f32	%f1885, 0f3F800000, %f2381, %p302;
-	add.f32 	%f1886, %f504, %f1885;
-	sub.f32 	%f516, %f1886, %f2118;
-	// inline asm
-	rcp.approx.ftz.f32 %f1883,%f1405;
-	// inline asm
-	mul.f32 	%f1887, %f1883, %f417;
-	mul.f32 	%f1888, %f1887, %f1887;
-	fma.rn.f32 	%f1891, %f2112, %f1888, %f2113;
-	fma.rn.f32 	%f1893, %f1891, %f1888, %f2114;
-	mul.rn.f32 	%f1894, %f1893, %f1888;
-	mul.rn.f32 	%f1895, %f1894, %f1887;
-	sub.f32 	%f1896, %f415, %f1887;
-	neg.f32 	%f1897, %f1887;
-	add.f32 	%f1898, %f1896, %f1896;
-	fma.rn.f32 	%f1899, %f1897, %f415, %f1898;
-	mul.rn.f32 	%f1900, %f1883, %f1899;
-	add.f32 	%f1901, %f1895, %f1887;
-	sub.f32 	%f1902, %f1887, %f1901;
-	add.f32 	%f1903, %f1895, %f1902;
-	add.f32 	%f1904, %f1900, %f1903;
-	add.f32 	%f1905, %f1901, %f1904;
-	sub.f32 	%f1906, %f1901, %f1905;
-	add.f32 	%f1907, %f1904, %f1906;
-	add.f32 	%f1908, %f418, %f1905;
-	sub.f32 	%f1909, %f418, %f1908;
-	add.f32 	%f1910, %f1905, %f1909;
-	add.f32 	%f1911, %f1907, %f1910;
-	add.f32 	%f1912, %f419, %f1911;
-	add.f32 	%f1913, %f1908, %f1912;
-	sub.f32 	%f1914, %f1908, %f1913;
-	add.f32 	%f1915, %f1912, %f1914;
-	mul.rn.f32 	%f1917, %f1025, %f1913;
-	neg.f32 	%f1918, %f1917;
-	fma.rn.f32 	%f1919, %f1025, %f1913, %f1918;
-	fma.rn.f32 	%f1920, %f1025, %f1915, %f1919;
-	fma.rn.f32 	%f1922, %f2115, %f1913, %f1920;
-	add.rn.f32 	%f1923, %f1917, %f1922;
-	neg.f32 	%f1924, %f1923;
-	add.rn.f32 	%f1925, %f1917, %f1924;
-	add.rn.f32 	%f1926, %f1925, %f1922;
-	mov.b32 	 %r397, %f1923;
-	setp.eq.s32	%p223, %r397, 1118925336;
-	add.s32 	%r398, %r397, -1;
-	mov.b32 	 %f1927, %r398;
-	add.f32 	%f1928, %f1926, 0f37000000;
-	selp.f32	%f1929, %f1927, %f1923, %p223;
-	selp.f32	%f517, %f1928, %f1926, %p223;
-	mul.f32 	%f1930, %f1929, 0f3FB8AA3B;
-	cvt.rzi.f32.f32	%f1931, %f1930;
-	fma.rn.f32 	%f1933, %f1931, %f2116, %f1929;
-	fma.rn.f32 	%f1935, %f1931, %f2117, %f1933;
-	mul.f32 	%f1936, %f1935, 0f3FB8AA3B;
-	ex2.approx.ftz.f32 	%f1937, %f1936;
-	add.f32 	%f1938, %f1931, 0f00000000;
-	ex2.approx.f32 	%f1939, %f1938;
-	mul.f32 	%f1940, %f1937, %f1939;
-	setp.lt.f32	%p224, %f1929, 0fC2D20000;
-	selp.f32	%f1941, 0f00000000, %f1940, %p224;
-	setp.gt.f32	%p225, %f1929, 0f42D20000;
-	selp.f32	%f2382, 0f7F800000, %f1941, %p225;
-	setp.eq.f32	%p226, %f2382, 0f7F800000;
-	@%p226 bra 	BB11_187;
-
-	fma.rn.f32 	%f2382, %f2382, %f517, %f2382;
-
-BB11_187:
-	setp.lt.f32	%p327, %f1021, 0f00000000;
-	and.pred  	%p326, %p327, %p36;
-	setp.eq.f32	%p303, %f1021, 0f00000000;
-	mov.b32 	 %r399, %f2382;
-	xor.b32  	%r400, %r399, -2147483648;
-	mov.b32 	 %f1942, %r400;
-	selp.f32	%f2384, %f1942, %f2382, %p326;
-	@%p303 bra 	BB11_190;
-	bra.uni 	BB11_188;
-
-BB11_190:
-	add.f32 	%f2132, %f1021, %f1021;
-	selp.f32	%f2384, %f2132, 0f00000000, %p36;
-	bra.uni 	BB11_191;
-
-BB11_188:
-	setp.geu.f32	%p228, %f1021, 0f00000000;
-	@%p228 bra 	BB11_191;
-
-	cvt.rzi.f32.f32	%f1944, %f1025;
-	setp.neu.f32	%p229, %f1944, 0f40000000;
-	selp.f32	%f2384, 0f7FFFFFFF, %f2384, %p229;
-
-BB11_191:
-	abs.f32 	%f2121, %f1021;
-	add.f32 	%f2120, %f2121, 0f40000000;
-	mov.b32 	 %r422, %f2120;
-	setp.lt.s32	%p304, %r422, 2139095040;
-	@%p304 bra 	BB11_196;
-
-	abs.f32 	%f2130, %f1021;
-	setp.gtu.f32	%p232, %f2130, 0f7F800000;
-	@%p232 bra 	BB11_195;
-	bra.uni 	BB11_193;
-
-BB11_195:
-	add.f32 	%f2384, %f1021, 0f40000000;
-	bra.uni 	BB11_196;
-
-BB11_193:
-	abs.f32 	%f2131, %f1021;
-	setp.neu.f32	%p233, %f2131, 0f7F800000;
-	@%p233 bra 	BB11_196;
-
-	setp.lt.f32	%p330, %f1021, 0f00000000;
-	and.pred  	%p329, %p330, %p36;
-	selp.f32	%f2384, 0fFF800000, 0f7F800000, %p329;
-
-BB11_196:
-	setp.eq.f32	%p305, %f1021, 0f3F800000;
-	mov.f32 	%f2129, 0fB5BFBE8E;
-	mov.f32 	%f2128, 0fBF317200;
-	mov.f32 	%f2127, 0f00000000;
-	mov.f32 	%f2126, 0f35BFBE8E;
-	mov.f32 	%f2125, 0f3F317200;
-	mov.f32 	%f2124, 0f3DAAAABD;
-	mov.f32 	%f2123, 0f3C4CAF63;
-	mov.f32 	%f2122, 0f3B18F0FE;
-	selp.f32	%f1948, 0f3F800000, %f2384, %p305;
-	add.f32 	%f528, %f516, %f1948;
-	abs.f32 	%f529, %f456;
-	setp.lt.f32	%p235, %f529, 0f00800000;
-	mul.f32 	%f1949, %f529, 0f4B800000;
-	selp.f32	%f1950, 0fC3170000, 0fC2FE0000, %p235;
-	selp.f32	%f1951, %f1949, %f529, %p235;
-	mov.b32 	 %r401, %f1951;
-	and.b32  	%r402, %r401, 8388607;
-	or.b32  	%r403, %r402, 1065353216;
-	mov.b32 	 %f1952, %r403;
-	shr.u32 	%r404, %r401, 23;
-	cvt.rn.f32.u32	%f1953, %r404;
-	add.f32 	%f1954, %f1950, %f1953;
-	setp.gt.f32	%p236, %f1952, 0f3FB504F3;
-	mul.f32 	%f1955, %f1952, 0f3F000000;
-	add.f32 	%f1956, %f1954, 0f3F800000;
-	selp.f32	%f1957, %f1955, %f1952, %p236;
-	selp.f32	%f1958, %f1956, %f1954, %p236;
-	add.f32 	%f1959, %f1957, 0fBF800000;
-	add.f32 	%f1947, %f1957, 0f3F800000;
-	// inline asm
-	rcp.approx.ftz.f32 %f1946,%f1947;
-	// inline asm
-	add.f32 	%f1960, %f1959, %f1959;
-	mul.f32 	%f1961, %f1946, %f1960;
-	mul.f32 	%f1962, %f1961, %f1961;
-	fma.rn.f32 	%f1965, %f2122, %f1962, %f2123;
-	fma.rn.f32 	%f1967, %f1965, %f1962, %f2124;
-	mul.rn.f32 	%f1968, %f1967, %f1962;
-	mul.rn.f32 	%f1969, %f1968, %f1961;
-	sub.f32 	%f1970, %f1959, %f1961;
-	neg.f32 	%f1971, %f1961;
-	add.f32 	%f1972, %f1970, %f1970;
-	fma.rn.f32 	%f1973, %f1971, %f1959, %f1972;
-	mul.rn.f32 	%f1974, %f1946, %f1973;
-	add.f32 	%f1975, %f1969, %f1961;
-	sub.f32 	%f1976, %f1961, %f1975;
-	add.f32 	%f1977, %f1969, %f1976;
-	add.f32 	%f1978, %f1974, %f1977;
-	add.f32 	%f1979, %f1975, %f1978;
-	sub.f32 	%f1980, %f1975, %f1979;
-	add.f32 	%f1981, %f1978, %f1980;
-	mul.rn.f32 	%f1983, %f1958, %f2125;
-	mul.rn.f32 	%f1985, %f1958, %f2126;
-	add.f32 	%f1986, %f1983, %f1979;
-	sub.f32 	%f1987, %f1983, %f1986;
-	add.f32 	%f1988, %f1979, %f1987;
-	add.f32 	%f1989, %f1981, %f1988;
-	add.f32 	%f1990, %f1985, %f1989;
-	add.f32 	%f1991, %f1986, %f1990;
-	sub.f32 	%f1992, %f1986, %f1991;
-	add.f32 	%f1993, %f1990, %f1992;
-	mul.rn.f32 	%f1995, %f1025, %f1991;
-	neg.f32 	%f1996, %f1995;
-	fma.rn.f32 	%f1997, %f1025, %f1991, %f1996;
-	fma.rn.f32 	%f1998, %f1025, %f1993, %f1997;
-	fma.rn.f32 	%f2000, %f2127, %f1991, %f1998;
-	add.rn.f32 	%f2001, %f1995, %f2000;
-	neg.f32 	%f2002, %f2001;
-	add.rn.f32 	%f2003, %f1995, %f2002;
-	add.rn.f32 	%f2004, %f2003, %f2000;
-	mov.b32 	 %r405, %f2001;
-	setp.eq.s32	%p237, %r405, 1118925336;
-	add.s32 	%r406, %r405, -1;
-	mov.b32 	 %f2005, %r406;
-	add.f32 	%f2006, %f2004, 0f37000000;
-	selp.f32	%f2007, %f2005, %f2001, %p237;
-	selp.f32	%f530, %f2006, %f2004, %p237;
-	mul.f32 	%f2008, %f2007, 0f3FB8AA3B;
-	cvt.rzi.f32.f32	%f2009, %f2008;
-	fma.rn.f32 	%f2011, %f2009, %f2128, %f2007;
-	fma.rn.f32 	%f2013, %f2009, %f2129, %f2011;
-	mul.f32 	%f2014, %f2013, 0f3FB8AA3B;
-	ex2.approx.ftz.f32 	%f2015, %f2014;
-	add.f32 	%f2016, %f2009, 0f00000000;
-	ex2.approx.f32 	%f2017, %f2016;
-	mul.f32 	%f2018, %f2015, %f2017;
-	setp.lt.f32	%p238, %f2007, 0fC2D20000;
-	selp.f32	%f2019, 0f00000000, %f2018, %p238;
-	setp.gt.f32	%p239, %f2007, 0f42D20000;
-	selp.f32	%f2385, 0f7F800000, %f2019, %p239;
-	setp.eq.f32	%p240, %f2385, 0f7F800000;
-	@%p240 bra 	BB11_198;
-
-	fma.rn.f32 	%f2385, %f2385, %f530, %f2385;
-
-BB11_198:
-	setp.lt.f32	%p241, %f456, 0f00000000;
-	and.pred  	%p9, %p241, %p36;
-	mov.b32 	 %r407, %f2385;
-	xor.b32  	%r408, %r407, -2147483648;
-	mov.b32 	 %f2020, %r408;
-	selp.f32	%f2387, %f2020, %f2385, %p9;
-	setp.eq.f32	%p243, %f456, 0f00000000;
-	@%p243 bra 	BB11_201;
-	bra.uni 	BB11_199;
-
-BB11_201:
-	add.f32 	%f2023, %f456, %f456;
-	selp.f32	%f2387, %f2023, 0f00000000, %p36;
-	bra.uni 	BB11_202;
-
-BB11_199:
-	setp.geu.f32	%p244, %f456, 0f00000000;
-	@%p244 bra 	BB11_202;
-
-	cvt.rzi.f32.f32	%f2022, %f1025;
-	setp.neu.f32	%p245, %f2022, 0f40000000;
-	selp.f32	%f2387, 0f7FFFFFFF, %f2387, %p245;
-
-BB11_202:
-	add.f32 	%f2024, %f529, 0f40000000;
-	mov.b32 	 %r409, %f2024;
-	setp.lt.s32	%p247, %r409, 2139095040;
-	@%p247 bra 	BB11_207;
-
-	setp.gtu.f32	%p248, %f529, 0f7F800000;
-	@%p248 bra 	BB11_206;
-	bra.uni 	BB11_204;
-
-BB11_206:
-	add.f32 	%f2387, %f456, 0f40000000;
-	bra.uni 	BB11_207;
-
-BB11_204:
-	setp.neu.f32	%p249, %f529, 0f7F800000;
-	@%p249 bra 	BB11_207;
-
-	selp.f32	%f2387, 0fFF800000, 0f7F800000, %p9;
-
-BB11_207:
-	setp.eq.f32	%p328, %f357, 0f00000000;
-	setp.eq.f32	%p251, %f456, 0f3F800000;
-	selp.f32	%f2026, 0f3F800000, %f2387, %p251;
-	sub.f32 	%f541, %f528, %f2026;
-	setp.eq.f32	%p252, %f480, 0f00000000;
-	and.pred  	%p254, %p252, %p328;
-	mov.pred 	%p333, -1;
-	@%p254 bra 	BB11_211;
-
-	neg.f32 	%f2027, %f541;
-	div.rn.f32 	%f2388, %f2027, %f357;
-	mul.f32 	%f2028, %f480, 0fC0800000;
-	mul.f32 	%f2029, %f2028, %f541;
-	fma.rn.f32 	%f543, %f357, %f357, %f2029;
-	setp.lt.f32	%p256, %f543, 0f00000000;
-	setp.neu.f32	%p257, %f480, 0f00000000;
-	and.pred  	%p258, %p256, %p257;
-	@%p258 bra 	BB11_209;
-	bra.uni 	BB11_210;
-
-BB11_209:
-	mov.f32 	%f2389, %f2388;
-	bra.uni 	BB11_211;
-
-BB11_210:
-	mov.b32 	 %r410, %f357;
-	and.b32  	%r411, %r410, -2147483648;
-	sqrt.rn.f32 	%f2030, %f543;
-	mov.b32 	 %r412, %f2030;
-	and.b32  	%r413, %r412, 2147483647;
-	or.b32  	%r414, %r413, %r411;
-	mov.b32 	 %f2031, %r414;
-	add.f32 	%f2032, %f357, %f2031;
-	mul.f32 	%f2033, %f2032, 0fBF000000;
-	div.rn.f32 	%f2034, %f2033, %f480;
-	div.rn.f32 	%f2035, %f541, %f2033;
-	min.f32 	%f2036, %f2034, %f2035;
-	max.f32 	%f2037, %f2034, %f2035;
-	selp.f32	%f2389, %f2388, %f2036, %p252;
-	selp.f32	%f2388, %f2388, %f2037, %p252;
-	mov.pred 	%p333, 0;
-
-BB11_211:
-	mov.u16 	%rs26, 0;
-	@%p333 bra 	BB11_215;
-
-	setp.leu.f32	%p261, %f2389, %f1016;
-	setp.geu.f32	%p262, %f2389, %f1017;
-	or.pred  	%p263, %p261, %p262;
-	@%p263 bra 	BB11_215;
-
-	fma.rn.f32 	%f548, %f2389, %f2343, %f2292;
-	setp.ltu.f32	%p264, %f548, %f1572;
-	@%p264 bra 	BB11_215;
-
-	setp.lt.f32	%p265, %f548, %f1573;
-	selp.u16	%rs26, 1, 0, %p265;
-
-BB11_215:
-	mov.f32 	%f2394, 0f7F800000;
-	mov.f32 	%f2390, %f2394;
-	@%p333 bra 	BB11_219;
-
-	setp.leu.f32	%p266, %f2388, %f1016;
-	setp.geu.f32	%p267, %f2388, %f1017;
+	mov.b32 	%f1491, %r396;
+	selp.f32 	%f2077, %f1491, %f2075, %p6;
+	setp.geu.f32 	%p116, %f348, 0f00000000;
+	@%p116 bra 	$L__BB11_108;
+
+	mov.f32 	%f1844, 0f40000000;
+	cvt.rzi.f32.f32 	%f1493, %f1844;
+	setp.eq.f32 	%p117, %f1493, 0f40000000;
+	@%p117 bra 	$L__BB11_108;
+
+	mov.f32 	%f2077, 0f7FFFFFFF;
+
+$L__BB11_108:
+	abs.f32 	%f1850, %f348;
+	add.f32 	%f1496, %f1850, 0f40000000;
+	mov.b32 	%r14, %f1496;
+	setp.lt.s32 	%p119, %r14, 2139095040;
+	@%p119 bra 	$L__BB11_113;
+
+	abs.f32 	%f1853, %f348;
+	setp.gtu.f32 	%p120, %f1853, 0f7F800000;
+	@%p120 bra 	$L__BB11_112;
+	bra.uni 	$L__BB11_110;
+
+$L__BB11_112:
+	add.f32 	%f2077, %f348, 0f40000000;
+	bra.uni 	$L__BB11_113;
+
+$L__BB11_110:
+	abs.f32 	%f1854, %f348;
+	setp.neu.f32 	%p121, %f1854, 0f7F800000;
+	@%p121 bra 	$L__BB11_113;
+
+	selp.f32 	%f2077, 0fFF800000, 0f7F800000, %p6;
+
+$L__BB11_113:
+	add.f32 	%f1852, %f2000, %f2000;
+	mul.f32 	%f1851, %f348, %f1852;
+	mov.f32 	%f1843, 0f3102E308;
+	mov.f32 	%f1842, 0fBF317218;
+	mov.f32 	%f1841, 0f3FB8AA3B;
+	mov.f32 	%f1840, 0f00000000;
+	mov.f32 	%f1839, 0f35BFBE8E;
+	mov.f32 	%f1838, 0f3F317200;
+	mov.f32 	%f1837, 0f3DAAAABD;
+	mov.f32 	%f1836, 0f3C4CAF63;
+	mov.f32 	%f1835, 0f3B18F0FE;
+	mov.f32 	%f1834, 0f40000000;
+	setp.eq.f32 	%p122, %f348, 0f3F800000;
+	selp.f32 	%f1498, 0f3F800000, %f2077, %p122;
+	sub.f32 	%f1499, %f413, %f1851;
+	add.f32 	%f426, %f1499, %f1498;
+	abs.f32 	%f427, %f346;
+	setp.lt.f32 	%p123, %f427, 0f00800000;
+	mul.f32 	%f1500, %f427, 0f4B800000;
+	selp.f32 	%f1501, %f1500, %f427, %p123;
+	selp.f32 	%f1502, 0fC3170000, 0fC2FE0000, %p123;
+	mov.b32 	%r397, %f1501;
+	and.b32  	%r398, %r397, 8388607;
+	or.b32  	%r399, %r398, 1065353216;
+	mov.b32 	%f1503, %r399;
+	shr.u32 	%r400, %r397, 23;
+	cvt.rn.f32.u32 	%f1504, %r400;
+	add.f32 	%f1505, %f1502, %f1504;
+	setp.gt.f32 	%p124, %f1503, 0f3FB504F3;
+	mul.f32 	%f1506, %f1503, 0f3F000000;
+	add.f32 	%f1507, %f1505, 0f3F800000;
+	selp.f32 	%f1508, %f1507, %f1505, %p124;
+	selp.f32 	%f1509, %f1506, %f1503, %p124;
+	add.f32 	%f1510, %f1509, 0fBF800000;
+	add.f32 	%f1511, %f1509, 0f3F800000;
+	rcp.approx.ftz.f32 	%f1512, %f1511;
+	add.f32 	%f1513, %f1510, %f1510;
+	mul.f32 	%f1515, %f1513, %f1512;
+	mul.f32 	%f1516, %f1515, %f1515;
+	fma.rn.f32 	%f1519, %f1835, %f1516, %f1836;
+	fma.rn.f32 	%f1521, %f1519, %f1516, %f1837;
+	mul.rn.f32 	%f1522, %f1521, %f1516;
+	mul.rn.f32 	%f1523, %f1522, %f1515;
+	sub.f32 	%f1524, %f1510, %f1515;
+	add.f32 	%f1525, %f1524, %f1524;
+	neg.f32 	%f1526, %f1515;
+	fma.rn.f32 	%f1527, %f1526, %f1510, %f1525;
+	mul.rn.f32 	%f1528, %f1512, %f1527;
+	add.f32 	%f1529, %f1523, %f1515;
+	sub.f32 	%f1530, %f1515, %f1529;
+	add.f32 	%f1531, %f1523, %f1530;
+	add.f32 	%f1532, %f1528, %f1531;
+	add.f32 	%f1533, %f1529, %f1532;
+	sub.f32 	%f1534, %f1529, %f1533;
+	add.f32 	%f1535, %f1532, %f1534;
+	mul.rn.f32 	%f1537, %f1508, %f1838;
+	mul.rn.f32 	%f1539, %f1508, %f1839;
+	add.f32 	%f1540, %f1537, %f1533;
+	sub.f32 	%f1541, %f1537, %f1540;
+	add.f32 	%f1542, %f1533, %f1541;
+	add.f32 	%f1543, %f1535, %f1542;
+	add.f32 	%f1544, %f1539, %f1543;
+	add.f32 	%f1545, %f1540, %f1544;
+	sub.f32 	%f1546, %f1540, %f1545;
+	add.f32 	%f1547, %f1544, %f1546;
+	mul.rn.f32 	%f1548, %f1834, %f1545;
+	neg.f32 	%f1549, %f1548;
+	fma.rn.f32 	%f1550, %f1834, %f1545, %f1549;
+	fma.rn.f32 	%f1551, %f1834, %f1547, %f1550;
+	fma.rn.f32 	%f1553, %f1840, %f1545, %f1551;
+	add.rn.f32 	%f1554, %f1548, %f1553;
+	neg.f32 	%f1555, %f1554;
+	add.rn.f32 	%f1556, %f1548, %f1555;
+	add.rn.f32 	%f1557, %f1556, %f1553;
+	mov.b32 	%r401, %f1554;
+	setp.eq.s32 	%p125, %r401, 1118925336;
+	add.s32 	%r402, %r401, -1;
+	mov.b32 	%f1558, %r402;
+	add.f32 	%f1559, %f1557, 0f37000000;
+	selp.f32 	%f428, %f1559, %f1557, %p125;
+	selp.f32 	%f1560, %f1558, %f1554, %p125;
+	mul.rn.f32 	%f1562, %f1560, %f1841;
+	cvt.rzi.f32.f32 	%f1563, %f1562;
+	abs.f32 	%f1564, %f1563;
+	setp.gt.f32 	%p126, %f1564, 0f42FC0000;
+	mov.b32 	%r403, %f1563;
+	and.b32  	%r404, %r403, -2147483648;
+	or.b32  	%r405, %r404, 1123811328;
+	mov.b32 	%f1565, %r405;
+	selp.f32 	%f1566, %f1565, %f1563, %p126;
+	fma.rn.f32 	%f1568, %f1566, %f1842, %f1560;
+	fma.rn.f32 	%f1570, %f1566, %f1843, %f1568;
+	mul.f32 	%f1571, %f1570, 0f3FB8AA3B;
+	add.f32 	%f1572, %f1566, 0f4B40007F;
+	mov.b32 	%r406, %f1572;
+	shl.b32 	%r407, %r406, 23;
+	mov.b32 	%f1573, %r407;
+	ex2.approx.ftz.f32 	%f1574, %f1571;
+	mul.f32 	%f429, %f1574, %f1573;
+	setp.eq.f32 	%p127, %f429, 0f7F800000;
+	mov.f32 	%f2078, 0f7F800000;
+	@%p127 bra 	$L__BB11_115;
+
+	fma.rn.f32 	%f2078, %f429, %f428, %f429;
+
+$L__BB11_115:
+	setp.lt.f32 	%p128, %f346, 0f00000000;
+	and.pred  	%p7, %p128, %p39;
+	setp.eq.f32 	%p130, %f346, 0f00000000;
+	@%p130 bra 	$L__BB11_119;
+	bra.uni 	$L__BB11_116;
+
+$L__BB11_119:
+	add.f32 	%f1579, %f346, %f346;
+	selp.f32 	%f2080, %f1579, 0f00000000, %p39;
+	bra.uni 	$L__BB11_120;
+
+$L__BB11_116:
+	mov.b32 	%r408, %f2078;
+	xor.b32  	%r409, %r408, -2147483648;
+	mov.b32 	%f1575, %r409;
+	selp.f32 	%f2080, %f1575, %f2078, %p7;
+	setp.geu.f32 	%p131, %f346, 0f00000000;
+	@%p131 bra 	$L__BB11_120;
+
+	mov.f32 	%f1821, 0f40000000;
+	cvt.rzi.f32.f32 	%f1577, %f1821;
+	setp.eq.f32 	%p132, %f1577, 0f40000000;
+	@%p132 bra 	$L__BB11_120;
+
+	mov.f32 	%f2080, 0f7FFFFFFF;
+
+$L__BB11_120:
+	abs.f32 	%f1769, %f346;
+	add.f32 	%f1580, %f1769, 0f40000000;
+	mov.b32 	%r410, %f1580;
+	setp.lt.s32 	%p134, %r410, 2139095040;
+	@%p134 bra 	$L__BB11_125;
+
+	abs.f32 	%f1819, %f346;
+	setp.gtu.f32 	%p135, %f1819, 0f7F800000;
+	@%p135 bra 	$L__BB11_124;
+	bra.uni 	$L__BB11_122;
+
+$L__BB11_124:
+	add.f32 	%f2080, %f346, 0f40000000;
+	bra.uni 	$L__BB11_125;
+
+$L__BB11_122:
+	abs.f32 	%f1820, %f346;
+	setp.neu.f32 	%p136, %f1820, 0f7F800000;
+	@%p136 bra 	$L__BB11_125;
+
+	selp.f32 	%f2080, 0fFF800000, 0f7F800000, %p7;
+
+$L__BB11_125:
+	setp.eq.f32 	%p138, %f346, 0f3F800000;
+	selp.f32 	%f1582, 0f3F800000, %f2080, %p138;
+	sub.f32 	%f438, %f426, %f1582;
+	setp.eq.f32 	%p139, %f375, 0f00000000;
+	setp.eq.f32 	%p140, %f372, 0f00000000;
+	and.pred  	%p141, %p139, %p140;
+	mov.pred 	%p316, 0;
+	@%p141 bra 	$L__BB11_128;
+
+	neg.f32 	%f1583, %f438;
+	div.rn.f32 	%f2081, %f1583, %f375;
+	mul.f32 	%f1584, %f372, 0fC0800000;
+	mul.f32 	%f1585, %f1584, %f438;
+	fma.rn.f32 	%f440, %f375, %f375, %f1585;
+	setp.neu.f32 	%p143, %f372, 0f00000000;
+	setp.lt.f32 	%p144, %f440, 0f00000000;
+	and.pred  	%p145, %p144, %p143;
+	mov.f32 	%f2082, %f2081;
+	@%p145 bra 	$L__BB11_128;
+
+	mov.b32 	%r411, %f375;
+	and.b32  	%r412, %r411, -2147483648;
+	sqrt.rn.f32 	%f1586, %f440;
+	mov.b32 	%r413, %f1586;
+	and.b32  	%r414, %r413, 2147483647;
+	or.b32  	%r415, %r414, %r412;
+	mov.b32 	%f1587, %r415;
+	add.f32 	%f1588, %f375, %f1587;
+	mul.f32 	%f1589, %f1588, 0fBF000000;
+	div.rn.f32 	%f1590, %f1589, %f372;
+	div.rn.f32 	%f1591, %f438, %f1589;
+	min.f32 	%f1592, %f1590, %f1591;
+	max.f32 	%f1593, %f1590, %f1591;
+	selp.f32 	%f2082, %f2081, %f1592, %p140;
+	selp.f32 	%f2081, %f2081, %f1593, %p140;
+	mov.pred 	%p316, -1;
+
+$L__BB11_128:
+	ld.v2.f32 	{%f1594, %f1595}, [%rd16+16];
+	setp.lt.f32 	%p148, %f2082, %f986;
+	setp.gt.f32 	%p149, %f2082, %f985;
+	and.pred  	%p150, %p149, %p148;
+	and.pred  	%p151, %p316, %p150;
+	mov.u16 	%rs9, 0;
+	not.pred 	%p152, %p151;
+	@%p152 bra 	$L__BB11_131;
+
+	fma.rn.f32 	%f447, %f2082, %f2059, %f2001;
+	setp.ltu.f32 	%p153, %f447, %f1594;
+	@%p153 bra 	$L__BB11_131;
+
+	setp.lt.f32 	%p154, %f447, %f1595;
+	selp.u16 	%rs9, 1, 0, %p154;
+
+$L__BB11_131:
+	setp.lt.f32 	%p156, %f2081, %f986;
+	setp.gt.f32 	%p157, %f2081, %f985;
+	and.pred  	%p158, %p157, %p156;
+	and.pred  	%p159, %p158, %p316;
+	mov.pred 	%p317, 0;
+	not.pred 	%p160, %p159;
+	@%p160 bra 	$L__BB11_134;
+
+	fma.rn.f32 	%f448, %f2081, %f2059, %f2001;
+	setp.ltu.f32 	%p162, %f448, %f1594;
+	@%p162 bra 	$L__BB11_134;
+
+	setp.lt.f32 	%p317, %f448, %f1595;
+
+$L__BB11_134:
+	setp.eq.f32 	%p276, %f352, 0f7F800000;
+	selp.f32 	%f1597, %f2081, 0f7F800000, %p317;
+	mov.f32 	%f2083, 0f7F800000;
+	setp.eq.s16 	%p163, %rs9, 0;
+	selp.f32 	%f449, %f1597, %f2082, %p163;
+	ld.f32 	%f450, [%rd16+4];
+	@%p276 bra 	$L__BB11_136;
+
+	fma.rn.f32 	%f2083, %f352, %f351, %f352;
+
+$L__BB11_136:
+	setp.eq.f32 	%p277, %f2057, 0f00000000;
+	@%p277 bra 	$L__BB11_140;
+	bra.uni 	$L__BB11_137;
+
+$L__BB11_140:
+	add.f32 	%f1602, %f2057, %f2057;
+	selp.f32 	%f2085, %f1602, 0f00000000, %p39;
+	bra.uni 	$L__BB11_141;
+
+$L__BB11_137:
+	mov.b32 	%r416, %f2083;
+	xor.b32  	%r417, %r416, -2147483648;
+	mov.b32 	%f1598, %r417;
+	selp.f32 	%f2085, %f1598, %f2083, %p1;
+	setp.geu.f32 	%p166, %f2057, 0f00000000;
+	@%p166 bra 	$L__BB11_141;
+
+	mov.f32 	%f1818, 0f40000000;
+	cvt.rzi.f32.f32 	%f1600, %f1818;
+	setp.eq.f32 	%p167, %f1600, 0f40000000;
+	@%p167 bra 	$L__BB11_141;
+
+	mov.f32 	%f2085, 0f7FFFFFFF;
+
+$L__BB11_141:
+	abs.f32 	%f1771, %f2057;
+	add.f32 	%f1770, %f1771, 0f40000000;
+	mov.b32 	%r449, %f1770;
+	setp.lt.s32 	%p278, %r449, 2139095040;
+	@%p278 bra 	$L__BB11_146;
+
+	abs.f32 	%f1816, %f2057;
+	setp.gtu.f32 	%p170, %f1816, 0f7F800000;
+	@%p170 bra 	$L__BB11_145;
+	bra.uni 	$L__BB11_143;
+
+$L__BB11_145:
+	add.f32 	%f2085, %f2057, 0f40000000;
+	bra.uni 	$L__BB11_146;
+
+$L__BB11_143:
+	abs.f32 	%f1817, %f2057;
+	setp.neu.f32 	%p171, %f1817, 0f7F800000;
+	@%p171 bra 	$L__BB11_146;
+
+	selp.f32 	%f2085, 0fFF800000, 0f7F800000, %p1;
+
+$L__BB11_146:
+	setp.eq.f32 	%p279, %f363, 0f7F800000;
+	mov.f32 	%f2086, 0f7F800000;
+	@%p279 bra 	$L__BB11_148;
+
+	fma.rn.f32 	%f2086, %f363, %f362, %f363;
+
+$L__BB11_148:
+	setp.eq.f32 	%p280, %f2058, 0f00000000;
+	@%p280 bra 	$L__BB11_152;
+	bra.uni 	$L__BB11_149;
+
+$L__BB11_152:
+	add.f32 	%f1608, %f2058, %f2058;
+	selp.f32 	%f2088, %f1608, 0f00000000, %p39;
+	bra.uni 	$L__BB11_153;
+
+$L__BB11_149:
+	setp.lt.f32 	%p301, %f2058, 0f00000000;
+	and.pred  	%p300, %p301, %p39;
+	mov.b32 	%r418, %f2086;
+	xor.b32  	%r419, %r418, -2147483648;
+	mov.b32 	%f1604, %r419;
+	selp.f32 	%f2088, %f1604, %f2086, %p300;
+	setp.geu.f32 	%p174, %f2058, 0f00000000;
+	@%p174 bra 	$L__BB11_153;
+
+	mov.f32 	%f1815, 0f40000000;
+	cvt.rzi.f32.f32 	%f1606, %f1815;
+	setp.eq.f32 	%p175, %f1606, 0f40000000;
+	@%p175 bra 	$L__BB11_153;
+
+	mov.f32 	%f2088, 0f7FFFFFFF;
+
+$L__BB11_153:
+	abs.f32 	%f1773, %f2058;
+	add.f32 	%f1772, %f1773, 0f40000000;
+	mov.b32 	%r450, %f1772;
+	setp.lt.s32 	%p281, %r450, 2139095040;
+	@%p281 bra 	$L__BB11_158;
+
+	abs.f32 	%f1813, %f2058;
+	setp.gtu.f32 	%p178, %f1813, 0f7F800000;
+	@%p178 bra 	$L__BB11_157;
+	bra.uni 	$L__BB11_155;
+
+$L__BB11_157:
+	add.f32 	%f2088, %f2058, 0f40000000;
+	bra.uni 	$L__BB11_158;
+
+$L__BB11_155:
+	abs.f32 	%f1814, %f2058;
+	setp.neu.f32 	%p179, %f1814, 0f7F800000;
+	@%p179 bra 	$L__BB11_158;
+
+	setp.lt.f32 	%p311, %f2058, 0f00000000;
+	and.pred  	%p310, %p311, %p39;
+	selp.f32 	%f2088, 0fFF800000, 0f7F800000, %p310;
+
+$L__BB11_158:
+	setp.eq.f32 	%p284, %f378, 0f7F800000;
+	setp.eq.f32 	%p283, %f2057, 0f3F800000;
+	setp.eq.f32 	%p282, %f2058, 0f3F800000;
+	selp.f32 	%f1610, 0f3F800000, %f2088, %p282;
+	selp.f32 	%f1611, 0f3F800000, %f2085, %p283;
+	add.f32 	%f467, %f1611, %f1610;
+	mov.f32 	%f2089, 0f7F800000;
+	@%p284 bra 	$L__BB11_160;
+
+	fma.rn.f32 	%f2089, %f378, %f377, %f378;
+
+$L__BB11_160:
+	setp.eq.f32 	%p285, %f1999, 0f00000000;
+	@%p285 bra 	$L__BB11_164;
+	bra.uni 	$L__BB11_161;
+
+$L__BB11_164:
+	add.f32 	%f1812, %f1999, %f1999;
+	selp.f32 	%f2091, %f1812, 0f00000000, %p39;
+	bra.uni 	$L__BB11_165;
+
+$L__BB11_161:
+	setp.lt.f32 	%p303, %f1999, 0f00000000;
+	and.pred  	%p302, %p303, %p39;
+	mov.b32 	%r420, %f2089;
+	xor.b32  	%r421, %r420, -2147483648;
+	mov.b32 	%f1612, %r421;
+	selp.f32 	%f2091, %f1612, %f2089, %p302;
+	setp.geu.f32 	%p184, %f1999, 0f00000000;
+	@%p184 bra 	$L__BB11_165;
+
+	mov.f32 	%f1811, 0f40000000;
+	cvt.rzi.f32.f32 	%f1614, %f1811;
+	setp.eq.f32 	%p185, %f1614, 0f40000000;
+	@%p185 bra 	$L__BB11_165;
+
+	mov.f32 	%f2091, 0f7FFFFFFF;
+
+$L__BB11_165:
+	abs.f32 	%f1775, %f1999;
+	add.f32 	%f1774, %f1775, 0f40000000;
+	mov.b32 	%r451, %f1774;
+	setp.lt.s32 	%p286, %r451, 2139095040;
+	@%p286 bra 	$L__BB11_170;
+
+	abs.f32 	%f1809, %f1999;
+	setp.gtu.f32 	%p188, %f1809, 0f7F800000;
+	@%p188 bra 	$L__BB11_169;
+	bra.uni 	$L__BB11_167;
+
+$L__BB11_169:
+	add.f32 	%f2091, %f1999, 0f40000000;
+	bra.uni 	$L__BB11_170;
+
+$L__BB11_167:
+	abs.f32 	%f1810, %f1999;
+	setp.neu.f32 	%p189, %f1810, 0f7F800000;
+	@%p189 bra 	$L__BB11_170;
+
+	setp.lt.f32 	%p309, %f1999, 0f00000000;
+	and.pred  	%p308, %p309, %p39;
+	selp.f32 	%f2091, 0fFF800000, 0f7F800000, %p308;
+
+$L__BB11_170:
+	setp.eq.f32 	%p288, %f390, 0f7F800000;
+	setp.eq.f32 	%p287, %f1999, 0f3F800000;
+	add.f32 	%f1777, %f1999, %f1999;
+	mul.f32 	%f1776, %f347, %f1777;
+	selp.f32 	%f1618, 0f3F800000, %f2091, %p287;
+	sub.f32 	%f476, %f1618, %f1776;
+	mov.f32 	%f2092, 0f7F800000;
+	@%p288 bra 	$L__BB11_172;
+
+	fma.rn.f32 	%f2092, %f390, %f389, %f390;
+
+$L__BB11_172:
+	setp.eq.f32 	%p289, %f347, 0f00000000;
+	@%p289 bra 	$L__BB11_176;
+	bra.uni 	$L__BB11_173;
+
+$L__BB11_176:
+	add.f32 	%f1808, %f347, %f347;
+	selp.f32 	%f2094, %f1808, 0f00000000, %p39;
+	bra.uni 	$L__BB11_177;
+
+$L__BB11_173:
+	setp.lt.f32 	%p305, %f347, 0f00000000;
+	and.pred  	%p304, %p305, %p39;
+	mov.b32 	%r422, %f2092;
+	xor.b32  	%r423, %r422, -2147483648;
+	mov.b32 	%f1619, %r423;
+	selp.f32 	%f2094, %f1619, %f2092, %p304;
+	setp.geu.f32 	%p193, %f347, 0f00000000;
+	@%p193 bra 	$L__BB11_177;
+
+	mov.f32 	%f1807, 0f40000000;
+	cvt.rzi.f32.f32 	%f1621, %f1807;
+	setp.eq.f32 	%p194, %f1621, 0f40000000;
+	@%p194 bra 	$L__BB11_177;
+
+	mov.f32 	%f2094, 0f7FFFFFFF;
+
+$L__BB11_177:
+	abs.f32 	%f1779, %f347;
+	add.f32 	%f1778, %f1779, 0f40000000;
+	mov.b32 	%r452, %f1778;
+	setp.lt.s32 	%p290, %r452, 2139095040;
+	@%p290 bra 	$L__BB11_182;
+
+	abs.f32 	%f1805, %f347;
+	setp.gtu.f32 	%p197, %f1805, 0f7F800000;
+	@%p197 bra 	$L__BB11_181;
+	bra.uni 	$L__BB11_179;
+
+$L__BB11_181:
+	add.f32 	%f2094, %f347, 0f40000000;
+	bra.uni 	$L__BB11_182;
+
+$L__BB11_179:
+	abs.f32 	%f1806, %f347;
+	setp.neu.f32 	%p198, %f1806, 0f7F800000;
+	@%p198 bra 	$L__BB11_182;
+
+	setp.lt.f32 	%p307, %f347, 0f00000000;
+	and.pred  	%p306, %p307, %p39;
+	selp.f32 	%f2094, 0fFF800000, 0f7F800000, %p306;
+
+$L__BB11_182:
+	setp.eq.f32 	%p292, %f404, 0f7F800000;
+	setp.eq.f32 	%p291, %f347, 0f3F800000;
+	selp.f32 	%f1625, 0f3F800000, %f2094, %p291;
+	add.f32 	%f485, %f476, %f1625;
+	mov.f32 	%f2095, 0f7F800000;
+	@%p292 bra 	$L__BB11_184;
+
+	fma.rn.f32 	%f2095, %f404, %f403, %f404;
+
+$L__BB11_184:
+	setp.eq.f32 	%p293, %f2000, 0f00000000;
+	@%p293 bra 	$L__BB11_188;
+	bra.uni 	$L__BB11_185;
+
+$L__BB11_188:
+	add.f32 	%f1804, %f2000, %f2000;
+	selp.f32 	%f2097, %f1804, 0f00000000, %p39;
+	bra.uni 	$L__BB11_189;
+
+$L__BB11_185:
+	setp.lt.f32 	%p313, %f2000, 0f00000000;
+	and.pred  	%p312, %p313, %p39;
+	mov.b32 	%r424, %f2095;
+	xor.b32  	%r425, %r424, -2147483648;
+	mov.b32 	%f1626, %r425;
+	selp.f32 	%f2097, %f1626, %f2095, %p312;
+	setp.geu.f32 	%p202, %f2000, 0f00000000;
+	@%p202 bra 	$L__BB11_189;
+
+	mov.f32 	%f1803, 0f40000000;
+	cvt.rzi.f32.f32 	%f1628, %f1803;
+	setp.eq.f32 	%p203, %f1628, 0f40000000;
+	@%p203 bra 	$L__BB11_189;
+
+	mov.f32 	%f2097, 0f7FFFFFFF;
+
+$L__BB11_189:
+	abs.f32 	%f1781, %f2000;
+	add.f32 	%f1780, %f1781, 0f40000000;
+	mov.b32 	%r453, %f1780;
+	setp.lt.s32 	%p294, %r453, 2139095040;
+	@%p294 bra 	$L__BB11_194;
+
+	abs.f32 	%f1801, %f2000;
+	setp.gtu.f32 	%p206, %f1801, 0f7F800000;
+	@%p206 bra 	$L__BB11_193;
+	bra.uni 	$L__BB11_191;
+
+$L__BB11_193:
+	add.f32 	%f2097, %f2000, 0f40000000;
+	bra.uni 	$L__BB11_194;
+
+$L__BB11_191:
+	abs.f32 	%f1802, %f2000;
+	setp.neu.f32 	%p207, %f1802, 0f7F800000;
+	@%p207 bra 	$L__BB11_194;
+
+	setp.lt.f32 	%p315, %f2000, 0f00000000;
+	and.pred  	%p314, %p315, %p39;
+	selp.f32 	%f2097, 0fFF800000, 0f7F800000, %p314;
+
+$L__BB11_194:
+	setp.eq.f32 	%p296, %f417, 0f7F800000;
+	setp.eq.f32 	%p295, %f2000, 0f3F800000;
+	add.f32 	%f1783, %f2000, %f2000;
+	mul.f32 	%f1782, %f348, %f1783;
+	selp.f32 	%f1632, 0f3F800000, %f2097, %p295;
+	add.f32 	%f1633, %f485, %f1632;
+	sub.f32 	%f494, %f1633, %f1782;
+	mov.f32 	%f2098, 0f7F800000;
+	@%p296 bra 	$L__BB11_196;
+
+	fma.rn.f32 	%f2098, %f417, %f416, %f417;
+
+$L__BB11_196:
+	setp.eq.f32 	%p297, %f348, 0f00000000;
+	@%p297 bra 	$L__BB11_200;
+	bra.uni 	$L__BB11_197;
+
+$L__BB11_200:
+	add.f32 	%f1800, %f348, %f348;
+	selp.f32 	%f2100, %f1800, 0f00000000, %p39;
+	bra.uni 	$L__BB11_201;
+
+$L__BB11_197:
+	mov.b32 	%r426, %f2098;
+	xor.b32  	%r427, %r426, -2147483648;
+	mov.b32 	%f1634, %r427;
+	selp.f32 	%f2100, %f1634, %f2098, %p6;
+	setp.geu.f32 	%p211, %f348, 0f00000000;
+	@%p211 bra 	$L__BB11_201;
+
+	mov.f32 	%f1799, 0f40000000;
+	cvt.rzi.f32.f32 	%f1636, %f1799;
+	setp.eq.f32 	%p212, %f1636, 0f40000000;
+	@%p212 bra 	$L__BB11_201;
+
+	mov.f32 	%f2100, 0f7FFFFFFF;
+
+$L__BB11_201:
+	abs.f32 	%f1785, %f348;
+	add.f32 	%f1784, %f1785, 0f40000000;
+	mov.b32 	%r454, %f1784;
+	setp.lt.s32 	%p298, %r454, 2139095040;
+	@%p298 bra 	$L__BB11_206;
+
+	abs.f32 	%f1797, %f348;
+	setp.gtu.f32 	%p215, %f1797, 0f7F800000;
+	@%p215 bra 	$L__BB11_205;
+	bra.uni 	$L__BB11_203;
+
+$L__BB11_205:
+	add.f32 	%f2100, %f348, 0f40000000;
+	bra.uni 	$L__BB11_206;
+
+$L__BB11_203:
+	abs.f32 	%f1798, %f348;
+	setp.neu.f32 	%p216, %f1798, 0f7F800000;
+	@%p216 bra 	$L__BB11_206;
+
+	selp.f32 	%f2100, 0fFF800000, 0f7F800000, %p6;
+
+$L__BB11_206:
+	setp.eq.f32 	%p299, %f348, 0f3F800000;
+	mov.f32 	%f1795, 0f3102E308;
+	mov.f32 	%f1794, 0fBF317218;
+	mov.f32 	%f1793, 0f3FB8AA3B;
+	mov.f32 	%f1792, 0f00000000;
+	mov.f32 	%f1791, 0f35BFBE8E;
+	mov.f32 	%f1790, 0f3F317200;
+	mov.f32 	%f1789, 0f3DAAAABD;
+	mov.f32 	%f1788, 0f3C4CAF63;
+	mov.f32 	%f1787, 0f3B18F0FE;
+	mov.f32 	%f1786, 0f40000000;
+	selp.f32 	%f1640, 0f3F800000, %f2100, %p299;
+	add.f32 	%f503, %f494, %f1640;
+	abs.f32 	%f504, %f450;
+	setp.lt.f32 	%p218, %f504, 0f00800000;
+	mul.f32 	%f1641, %f504, 0f4B800000;
+	selp.f32 	%f1642, %f1641, %f504, %p218;
+	selp.f32 	%f1643, 0fC3170000, 0fC2FE0000, %p218;
+	mov.b32 	%r428, %f1642;
+	and.b32  	%r429, %r428, 8388607;
+	or.b32  	%r430, %r429, 1065353216;
+	mov.b32 	%f1644, %r430;
+	shr.u32 	%r431, %r428, 23;
+	cvt.rn.f32.u32 	%f1645, %r431;
+	add.f32 	%f1646, %f1643, %f1645;
+	setp.gt.f32 	%p219, %f1644, 0f3FB504F3;
+	mul.f32 	%f1647, %f1644, 0f3F000000;
+	add.f32 	%f1648, %f1646, 0f3F800000;
+	selp.f32 	%f1649, %f1648, %f1646, %p219;
+	selp.f32 	%f1650, %f1647, %f1644, %p219;
+	add.f32 	%f1651, %f1650, 0fBF800000;
+	add.f32 	%f1652, %f1650, 0f3F800000;
+	rcp.approx.ftz.f32 	%f1653, %f1652;
+	add.f32 	%f1654, %f1651, %f1651;
+	mul.f32 	%f1656, %f1654, %f1653;
+	mul.f32 	%f1657, %f1656, %f1656;
+	fma.rn.f32 	%f1660, %f1787, %f1657, %f1788;
+	fma.rn.f32 	%f1662, %f1660, %f1657, %f1789;
+	mul.rn.f32 	%f1663, %f1662, %f1657;
+	mul.rn.f32 	%f1664, %f1663, %f1656;
+	sub.f32 	%f1665, %f1651, %f1656;
+	add.f32 	%f1666, %f1665, %f1665;
+	neg.f32 	%f1667, %f1656;
+	fma.rn.f32 	%f1668, %f1667, %f1651, %f1666;
+	mul.rn.f32 	%f1669, %f1653, %f1668;
+	add.f32 	%f1670, %f1664, %f1656;
+	sub.f32 	%f1671, %f1656, %f1670;
+	add.f32 	%f1672, %f1664, %f1671;
+	add.f32 	%f1673, %f1669, %f1672;
+	add.f32 	%f1674, %f1670, %f1673;
+	sub.f32 	%f1675, %f1670, %f1674;
+	add.f32 	%f1676, %f1673, %f1675;
+	mul.rn.f32 	%f1678, %f1649, %f1790;
+	mul.rn.f32 	%f1680, %f1649, %f1791;
+	add.f32 	%f1681, %f1678, %f1674;
+	sub.f32 	%f1682, %f1678, %f1681;
+	add.f32 	%f1683, %f1674, %f1682;
+	add.f32 	%f1684, %f1676, %f1683;
+	add.f32 	%f1685, %f1680, %f1684;
+	add.f32 	%f1686, %f1681, %f1685;
+	sub.f32 	%f1687, %f1681, %f1686;
+	add.f32 	%f1688, %f1685, %f1687;
+	mul.rn.f32 	%f1689, %f1786, %f1686;
+	neg.f32 	%f1690, %f1689;
+	fma.rn.f32 	%f1691, %f1786, %f1686, %f1690;
+	fma.rn.f32 	%f1692, %f1786, %f1688, %f1691;
+	fma.rn.f32 	%f1694, %f1792, %f1686, %f1692;
+	add.rn.f32 	%f1695, %f1689, %f1694;
+	neg.f32 	%f1696, %f1695;
+	add.rn.f32 	%f1697, %f1689, %f1696;
+	add.rn.f32 	%f1698, %f1697, %f1694;
+	mov.b32 	%r432, %f1695;
+	setp.eq.s32 	%p220, %r432, 1118925336;
+	add.s32 	%r433, %r432, -1;
+	mov.b32 	%f1699, %r433;
+	add.f32 	%f1700, %f1698, 0f37000000;
+	selp.f32 	%f505, %f1700, %f1698, %p220;
+	selp.f32 	%f1701, %f1699, %f1695, %p220;
+	mul.rn.f32 	%f1703, %f1701, %f1793;
+	cvt.rzi.f32.f32 	%f1704, %f1703;
+	abs.f32 	%f1705, %f1704;
+	setp.gt.f32 	%p221, %f1705, 0f42FC0000;
+	mov.b32 	%r434, %f1704;
+	and.b32  	%r435, %r434, -2147483648;
+	or.b32  	%r436, %r435, 1123811328;
+	mov.b32 	%f1706, %r436;
+	selp.f32 	%f1707, %f1706, %f1704, %p221;
+	fma.rn.f32 	%f1709, %f1707, %f1794, %f1701;
+	fma.rn.f32 	%f1711, %f1707, %f1795, %f1709;
+	mul.f32 	%f1712, %f1711, 0f3FB8AA3B;
+	add.f32 	%f1713, %f1707, 0f4B40007F;
+	mov.b32 	%r437, %f1713;
+	shl.b32 	%r438, %r437, 23;
+	mov.b32 	%f1714, %r438;
+	ex2.approx.ftz.f32 	%f1715, %f1712;
+	mul.f32 	%f506, %f1715, %f1714;
+	setp.eq.f32 	%p222, %f506, 0f7F800000;
+	mov.f32 	%f2101, 0f7F800000;
+	@%p222 bra 	$L__BB11_208;
+
+	fma.rn.f32 	%f2101, %f506, %f505, %f506;
+
+$L__BB11_208:
+	setp.lt.f32 	%p223, %f450, 0f00000000;
+	and.pred  	%p11, %p223, %p39;
+	setp.eq.f32 	%p225, %f450, 0f00000000;
+	@%p225 bra 	$L__BB11_212;
+	bra.uni 	$L__BB11_209;
+
+$L__BB11_212:
+	add.f32 	%f1720, %f450, %f450;
+	selp.f32 	%f2103, %f1720, 0f00000000, %p39;
+	bra.uni 	$L__BB11_213;
+
+$L__BB11_209:
+	mov.b32 	%r439, %f2101;
+	xor.b32  	%r440, %r439, -2147483648;
+	mov.b32 	%f1716, %r440;
+	selp.f32 	%f2103, %f1716, %f2101, %p11;
+	setp.geu.f32 	%p226, %f450, 0f00000000;
+	@%p226 bra 	$L__BB11_213;
+
+	mov.f32 	%f1796, 0f40000000;
+	cvt.rzi.f32.f32 	%f1718, %f1796;
+	setp.eq.f32 	%p227, %f1718, 0f40000000;
+	@%p227 bra 	$L__BB11_213;
+
+	mov.f32 	%f2103, 0f7FFFFFFF;
+
+$L__BB11_213:
+	add.f32 	%f1721, %f504, 0f40000000;
+	mov.b32 	%r441, %f1721;
+	setp.lt.s32 	%p229, %r441, 2139095040;
+	@%p229 bra 	$L__BB11_218;
+
+	setp.gtu.f32 	%p230, %f504, 0f7F800000;
+	@%p230 bra 	$L__BB11_217;
+	bra.uni 	$L__BB11_215;
+
+$L__BB11_217:
+	add.f32 	%f2103, %f450, 0f40000000;
+	bra.uni 	$L__BB11_218;
+
+$L__BB11_215:
+	setp.neu.f32 	%p231, %f504, 0f7F800000;
+	@%p231 bra 	$L__BB11_218;
+
+	selp.f32 	%f2103, 0fFF800000, 0f7F800000, %p11;
+
+$L__BB11_218:
+	setp.eq.f32 	%p233, %f450, 0f3F800000;
+	selp.f32 	%f1723, 0f3F800000, %f2103, %p233;
+	sub.f32 	%f515, %f503, %f1723;
+	setp.eq.f32 	%p234, %f467, 0f00000000;
+	and.pred  	%p236, %p139, %p234;
+	mov.pred 	%p318, 0;
+	@%p236 bra 	$L__BB11_221;
+
+	neg.f32 	%f1724, %f515;
+	div.rn.f32 	%f2104, %f1724, %f375;
+	mul.f32 	%f1725, %f467, 0fC0800000;
+	mul.f32 	%f1726, %f1725, %f515;
+	fma.rn.f32 	%f517, %f375, %f375, %f1726;
+	setp.neu.f32 	%p238, %f467, 0f00000000;
+	setp.lt.f32 	%p239, %f517, 0f00000000;
+	and.pred  	%p240, %p239, %p238;
+	mov.f32 	%f2105, %f2104;
+	@%p240 bra 	$L__BB11_221;
+
+	mov.b32 	%r442, %f375;
+	and.b32  	%r443, %r442, -2147483648;
+	sqrt.rn.f32 	%f1727, %f517;
+	mov.b32 	%r444, %f1727;
+	and.b32  	%r445, %r444, 2147483647;
+	or.b32  	%r446, %r445, %r443;
+	mov.b32 	%f1728, %r446;
+	add.f32 	%f1729, %f375, %f1728;
+	mul.f32 	%f1730, %f1729, 0fBF000000;
+	div.rn.f32 	%f1731, %f1730, %f467;
+	div.rn.f32 	%f1732, %f515, %f1730;
+	min.f32 	%f1733, %f1731, %f1732;
+	max.f32 	%f1734, %f1731, %f1732;
+	selp.f32 	%f2105, %f2104, %f1733, %p234;
+	selp.f32 	%f2104, %f2104, %f1734, %p234;
+	mov.pred 	%p318, -1;
+
+$L__BB11_221:
+	setp.lt.f32 	%p243, %f2105, %f986;
+	setp.gt.f32 	%p244, %f2105, %f985;
+	and.pred  	%p245, %p244, %p243;
+	and.pred  	%p246, %p318, %p245;
+	mov.u16 	%rs10, 0;
+	not.pred 	%p247, %p246;
+	@%p247 bra 	$L__BB11_224;
+
+	fma.rn.f32 	%f522, %f2105, %f2059, %f2001;
+	setp.ltu.f32 	%p248, %f522, %f1594;
+	@%p248 bra 	$L__BB11_224;
+
+	setp.lt.f32 	%p249, %f522, %f1595;
+	selp.u16 	%rs10, 1, 0, %p249;
+
+$L__BB11_224:
+	setp.lt.f32 	%p250, %f2104, %f986;
+	setp.gt.f32 	%p251, %f2104, %f985;
+	and.pred  	%p252, %p251, %p250;
+	and.pred  	%p253, %p252, %p318;
+	not.pred 	%p254, %p253;
+	mov.f32 	%f2108, 0f7F800000;
+	mov.f32 	%f2106, %f2108;
+	@%p254 bra 	$L__BB11_227;
+
+	fma.rn.f32 	%f523, %f2104, %f2059, %f2001;
+	setp.ltu.f32 	%p255, %f523, %f1594;
+	@%p255 bra 	$L__BB11_227;
+
+	setp.lt.f32 	%p256, %f523, %f1595;
+	selp.f32 	%f2106, %f2104, 0f7F800000, %p256;
+
+$L__BB11_227:
+	add.f32 	%f526, %f347, 0f00000000;
+	add.f32 	%f527, %f348, 0f00000000;
+	ld.f32 	%f528, [%rd16+-8];
+	add.f32 	%f529, %f528, 0f00000000;
+	setp.eq.s16 	%p257, %rs10, 0;
+	selp.f32 	%f530, %f2106, %f2105, %p257;
+	setp.eq.f32 	%p258, %f2059, 0f00000000;
+	mov.f32 	%f2107, %f2108;
+	@%p258 bra 	$L__BB11_229;
+
+	sub.f32 	%f1738, %f529, %f2001;
+	div.rn.f32 	%f2107, %f1738, %f2059;
+
+$L__BB11_229:
+	setp.gtu.f32 	%p259, %f2107, %f986;
+	setp.ltu.f32 	%p260, %f2107, %f985;
+	or.pred  	%p261, %p259, %p260;
+	selp.f32 	%f1740, 0f7F800000, %f2107, %p261;
+	fma.rn.f32 	%f1741, %f1740, %f2057, %f1999;
+	fma.rn.f32 	%f1742, %f1740, %f2058, %f2000;
+	fma.rn.f32 	%f1743, %f1740, %f2059, %f2001;
+	sub.f32 	%f1744, %f526, %f1741;
+	sub.f32 	%f1745, %f527, %f1742;
+	sub.f32 	%f1746, %f529, %f1743;
+	mul.f32 	%f1747, %f1744, %f1744;
+	fma.rn.f32 	%f1748, %f1745, %f1745, %f1747;
+	fma.rn.f32 	%f1749, %f1746, %f1746, %f1748;
+	sqrt.rn.f32 	%f1750, %f1749;
+	setp.ge.f32 	%p262, %f1750, %f346;
+	setp.le.f32 	%p263, %f1750, %f450;
+	and.pred  	%p264, %p263, %p262;
+	selp.f32 	%f533, %f1740, 0f7F800000, %p264;
+	ld.f32 	%f1751, [%rd16+12];
+	add.f32 	%f534, %f528, %f1751;
+	@%p258 bra 	$L__BB11_231;
+
+	sub.f32 	%f1752, %f534, %f2001;
+	div.rn.f32 	%f2108, %f1752, %f2059;
+
+$L__BB11_231:
+	setp.gtu.f32 	%p266, %f2108, %f986;
+	setp.ltu.f32 	%p267, %f2108, %f985;
 	or.pred  	%p268, %p266, %p267;
-	mov.f32 	%f2390, %f2394;
-	@%p268 bra 	BB11_219;
-
-	fma.rn.f32 	%f549, %f2388, %f2343, %f2292;
-	setp.ltu.f32	%p269, %f549, %f1572;
-	mov.f32 	%f2390, %f2394;
-	@%p269 bra 	BB11_219;
-
-	setp.lt.f32	%p270, %f549, %f1573;
-	selp.f32	%f2390, %f2388, 0f7F800000, %p270;
-
-BB11_219:
-	setp.eq.s16	%p271, %rs26, 0;
-	selp.f32	%f552, %f2390, %f2389, %p271;
-	add.f32 	%f553, %f1020, 0f00000000;
-	add.f32 	%f554, %f1021, 0f00000000;
-	ld.f32 	%f555, [%rd1+296];
-	add.f32 	%f556, %f555, 0f00000000;
-	setp.eq.f32	%p272, %f2343, 0f00000000;
-	mov.f32 	%f2391, %f2394;
-	@%p272 bra 	BB11_221;
-
-	sub.f32 	%f2042, %f556, %f2292;
-	div.rn.f32 	%f2391, %f2042, %f2343;
-
-BB11_221:
-	setp.gtu.f32	%p273, %f2391, %f1017;
-	setp.ltu.f32	%p274, %f2391, %f1016;
-	or.pred  	%p275, %p274, %p273;
-	selp.f32	%f559, 0f7F800000, %f2391, %p275;
-	fma.rn.f32 	%f2044, %f559, %f2341, %f2294;
-	fma.rn.f32 	%f2045, %f559, %f2342, %f2293;
-	fma.rn.f32 	%f2046, %f559, %f2343, %f2292;
-	sub.f32 	%f2047, %f553, %f2044;
-	sub.f32 	%f2048, %f554, %f2045;
-	sub.f32 	%f2049, %f556, %f2046;
-	mul.f32 	%f2050, %f2047, %f2047;
-	fma.rn.f32 	%f2051, %f2048, %f2048, %f2050;
-	fma.rn.f32 	%f2052, %f2049, %f2049, %f2051;
-	sqrt.rn.f32 	%f560, %f2052;
-	setp.ltu.f32	%p276, %f560, %f315;
-	mov.f32 	%f2392, %f2394;
-	@%p276 bra 	BB11_223;
-
-	setp.le.f32	%p277, %f560, %f456;
-	selp.f32	%f2392, %f559, 0f7F800000, %p277;
-
-BB11_223:
-	ld.f32 	%f2054, [%rd1+316];
-	add.f32 	%f563, %f555, %f2054;
-	mov.f32 	%f2393, %f2394;
-	@%p272 bra 	BB11_225;
-
-	sub.f32 	%f2055, %f563, %f2292;
-	div.rn.f32 	%f2393, %f2055, %f2343;
-
-BB11_225:
-	setp.gtu.f32	%p279, %f2393, %f1017;
-	setp.ltu.f32	%p280, %f2393, %f1016;
-	or.pred  	%p281, %p280, %p279;
-	selp.f32	%f566, 0f7F800000, %f2393, %p281;
-	fma.rn.f32 	%f2057, %f566, %f2341, %f2294;
-	fma.rn.f32 	%f2058, %f566, %f2342, %f2293;
-	fma.rn.f32 	%f2059, %f566, %f2343, %f2292;
-	sub.f32 	%f2060, %f553, %f2057;
-	sub.f32 	%f2061, %f554, %f2058;
-	sub.f32 	%f2062, %f563, %f2059;
-	mul.f32 	%f2063, %f2060, %f2060;
-	fma.rn.f32 	%f2064, %f2061, %f2061, %f2063;
-	fma.rn.f32 	%f2065, %f2062, %f2062, %f2064;
-	sqrt.rn.f32 	%f567, %f2065;
-	setp.ltu.f32	%p282, %f567, %f315;
-	@%p282 bra 	BB11_227;
-
-	setp.le.f32	%p283, %f567, %f456;
-	selp.f32	%f2394, %f566, 0f7F800000, %p283;
-
-BB11_227:
-	setp.lt.f32	%p284, %f455, %f552;
-	selp.f32	%f2066, %f455, %f552, %p284;
-	setp.geu.f32	%p285, %f2066, %f2392;
-	selp.f32	%f2067, %f2392, %f2066, %p285;
-	setp.geu.f32	%p286, %f2067, %f2394;
-	selp.f32	%f570, %f2394, %f2067, %p286;
-	setp.eq.f32	%p287, %f570, 0f7F800000;
-	@%p287 bra 	BB11_229;
-
-	mov.u32 	%r416, 254;
-	// inline asm
-	call (%r415), _optix_report_intersection_0, (%f570, %r416);
-	// inline asm
-
-BB11_229:
+	selp.f32 	%f1753, 0f7F800000, %f2108, %p268;
+	fma.rn.f32 	%f1754, %f1753, %f2057, %f1999;
+	fma.rn.f32 	%f1755, %f1753, %f2058, %f2000;
+	fma.rn.f32 	%f1756, %f1753, %f2059, %f2001;
+	sub.f32 	%f1757, %f526, %f1754;
+	sub.f32 	%f1758, %f527, %f1755;
+	sub.f32 	%f1759, %f534, %f1756;
+	mul.f32 	%f1760, %f1757, %f1757;
+	fma.rn.f32 	%f1761, %f1758, %f1758, %f1760;
+	fma.rn.f32 	%f1762, %f1759, %f1759, %f1761;
+	sqrt.rn.f32 	%f1763, %f1762;
+	setp.ge.f32 	%p269, %f1763, %f346;
+	setp.le.f32 	%p270, %f1763, %f450;
+	and.pred  	%p271, %p270, %p269;
+	selp.f32 	%f1764, %f1753, 0f7F800000, %p271;
+	setp.lt.f32 	%p272, %f449, %f530;
+	selp.f32 	%f1765, %f449, %f530, %p272;
+	setp.lt.f32 	%p273, %f1765, %f533;
+	selp.f32 	%f1766, %f1765, %f533, %p273;
+	setp.lt.f32 	%p274, %f1766, %f1764;
+	selp.f32 	%f537, %f1766, %f1764, %p274;
+	setp.eq.f32 	%p275, %f537, 0f7F800000;
+	@%p275 bra 	$L__BB11_233;
+
+	mov.u32 	%r448, 254;
+	// begin inline asm
+	call (%r447), _optix_report_intersection_0, (%f537, %r448);
+	// end inline asm
+
+$L__BB11_233:
 	ret;
-}
 
+}
 	// .globl	__closesthit__cylhollow
-.visible .entry __closesthit__cylhollow(
-
-)
+.visible .entry __closesthit__cylhollow()
 {
-	.reg .pred 	%p<55>;
-	.reg .b16 	%rs<18>;
-	.reg .f32 	%f<1924>;
-	.reg .b32 	%r<635>;
-	.reg .b64 	%rd<647>;
-
-
-	// inline asm
-	call (%r19), _optix_get_launch_dimension_x, ();
-	// inline asm
-	// inline asm
-	call (%r20), _optix_get_launch_dimension_y, ();
-	// inline asm
-	// inline asm
-	call (%r22), _optix_get_launch_index_x, ();
-	// inline asm
-	// inline asm
-	call (%r23), _optix_get_launch_index_y, ();
-	// inline asm
-	// inline asm
-	call (%r24), _optix_get_launch_index_z, ();
-	// inline asm
-	mad.lo.s32 	%r25, %r24, %r20, %r23;
-	mad.lo.s32 	%r1, %r25, %r19, %r22;
+	.reg .pred 	%p<58>;
+	.reg .b16 	%rs<2>;
+	.reg .f32 	%f<1985>;
+	.reg .b32 	%r<647>;
+	.reg .b64 	%rd<634>;
+
+
+	// begin inline asm
+	call (%r23), _optix_get_launch_dimension_x, ();
+	// end inline asm
+	// begin inline asm
+	call (%r24), _optix_get_launch_dimension_y, ();
+	// end inline asm
+	// begin inline asm
+	call (%r26), _optix_get_launch_index_x, ();
+	// end inline asm
+	// begin inline asm
+	call (%r27), _optix_get_launch_index_y, ();
+	// end inline asm
+	// begin inline asm
+	call (%r28), _optix_get_launch_index_z, ();
+	// end inline asm
+	mad.lo.s32 	%r29, %r28, %r24, %r27;
+	mad.lo.s32 	%r1, %r29, %r23, %r26;
 	ld.const.u64 	%rd1, [params+352];
-	setp.eq.s64	%p1, %rd1, 0;
-	@%p1 bra 	BB12_2;
+	setp.eq.s64 	%p1, %rd1, 0;
+	@%p1 bra 	$L__BB12_2;
 
-	cvta.to.global.u64 	%rd46, %rd1;
-	cvt.u64.u32	%rd47, %r1;
-	add.s64 	%rd48, %rd46, %rd47;
+	cvta.to.global.u64 	%rd42, %rd1;
+	cvt.u64.u32 	%rd43, %r1;
+	add.s64 	%rd44, %rd42, %rd43;
 	mov.u16 	%rs1, 1;
-	st.global.u8 	[%rd48], %rs1;
-	bra.uni 	BB12_112;
-
-BB12_2:
-	// inline asm
-	call (%rd49), _optix_get_sbt_data_ptr_64, ();
-	// inline asm
-	ld.u64 	%rd3, [%rd49+8];
-	// inline asm
-	call (%f667), _optix_get_world_ray_origin_x, ();
-	// inline asm
-	// inline asm
-	call (%f668), _optix_get_world_ray_origin_y, ();
-	// inline asm
-	// inline asm
-	call (%f1723), _optix_get_world_ray_origin_z, ();
-	// inline asm
-	// inline asm
-	call (%r26), _optix_get_transform_list_size, ();
-	// inline asm
-	setp.eq.s32	%p2, %r26, 0;
-	@%p2 bra 	BB12_3;
-
-	mov.u32 	%r631, 0;
-	// inline asm
-	call (%f670), _optix_get_ray_time, ();
-	// inline asm
-
-BB12_5:
+	st.global.u8 	[%rd44], %rs1;
+	bra.uni 	$L__BB12_112;
+
+$L__BB12_2:
+	// begin inline asm
+	call (%rd45), _optix_get_sbt_data_ptr_64, ();
+	// end inline asm
+	ld.u64 	%rd3, [%rd45+8];
+	// begin inline asm
+	call (%f1775), _optix_get_world_ray_origin_x, ();
+	// end inline asm
+	// begin inline asm
+	call (%f1776), _optix_get_world_ray_origin_y, ();
+	// end inline asm
+	// begin inline asm
+	call (%f1777), _optix_get_world_ray_origin_z, ();
+	// end inline asm
+	// begin inline asm
+	call (%r30), _optix_get_transform_list_size, ();
+	// end inline asm
+	setp.eq.s32 	%p2, %r30, 0;
+	@%p2 bra 	$L__BB12_23;
+
+	// begin inline asm
+	call (%r31), _optix_get_transform_list_size, ();
+	// end inline asm
+	// begin inline asm
+	call (%f704), _optix_get_ray_time, ();
+	// end inline asm
+	setp.eq.s32 	%p3, %r31, 0;
+	@%p3 bra 	$L__BB12_21;
+
+	mov.u32 	%r642, 0;
+
+$L__BB12_5:
 	.pragma "nounroll";
-	// inline asm
-	call (%rd50), _optix_get_transform_list_handle, (%r631);
-	// inline asm
-	// inline asm
-	call (%r29), _optix_get_transform_type_from_handle, (%rd50);
-	// inline asm
-	and.b32  	%r30, %r29, -2;
-	setp.eq.s32	%p3, %r30, 2;
-	@%p3 bra 	BB12_11;
-	bra.uni 	BB12_6;
-
-BB12_11:
-	setp.eq.s32	%p6, %r29, 2;
-	@%p6 bra 	BB12_15;
-	bra.uni 	BB12_12;
-
-BB12_15:
-	// inline asm
-	call (%rd124), _optix_get_matrix_motion_transform_from_handle, (%rd50);
-	// inline asm
-	// inline asm
-	cvta.to.global.u64 %rd126, %rd124;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r118,%r119,%r120,%r121}, [%rd126];
-	// inline asm
-	mov.b32	{%rs4, %rs5}, %r120;
-	add.s64 	%rd130, %rd124, 16;
-	// inline asm
+	// begin inline asm
+	call (%rd46), _optix_get_transform_list_handle, (%r642);
+	// end inline asm
+	// begin inline asm
+	call (%r34), _optix_get_transform_type_from_handle, (%rd46);
+	// end inline asm
+	or.b32  	%r35, %r34, 1;
+	setp.eq.s32 	%p4, %r35, 3;
+	@%p4 bra 	$L__BB12_11;
+	bra.uni 	$L__BB12_6;
+
+$L__BB12_11:
+	setp.eq.s32 	%p7, %r34, 2;
+	@%p7 bra 	$L__BB12_15;
+	bra.uni 	$L__BB12_12;
+
+$L__BB12_15:
+	// begin inline asm
+	call (%rd118), _optix_get_matrix_motion_transform_from_handle, (%rd46);
+	// end inline asm
+	// begin inline asm
+	cvta.to.global.u64 %rd120, %rd118;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r123,%r124,%r125,%r126}, [%rd120];
+	// end inline asm
+	add.s64 	%rd124, %rd118, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd123, %rd124;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r127,%r128,%r129,%r130}, [%rd123];
+	// end inline asm
+	add.s64 	%rd127, %rd118, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd126, %rd127;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r131,%r132,%r133,%r134}, [%rd126];
+	// end inline asm
+	add.s64 	%rd130, %rd118, 48;
+	// begin inline asm
 	cvta.to.global.u64 %rd129, %rd130;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r122,%r123,%r124,%r125}, [%rd129];
-	// inline asm
-	add.s64 	%rd133, %rd124, 32;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r135,%r136,%r137,%r138}, [%rd129];
+	// end inline asm
+	add.s64 	%rd133, %rd118, 64;
+	// begin inline asm
 	cvta.to.global.u64 %rd132, %rd133;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r126,%r127,%r128,%r129}, [%rd132];
-	// inline asm
-	add.s64 	%rd136, %rd124, 48;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r139,%r140,%r141,%r142}, [%rd132];
+	// end inline asm
+	add.s64 	%rd136, %rd118, 80;
+	// begin inline asm
 	cvta.to.global.u64 %rd135, %rd136;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r130,%r131,%r132,%r133}, [%rd135];
-	// inline asm
-	add.s64 	%rd139, %rd124, 64;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r143,%r144,%r145,%r146}, [%rd135];
+	// end inline asm
+	add.s64 	%rd139, %rd118, 96;
+	// begin inline asm
 	cvta.to.global.u64 %rd138, %rd139;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r134,%r135,%r136,%r137}, [%rd138];
-	// inline asm
-	add.s64 	%rd142, %rd124, 80;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r147,%r148,%r149,%r150}, [%rd138];
+	// end inline asm
+	add.s64 	%rd142, %rd118, 112;
+	// begin inline asm
 	cvta.to.global.u64 %rd141, %rd142;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r138,%r139,%r140,%r141}, [%rd141];
-	// inline asm
-	add.s64 	%rd145, %rd124, 96;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r151,%r152,%r153,%r154}, [%rd141];
+	// end inline asm
+	mov.b32 	%f832, %r126;
+	mov.b32 	%f833, %r127;
+	and.b32  	%r167, %r125, 65535;
+	add.s32 	%r168, %r167, -1;
+	cvt.rn.f32.s32 	%f834, %r168;
+	sub.f32 	%f835, %f704, %f832;
+	mul.f32 	%f836, %f835, %f834;
+	sub.f32 	%f837, %f833, %f832;
+	div.rn.f32 	%f838, %f836, %f837;
+	min.f32 	%f839, %f834, %f838;
+	mov.f32 	%f840, 0f00000000;
+	max.f32 	%f841, %f840, %f839;
+	cvt.rmi.f32.f32 	%f842, %f841;
+	sub.f32 	%f90, %f841, %f842;
+	cvt.rzi.s32.f32 	%r169, %f842;
+	mul.wide.s32 	%rd153, %r169, 48;
+	add.s64 	%rd145, %rd127, %rd153;
+	// begin inline asm
 	cvta.to.global.u64 %rd144, %rd145;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r142,%r143,%r144,%r145}, [%rd144];
-	// inline asm
-	add.s64 	%rd148, %rd124, 112;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r155,%r156,%r157,%r158}, [%rd144];
+	// end inline asm
+	mov.b32 	%f1730, %r155;
+	mov.b32 	%f1729, %r156;
+	mov.b32 	%f1728, %r157;
+	mov.b32 	%f1727, %r158;
+	add.s64 	%rd148, %rd145, 16;
+	// begin inline asm
 	cvta.to.global.u64 %rd147, %rd148;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r146,%r147,%r148,%r149}, [%rd147];
-	// inline asm
-	mov.b32 	 %f797, %r121;
-	mov.b32 	 %f798, %r122;
-	cvt.u32.u16	%r162, %rs4;
-	add.s32 	%r163, %r162, -1;
-	cvt.rn.f32.s32	%f799, %r163;
-	sub.f32 	%f800, %f670, %f797;
-	mul.f32 	%f801, %f800, %f799;
-	sub.f32 	%f802, %f798, %f797;
-	div.rn.f32 	%f803, %f801, %f802;
-	min.f32 	%f804, %f799, %f803;
-	mov.f32 	%f805, 0f00000000;
-	max.f32 	%f806, %f805, %f804;
-	cvt.rmi.f32.f32	%f807, %f806;
-	cvt.rzi.s32.f32	%r164, %f807;
-	mul.wide.s32 	%rd159, %r164, 48;
-	add.s64 	%rd151, %rd133, %rd159;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r159,%r160,%r161,%r162}, [%rd147];
+	// end inline asm
+	mov.b32 	%f1734, %r159;
+	mov.b32 	%f1733, %r160;
+	mov.b32 	%f1732, %r161;
+	mov.b32 	%f1731, %r162;
+	add.s64 	%rd151, %rd145, 32;
+	// begin inline asm
 	cvta.to.global.u64 %rd150, %rd151;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r150,%r151,%r152,%r153}, [%rd150];
-	// inline asm
-	mov.b32 	 %f1690, %r150;
-	mov.b32 	 %f1689, %r151;
-	mov.b32 	 %f1688, %r152;
-	mov.b32 	 %f1687, %r153;
-	add.s64 	%rd154, %rd151, 16;
-	// inline asm
-	cvta.to.global.u64 %rd153, %rd154;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r154,%r155,%r156,%r157}, [%rd153];
-	// inline asm
-	mov.b32 	 %f1694, %r154;
-	mov.b32 	 %f1693, %r155;
-	mov.b32 	 %f1692, %r156;
-	mov.b32 	 %f1691, %r157;
-	add.s64 	%rd157, %rd151, 32;
-	// inline asm
-	cvta.to.global.u64 %rd156, %rd157;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r158,%r159,%r160,%r161}, [%rd156];
-	// inline asm
-	sub.f32 	%f98, %f806, %f807;
-	mov.b32 	 %f1698, %r158;
-	mov.b32 	 %f1697, %r159;
-	mov.b32 	 %f1696, %r160;
-	mov.b32 	 %f1695, %r161;
-	setp.leu.f32	%p8, %f98, 0f00000000;
-	@%p8 bra 	BB12_17;
-
-	cvt.rmi.f32.f32	%f1658, %f806;
-	cvt.rzi.s32.f32	%r630, %f1658;
-	cvt.s64.s32	%rd642, %r630;
-	mul.lo.s64 	%rd169, %rd642, 48;
-	add.s64 	%rd170, %rd124, %rd169;
-	add.s64 	%rd161, %rd170, 80;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r163,%r164,%r165,%r166}, [%rd150];
+	// end inline asm
+	mov.b32 	%f1738, %r163;
+	mov.b32 	%f1737, %r164;
+	mov.b32 	%f1736, %r165;
+	mov.b32 	%f1735, %r166;
+	setp.leu.f32 	%p9, %f90, 0f00000000;
+	@%p9 bra 	$L__BB12_17;
+
+	cvt.rmi.f32.f32 	%f1698, %f841;
+	cvt.rzi.s32.f32 	%r641, %f1698;
+	cvt.s64.s32 	%rd629, %r641;
+	mov.f32 	%f843, 0f3F800000;
+	sub.f32 	%f844, %f843, %f90;
+	mul.lo.s64 	%rd163, %rd629, 48;
+	add.s64 	%rd164, %rd118, %rd163;
+	add.s64 	%rd155, %rd164, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd154, %rd155;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r170,%r171,%r172,%r173}, [%rd154];
+	// end inline asm
+	mov.b32 	%f845, %r170;
+	mov.b32 	%f846, %r171;
+	mov.b32 	%f847, %r172;
+	mov.b32 	%f848, %r173;
+	mul.f32 	%f849, %f90, %f845;
+	mul.f32 	%f850, %f90, %f846;
+	mul.f32 	%f851, %f90, %f847;
+	mul.f32 	%f852, %f90, %f848;
+	fma.rn.f32 	%f1730, %f844, %f1730, %f849;
+	fma.rn.f32 	%f1729, %f844, %f1729, %f850;
+	fma.rn.f32 	%f1728, %f844, %f1728, %f851;
+	fma.rn.f32 	%f1727, %f844, %f1727, %f852;
+	add.s64 	%rd158, %rd164, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd157, %rd158;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r174,%r175,%r176,%r177}, [%rd157];
+	// end inline asm
+	mov.b32 	%f853, %r174;
+	mov.b32 	%f854, %r175;
+	mov.b32 	%f855, %r176;
+	mov.b32 	%f856, %r177;
+	mul.f32 	%f857, %f90, %f853;
+	mul.f32 	%f858, %f90, %f854;
+	mul.f32 	%f859, %f90, %f855;
+	mul.f32 	%f860, %f90, %f856;
+	fma.rn.f32 	%f1734, %f844, %f1734, %f857;
+	fma.rn.f32 	%f1733, %f844, %f1733, %f858;
+	fma.rn.f32 	%f1732, %f844, %f1732, %f859;
+	fma.rn.f32 	%f1731, %f844, %f1731, %f860;
+	add.s64 	%rd161, %rd164, 112;
+	// begin inline asm
 	cvta.to.global.u64 %rd160, %rd161;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r165,%r166,%r167,%r168}, [%rd160];
-	// inline asm
-	mov.b32 	 %f808, %r165;
-	mov.b32 	 %f809, %r166;
-	mov.b32 	 %f810, %r167;
-	mov.b32 	 %f811, %r168;
-	add.s64 	%rd164, %rd170, 96;
-	// inline asm
-	cvta.to.global.u64 %rd163, %rd164;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r169,%r170,%r171,%r172}, [%rd163];
-	// inline asm
-	mov.b32 	 %f812, %r169;
-	mov.b32 	 %f813, %r170;
-	mov.b32 	 %f814, %r171;
-	mov.b32 	 %f815, %r172;
-	add.s64 	%rd167, %rd170, 112;
-	// inline asm
-	cvta.to.global.u64 %rd166, %rd167;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r173,%r174,%r175,%r176}, [%rd166];
-	// inline asm
-	mov.f32 	%f816, 0f3F800000;
-	sub.f32 	%f817, %f816, %f98;
-	mul.f32 	%f818, %f98, %f808;
-	mul.f32 	%f819, %f98, %f809;
-	mul.f32 	%f820, %f98, %f810;
-	mul.f32 	%f821, %f98, %f811;
-	fma.rn.f32 	%f1690, %f817, %f1690, %f818;
-	fma.rn.f32 	%f1689, %f817, %f1689, %f819;
-	fma.rn.f32 	%f1688, %f817, %f1688, %f820;
-	fma.rn.f32 	%f1687, %f817, %f1687, %f821;
-	mul.f32 	%f822, %f98, %f812;
-	mul.f32 	%f823, %f98, %f813;
-	mul.f32 	%f824, %f98, %f814;
-	mul.f32 	%f825, %f98, %f815;
-	fma.rn.f32 	%f1694, %f817, %f1694, %f822;
-	fma.rn.f32 	%f1693, %f817, %f1693, %f823;
-	fma.rn.f32 	%f1692, %f817, %f1692, %f824;
-	fma.rn.f32 	%f1691, %f817, %f1691, %f825;
-	mov.b32 	 %f826, %r173;
-	mov.b32 	 %f827, %r174;
-	mov.b32 	 %f828, %r175;
-	mov.b32 	 %f829, %r176;
-	mul.f32 	%f830, %f98, %f826;
-	mul.f32 	%f831, %f98, %f827;
-	mul.f32 	%f832, %f98, %f828;
-	mul.f32 	%f833, %f98, %f829;
-	fma.rn.f32 	%f1698, %f817, %f1698, %f830;
-	fma.rn.f32 	%f1697, %f817, %f1697, %f831;
-	fma.rn.f32 	%f1696, %f817, %f1696, %f832;
-	fma.rn.f32 	%f1695, %f817, %f1695, %f833;
-	bra.uni 	BB12_17;
-
-BB12_6:
-	mov.f32 	%f1699, 0f00000000;
-	mov.f32 	%f1702, 0f3F800000;
-	setp.eq.s32	%p4, %r29, 4;
-	@%p4 bra 	BB12_9;
-	bra.uni 	BB12_7;
-
-BB12_9:
-	// inline asm
-	call (%rd643), _optix_get_instance_inverse_transform_from_handle, (%rd50);
-	// inline asm
-	bra.uni 	BB12_10;
-
-BB12_12:
-	// inline asm
-	call (%rd65), _optix_get_srt_motion_transform_from_handle, (%rd50);
-	// inline asm
-	// inline asm
-	cvta.to.global.u64 %rd67, %rd65;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r43,%r44,%r45,%r46}, [%rd67];
-	// inline asm
-	mov.b32	{%rs2, %rs3}, %r45;
-	add.s64 	%rd71, %rd65, 16;
-	// inline asm
-	cvta.to.global.u64 %rd70, %rd71;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r47,%r48,%r49,%r50}, [%rd70];
-	// inline asm
-	add.s64 	%rd74, %rd65, 32;
-	// inline asm
-	cvta.to.global.u64 %rd73, %rd74;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r51,%r52,%r53,%r54}, [%rd73];
-	// inline asm
-	add.s64 	%rd77, %rd65, 48;
-	// inline asm
-	cvta.to.global.u64 %rd76, %rd77;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r55,%r56,%r57,%r58}, [%rd76];
-	// inline asm
-	add.s64 	%rd80, %rd65, 64;
-	// inline asm
-	cvta.to.global.u64 %rd79, %rd80;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r59,%r60,%r61,%r62}, [%rd79];
-	// inline asm
-	add.s64 	%rd83, %rd65, 80;
-	// inline asm
-	cvta.to.global.u64 %rd82, %rd83;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r63,%r64,%r65,%r66}, [%rd82];
-	// inline asm
-	add.s64 	%rd86, %rd65, 96;
-	// inline asm
-	cvta.to.global.u64 %rd85, %rd86;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r67,%r68,%r69,%r70}, [%rd85];
-	// inline asm
-	add.s64 	%rd89, %rd65, 112;
-	// inline asm
-	cvta.to.global.u64 %rd88, %rd89;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r71,%r72,%r73,%r74}, [%rd88];
-	// inline asm
-	add.s64 	%rd92, %rd65, 128;
-	// inline asm
-	cvta.to.global.u64 %rd91, %rd92;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r75,%r76,%r77,%r78}, [%rd91];
-	// inline asm
-	add.s64 	%rd95, %rd65, 144;
-	// inline asm
-	cvta.to.global.u64 %rd94, %rd95;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r79,%r80,%r81,%r82}, [%rd94];
-	// inline asm
-	mov.b32 	 %f684, %r46;
-	mov.b32 	 %f685, %r47;
-	cvt.u32.u16	%r99, %rs2;
-	add.s32 	%r100, %r99, -1;
-	cvt.rn.f32.s32	%f686, %r100;
-	sub.f32 	%f687, %f670, %f684;
-	mul.f32 	%f688, %f687, %f686;
-	sub.f32 	%f689, %f685, %f684;
-	div.rn.f32 	%f690, %f688, %f689;
-	min.f32 	%f691, %f686, %f690;
-	mov.f32 	%f692, 0f00000000;
-	max.f32 	%f693, %f692, %f691;
-	cvt.rmi.f32.f32	%f694, %f693;
-	cvt.rzi.s32.f32	%r101, %f694;
-	mul.wide.s32 	%rd109, %r101, 64;
-	add.s64 	%rd98, %rd74, %rd109;
-	// inline asm
-	cvta.to.global.u64 %rd97, %rd98;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r83,%r84,%r85,%r86}, [%rd97];
-	// inline asm
-	mov.b32 	 %f1671, %r83;
-	mov.b32 	 %f1672, %r84;
-	mov.b32 	 %f1673, %r85;
-	mov.b32 	 %f1674, %r86;
-	add.s64 	%rd101, %rd98, 16;
-	// inline asm
-	cvta.to.global.u64 %rd100, %rd101;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r87,%r88,%r89,%r90}, [%rd100];
-	// inline asm
-	mov.b32 	 %f1675, %r87;
-	mov.b32 	 %f1676, %r88;
-	mov.b32 	 %f1677, %r89;
-	mov.b32 	 %f1678, %r90;
-	add.s64 	%rd104, %rd98, 32;
-	// inline asm
-	cvta.to.global.u64 %rd103, %rd104;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r91,%r92,%r93,%r94}, [%rd103];
-	// inline asm
-	sub.f32 	%f37, %f693, %f694;
-	mov.b32 	 %f1679, %r91;
-	mov.b32 	 %f1680, %r92;
-	mov.b32 	 %f1681, %r93;
-	mov.b32 	 %f1682, %r94;
-	add.s64 	%rd107, %rd98, 48;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r178,%r179,%r180,%r181}, [%rd160];
+	// end inline asm
+	mov.b32 	%f861, %r178;
+	mov.b32 	%f862, %r179;
+	mov.b32 	%f863, %r180;
+	mov.b32 	%f864, %r181;
+	mul.f32 	%f865, %f90, %f861;
+	mul.f32 	%f866, %f90, %f862;
+	mul.f32 	%f867, %f90, %f863;
+	mul.f32 	%f868, %f90, %f864;
+	fma.rn.f32 	%f1738, %f844, %f1738, %f865;
+	fma.rn.f32 	%f1737, %f844, %f1737, %f866;
+	fma.rn.f32 	%f1736, %f844, %f1736, %f867;
+	fma.rn.f32 	%f1735, %f844, %f1735, %f868;
+	bra.uni 	$L__BB12_17;
+
+$L__BB12_6:
+	mov.f32 	%f1739, 0f00000000;
+	mov.f32 	%f1742, 0f3F800000;
+	setp.eq.s32 	%p5, %r34, 4;
+	@%p5 bra 	$L__BB12_9;
+
+	setp.ne.s32 	%p6, %r34, 1;
+	mov.f32 	%f1740, %f1739;
+	mov.f32 	%f1741, %f1739;
+	mov.f32 	%f1743, %f1739;
+	mov.f32 	%f1744, %f1739;
+	mov.f32 	%f1745, %f1742;
+	mov.f32 	%f1746, %f1739;
+	mov.f32 	%f1747, %f1739;
+	mov.f32 	%f1748, %f1742;
+	mov.f32 	%f1749, %f1739;
+	mov.f32 	%f1750, %f1739;
+	@%p6 bra 	$L__BB12_18;
+
+	// begin inline asm
+	call (%rd48), _optix_get_static_transform_from_handle, (%rd46);
+	// end inline asm
+	add.s64 	%rd630, %rd48, 64;
+	bra.uni 	$L__BB12_10;
+
+$L__BB12_12:
+	// begin inline asm
+	call (%rd61), _optix_get_srt_motion_transform_from_handle, (%rd46);
+	// end inline asm
+	// begin inline asm
+	cvta.to.global.u64 %rd63, %rd61;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r48,%r49,%r50,%r51}, [%rd63];
+	// end inline asm
+	add.s64 	%rd67, %rd61, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd66, %rd67;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r52,%r53,%r54,%r55}, [%rd66];
+	// end inline asm
+	add.s64 	%rd70, %rd61, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd69, %rd70;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r56,%r57,%r58,%r59}, [%rd69];
+	// end inline asm
+	add.s64 	%rd73, %rd61, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd72, %rd73;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r60,%r61,%r62,%r63}, [%rd72];
+	// end inline asm
+	add.s64 	%rd76, %rd61, 64;
+	// begin inline asm
+	cvta.to.global.u64 %rd75, %rd76;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r64,%r65,%r66,%r67}, [%rd75];
+	// end inline asm
+	add.s64 	%rd79, %rd61, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd78, %rd79;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r68,%r69,%r70,%r71}, [%rd78];
+	// end inline asm
+	add.s64 	%rd82, %rd61, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd81, %rd82;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r72,%r73,%r74,%r75}, [%rd81];
+	// end inline asm
+	add.s64 	%rd85, %rd61, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd84, %rd85;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r76,%r77,%r78,%r79}, [%rd84];
+	// end inline asm
+	add.s64 	%rd88, %rd61, 128;
+	// begin inline asm
+	cvta.to.global.u64 %rd87, %rd88;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r80,%r81,%r82,%r83}, [%rd87];
+	// end inline asm
+	add.s64 	%rd91, %rd61, 144;
+	// begin inline asm
+	cvta.to.global.u64 %rd90, %rd91;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r84,%r85,%r86,%r87}, [%rd90];
+	// end inline asm
+	mov.b32 	%f719, %r51;
+	mov.b32 	%f720, %r52;
+	and.b32  	%r104, %r50, 65535;
+	add.s32 	%r105, %r104, -1;
+	cvt.rn.f32.s32 	%f721, %r105;
+	sub.f32 	%f722, %f704, %f719;
+	mul.f32 	%f723, %f722, %f721;
+	sub.f32 	%f724, %f720, %f719;
+	div.rn.f32 	%f725, %f723, %f724;
+	min.f32 	%f726, %f721, %f725;
+	mov.f32 	%f727, 0f00000000;
+	max.f32 	%f728, %f727, %f726;
+	cvt.rmi.f32.f32 	%f729, %f728;
+	sub.f32 	%f29, %f728, %f729;
+	cvt.rzi.s32.f32 	%r106, %f729;
+	mul.wide.s32 	%rd105, %r106, 64;
+	add.s64 	%rd94, %rd70, %rd105;
+	// begin inline asm
+	cvta.to.global.u64 %rd93, %rd94;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r88,%r89,%r90,%r91}, [%rd93];
+	// end inline asm
+	mov.b32 	%f1711, %r88;
+	mov.b32 	%f1712, %r89;
+	mov.b32 	%f1713, %r90;
+	mov.b32 	%f1714, %r91;
+	add.s64 	%rd97, %rd94, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd96, %rd97;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r92,%r93,%r94,%r95}, [%rd96];
+	// end inline asm
+	mov.b32 	%f1715, %r92;
+	mov.b32 	%f1716, %r93;
+	mov.b32 	%f1717, %r94;
+	mov.b32 	%f1718, %r95;
+	add.s64 	%rd100, %rd94, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd99, %rd100;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r96,%r97,%r98,%r99}, [%rd99];
+	// end inline asm
+	mov.b32 	%f1719, %r96;
+	mov.b32 	%f1720, %r97;
+	mov.b32 	%f1721, %r98;
+	mov.b32 	%f1722, %r99;
+	add.s64 	%rd103, %rd94, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd102, %rd103;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r100,%r101,%r102,%r103}, [%rd102];
+	// end inline asm
+	mov.b32 	%f1723, %r100;
+	mov.b32 	%f1724, %r101;
+	mov.b32 	%f1725, %r102;
+	mov.b32 	%f1726, %r103;
+	setp.leu.f32 	%p8, %f29, 0f00000000;
+	@%p8 bra 	$L__BB12_14;
+
+	mov.f32 	%f730, 0f3F800000;
+	sub.f32 	%f731, %f730, %f29;
+	add.s64 	%rd107, %rd94, 64;
+	// begin inline asm
 	cvta.to.global.u64 %rd106, %rd107;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r95,%r96,%r97,%r98}, [%rd106];
-	// inline asm
-	mov.b32 	 %f1683, %r95;
-	mov.b32 	 %f1684, %r96;
-	mov.b32 	 %f1685, %r97;
-	mov.b32 	 %f1686, %r98;
-	setp.leu.f32	%p7, %f37, 0f00000000;
-	@%p7 bra 	BB12_14;
-
-	cvt.rmi.f32.f32	%f1657, %f693;
-	cvt.rzi.s32.f32	%r629, %f1657;
-	cvt.s64.s32	%rd641, %r629;
-	shl.b64 	%rd122, %rd641, 6;
-	add.s64 	%rd123, %rd122, %rd65;
-	add.s64 	%rd111, %rd123, 96;
-	// inline asm
-	cvta.to.global.u64 %rd110, %rd111;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r102,%r103,%r104,%r105}, [%rd110];
-	// inline asm
-	mov.b32 	 %f695, %r102;
-	mov.b32 	 %f696, %r103;
-	mov.b32 	 %f697, %r104;
-	mov.b32 	 %f698, %r105;
-	add.s64 	%rd114, %rd123, 112;
-	// inline asm
-	cvta.to.global.u64 %rd113, %rd114;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r106,%r107,%r108,%r109}, [%rd113];
-	// inline asm
-	mov.b32 	 %f699, %r106;
-	mov.b32 	 %f700, %r107;
-	mov.b32 	 %f701, %r108;
-	mov.b32 	 %f702, %r109;
-	add.s64 	%rd117, %rd123, 128;
-	// inline asm
-	cvta.to.global.u64 %rd116, %rd117;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r110,%r111,%r112,%r113}, [%rd116];
-	// inline asm
-	mov.b32 	 %f703, %r110;
-	mov.b32 	 %f704, %r111;
-	mov.b32 	 %f705, %r112;
-	mov.b32 	 %f706, %r113;
-	add.s64 	%rd120, %rd123, 144;
-	// inline asm
-	cvta.to.global.u64 %rd119, %rd120;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r114,%r115,%r116,%r117}, [%rd119];
-	// inline asm
-	mov.f32 	%f707, 0f3F800000;
-	sub.f32 	%f708, %f707, %f37;
-	mul.f32 	%f709, %f37, %f695;
-	mul.f32 	%f710, %f37, %f696;
-	mul.f32 	%f711, %f37, %f697;
-	mul.f32 	%f712, %f37, %f698;
-	fma.rn.f32 	%f1671, %f708, %f1671, %f709;
-	fma.rn.f32 	%f1672, %f708, %f1672, %f710;
-	fma.rn.f32 	%f1673, %f708, %f1673, %f711;
-	fma.rn.f32 	%f1674, %f708, %f1674, %f712;
-	mul.f32 	%f713, %f37, %f699;
-	mul.f32 	%f714, %f37, %f700;
-	mul.f32 	%f715, %f37, %f701;
-	mul.f32 	%f716, %f37, %f702;
-	fma.rn.f32 	%f1675, %f708, %f1675, %f713;
-	fma.rn.f32 	%f1676, %f708, %f1676, %f714;
-	fma.rn.f32 	%f1677, %f708, %f1677, %f715;
-	fma.rn.f32 	%f1678, %f708, %f1678, %f716;
-	mul.f32 	%f717, %f37, %f703;
-	mul.f32 	%f718, %f37, %f704;
-	mul.f32 	%f719, %f37, %f705;
-	mul.f32 	%f720, %f37, %f706;
-	fma.rn.f32 	%f1679, %f708, %f1679, %f717;
-	fma.rn.f32 	%f721, %f708, %f1680, %f718;
-	fma.rn.f32 	%f722, %f708, %f1681, %f719;
-	fma.rn.f32 	%f723, %f708, %f1682, %f720;
-	mov.b32 	 %f724, %r114;
-	mov.b32 	 %f725, %r115;
-	mov.b32 	 %f726, %r116;
-	mov.b32 	 %f727, %r117;
-	mul.f32 	%f728, %f37, %f724;
-	mul.f32 	%f729, %f37, %f725;
-	mul.f32 	%f730, %f37, %f726;
-	mul.f32 	%f731, %f37, %f727;
-	fma.rn.f32 	%f732, %f708, %f1683, %f728;
-	fma.rn.f32 	%f1684, %f708, %f1684, %f729;
-	fma.rn.f32 	%f1685, %f708, %f1685, %f730;
-	fma.rn.f32 	%f1686, %f708, %f1686, %f731;
-	mul.f32 	%f733, %f722, %f722;
-	fma.rn.f32 	%f734, %f721, %f721, %f733;
-	fma.rn.f32 	%f735, %f723, %f723, %f734;
-	fma.rn.f32 	%f736, %f732, %f732, %f735;
-	sqrt.rn.f32 	%f737, %f736;
-	rcp.rn.f32 	%f738, %f737;
-	mul.f32 	%f1680, %f721, %f738;
-	mul.f32 	%f1681, %f722, %f738;
-	mul.f32 	%f1682, %f723, %f738;
-	mul.f32 	%f1683, %f732, %f738;
-
-BB12_14:
-	mul.f32 	%f739, %f1681, %f1681;
-	fma.rn.f32 	%f740, %f1680, %f1680, %f739;
-	fma.rn.f32 	%f741, %f1682, %f1682, %f740;
-	fma.rn.f32 	%f742, %f1683, %f1683, %f741;
-	rcp.rn.f32 	%f743, %f742;
-	mul.f32 	%f744, %f1680, %f743;
-	mul.f32 	%f745, %f1681, %f743;
-	mul.f32 	%f746, %f1682, %f743;
-	mul.f32 	%f747, %f1683, %f743;
-	mul.f32 	%f748, %f1680, %f744;
-	mul.f32 	%f749, %f1681, %f745;
-	mul.f32 	%f750, %f1682, %f746;
-	mul.f32 	%f751, %f1680, %f745;
-	mul.f32 	%f752, %f1682, %f747;
-	mul.f32 	%f753, %f1680, %f746;
-	mul.f32 	%f754, %f1681, %f747;
-	mul.f32 	%f755, %f1681, %f746;
-	mul.f32 	%f756, %f1680, %f747;
-	sub.f32 	%f757, %f748, %f749;
-	sub.f32 	%f758, %f757, %f750;
-	fma.rn.f32 	%f759, %f1683, %f747, %f758;
-	sub.f32 	%f760, %f751, %f752;
-	add.f32 	%f761, %f760, %f760;
-	add.f32 	%f762, %f753, %f754;
-	add.f32 	%f763, %f762, %f762;
-	add.f32 	%f764, %f751, %f752;
-	add.f32 	%f765, %f764, %f764;
-	sub.f32 	%f766, %f749, %f748;
-	sub.f32 	%f767, %f766, %f750;
-	fma.rn.f32 	%f768, %f1683, %f747, %f767;
-	sub.f32 	%f769, %f755, %f756;
-	add.f32 	%f770, %f769, %f769;
-	sub.f32 	%f771, %f753, %f754;
-	add.f32 	%f772, %f771, %f771;
-	add.f32 	%f773, %f755, %f756;
-	add.f32 	%f774, %f773, %f773;
-	neg.f32 	%f775, %f748;
-	sub.f32 	%f776, %f775, %f749;
-	add.f32 	%f777, %f750, %f776;
-	fma.rn.f32 	%f778, %f1683, %f747, %f777;
-	mul.f32 	%f779, %f1674, %f759;
-	fma.rn.f32 	%f780, %f1677, %f761, %f779;
-	fma.rn.f32 	%f781, %f1679, %f763, %f780;
-	sub.f32 	%f1687, %f1684, %f781;
-	mul.f32 	%f782, %f1677, %f768;
-	fma.rn.f32 	%f783, %f1674, %f765, %f782;
-	fma.rn.f32 	%f784, %f1679, %f770, %f783;
-	sub.f32 	%f1691, %f1685, %f784;
-	mul.f32 	%f785, %f1677, %f774;
-	fma.rn.f32 	%f786, %f1674, %f772, %f785;
-	fma.rn.f32 	%f787, %f1679, %f778, %f786;
-	sub.f32 	%f1695, %f1686, %f787;
-	mul.f32 	%f788, %f1673, %f759;
-	fma.rn.f32 	%f789, %f1676, %f761, %f788;
-	fma.rn.f32 	%f1688, %f1678, %f763, %f789;
-	mul.f32 	%f790, %f1676, %f768;
-	fma.rn.f32 	%f791, %f1673, %f765, %f790;
-	fma.rn.f32 	%f1692, %f1678, %f770, %f791;
-	mul.f32 	%f792, %f1676, %f774;
-	fma.rn.f32 	%f793, %f1673, %f772, %f792;
-	fma.rn.f32 	%f1696, %f1678, %f778, %f793;
-	mul.f32 	%f794, %f1672, %f759;
-	fma.rn.f32 	%f1689, %f1675, %f761, %f794;
-	mul.f32 	%f795, %f1675, %f768;
-	fma.rn.f32 	%f1693, %f1672, %f765, %f795;
-	mul.f32 	%f796, %f1675, %f774;
-	fma.rn.f32 	%f1697, %f1672, %f772, %f796;
-	mul.f32 	%f1690, %f1671, %f759;
-	mul.f32 	%f1694, %f1671, %f765;
-	mul.f32 	%f1698, %f1671, %f772;
-
-BB12_17:
-	mul.f32 	%f834, %f1692, %f1697;
-	mul.f32 	%f835, %f1693, %f1696;
-	sub.f32 	%f836, %f835, %f834;
-	mul.f32 	%f837, %f1690, %f836;
-	mul.f32 	%f838, %f1692, %f1698;
-	mul.f32 	%f839, %f1694, %f1696;
-	sub.f32 	%f840, %f839, %f838;
-	mul.f32 	%f841, %f1689, %f840;
-	sub.f32 	%f842, %f837, %f841;
-	mul.f32 	%f843, %f1693, %f1698;
-	mul.f32 	%f844, %f1694, %f1697;
-	sub.f32 	%f845, %f844, %f843;
-	fma.rn.f32 	%f846, %f1688, %f845, %f842;
-	rcp.rn.f32 	%f847, %f846;
-	mul.f32 	%f1702, %f847, %f836;
-	mul.f32 	%f848, %f1689, %f1696;
-	mul.f32 	%f849, %f1688, %f1697;
-	sub.f32 	%f850, %f849, %f848;
-	mul.f32 	%f1701, %f847, %f850;
-	mul.f32 	%f851, %f1688, %f1693;
-	mul.f32 	%f852, %f1689, %f1692;
-	sub.f32 	%f853, %f852, %f851;
-	mul.f32 	%f1700, %f853, %f847;
-	sub.f32 	%f854, %f838, %f839;
-	mul.f32 	%f1706, %f847, %f854;
-	mul.f32 	%f855, %f1688, %f1698;
-	mul.f32 	%f856, %f1690, %f1696;
-	sub.f32 	%f857, %f856, %f855;
-	mul.f32 	%f1705, %f847, %f857;
-	mul.f32 	%f858, %f1690, %f1692;
-	mul.f32 	%f859, %f1688, %f1694;
-	sub.f32 	%f860, %f859, %f858;
-	mul.f32 	%f1704, %f860, %f847;
-	mul.f32 	%f1710, %f847, %f845;
-	mul.f32 	%f861, %f1690, %f1697;
-	mul.f32 	%f862, %f1689, %f1698;
-	sub.f32 	%f863, %f862, %f861;
-	mul.f32 	%f1709, %f847, %f863;
-	mul.f32 	%f864, %f1689, %f1694;
-	mul.f32 	%f865, %f1690, %f1693;
-	sub.f32 	%f866, %f865, %f864;
-	mul.f32 	%f1708, %f866, %f847;
-	mul.f32 	%f867, %f1687, %f1702;
-	neg.f32 	%f868, %f867;
-	mul.f32 	%f869, %f1691, %f1701;
-	sub.f32 	%f870, %f868, %f869;
-	mul.f32 	%f871, %f1695, %f1700;
-	sub.f32 	%f1699, %f870, %f871;
-	mul.f32 	%f872, %f1687, %f1706;
-	neg.f32 	%f873, %f872;
-	mul.f32 	%f874, %f1691, %f1705;
-	sub.f32 	%f875, %f873, %f874;
-	mul.f32 	%f876, %f1695, %f1704;
-	sub.f32 	%f1703, %f875, %f876;
-	mul.f32 	%f877, %f1687, %f1710;
-	neg.f32 	%f878, %f877;
-	mul.f32 	%f879, %f1691, %f1709;
-	sub.f32 	%f880, %f878, %f879;
-	mul.f32 	%f881, %f1695, %f1708;
-	sub.f32 	%f1707, %f880, %f881;
-	bra.uni 	BB12_18;
-
-BB12_7:
-	setp.ne.s32	%p5, %r29, 1;
-	mov.f32 	%f1700, %f1699;
-	mov.f32 	%f1701, %f1699;
-	mov.f32 	%f1703, %f1699;
-	mov.f32 	%f1704, %f1699;
-	mov.f32 	%f1705, %f1702;
-	mov.f32 	%f1706, %f1699;
-	mov.f32 	%f1707, %f1699;
-	mov.f32 	%f1708, %f1702;
-	mov.f32 	%f1709, %f1699;
-	mov.f32 	%f1710, %f1699;
-	@%p5 bra 	BB12_18;
-
-	// inline asm
-	call (%rd52), _optix_get_static_transform_from_handle, (%rd50);
-	// inline asm
-	add.s64 	%rd643, %rd52, 64;
-
-BB12_10:
-	// inline asm
-	cvta.to.global.u64 %rd56, %rd643;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r31,%r32,%r33,%r34}, [%rd56];
-	// inline asm
-	mov.b32 	 %f1702, %r31;
-	mov.b32 	 %f1701, %r32;
-	mov.b32 	 %f1700, %r33;
-	mov.b32 	 %f1699, %r34;
-	add.s64 	%rd60, %rd643, 16;
-	// inline asm
-	cvta.to.global.u64 %rd59, %rd60;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r35,%r36,%r37,%r38}, [%rd59];
-	// inline asm
-	mov.b32 	 %f1706, %r35;
-	mov.b32 	 %f1705, %r36;
-	mov.b32 	 %f1704, %r37;
-	mov.b32 	 %f1703, %r38;
-	add.s64 	%rd63, %rd643, 32;
-	// inline asm
-	cvta.to.global.u64 %rd62, %rd63;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r39,%r40,%r41,%r42}, [%rd62];
-	// inline asm
-	mov.b32 	 %f1710, %r39;
-	mov.b32 	 %f1709, %r40;
-	mov.b32 	 %f1708, %r41;
-	mov.b32 	 %f1707, %r42;
-
-BB12_18:
-	setp.eq.s32	%p9, %r631, 0;
-	@%p9 bra 	BB12_19;
-	bra.uni 	BB12_20;
-
-BB12_19:
-	mov.f32 	%f1670, %f1699;
-	mov.f32 	%f1669, %f1700;
-	mov.f32 	%f1668, %f1701;
-	mov.f32 	%f1667, %f1702;
-	mov.f32 	%f1666, %f1703;
-	mov.f32 	%f1665, %f1704;
-	mov.f32 	%f1664, %f1705;
-	mov.f32 	%f1663, %f1706;
-	mov.f32 	%f1662, %f1707;
-	mov.f32 	%f1661, %f1708;
-	mov.f32 	%f1660, %f1709;
-	mov.f32 	%f1659, %f1710;
-	bra.uni 	BB12_21;
-
-BB12_20:
-	mul.f32 	%f882, %f1667, %f1702;
-	fma.rn.f32 	%f883, %f1663, %f1701, %f882;
-	fma.rn.f32 	%f151, %f1659, %f1700, %f883;
-	mul.f32 	%f884, %f1668, %f1702;
-	fma.rn.f32 	%f885, %f1664, %f1701, %f884;
-	fma.rn.f32 	%f152, %f1660, %f1700, %f885;
-	mul.f32 	%f886, %f1669, %f1702;
-	fma.rn.f32 	%f887, %f1665, %f1701, %f886;
-	fma.rn.f32 	%f153, %f1661, %f1700, %f887;
-	mul.f32 	%f888, %f1670, %f1702;
-	fma.rn.f32 	%f889, %f1666, %f1701, %f888;
-	fma.rn.f32 	%f890, %f1662, %f1700, %f889;
-	add.f32 	%f154, %f1699, %f890;
-	mul.f32 	%f891, %f1667, %f1706;
-	fma.rn.f32 	%f892, %f1663, %f1705, %f891;
-	fma.rn.f32 	%f155, %f1659, %f1704, %f892;
-	mul.f32 	%f893, %f1668, %f1706;
-	fma.rn.f32 	%f894, %f1664, %f1705, %f893;
-	fma.rn.f32 	%f156, %f1660, %f1704, %f894;
-	mul.f32 	%f895, %f1669, %f1706;
-	fma.rn.f32 	%f896, %f1665, %f1705, %f895;
-	fma.rn.f32 	%f157, %f1661, %f1704, %f896;
-	mul.f32 	%f897, %f1670, %f1706;
-	fma.rn.f32 	%f898, %f1666, %f1705, %f897;
-	fma.rn.f32 	%f899, %f1662, %f1704, %f898;
-	add.f32 	%f158, %f1703, %f899;
-	mul.f32 	%f900, %f1667, %f1710;
-	fma.rn.f32 	%f901, %f1663, %f1709, %f900;
-	fma.rn.f32 	%f1659, %f1659, %f1708, %f901;
-	mul.f32 	%f902, %f1668, %f1710;
-	fma.rn.f32 	%f903, %f1664, %f1709, %f902;
-	fma.rn.f32 	%f1660, %f1660, %f1708, %f903;
-	mul.f32 	%f904, %f1669, %f1710;
-	fma.rn.f32 	%f905, %f1665, %f1709, %f904;
-	fma.rn.f32 	%f1661, %f1661, %f1708, %f905;
-	mul.f32 	%f906, %f1670, %f1710;
-	fma.rn.f32 	%f907, %f1666, %f1709, %f906;
-	fma.rn.f32 	%f908, %f1662, %f1708, %f907;
-	add.f32 	%f1662, %f1707, %f908;
-	mov.f32 	%f1670, %f154;
-	mov.f32 	%f1669, %f153;
-	mov.f32 	%f1668, %f152;
-	mov.f32 	%f1667, %f151;
-	mov.f32 	%f1666, %f158;
-	mov.f32 	%f1665, %f157;
-	mov.f32 	%f1664, %f156;
-	mov.f32 	%f1663, %f155;
-
-BB12_21:
-	add.s32 	%r631, %r631, 1;
-	setp.lt.u32	%p10, %r631, %r26;
-	@%p10 bra 	BB12_5;
-
-	mul.f32 	%f909, %f667, %f1667;
-	fma.rn.f32 	%f910, %f668, %f1668, %f909;
-	fma.rn.f32 	%f911, %f1723, %f1669, %f910;
-	add.f32 	%f1725, %f1670, %f911;
-	mul.f32 	%f912, %f667, %f1663;
-	fma.rn.f32 	%f913, %f668, %f1664, %f912;
-	fma.rn.f32 	%f914, %f1723, %f1665, %f913;
-	add.f32 	%f1724, %f1666, %f914;
-	mul.f32 	%f915, %f667, %f1659;
-	fma.rn.f32 	%f916, %f668, %f1660, %f915;
-	fma.rn.f32 	%f917, %f1723, %f1661, %f916;
-	add.f32 	%f1723, %f1662, %f917;
-	bra.uni 	BB12_23;
-
-BB12_3:
-	mov.f32 	%f1724, %f668;
-	mov.f32 	%f1725, %f667;
-
-BB12_23:
-	// inline asm
-	call (%f918), _optix_get_world_ray_direction_x, ();
-	// inline asm
-	// inline asm
-	call (%f919), _optix_get_world_ray_direction_y, ();
-	// inline asm
-	// inline asm
-	call (%f1774), _optix_get_world_ray_direction_z, ();
-	// inline asm
-	// inline asm
-	call (%f921), _optix_get_ray_time, ();
-	// inline asm
-	mov.u32 	%r632, 0;
-	@%p2 bra 	BB12_24;
-
-BB12_25:
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r107,%r108,%r109,%r110}, [%rd106];
+	// end inline asm
+	mov.b32 	%f732, %r107;
+	mov.b32 	%f733, %r108;
+	mov.b32 	%f734, %r109;
+	mov.b32 	%f735, %r110;
+	mul.f32 	%f736, %f29, %f732;
+	mul.f32 	%f737, %f29, %f733;
+	mul.f32 	%f738, %f29, %f734;
+	mul.f32 	%f739, %f29, %f735;
+	fma.rn.f32 	%f1711, %f731, %f1711, %f736;
+	fma.rn.f32 	%f1712, %f731, %f1712, %f737;
+	fma.rn.f32 	%f1713, %f731, %f1713, %f738;
+	fma.rn.f32 	%f1714, %f731, %f1714, %f739;
+	add.s64 	%rd110, %rd94, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd109, %rd110;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r111,%r112,%r113,%r114}, [%rd109];
+	// end inline asm
+	mov.b32 	%f740, %r111;
+	mov.b32 	%f741, %r112;
+	mov.b32 	%f742, %r113;
+	mov.b32 	%f743, %r114;
+	mul.f32 	%f744, %f29, %f740;
+	mul.f32 	%f745, %f29, %f741;
+	mul.f32 	%f746, %f29, %f742;
+	mul.f32 	%f747, %f29, %f743;
+	fma.rn.f32 	%f1715, %f731, %f1715, %f744;
+	fma.rn.f32 	%f1716, %f731, %f1716, %f745;
+	fma.rn.f32 	%f1717, %f731, %f1717, %f746;
+	fma.rn.f32 	%f1718, %f731, %f1718, %f747;
+	add.s64 	%rd113, %rd94, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd112, %rd113;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r115,%r116,%r117,%r118}, [%rd112];
+	// end inline asm
+	mov.b32 	%f748, %r115;
+	mov.b32 	%f749, %r116;
+	mov.b32 	%f750, %r117;
+	mov.b32 	%f751, %r118;
+	mul.f32 	%f752, %f29, %f748;
+	mul.f32 	%f753, %f29, %f749;
+	mul.f32 	%f754, %f29, %f750;
+	mul.f32 	%f755, %f29, %f751;
+	fma.rn.f32 	%f1719, %f731, %f1719, %f752;
+	fma.rn.f32 	%f756, %f731, %f1720, %f753;
+	fma.rn.f32 	%f757, %f731, %f1721, %f754;
+	fma.rn.f32 	%f758, %f731, %f1722, %f755;
+	add.s64 	%rd116, %rd94, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd115, %rd116;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r119,%r120,%r121,%r122}, [%rd115];
+	// end inline asm
+	mov.b32 	%f759, %r119;
+	mov.b32 	%f760, %r120;
+	mov.b32 	%f761, %r121;
+	mov.b32 	%f762, %r122;
+	mul.f32 	%f763, %f29, %f759;
+	mul.f32 	%f764, %f29, %f760;
+	mul.f32 	%f765, %f29, %f761;
+	mul.f32 	%f766, %f29, %f762;
+	fma.rn.f32 	%f767, %f731, %f1723, %f763;
+	fma.rn.f32 	%f1724, %f731, %f1724, %f764;
+	fma.rn.f32 	%f1725, %f731, %f1725, %f765;
+	fma.rn.f32 	%f1726, %f731, %f1726, %f766;
+	mul.f32 	%f768, %f757, %f757;
+	fma.rn.f32 	%f769, %f756, %f756, %f768;
+	fma.rn.f32 	%f770, %f758, %f758, %f769;
+	fma.rn.f32 	%f771, %f767, %f767, %f770;
+	sqrt.rn.f32 	%f772, %f771;
+	rcp.rn.f32 	%f773, %f772;
+	mul.f32 	%f1720, %f756, %f773;
+	mul.f32 	%f1721, %f757, %f773;
+	mul.f32 	%f1722, %f758, %f773;
+	mul.f32 	%f1723, %f773, %f767;
+
+$L__BB12_14:
+	mul.f32 	%f774, %f1721, %f1721;
+	fma.rn.f32 	%f775, %f1720, %f1720, %f774;
+	fma.rn.f32 	%f776, %f1722, %f1722, %f775;
+	fma.rn.f32 	%f777, %f1723, %f1723, %f776;
+	rcp.rn.f32 	%f778, %f777;
+	mul.f32 	%f779, %f1720, %f778;
+	mul.f32 	%f780, %f1721, %f778;
+	mul.f32 	%f781, %f1722, %f778;
+	mul.f32 	%f782, %f1723, %f778;
+	mul.f32 	%f783, %f1720, %f779;
+	mul.f32 	%f784, %f1721, %f780;
+	mul.f32 	%f785, %f1722, %f781;
+	mul.f32 	%f786, %f1720, %f780;
+	mul.f32 	%f787, %f1722, %f782;
+	mul.f32 	%f788, %f1720, %f781;
+	mul.f32 	%f789, %f1721, %f782;
+	mul.f32 	%f790, %f1721, %f781;
+	mul.f32 	%f791, %f1720, %f782;
+	sub.f32 	%f792, %f783, %f784;
+	sub.f32 	%f793, %f792, %f785;
+	fma.rn.f32 	%f794, %f1723, %f782, %f793;
+	sub.f32 	%f795, %f786, %f787;
+	add.f32 	%f796, %f795, %f795;
+	add.f32 	%f797, %f788, %f789;
+	add.f32 	%f798, %f797, %f797;
+	add.f32 	%f799, %f786, %f787;
+	add.f32 	%f800, %f799, %f799;
+	sub.f32 	%f801, %f784, %f783;
+	sub.f32 	%f802, %f801, %f785;
+	fma.rn.f32 	%f803, %f1723, %f782, %f802;
+	sub.f32 	%f804, %f790, %f791;
+	add.f32 	%f805, %f804, %f804;
+	sub.f32 	%f806, %f788, %f789;
+	add.f32 	%f807, %f806, %f806;
+	add.f32 	%f808, %f790, %f791;
+	add.f32 	%f809, %f808, %f808;
+	neg.f32 	%f810, %f783;
+	sub.f32 	%f811, %f810, %f784;
+	add.f32 	%f812, %f785, %f811;
+	fma.rn.f32 	%f813, %f1723, %f782, %f812;
+	mul.f32 	%f814, %f1714, %f794;
+	fma.rn.f32 	%f815, %f1717, %f796, %f814;
+	fma.rn.f32 	%f816, %f1719, %f798, %f815;
+	sub.f32 	%f1727, %f1724, %f816;
+	mul.f32 	%f817, %f1717, %f803;
+	fma.rn.f32 	%f818, %f1714, %f800, %f817;
+	fma.rn.f32 	%f819, %f1719, %f805, %f818;
+	sub.f32 	%f1731, %f1725, %f819;
+	mul.f32 	%f820, %f1717, %f809;
+	fma.rn.f32 	%f821, %f1714, %f807, %f820;
+	fma.rn.f32 	%f822, %f1719, %f813, %f821;
+	sub.f32 	%f1735, %f1726, %f822;
+	mul.f32 	%f823, %f1713, %f794;
+	fma.rn.f32 	%f824, %f1716, %f796, %f823;
+	fma.rn.f32 	%f1728, %f1718, %f798, %f824;
+	mul.f32 	%f825, %f1716, %f803;
+	fma.rn.f32 	%f826, %f1713, %f800, %f825;
+	fma.rn.f32 	%f1732, %f1718, %f805, %f826;
+	mul.f32 	%f827, %f1716, %f809;
+	fma.rn.f32 	%f828, %f1713, %f807, %f827;
+	fma.rn.f32 	%f1736, %f1718, %f813, %f828;
+	mul.f32 	%f829, %f1712, %f794;
+	fma.rn.f32 	%f1729, %f1715, %f796, %f829;
+	mul.f32 	%f830, %f1715, %f803;
+	fma.rn.f32 	%f1733, %f1712, %f800, %f830;
+	mul.f32 	%f831, %f1715, %f809;
+	fma.rn.f32 	%f1737, %f1712, %f807, %f831;
+	mul.f32 	%f1730, %f1711, %f794;
+	mul.f32 	%f1734, %f1711, %f800;
+	mul.f32 	%f1738, %f1711, %f807;
+
+$L__BB12_17:
+	mul.f32 	%f869, %f1732, %f1737;
+	mul.f32 	%f870, %f1733, %f1736;
+	sub.f32 	%f871, %f870, %f869;
+	mul.f32 	%f872, %f1730, %f871;
+	mul.f32 	%f873, %f1732, %f1738;
+	mul.f32 	%f874, %f1734, %f1736;
+	sub.f32 	%f875, %f874, %f873;
+	mul.f32 	%f876, %f1729, %f875;
+	sub.f32 	%f877, %f872, %f876;
+	mul.f32 	%f878, %f1733, %f1738;
+	mul.f32 	%f879, %f1734, %f1737;
+	sub.f32 	%f880, %f879, %f878;
+	fma.rn.f32 	%f881, %f1728, %f880, %f877;
+	rcp.rn.f32 	%f882, %f881;
+	mul.f32 	%f1742, %f871, %f882;
+	mul.f32 	%f883, %f1729, %f1736;
+	mul.f32 	%f884, %f1728, %f1737;
+	sub.f32 	%f885, %f884, %f883;
+	mul.f32 	%f1741, %f885, %f882;
+	mul.f32 	%f886, %f1728, %f1733;
+	mul.f32 	%f887, %f1729, %f1732;
+	sub.f32 	%f888, %f887, %f886;
+	mul.f32 	%f1740, %f888, %f882;
+	sub.f32 	%f889, %f873, %f874;
+	mul.f32 	%f1746, %f889, %f882;
+	mul.f32 	%f890, %f1728, %f1738;
+	mul.f32 	%f891, %f1730, %f1736;
+	sub.f32 	%f892, %f891, %f890;
+	mul.f32 	%f1745, %f892, %f882;
+	mul.f32 	%f893, %f1730, %f1732;
+	mul.f32 	%f894, %f1728, %f1734;
+	sub.f32 	%f895, %f894, %f893;
+	mul.f32 	%f1744, %f895, %f882;
+	mul.f32 	%f1750, %f880, %f882;
+	mul.f32 	%f896, %f1730, %f1737;
+	mul.f32 	%f897, %f1729, %f1738;
+	sub.f32 	%f898, %f897, %f896;
+	mul.f32 	%f1749, %f898, %f882;
+	mul.f32 	%f899, %f1729, %f1734;
+	mul.f32 	%f900, %f1730, %f1733;
+	sub.f32 	%f901, %f900, %f899;
+	mul.f32 	%f1748, %f901, %f882;
+	mul.f32 	%f902, %f1727, %f1742;
+	neg.f32 	%f903, %f902;
+	mul.f32 	%f904, %f1731, %f1741;
+	sub.f32 	%f905, %f903, %f904;
+	mul.f32 	%f906, %f1735, %f1740;
+	sub.f32 	%f1739, %f905, %f906;
+	mul.f32 	%f907, %f1727, %f1746;
+	neg.f32 	%f908, %f907;
+	mul.f32 	%f909, %f1731, %f1745;
+	sub.f32 	%f910, %f908, %f909;
+	mul.f32 	%f911, %f1735, %f1744;
+	sub.f32 	%f1743, %f910, %f911;
+	mul.f32 	%f912, %f1727, %f1750;
+	neg.f32 	%f913, %f912;
+	mul.f32 	%f914, %f1731, %f1749;
+	sub.f32 	%f915, %f913, %f914;
+	mul.f32 	%f916, %f1735, %f1748;
+	sub.f32 	%f1747, %f915, %f916;
+	bra.uni 	$L__BB12_18;
+
+$L__BB12_9:
+	// begin inline asm
+	call (%rd630), _optix_get_instance_inverse_transform_from_handle, (%rd46);
+	// end inline asm
+
+$L__BB12_10:
+	// begin inline asm
+	cvta.to.global.u64 %rd52, %rd630;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r36,%r37,%r38,%r39}, [%rd52];
+	// end inline asm
+	mov.b32 	%f1742, %r36;
+	mov.b32 	%f1741, %r37;
+	mov.b32 	%f1740, %r38;
+	mov.b32 	%f1739, %r39;
+	add.s64 	%rd56, %rd630, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd55, %rd56;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r40,%r41,%r42,%r43}, [%rd55];
+	// end inline asm
+	mov.b32 	%f1746, %r40;
+	mov.b32 	%f1745, %r41;
+	mov.b32 	%f1744, %r42;
+	mov.b32 	%f1743, %r43;
+	add.s64 	%rd59, %rd630, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd58, %rd59;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r44,%r45,%r46,%r47}, [%rd58];
+	// end inline asm
+	mov.b32 	%f1750, %r44;
+	mov.b32 	%f1749, %r45;
+	mov.b32 	%f1748, %r46;
+	mov.b32 	%f1747, %r47;
+
+$L__BB12_18:
+	setp.eq.s32 	%p10, %r642, 0;
+	@%p10 bra 	$L__BB12_20;
+
+	mul.f32 	%f917, %f1707, %f1742;
+	fma.rn.f32 	%f918, %f1703, %f1741, %f917;
+	fma.rn.f32 	%f151, %f1699, %f1740, %f918;
+	mul.f32 	%f919, %f1708, %f1742;
+	fma.rn.f32 	%f920, %f1704, %f1741, %f919;
+	fma.rn.f32 	%f152, %f1700, %f1740, %f920;
+	mul.f32 	%f921, %f1709, %f1742;
+	fma.rn.f32 	%f922, %f1705, %f1741, %f921;
+	fma.rn.f32 	%f153, %f1701, %f1740, %f922;
+	mul.f32 	%f923, %f1710, %f1742;
+	fma.rn.f32 	%f924, %f1706, %f1741, %f923;
+	fma.rn.f32 	%f925, %f1702, %f1740, %f924;
+	add.f32 	%f1739, %f1739, %f925;
+	mul.f32 	%f926, %f1707, %f1746;
+	fma.rn.f32 	%f927, %f1703, %f1745, %f926;
+	fma.rn.f32 	%f155, %f1699, %f1744, %f927;
+	mul.f32 	%f928, %f1708, %f1746;
+	fma.rn.f32 	%f929, %f1704, %f1745, %f928;
+	fma.rn.f32 	%f156, %f1700, %f1744, %f929;
+	mul.f32 	%f930, %f1709, %f1746;
+	fma.rn.f32 	%f931, %f1705, %f1745, %f930;
+	fma.rn.f32 	%f157, %f1701, %f1744, %f931;
+	mul.f32 	%f932, %f1710, %f1746;
+	fma.rn.f32 	%f933, %f1706, %f1745, %f932;
+	fma.rn.f32 	%f934, %f1702, %f1744, %f933;
+	add.f32 	%f1743, %f1743, %f934;
+	mul.f32 	%f935, %f1707, %f1750;
+	fma.rn.f32 	%f936, %f1703, %f1749, %f935;
+	fma.rn.f32 	%f159, %f1699, %f1748, %f936;
+	mul.f32 	%f937, %f1708, %f1750;
+	fma.rn.f32 	%f938, %f1704, %f1749, %f937;
+	fma.rn.f32 	%f160, %f1700, %f1748, %f938;
+	mul.f32 	%f939, %f1709, %f1750;
+	fma.rn.f32 	%f940, %f1705, %f1749, %f939;
+	fma.rn.f32 	%f161, %f1701, %f1748, %f940;
+	mul.f32 	%f941, %f1710, %f1750;
+	fma.rn.f32 	%f942, %f1706, %f1749, %f941;
+	fma.rn.f32 	%f943, %f1702, %f1748, %f942;
+	add.f32 	%f1747, %f1747, %f943;
+	mov.f32 	%f1740, %f153;
+	mov.f32 	%f1741, %f152;
+	mov.f32 	%f1742, %f151;
+	mov.f32 	%f1744, %f157;
+	mov.f32 	%f1745, %f156;
+	mov.f32 	%f1746, %f155;
+	mov.f32 	%f1748, %f161;
+	mov.f32 	%f1749, %f160;
+	mov.f32 	%f1750, %f159;
+
+$L__BB12_20:
+	add.s32 	%r642, %r642, 1;
+	setp.lt.u32 	%p11, %r642, %r31;
+	mov.f32 	%f1699, %f1750;
+	mov.f32 	%f1700, %f1749;
+	mov.f32 	%f1701, %f1748;
+	mov.f32 	%f1702, %f1747;
+	mov.f32 	%f1703, %f1746;
+	mov.f32 	%f1704, %f1745;
+	mov.f32 	%f1705, %f1744;
+	mov.f32 	%f1706, %f1743;
+	mov.f32 	%f1707, %f1742;
+	mov.f32 	%f1708, %f1741;
+	mov.f32 	%f1709, %f1740;
+	mov.f32 	%f1710, %f1739;
+	@%p11 bra 	$L__BB12_5;
+
+$L__BB12_21:
+	mul.f32 	%f944, %f1775, %f1742;
+	fma.rn.f32 	%f945, %f1776, %f1741, %f944;
+	fma.rn.f32 	%f946, %f1777, %f1740, %f945;
+	mul.f32 	%f947, %f1775, %f1746;
+	fma.rn.f32 	%f948, %f1776, %f1745, %f947;
+	fma.rn.f32 	%f949, %f1777, %f1744, %f948;
+	mul.f32 	%f950, %f1775, %f1750;
+	fma.rn.f32 	%f951, %f1776, %f1749, %f950;
+	fma.rn.f32 	%f952, %f1777, %f1748, %f951;
+	add.f32 	%f1777, %f1747, %f952;
+	add.f32 	%f1776, %f1743, %f949;
+	add.f32 	%f1775, %f1739, %f946;
+
+$L__BB12_23:
+	// begin inline asm
+	call (%f1833), _optix_get_world_ray_direction_x, ();
+	// end inline asm
+	// begin inline asm
+	call (%f1834), _optix_get_world_ray_direction_y, ();
+	// end inline asm
+	// begin inline asm
+	call (%f955), _optix_get_world_ray_direction_z, ();
+	// end inline asm
+	// begin inline asm
+	call (%r182), _optix_get_transform_list_size, ();
+	// end inline asm
+	setp.eq.s32 	%p12, %r182, 0;
+	@%p12 bra 	$L__BB12_43;
+
+	// begin inline asm
+	call (%r183), _optix_get_transform_list_size, ();
+	// end inline asm
+	// begin inline asm
+	call (%f956), _optix_get_ray_time, ();
+	// end inline asm
+	setp.eq.s32 	%p13, %r183, 0;
+	@%p13 bra 	$L__BB12_42;
+
+	mov.u32 	%r643, 0;
+
+$L__BB12_26:
 	.pragma "nounroll";
-	// inline asm
-	call (%rd171), _optix_get_transform_list_handle, (%r632);
-	// inline asm
-	// inline asm
-	call (%r179), _optix_get_transform_type_from_handle, (%rd171);
-	// inline asm
-	and.b32  	%r180, %r179, -2;
-	setp.eq.s32	%p12, %r180, 2;
-	@%p12 bra 	BB12_31;
-	bra.uni 	BB12_26;
-
-BB12_31:
-	setp.eq.s32	%p15, %r179, 2;
-	@%p15 bra 	BB12_35;
-	bra.uni 	BB12_32;
-
-BB12_35:
-	// inline asm
-	call (%rd245), _optix_get_matrix_motion_transform_from_handle, (%rd171);
-	// inline asm
-	// inline asm
-	cvta.to.global.u64 %rd247, %rd245;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r268,%r269,%r270,%r271}, [%rd247];
-	// inline asm
-	mov.b32	{%rs8, %rs9}, %r270;
-	add.s64 	%rd251, %rd245, 16;
-	// inline asm
-	cvta.to.global.u64 %rd250, %rd251;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r272,%r273,%r274,%r275}, [%rd250];
-	// inline asm
-	add.s64 	%rd254, %rd245, 32;
-	// inline asm
-	cvta.to.global.u64 %rd253, %rd254;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r276,%r277,%r278,%r279}, [%rd253];
-	// inline asm
-	add.s64 	%rd257, %rd245, 48;
-	// inline asm
-	cvta.to.global.u64 %rd256, %rd257;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r280,%r281,%r282,%r283}, [%rd256];
-	// inline asm
-	add.s64 	%rd260, %rd245, 64;
-	// inline asm
-	cvta.to.global.u64 %rd259, %rd260;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r284,%r285,%r286,%r287}, [%rd259];
-	// inline asm
-	add.s64 	%rd263, %rd245, 80;
-	// inline asm
-	cvta.to.global.u64 %rd262, %rd263;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r288,%r289,%r290,%r291}, [%rd262];
-	// inline asm
-	add.s64 	%rd266, %rd245, 96;
-	// inline asm
-	cvta.to.global.u64 %rd265, %rd266;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r292,%r293,%r294,%r295}, [%rd265];
-	// inline asm
-	add.s64 	%rd269, %rd245, 112;
-	// inline asm
-	cvta.to.global.u64 %rd268, %rd269;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r296,%r297,%r298,%r299}, [%rd268];
-	// inline asm
-	mov.b32 	 %f1024, %r271;
-	mov.b32 	 %f1025, %r272;
-	cvt.u32.u16	%r312, %rs8;
-	add.s32 	%r313, %r312, -1;
-	cvt.rn.f32.s32	%f1026, %r313;
-	sub.f32 	%f1027, %f921, %f1024;
-	mul.f32 	%f1028, %f1027, %f1026;
-	sub.f32 	%f1029, %f1025, %f1024;
-	div.rn.f32 	%f1030, %f1028, %f1029;
-	min.f32 	%f1031, %f1026, %f1030;
-	mov.f32 	%f1032, 0f00000000;
-	max.f32 	%f1033, %f1032, %f1031;
-	cvt.rmi.f32.f32	%f1034, %f1033;
-	cvt.rzi.s32.f32	%r314, %f1034;
-	cvt.s64.s32	%rd19, %r314;
-	mul.wide.s32 	%rd280, %r314, 48;
-	add.s64 	%rd272, %rd254, %rd280;
-	// inline asm
-	cvta.to.global.u64 %rd271, %rd272;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r300,%r301,%r302,%r303}, [%rd271];
-	// inline asm
-	mov.b32 	 %f1751, %r300;
-	mov.b32 	 %f1752, %r301;
-	mov.b32 	 %f1753, %r302;
-	add.s64 	%rd275, %rd272, 16;
-	// inline asm
-	cvta.to.global.u64 %rd274, %rd275;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r304,%r305,%r306,%r307}, [%rd274];
-	// inline asm
-	mov.b32 	 %f1748, %r304;
-	mov.b32 	 %f1749, %r305;
-	mov.b32 	 %f1750, %r306;
-	add.s64 	%rd278, %rd272, 32;
-	// inline asm
-	cvta.to.global.u64 %rd277, %rd278;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r308,%r309,%r310,%r311}, [%rd277];
-	// inline asm
-	sub.f32 	%f249, %f1033, %f1034;
-	mov.b32 	 %f1745, %r308;
-	mov.b32 	 %f1746, %r309;
-	mov.b32 	 %f1747, %r310;
-	setp.leu.f32	%p17, %f249, 0f00000000;
-	@%p17 bra 	BB12_37;
-
-	mul.lo.s64 	%rd290, %rd19, 48;
-	add.s64 	%rd291, %rd245, %rd290;
-	add.s64 	%rd282, %rd291, 80;
-	// inline asm
-	cvta.to.global.u64 %rd281, %rd282;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r315,%r316,%r317,%r318}, [%rd281];
-	// inline asm
-	mov.b32 	 %f1035, %r315;
-	mov.b32 	 %f1036, %r316;
-	mov.b32 	 %f1037, %r317;
-	add.s64 	%rd285, %rd291, 96;
-	// inline asm
-	cvta.to.global.u64 %rd284, %rd285;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r319,%r320,%r321,%r322}, [%rd284];
-	// inline asm
-	mov.b32 	 %f1038, %r319;
-	mov.b32 	 %f1039, %r320;
-	mov.b32 	 %f1040, %r321;
-	add.s64 	%rd288, %rd291, 112;
-	// inline asm
-	cvta.to.global.u64 %rd287, %rd288;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r323,%r324,%r325,%r326}, [%rd287];
-	// inline asm
-	mov.f32 	%f1041, 0f3F800000;
-	sub.f32 	%f1042, %f1041, %f249;
-	mul.f32 	%f1043, %f249, %f1035;
-	mul.f32 	%f1044, %f249, %f1036;
-	mul.f32 	%f1045, %f249, %f1037;
-	fma.rn.f32 	%f1751, %f1042, %f1751, %f1043;
-	fma.rn.f32 	%f1752, %f1042, %f1752, %f1044;
-	fma.rn.f32 	%f1753, %f1042, %f1753, %f1045;
-	mul.f32 	%f1046, %f249, %f1038;
-	mul.f32 	%f1047, %f249, %f1039;
-	mul.f32 	%f1048, %f249, %f1040;
-	fma.rn.f32 	%f1748, %f1042, %f1748, %f1046;
-	fma.rn.f32 	%f1749, %f1042, %f1749, %f1047;
-	fma.rn.f32 	%f1750, %f1042, %f1750, %f1048;
-	mov.b32 	 %f1049, %r323;
-	mov.b32 	 %f1050, %r324;
-	mov.b32 	 %f1051, %r325;
-	mul.f32 	%f1052, %f249, %f1049;
-	mul.f32 	%f1053, %f249, %f1050;
-	mul.f32 	%f1054, %f249, %f1051;
-	fma.rn.f32 	%f1745, %f1042, %f1745, %f1052;
-	fma.rn.f32 	%f1746, %f1042, %f1746, %f1053;
-	fma.rn.f32 	%f1747, %f1042, %f1747, %f1054;
-	bra.uni 	BB12_37;
-
-BB12_26:
-	mov.f32 	%f1754, 0f00000000;
-	mov.f32 	%f1756, 0f3F800000;
-	setp.eq.s32	%p13, %r179, 4;
-	@%p13 bra 	BB12_29;
-	bra.uni 	BB12_27;
-
-BB12_29:
-	// inline asm
-	call (%rd644), _optix_get_instance_inverse_transform_from_handle, (%rd171);
-	// inline asm
-	bra.uni 	BB12_30;
-
-BB12_32:
-	// inline asm
-	call (%rd186), _optix_get_srt_motion_transform_from_handle, (%rd171);
-	// inline asm
-	// inline asm
-	cvta.to.global.u64 %rd188, %rd186;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r193,%r194,%r195,%r196}, [%rd188];
-	// inline asm
-	mov.b32	{%rs6, %rs7}, %r195;
-	add.s64 	%rd192, %rd186, 16;
-	// inline asm
+	// begin inline asm
+	call (%rd165), _optix_get_transform_list_handle, (%r643);
+	// end inline asm
+	// begin inline asm
+	call (%r186), _optix_get_transform_type_from_handle, (%rd165);
+	// end inline asm
+	or.b32  	%r187, %r186, 1;
+	setp.eq.s32 	%p14, %r187, 3;
+	@%p14 bra 	$L__BB12_32;
+	bra.uni 	$L__BB12_27;
+
+$L__BB12_32:
+	setp.eq.s32 	%p17, %r186, 2;
+	@%p17 bra 	$L__BB12_36;
+	bra.uni 	$L__BB12_33;
+
+$L__BB12_36:
+	// begin inline asm
+	call (%rd237), _optix_get_matrix_motion_transform_from_handle, (%rd165);
+	// end inline asm
+	// begin inline asm
+	cvta.to.global.u64 %rd239, %rd237;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r275,%r276,%r277,%r278}, [%rd239];
+	// end inline asm
+	add.s64 	%rd243, %rd237, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd242, %rd243;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r279,%r280,%r281,%r282}, [%rd242];
+	// end inline asm
+	add.s64 	%rd246, %rd237, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd245, %rd246;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r283,%r284,%r285,%r286}, [%rd245];
+	// end inline asm
+	add.s64 	%rd249, %rd237, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd248, %rd249;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r287,%r288,%r289,%r290}, [%rd248];
+	// end inline asm
+	add.s64 	%rd252, %rd237, 64;
+	// begin inline asm
+	cvta.to.global.u64 %rd251, %rd252;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r291,%r292,%r293,%r294}, [%rd251];
+	// end inline asm
+	add.s64 	%rd255, %rd237, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd254, %rd255;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r295,%r296,%r297,%r298}, [%rd254];
+	// end inline asm
+	add.s64 	%rd258, %rd237, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd257, %rd258;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r299,%r300,%r301,%r302}, [%rd257];
+	// end inline asm
+	add.s64 	%rd261, %rd237, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd260, %rd261;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r303,%r304,%r305,%r306}, [%rd260];
+	// end inline asm
+	mov.b32 	%f1060, %r278;
+	mov.b32 	%f1061, %r279;
+	and.b32  	%r319, %r277, 65535;
+	add.s32 	%r320, %r319, -1;
+	cvt.rn.f32.s32 	%f1062, %r320;
+	sub.f32 	%f1063, %f956, %f1060;
+	mul.f32 	%f1064, %f1063, %f1062;
+	sub.f32 	%f1065, %f1061, %f1060;
+	div.rn.f32 	%f1066, %f1064, %f1065;
+	min.f32 	%f1067, %f1062, %f1066;
+	mov.f32 	%f1068, 0f00000000;
+	max.f32 	%f1069, %f1068, %f1067;
+	cvt.rmi.f32.f32 	%f1070, %f1069;
+	sub.f32 	%f258, %f1069, %f1070;
+	cvt.rzi.s32.f32 	%r321, %f1070;
+	cvt.s64.s32 	%rd17, %r321;
+	mul.wide.s32 	%rd272, %r321, 48;
+	add.s64 	%rd264, %rd246, %rd272;
+	// begin inline asm
+	cvta.to.global.u64 %rd263, %rd264;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r307,%r308,%r309,%r310}, [%rd263];
+	// end inline asm
+	mov.b32 	%f1803, %r307;
+	mov.b32 	%f1804, %r308;
+	mov.b32 	%f1805, %r309;
+	add.s64 	%rd267, %rd264, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd266, %rd267;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r311,%r312,%r313,%r314}, [%rd266];
+	// end inline asm
+	mov.b32 	%f1800, %r311;
+	mov.b32 	%f1801, %r312;
+	mov.b32 	%f1802, %r313;
+	add.s64 	%rd270, %rd264, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd269, %rd270;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r315,%r316,%r317,%r318}, [%rd269];
+	// end inline asm
+	mov.b32 	%f1797, %r315;
+	mov.b32 	%f1798, %r316;
+	mov.b32 	%f1799, %r317;
+	setp.leu.f32 	%p19, %f258, 0f00000000;
+	@%p19 bra 	$L__BB12_38;
+
+	mov.f32 	%f1071, 0f3F800000;
+	sub.f32 	%f1072, %f1071, %f258;
+	mul.lo.s64 	%rd282, %rd17, 48;
+	add.s64 	%rd283, %rd237, %rd282;
+	add.s64 	%rd274, %rd283, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd273, %rd274;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r322,%r323,%r324,%r325}, [%rd273];
+	// end inline asm
+	mov.b32 	%f1073, %r322;
+	mov.b32 	%f1074, %r323;
+	mov.b32 	%f1075, %r324;
+	mul.f32 	%f1076, %f258, %f1073;
+	mul.f32 	%f1077, %f258, %f1074;
+	mul.f32 	%f1078, %f258, %f1075;
+	fma.rn.f32 	%f1803, %f1072, %f1803, %f1076;
+	fma.rn.f32 	%f1804, %f1072, %f1804, %f1077;
+	fma.rn.f32 	%f1805, %f1072, %f1805, %f1078;
+	add.s64 	%rd277, %rd283, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd276, %rd277;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r326,%r327,%r328,%r329}, [%rd276];
+	// end inline asm
+	mov.b32 	%f1079, %r326;
+	mov.b32 	%f1080, %r327;
+	mov.b32 	%f1081, %r328;
+	mul.f32 	%f1082, %f258, %f1079;
+	mul.f32 	%f1083, %f258, %f1080;
+	mul.f32 	%f1084, %f258, %f1081;
+	fma.rn.f32 	%f1800, %f1072, %f1800, %f1082;
+	fma.rn.f32 	%f1801, %f1072, %f1801, %f1083;
+	fma.rn.f32 	%f1802, %f1072, %f1802, %f1084;
+	add.s64 	%rd280, %rd283, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd279, %rd280;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r330,%r331,%r332,%r333}, [%rd279];
+	// end inline asm
+	mov.b32 	%f1085, %r330;
+	mov.b32 	%f1086, %r331;
+	mov.b32 	%f1087, %r332;
+	mul.f32 	%f1088, %f258, %f1085;
+	mul.f32 	%f1089, %f258, %f1086;
+	mul.f32 	%f1090, %f258, %f1087;
+	fma.rn.f32 	%f1797, %f1072, %f1797, %f1088;
+	fma.rn.f32 	%f1798, %f1072, %f1798, %f1089;
+	fma.rn.f32 	%f1799, %f1072, %f1799, %f1090;
+	bra.uni 	$L__BB12_38;
+
+$L__BB12_27:
+	mov.f32 	%f1806, 0f00000000;
+	mov.f32 	%f1808, 0f3F800000;
+	setp.eq.s32 	%p15, %r186, 4;
+	@%p15 bra 	$L__BB12_30;
+
+	setp.ne.s32 	%p16, %r186, 1;
+	mov.f32 	%f1807, %f1806;
+	mov.f32 	%f1809, %f1806;
+	mov.f32 	%f1810, %f1808;
+	mov.f32 	%f1811, %f1806;
+	mov.f32 	%f1812, %f1808;
+	mov.f32 	%f1813, %f1806;
+	mov.f32 	%f1814, %f1806;
+	@%p16 bra 	$L__BB12_39;
+
+	// begin inline asm
+	call (%rd167), _optix_get_static_transform_from_handle, (%rd165);
+	// end inline asm
+	add.s64 	%rd631, %rd167, 64;
+	bra.uni 	$L__BB12_31;
+
+$L__BB12_33:
+	// begin inline asm
+	call (%rd180), _optix_get_srt_motion_transform_from_handle, (%rd165);
+	// end inline asm
+	// begin inline asm
+	cvta.to.global.u64 %rd182, %rd180;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r200,%r201,%r202,%r203}, [%rd182];
+	// end inline asm
+	add.s64 	%rd186, %rd180, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd185, %rd186;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r204,%r205,%r206,%r207}, [%rd185];
+	// end inline asm
+	add.s64 	%rd189, %rd180, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd188, %rd189;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r208,%r209,%r210,%r211}, [%rd188];
+	// end inline asm
+	add.s64 	%rd192, %rd180, 48;
+	// begin inline asm
 	cvta.to.global.u64 %rd191, %rd192;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r197,%r198,%r199,%r200}, [%rd191];
-	// inline asm
-	add.s64 	%rd195, %rd186, 32;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r212,%r213,%r214,%r215}, [%rd191];
+	// end inline asm
+	add.s64 	%rd195, %rd180, 64;
+	// begin inline asm
 	cvta.to.global.u64 %rd194, %rd195;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r201,%r202,%r203,%r204}, [%rd194];
-	// inline asm
-	add.s64 	%rd198, %rd186, 48;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r216,%r217,%r218,%r219}, [%rd194];
+	// end inline asm
+	add.s64 	%rd198, %rd180, 80;
+	// begin inline asm
 	cvta.to.global.u64 %rd197, %rd198;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r205,%r206,%r207,%r208}, [%rd197];
-	// inline asm
-	add.s64 	%rd201, %rd186, 64;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r220,%r221,%r222,%r223}, [%rd197];
+	// end inline asm
+	add.s64 	%rd201, %rd180, 96;
+	// begin inline asm
 	cvta.to.global.u64 %rd200, %rd201;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r209,%r210,%r211,%r212}, [%rd200];
-	// inline asm
-	add.s64 	%rd204, %rd186, 80;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r224,%r225,%r226,%r227}, [%rd200];
+	// end inline asm
+	add.s64 	%rd204, %rd180, 112;
+	// begin inline asm
 	cvta.to.global.u64 %rd203, %rd204;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r213,%r214,%r215,%r216}, [%rd203];
-	// inline asm
-	add.s64 	%rd207, %rd186, 96;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r228,%r229,%r230,%r231}, [%rd203];
+	// end inline asm
+	add.s64 	%rd207, %rd180, 128;
+	// begin inline asm
 	cvta.to.global.u64 %rd206, %rd207;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r217,%r218,%r219,%r220}, [%rd206];
-	// inline asm
-	add.s64 	%rd210, %rd186, 112;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r232,%r233,%r234,%r235}, [%rd206];
+	// end inline asm
+	add.s64 	%rd210, %rd180, 144;
+	// begin inline asm
 	cvta.to.global.u64 %rd209, %rd210;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r221,%r222,%r223,%r224}, [%rd209];
-	// inline asm
-	add.s64 	%rd213, %rd186, 128;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r236,%r237,%r238,%r239}, [%rd209];
+	// end inline asm
+	mov.b32 	%f968, %r203;
+	mov.b32 	%f969, %r204;
+	and.b32  	%r256, %r202, 65535;
+	add.s32 	%r257, %r256, -1;
+	cvt.rn.f32.s32 	%f970, %r257;
+	sub.f32 	%f971, %f956, %f968;
+	mul.f32 	%f972, %f971, %f970;
+	sub.f32 	%f973, %f969, %f968;
+	div.rn.f32 	%f974, %f972, %f973;
+	min.f32 	%f975, %f970, %f974;
+	mov.f32 	%f976, 0f00000000;
+	max.f32 	%f977, %f976, %f975;
+	cvt.rmi.f32.f32 	%f978, %f977;
+	sub.f32 	%f218, %f977, %f978;
+	cvt.rzi.s32.f32 	%r258, %f978;
+	mul.wide.s32 	%rd224, %r258, 64;
+	add.s64 	%rd213, %rd189, %rd224;
+	// begin inline asm
 	cvta.to.global.u64 %rd212, %rd213;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r225,%r226,%r227,%r228}, [%rd212];
-	// inline asm
-	add.s64 	%rd216, %rd186, 144;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r240,%r241,%r242,%r243}, [%rd212];
+	// end inline asm
+	mov.b32 	%f1787, %r240;
+	mov.b32 	%f1788, %r241;
+	mov.b32 	%f1789, %r242;
+	add.s64 	%rd216, %rd213, 16;
+	// begin inline asm
 	cvta.to.global.u64 %rd215, %rd216;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r229,%r230,%r231,%r232}, [%rd215];
-	// inline asm
-	mov.b32 	 %f932, %r196;
-	mov.b32 	 %f933, %r197;
-	cvt.u32.u16	%r249, %rs6;
-	add.s32 	%r250, %r249, -1;
-	cvt.rn.f32.s32	%f934, %r250;
-	sub.f32 	%f935, %f921, %f932;
-	mul.f32 	%f936, %f935, %f934;
-	sub.f32 	%f937, %f933, %f932;
-	div.rn.f32 	%f938, %f936, %f937;
-	min.f32 	%f939, %f934, %f938;
-	mov.f32 	%f940, 0f00000000;
-	max.f32 	%f941, %f940, %f939;
-	cvt.rmi.f32.f32	%f942, %f941;
-	cvt.rzi.s32.f32	%r251, %f942;
-	cvt.s64.s32	%rd17, %r251;
-	mul.wide.s32 	%rd230, %r251, 64;
-	add.s64 	%rd219, %rd195, %rd230;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r244,%r245,%r246,%r247}, [%rd215];
+	// end inline asm
+	mov.b32 	%f1790, %r244;
+	mov.b32 	%f1791, %r245;
+	mov.b32 	%f1792, %r247;
+	add.s64 	%rd219, %rd213, 32;
+	// begin inline asm
 	cvta.to.global.u64 %rd218, %rd219;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r233,%r234,%r235,%r236}, [%rd218];
-	// inline asm
-	mov.b32 	 %f1735, %r233;
-	mov.b32 	 %f1736, %r234;
-	mov.b32 	 %f1737, %r235;
-	add.s64 	%rd222, %rd219, 16;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r248,%r249,%r250,%r251}, [%rd218];
+	// end inline asm
+	mov.b32 	%f1793, %r249;
+	mov.b32 	%f1794, %r250;
+	mov.b32 	%f1795, %r251;
+	add.s64 	%rd222, %rd213, 48;
+	// begin inline asm
 	cvta.to.global.u64 %rd221, %rd222;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r237,%r238,%r239,%r240}, [%rd221];
-	// inline asm
-	mov.b32 	 %f1738, %r237;
-	mov.b32 	 %f1739, %r238;
-	mov.b32 	 %f1740, %r240;
-	add.s64 	%rd225, %rd219, 32;
-	// inline asm
-	cvta.to.global.u64 %rd224, %rd225;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r241,%r242,%r243,%r244}, [%rd224];
-	// inline asm
-	sub.f32 	%f209, %f941, %f942;
-	mov.b32 	 %f1741, %r242;
-	mov.b32 	 %f1742, %r243;
-	mov.b32 	 %f1743, %r244;
-	add.s64 	%rd228, %rd219, 48;
-	// inline asm
-	cvta.to.global.u64 %rd227, %rd228;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r245,%r246,%r247,%r248}, [%rd227];
-	// inline asm
-	mov.b32 	 %f1744, %r245;
-	setp.leu.f32	%p16, %f209, 0f00000000;
-	@%p16 bra 	BB12_34;
-
-	shl.b64 	%rd243, %rd17, 6;
-	add.s64 	%rd244, %rd243, %rd186;
-	add.s64 	%rd232, %rd244, 96;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r252,%r253,%r254,%r255}, [%rd221];
+	// end inline asm
+	mov.b32 	%f1796, %r252;
+	setp.leu.f32 	%p18, %f218, 0f00000000;
+	@%p18 bra 	$L__BB12_35;
+
+	mov.f32 	%f979, 0f3F800000;
+	sub.f32 	%f980, %f979, %f218;
+	add.s64 	%rd226, %rd213, 64;
+	// begin inline asm
+	cvta.to.global.u64 %rd225, %rd226;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r259,%r260,%r261,%r262}, [%rd225];
+	// end inline asm
+	mov.b32 	%f981, %r259;
+	mov.b32 	%f982, %r260;
+	mov.b32 	%f983, %r261;
+	mul.f32 	%f984, %f218, %f981;
+	mul.f32 	%f985, %f218, %f982;
+	mul.f32 	%f986, %f218, %f983;
+	fma.rn.f32 	%f1787, %f980, %f1787, %f984;
+	fma.rn.f32 	%f1788, %f980, %f1788, %f985;
+	fma.rn.f32 	%f1789, %f980, %f1789, %f986;
+	add.s64 	%rd229, %rd213, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd228, %rd229;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r263,%r264,%r265,%r266}, [%rd228];
+	// end inline asm
+	mov.b32 	%f987, %r263;
+	mov.b32 	%f988, %r264;
+	mov.b32 	%f989, %r266;
+	mul.f32 	%f990, %f218, %f987;
+	mul.f32 	%f991, %f218, %f988;
+	mul.f32 	%f992, %f218, %f989;
+	fma.rn.f32 	%f1790, %f980, %f1790, %f990;
+	fma.rn.f32 	%f1791, %f980, %f1791, %f991;
+	fma.rn.f32 	%f1792, %f980, %f1792, %f992;
+	add.s64 	%rd232, %rd213, 96;
+	// begin inline asm
 	cvta.to.global.u64 %rd231, %rd232;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r252,%r253,%r254,%r255}, [%rd231];
-	// inline asm
-	mov.b32 	 %f943, %r252;
-	mov.b32 	 %f944, %r253;
-	mov.b32 	 %f945, %r254;
-	add.s64 	%rd235, %rd244, 112;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r267,%r268,%r269,%r270}, [%rd231];
+	// end inline asm
+	mov.b32 	%f993, %r268;
+	mov.b32 	%f994, %r269;
+	mov.b32 	%f995, %r270;
+	mul.f32 	%f996, %f218, %f993;
+	mul.f32 	%f997, %f218, %f994;
+	mul.f32 	%f998, %f218, %f995;
+	fma.rn.f32 	%f999, %f980, %f1793, %f996;
+	fma.rn.f32 	%f1000, %f980, %f1794, %f997;
+	fma.rn.f32 	%f1001, %f980, %f1795, %f998;
+	add.s64 	%rd235, %rd213, 112;
+	// begin inline asm
 	cvta.to.global.u64 %rd234, %rd235;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r256,%r257,%r258,%r259}, [%rd234];
-	// inline asm
-	mov.b32 	 %f946, %r256;
-	mov.b32 	 %f947, %r257;
-	mov.b32 	 %f948, %r259;
-	add.s64 	%rd238, %rd244, 128;
-	// inline asm
-	cvta.to.global.u64 %rd237, %rd238;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r260,%r261,%r262,%r263}, [%rd237];
-	// inline asm
-	mov.b32 	 %f949, %r261;
-	mov.b32 	 %f950, %r262;
-	mov.b32 	 %f951, %r263;
-	add.s64 	%rd241, %rd244, 144;
-	// inline asm
-	cvta.to.global.u64 %rd240, %rd241;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r264,%r265,%r266,%r267}, [%rd240];
-	// inline asm
-	mov.f32 	%f952, 0f3F800000;
-	sub.f32 	%f953, %f952, %f209;
-	mul.f32 	%f954, %f209, %f943;
-	mul.f32 	%f955, %f209, %f944;
-	mul.f32 	%f956, %f209, %f945;
-	fma.rn.f32 	%f1735, %f953, %f1735, %f954;
-	fma.rn.f32 	%f1736, %f953, %f1736, %f955;
-	fma.rn.f32 	%f1737, %f953, %f1737, %f956;
-	mul.f32 	%f957, %f209, %f946;
-	mul.f32 	%f958, %f209, %f947;
-	mul.f32 	%f959, %f209, %f948;
-	fma.rn.f32 	%f1738, %f953, %f1738, %f957;
-	fma.rn.f32 	%f1739, %f953, %f1739, %f958;
-	fma.rn.f32 	%f1740, %f953, %f1740, %f959;
-	mul.f32 	%f960, %f209, %f949;
-	mul.f32 	%f961, %f209, %f950;
-	mul.f32 	%f962, %f209, %f951;
-	fma.rn.f32 	%f963, %f953, %f1741, %f960;
-	fma.rn.f32 	%f964, %f953, %f1742, %f961;
-	fma.rn.f32 	%f965, %f953, %f1743, %f962;
-	mov.b32 	 %f966, %r264;
-	mul.f32 	%f967, %f209, %f966;
-	fma.rn.f32 	%f968, %f953, %f1744, %f967;
-	mul.f32 	%f969, %f964, %f964;
-	fma.rn.f32 	%f970, %f963, %f963, %f969;
-	fma.rn.f32 	%f971, %f965, %f965, %f970;
-	fma.rn.f32 	%f972, %f968, %f968, %f971;
-	sqrt.rn.f32 	%f973, %f972;
-	rcp.rn.f32 	%f974, %f973;
-	mul.f32 	%f1741, %f963, %f974;
-	mul.f32 	%f1742, %f964, %f974;
-	mul.f32 	%f1743, %f965, %f974;
-	mul.f32 	%f1744, %f968, %f974;
-
-BB12_34:
-	mul.f32 	%f975, %f1742, %f1742;
-	fma.rn.f32 	%f976, %f1741, %f1741, %f975;
-	fma.rn.f32 	%f977, %f1743, %f1743, %f976;
-	fma.rn.f32 	%f978, %f1744, %f1744, %f977;
-	rcp.rn.f32 	%f979, %f978;
-	mul.f32 	%f980, %f1741, %f979;
-	mul.f32 	%f981, %f1742, %f979;
-	mul.f32 	%f982, %f1743, %f979;
-	mul.f32 	%f983, %f1744, %f979;
-	mul.f32 	%f984, %f1741, %f980;
-	mul.f32 	%f985, %f1742, %f981;
-	mul.f32 	%f986, %f1743, %f982;
-	mul.f32 	%f987, %f1741, %f981;
-	mul.f32 	%f988, %f1743, %f983;
-	mul.f32 	%f989, %f1741, %f982;
-	mul.f32 	%f990, %f1742, %f983;
-	mul.f32 	%f991, %f1742, %f982;
-	mul.f32 	%f992, %f1741, %f983;
-	sub.f32 	%f993, %f984, %f985;
-	sub.f32 	%f994, %f993, %f986;
-	fma.rn.f32 	%f995, %f1744, %f983, %f994;
-	sub.f32 	%f996, %f987, %f988;
-	add.f32 	%f997, %f996, %f996;
-	add.f32 	%f998, %f989, %f990;
-	add.f32 	%f999, %f998, %f998;
-	add.f32 	%f1000, %f987, %f988;
-	add.f32 	%f1001, %f1000, %f1000;
-	sub.f32 	%f1002, %f985, %f984;
-	sub.f32 	%f1003, %f1002, %f986;
-	fma.rn.f32 	%f1004, %f1744, %f983, %f1003;
-	sub.f32 	%f1005, %f991, %f992;
-	add.f32 	%f1006, %f1005, %f1005;
-	sub.f32 	%f1007, %f989, %f990;
-	add.f32 	%f1008, %f1007, %f1007;
-	add.f32 	%f1009, %f991, %f992;
-	add.f32 	%f1010, %f1009, %f1009;
-	neg.f32 	%f1011, %f984;
-	sub.f32 	%f1012, %f1011, %f985;
-	add.f32 	%f1013, %f986, %f1012;
-	fma.rn.f32 	%f1014, %f1744, %f983, %f1013;
-	mul.f32 	%f1015, %f1737, %f995;
-	fma.rn.f32 	%f1016, %f1739, %f997, %f1015;
-	fma.rn.f32 	%f1753, %f1740, %f999, %f1016;
-	mul.f32 	%f1017, %f1739, %f1004;
-	fma.rn.f32 	%f1018, %f1737, %f1001, %f1017;
-	fma.rn.f32 	%f1750, %f1740, %f1006, %f1018;
-	mul.f32 	%f1019, %f1739, %f1010;
-	fma.rn.f32 	%f1020, %f1737, %f1008, %f1019;
-	fma.rn.f32 	%f1747, %f1740, %f1014, %f1020;
-	mul.f32 	%f1021, %f1736, %f995;
-	fma.rn.f32 	%f1752, %f1738, %f997, %f1021;
-	mul.f32 	%f1022, %f1738, %f1004;
-	fma.rn.f32 	%f1749, %f1736, %f1001, %f1022;
-	mul.f32 	%f1023, %f1738, %f1010;
-	fma.rn.f32 	%f1746, %f1736, %f1008, %f1023;
-	mul.f32 	%f1751, %f1735, %f995;
-	mul.f32 	%f1748, %f1735, %f1001;
-	mul.f32 	%f1745, %f1735, %f1008;
-
-BB12_37:
-	mul.f32 	%f1055, %f1746, %f1750;
-	mul.f32 	%f1056, %f1747, %f1749;
-	sub.f32 	%f1057, %f1056, %f1055;
-	mul.f32 	%f1058, %f1751, %f1057;
-	mul.f32 	%f1059, %f1745, %f1750;
-	mul.f32 	%f1060, %f1747, %f1748;
-	sub.f32 	%f1061, %f1060, %f1059;
-	mul.f32 	%f1062, %f1061, %f1752;
-	sub.f32 	%f1063, %f1058, %f1062;
-	mul.f32 	%f1064, %f1745, %f1749;
-	mul.f32 	%f1065, %f1746, %f1748;
-	sub.f32 	%f1066, %f1065, %f1064;
-	fma.rn.f32 	%f1067, %f1066, %f1753, %f1063;
-	rcp.rn.f32 	%f1068, %f1067;
-	mul.f32 	%f1760, %f1057, %f1068;
-	mul.f32 	%f1069, %f1747, %f1752;
-	mul.f32 	%f1070, %f1746, %f1753;
-	sub.f32 	%f1071, %f1070, %f1069;
-	mul.f32 	%f1761, %f1068, %f1071;
-	mul.f32 	%f1072, %f1749, %f1753;
-	mul.f32 	%f1073, %f1750, %f1752;
-	sub.f32 	%f1074, %f1073, %f1072;
-	mul.f32 	%f1762, %f1068, %f1074;
-	sub.f32 	%f1075, %f1059, %f1060;
-	mul.f32 	%f1757, %f1075, %f1068;
-	mul.f32 	%f1076, %f1745, %f1753;
-	mul.f32 	%f1077, %f1747, %f1751;
-	sub.f32 	%f1078, %f1077, %f1076;
-	mul.f32 	%f1758, %f1068, %f1078;
-	mul.f32 	%f1079, %f1750, %f1751;
-	mul.f32 	%f1080, %f1748, %f1753;
-	sub.f32 	%f1081, %f1080, %f1079;
-	mul.f32 	%f1759, %f1068, %f1081;
-	mul.f32 	%f1754, %f1066, %f1068;
-	mul.f32 	%f1082, %f1746, %f1751;
-	mul.f32 	%f1083, %f1745, %f1752;
-	sub.f32 	%f1084, %f1083, %f1082;
-	mul.f32 	%f1755, %f1084, %f1068;
-	mul.f32 	%f1085, %f1748, %f1752;
-	mul.f32 	%f1086, %f1749, %f1751;
-	sub.f32 	%f1087, %f1086, %f1085;
-	mul.f32 	%f1756, %f1087, %f1068;
-	bra.uni 	BB12_38;
-
-BB12_27:
-	setp.ne.s32	%p14, %r179, 1;
-	mov.f32 	%f1755, %f1754;
-	mov.f32 	%f1757, %f1754;
-	mov.f32 	%f1758, %f1756;
-	mov.f32 	%f1759, %f1754;
-	mov.f32 	%f1760, %f1756;
-	mov.f32 	%f1761, %f1754;
-	mov.f32 	%f1762, %f1754;
-	@%p14 bra 	BB12_38;
-
-	// inline asm
-	call (%rd173), _optix_get_static_transform_from_handle, (%rd171);
-	// inline asm
-	add.s64 	%rd644, %rd173, 64;
-
-BB12_30:
-	// inline asm
-	cvta.to.global.u64 %rd177, %rd644;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r181,%r182,%r183,%r184}, [%rd177];
-	// inline asm
-	mov.b32 	 %f1760, %r181;
-	mov.b32 	 %f1761, %r182;
-	mov.b32 	 %f1762, %r183;
-	add.s64 	%rd181, %rd644, 16;
-	// inline asm
-	cvta.to.global.u64 %rd180, %rd181;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r185,%r186,%r187,%r188}, [%rd180];
-	// inline asm
-	mov.b32 	 %f1757, %r185;
-	mov.b32 	 %f1758, %r186;
-	mov.b32 	 %f1759, %r187;
-	add.s64 	%rd184, %rd644, 32;
-	// inline asm
-	cvta.to.global.u64 %rd183, %rd184;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r189,%r190,%r191,%r192}, [%rd183];
-	// inline asm
-	mov.b32 	 %f1754, %r189;
-	mov.b32 	 %f1755, %r190;
-	mov.b32 	 %f1756, %r191;
-
-BB12_38:
-	setp.eq.s32	%p18, %r632, 0;
-	@%p18 bra 	BB12_39;
-	bra.uni 	BB12_40;
-
-BB12_39:
-	mov.f32 	%f1734, %f1754;
-	mov.f32 	%f1733, %f1755;
-	mov.f32 	%f1732, %f1756;
-	mov.f32 	%f1731, %f1757;
-	mov.f32 	%f1730, %f1758;
-	mov.f32 	%f1729, %f1759;
-	mov.f32 	%f1728, %f1760;
-	mov.f32 	%f1727, %f1761;
-	mov.f32 	%f1726, %f1762;
-	bra.uni 	BB12_41;
-
-BB12_40:
-	mul.f32 	%f1088, %f1731, %f1761;
-	fma.rn.f32 	%f1089, %f1728, %f1760, %f1088;
-	fma.rn.f32 	%f289, %f1734, %f1762, %f1089;
-	mul.f32 	%f1090, %f1730, %f1761;
-	fma.rn.f32 	%f1091, %f1727, %f1760, %f1090;
-	fma.rn.f32 	%f290, %f1733, %f1762, %f1091;
-	mul.f32 	%f1092, %f1729, %f1761;
-	fma.rn.f32 	%f1093, %f1726, %f1760, %f1092;
-	fma.rn.f32 	%f291, %f1732, %f1762, %f1093;
-	mul.f32 	%f1094, %f1731, %f1758;
-	fma.rn.f32 	%f1095, %f1728, %f1757, %f1094;
-	fma.rn.f32 	%f292, %f1734, %f1759, %f1095;
-	mul.f32 	%f1096, %f1730, %f1758;
-	fma.rn.f32 	%f1097, %f1727, %f1757, %f1096;
-	fma.rn.f32 	%f293, %f1733, %f1759, %f1097;
-	mul.f32 	%f1098, %f1729, %f1758;
-	fma.rn.f32 	%f1099, %f1726, %f1757, %f1098;
-	fma.rn.f32 	%f294, %f1732, %f1759, %f1099;
-	mul.f32 	%f1100, %f1731, %f1755;
-	fma.rn.f32 	%f1101, %f1728, %f1754, %f1100;
-	fma.rn.f32 	%f1734, %f1734, %f1756, %f1101;
-	mul.f32 	%f1102, %f1730, %f1755;
-	fma.rn.f32 	%f1103, %f1727, %f1754, %f1102;
-	fma.rn.f32 	%f1733, %f1733, %f1756, %f1103;
-	mul.f32 	%f1104, %f1729, %f1755;
-	fma.rn.f32 	%f1105, %f1726, %f1754, %f1104;
-	fma.rn.f32 	%f1732, %f1732, %f1756, %f1105;
-	mov.f32 	%f1731, %f292;
-	mov.f32 	%f1730, %f293;
-	mov.f32 	%f1729, %f294;
-	mov.f32 	%f1728, %f289;
-	mov.f32 	%f1727, %f290;
-	mov.f32 	%f1726, %f291;
-
-BB12_41:
-	add.s32 	%r632, %r632, 1;
-	setp.lt.u32	%p19, %r632, %r26;
-	@%p19 bra 	BB12_25;
-
-	mul.f32 	%f1106, %f919, %f1727;
-	fma.rn.f32 	%f1107, %f918, %f1728, %f1106;
-	fma.rn.f32 	%f1772, %f1774, %f1726, %f1107;
-	mul.f32 	%f1108, %f919, %f1730;
-	fma.rn.f32 	%f1109, %f918, %f1731, %f1108;
-	fma.rn.f32 	%f1773, %f1774, %f1729, %f1109;
-	mul.f32 	%f1110, %f919, %f1733;
-	fma.rn.f32 	%f1111, %f918, %f1734, %f1110;
-	fma.rn.f32 	%f1774, %f1774, %f1732, %f1111;
-	bra.uni 	BB12_43;
-
-BB12_24:
-	mov.f32 	%f1772, %f918;
-	mov.f32 	%f1773, %f919;
-
-BB12_43:
-	// inline asm
-	call (%f1113), _optix_get_ray_tmax, ();
-	// inline asm
-	fma.rn.f32 	%f1923, %f1113, %f1774, %f1723;
-	ld.f32 	%f315, [%rd3+320];
-	setp.eq.f32	%p20, %f1923, %f315;
-	@%p20 bra 	BB12_48;
-
-	ld.f32 	%f1114, [%rd3+324];
-	setp.eq.f32	%p21, %f1923, %f1114;
-	@%p21 bra 	BB12_48;
-	bra.uni 	BB12_45;
-
-BB12_48:
-	mov.f32 	%f1144, 0f00000000;
-	fma.rn.f32 	%f326, %f1144, %f1144, %f1144;
-	@%p20 bra 	BB12_50;
-	bra.uni 	BB12_49;
-
-BB12_50:
-	mov.f32 	%f1149, 0fBF800000;
-	fma.rn.f32 	%f1150, %f1149, %f1149, %f326;
-	sqrt.rn.f32 	%f1151, %f1150;
-	div.rn.f32 	%f1775, %f1144, %f1151;
-	div.rn.f32 	%f1777, %f1149, %f1151;
-	bra.uni 	BB12_51;
-
-BB12_49:
-	mov.f32 	%f1145, 0f3F800000;
-	fma.rn.f32 	%f1146, %f1145, %f1145, %f326;
-	sqrt.rn.f32 	%f1147, %f1146;
-	div.rn.f32 	%f1775, %f1144, %f1147;
-	rcp.rn.f32 	%f1777, %f1147;
-
-BB12_51:
-	mov.f32 	%f1776, %f1775;
-
-BB12_52:
-	fma.rn.f32 	%f1921, %f1113, %f1772, %f1725;
-	fma.rn.f32 	%f1922, %f1113, %f1773, %f1724;
-	ld.u64 	%rd21, [%rd49];
-	ld.const.u64 	%rd292, [params+344];
-	cvta.to.global.u64 	%rd293, %rd292;
-	cvt.u64.u32	%rd22, %r1;
-	mul.wide.u32 	%rd294, %r1, 4;
-	add.s64 	%rd23, %rd293, %rd294;
-	ld.global.u32 	%r9, [%rd23];
-	setp.eq.s32	%p24, %r9, 0;
-	mov.f32 	%f1909, 0f00000000;
-	@%p24 bra 	BB12_53;
-
-	// inline asm
-	call (%r327), _optix_read_instance_id, ();
-	// inline asm
-	setp.ge.u32	%p25, %r327, %r9;
-	@%p25 bra 	BB12_53;
-
-	mov.f32 	%f1843, 0f00000000;
-	mov.f32 	%f1844, 0f3F800000;
-	mov.f32 	%f1781, %f1844;
-	mov.f32 	%f1780, %f1843;
-	mov.f32 	%f1779, %f1843;
-	mov.f32 	%f1778, %f1843;
-	mov.f32 	%f1785, %f1843;
-	mov.f32 	%f1784, %f1844;
-	mov.f32 	%f1783, %f1843;
-	mov.f32 	%f1782, %f1843;
-	mov.f32 	%f1789, %f1843;
-	mov.f32 	%f1788, %f1843;
-	mov.f32 	%f1787, %f1844;
-	mov.f32 	%f1786, %f1843;
-	@%p2 bra 	BB12_73;
-
-	add.s32 	%r633, %r26, -1;
-	setp.lt.s32	%p27, %r633, 0;
-	@%p27 bra 	BB12_73;
-
-BB12_57:
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r271,%r272,%r273,%r274}, [%rd234];
+	// end inline asm
+	mov.b32 	%f1002, %r271;
+	mul.f32 	%f1003, %f218, %f1002;
+	fma.rn.f32 	%f1004, %f980, %f1796, %f1003;
+	mul.f32 	%f1005, %f1000, %f1000;
+	fma.rn.f32 	%f1006, %f999, %f999, %f1005;
+	fma.rn.f32 	%f1007, %f1001, %f1001, %f1006;
+	fma.rn.f32 	%f1008, %f1004, %f1004, %f1007;
+	sqrt.rn.f32 	%f1009, %f1008;
+	rcp.rn.f32 	%f1010, %f1009;
+	mul.f32 	%f1793, %f999, %f1010;
+	mul.f32 	%f1794, %f1000, %f1010;
+	mul.f32 	%f1795, %f1001, %f1010;
+	mul.f32 	%f1796, %f1010, %f1004;
+
+$L__BB12_35:
+	mul.f32 	%f1011, %f1794, %f1794;
+	fma.rn.f32 	%f1012, %f1793, %f1793, %f1011;
+	fma.rn.f32 	%f1013, %f1795, %f1795, %f1012;
+	fma.rn.f32 	%f1014, %f1796, %f1796, %f1013;
+	rcp.rn.f32 	%f1015, %f1014;
+	mul.f32 	%f1016, %f1793, %f1015;
+	mul.f32 	%f1017, %f1794, %f1015;
+	mul.f32 	%f1018, %f1795, %f1015;
+	mul.f32 	%f1019, %f1796, %f1015;
+	mul.f32 	%f1020, %f1793, %f1016;
+	mul.f32 	%f1021, %f1794, %f1017;
+	mul.f32 	%f1022, %f1795, %f1018;
+	mul.f32 	%f1023, %f1793, %f1017;
+	mul.f32 	%f1024, %f1795, %f1019;
+	mul.f32 	%f1025, %f1793, %f1018;
+	mul.f32 	%f1026, %f1794, %f1019;
+	mul.f32 	%f1027, %f1794, %f1018;
+	mul.f32 	%f1028, %f1793, %f1019;
+	sub.f32 	%f1029, %f1020, %f1021;
+	sub.f32 	%f1030, %f1029, %f1022;
+	fma.rn.f32 	%f1031, %f1796, %f1019, %f1030;
+	sub.f32 	%f1032, %f1023, %f1024;
+	add.f32 	%f1033, %f1032, %f1032;
+	add.f32 	%f1034, %f1025, %f1026;
+	add.f32 	%f1035, %f1034, %f1034;
+	add.f32 	%f1036, %f1023, %f1024;
+	add.f32 	%f1037, %f1036, %f1036;
+	sub.f32 	%f1038, %f1021, %f1020;
+	sub.f32 	%f1039, %f1038, %f1022;
+	fma.rn.f32 	%f1040, %f1796, %f1019, %f1039;
+	sub.f32 	%f1041, %f1027, %f1028;
+	add.f32 	%f1042, %f1041, %f1041;
+	sub.f32 	%f1043, %f1025, %f1026;
+	add.f32 	%f1044, %f1043, %f1043;
+	add.f32 	%f1045, %f1027, %f1028;
+	add.f32 	%f1046, %f1045, %f1045;
+	neg.f32 	%f1047, %f1020;
+	sub.f32 	%f1048, %f1047, %f1021;
+	add.f32 	%f1049, %f1022, %f1048;
+	fma.rn.f32 	%f1050, %f1796, %f1019, %f1049;
+	mul.f32 	%f1051, %f1789, %f1031;
+	fma.rn.f32 	%f1052, %f1791, %f1033, %f1051;
+	fma.rn.f32 	%f1805, %f1792, %f1035, %f1052;
+	mul.f32 	%f1053, %f1791, %f1040;
+	fma.rn.f32 	%f1054, %f1789, %f1037, %f1053;
+	fma.rn.f32 	%f1802, %f1792, %f1042, %f1054;
+	mul.f32 	%f1055, %f1791, %f1046;
+	fma.rn.f32 	%f1056, %f1789, %f1044, %f1055;
+	fma.rn.f32 	%f1799, %f1792, %f1050, %f1056;
+	mul.f32 	%f1057, %f1788, %f1031;
+	fma.rn.f32 	%f1804, %f1790, %f1033, %f1057;
+	mul.f32 	%f1058, %f1790, %f1040;
+	fma.rn.f32 	%f1801, %f1788, %f1037, %f1058;
+	mul.f32 	%f1059, %f1790, %f1046;
+	fma.rn.f32 	%f1798, %f1788, %f1044, %f1059;
+	mul.f32 	%f1803, %f1787, %f1031;
+	mul.f32 	%f1800, %f1787, %f1037;
+	mul.f32 	%f1797, %f1787, %f1044;
+
+$L__BB12_38:
+	mul.f32 	%f1091, %f1798, %f1802;
+	mul.f32 	%f1092, %f1799, %f1801;
+	sub.f32 	%f1093, %f1092, %f1091;
+	mul.f32 	%f1094, %f1803, %f1093;
+	mul.f32 	%f1095, %f1797, %f1802;
+	mul.f32 	%f1096, %f1799, %f1800;
+	sub.f32 	%f1097, %f1096, %f1095;
+	mul.f32 	%f1098, %f1097, %f1804;
+	sub.f32 	%f1099, %f1094, %f1098;
+	mul.f32 	%f1100, %f1797, %f1801;
+	mul.f32 	%f1101, %f1798, %f1800;
+	sub.f32 	%f1102, %f1101, %f1100;
+	fma.rn.f32 	%f1103, %f1102, %f1805, %f1099;
+	rcp.rn.f32 	%f1104, %f1103;
+	mul.f32 	%f1812, %f1093, %f1104;
+	mul.f32 	%f1105, %f1799, %f1804;
+	mul.f32 	%f1106, %f1798, %f1805;
+	sub.f32 	%f1107, %f1106, %f1105;
+	mul.f32 	%f1813, %f1107, %f1104;
+	mul.f32 	%f1108, %f1801, %f1805;
+	mul.f32 	%f1109, %f1802, %f1804;
+	sub.f32 	%f1110, %f1109, %f1108;
+	mul.f32 	%f1814, %f1110, %f1104;
+	sub.f32 	%f1111, %f1095, %f1096;
+	mul.f32 	%f1809, %f1111, %f1104;
+	mul.f32 	%f1112, %f1797, %f1805;
+	mul.f32 	%f1113, %f1799, %f1803;
+	sub.f32 	%f1114, %f1113, %f1112;
+	mul.f32 	%f1810, %f1114, %f1104;
+	mul.f32 	%f1115, %f1802, %f1803;
+	mul.f32 	%f1116, %f1800, %f1805;
+	sub.f32 	%f1117, %f1116, %f1115;
+	mul.f32 	%f1811, %f1117, %f1104;
+	mul.f32 	%f1806, %f1102, %f1104;
+	mul.f32 	%f1118, %f1798, %f1803;
+	mul.f32 	%f1119, %f1797, %f1804;
+	sub.f32 	%f1120, %f1119, %f1118;
+	mul.f32 	%f1807, %f1120, %f1104;
+	mul.f32 	%f1121, %f1800, %f1804;
+	mul.f32 	%f1122, %f1801, %f1803;
+	sub.f32 	%f1123, %f1122, %f1121;
+	mul.f32 	%f1808, %f1123, %f1104;
+	bra.uni 	$L__BB12_39;
+
+$L__BB12_30:
+	// begin inline asm
+	call (%rd631), _optix_get_instance_inverse_transform_from_handle, (%rd165);
+	// end inline asm
+
+$L__BB12_31:
+	// begin inline asm
+	cvta.to.global.u64 %rd171, %rd631;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r188,%r189,%r190,%r191}, [%rd171];
+	// end inline asm
+	mov.b32 	%f1812, %r188;
+	mov.b32 	%f1813, %r189;
+	mov.b32 	%f1814, %r190;
+	add.s64 	%rd175, %rd631, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd174, %rd175;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r192,%r193,%r194,%r195}, [%rd174];
+	// end inline asm
+	mov.b32 	%f1809, %r192;
+	mov.b32 	%f1810, %r193;
+	mov.b32 	%f1811, %r194;
+	add.s64 	%rd178, %rd631, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd177, %rd178;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r196,%r197,%r198,%r199}, [%rd177];
+	// end inline asm
+	mov.b32 	%f1806, %r196;
+	mov.b32 	%f1807, %r197;
+	mov.b32 	%f1808, %r198;
+
+$L__BB12_39:
+	setp.eq.s32 	%p20, %r643, 0;
+	@%p20 bra 	$L__BB12_41;
+
+	mul.f32 	%f1124, %f1783, %f1813;
+	fma.rn.f32 	%f1125, %f1780, %f1812, %f1124;
+	fma.rn.f32 	%f304, %f1786, %f1814, %f1125;
+	mul.f32 	%f1126, %f1782, %f1813;
+	fma.rn.f32 	%f1127, %f1779, %f1812, %f1126;
+	fma.rn.f32 	%f305, %f1785, %f1814, %f1127;
+	mul.f32 	%f1128, %f1781, %f1813;
+	fma.rn.f32 	%f1129, %f1778, %f1812, %f1128;
+	fma.rn.f32 	%f1814, %f1784, %f1814, %f1129;
+	mul.f32 	%f1130, %f1783, %f1810;
+	fma.rn.f32 	%f1131, %f1780, %f1809, %f1130;
+	fma.rn.f32 	%f307, %f1786, %f1811, %f1131;
+	mul.f32 	%f1132, %f1782, %f1810;
+	fma.rn.f32 	%f1133, %f1779, %f1809, %f1132;
+	fma.rn.f32 	%f308, %f1785, %f1811, %f1133;
+	mul.f32 	%f1134, %f1781, %f1810;
+	fma.rn.f32 	%f1135, %f1778, %f1809, %f1134;
+	fma.rn.f32 	%f1811, %f1784, %f1811, %f1135;
+	mul.f32 	%f1136, %f1783, %f1807;
+	fma.rn.f32 	%f1137, %f1780, %f1806, %f1136;
+	fma.rn.f32 	%f310, %f1786, %f1808, %f1137;
+	mul.f32 	%f1138, %f1782, %f1807;
+	fma.rn.f32 	%f1139, %f1779, %f1806, %f1138;
+	fma.rn.f32 	%f311, %f1785, %f1808, %f1139;
+	mul.f32 	%f1140, %f1781, %f1807;
+	fma.rn.f32 	%f1141, %f1778, %f1806, %f1140;
+	fma.rn.f32 	%f1808, %f1784, %f1808, %f1141;
+	mov.f32 	%f1806, %f310;
+	mov.f32 	%f1807, %f311;
+	mov.f32 	%f1809, %f307;
+	mov.f32 	%f1810, %f308;
+	mov.f32 	%f1812, %f304;
+	mov.f32 	%f1813, %f305;
+
+$L__BB12_41:
+	add.s32 	%r643, %r643, 1;
+	setp.lt.u32 	%p21, %r643, %r183;
+	mov.f32 	%f1778, %f1814;
+	mov.f32 	%f1779, %f1813;
+	mov.f32 	%f1780, %f1812;
+	mov.f32 	%f1781, %f1811;
+	mov.f32 	%f1782, %f1810;
+	mov.f32 	%f1783, %f1809;
+	mov.f32 	%f1784, %f1808;
+	mov.f32 	%f1785, %f1807;
+	mov.f32 	%f1786, %f1806;
+	@%p21 bra 	$L__BB12_26;
+
+$L__BB12_42:
+	mul.f32 	%f1142, %f1834, %f1813;
+	fma.rn.f32 	%f1143, %f1833, %f1812, %f1142;
+	mul.f32 	%f1144, %f1834, %f1810;
+	fma.rn.f32 	%f1145, %f1833, %f1809, %f1144;
+	mul.f32 	%f1146, %f1834, %f1807;
+	fma.rn.f32 	%f1147, %f1833, %f1806, %f1146;
+	fma.rn.f32 	%f1835, %f955, %f1808, %f1147;
+	fma.rn.f32 	%f1834, %f955, %f1811, %f1145;
+	fma.rn.f32 	%f1833, %f955, %f1814, %f1143;
+	bra.uni 	$L__BB12_44;
+
+$L__BB12_43:
+	mov.f32 	%f1835, %f955;
+
+$L__BB12_44:
+	// begin inline asm
+	call (%f1149), _optix_get_ray_tmax, ();
+	// end inline asm
+	fma.rn.f32 	%f1984, %f1149, %f1835, %f1777;
+	add.s64 	%rd18, %rd3, 320;
+	ld.f32 	%f346, [%rd3+320];
+	setp.eq.f32 	%p22, %f1984, %f346;
+	@%p22 bra 	$L__BB12_49;
+
+	ld.f32 	%f1151, [%rd18+4];
+	setp.eq.f32 	%p23, %f1984, %f1151;
+	@%p23 bra 	$L__BB12_49;
+	bra.uni 	$L__BB12_46;
+
+$L__BB12_49:
+	mov.f32 	%f1175, 0f00000000;
+	fma.rn.f32 	%f358, %f1175, %f1175, %f1175;
+	@%p22 bra 	$L__BB12_51;
+	bra.uni 	$L__BB12_50;
+
+$L__BB12_51:
+	mov.f32 	%f1180, 0fBF800000;
+	fma.rn.f32 	%f1181, %f1180, %f1180, %f358;
+	sqrt.rn.f32 	%f1182, %f1181;
+	div.rn.f32 	%f1837, %f1175, %f1182;
+	div.rn.f32 	%f1836, %f1180, %f1182;
+	mov.f32 	%f1838, %f1837;
+	bra.uni 	$L__BB12_52;
+
+$L__BB12_50:
+	mov.f32 	%f1176, 0f3F800000;
+	fma.rn.f32 	%f1177, %f1176, %f1176, %f358;
+	sqrt.rn.f32 	%f1178, %f1177;
+	div.rn.f32 	%f1837, %f1175, %f1178;
+	rcp.rn.f32 	%f1836, %f1178;
+	mov.f32 	%f1838, %f1837;
+
+$L__BB12_52:
+	fma.rn.f32 	%f1982, %f1149, %f1833, %f1775;
+	fma.rn.f32 	%f1983, %f1149, %f1834, %f1776;
+	ld.u64 	%rd19, [%rd45];
+	ld.const.u64 	%rd284, [params+344];
+	cvta.to.global.u64 	%rd285, %rd284;
+	cvt.u64.u32 	%rd20, %r1;
+	mul.wide.u32 	%rd286, %r1, 4;
+	add.s64 	%rd21, %rd285, %rd286;
+	ld.global.u32 	%r10, [%rd21];
+	setp.eq.s32 	%p26, %r10, 0;
+	mov.f32 	%f1970, 0f00000000;
+	mov.f32 	%f1971, 0f00000000;
+	mov.f32 	%f1972, 0f00000000;
+	mov.f32 	%f1973, 0f00000000;
+	mov.f32 	%f1974, 0f00000000;
+	mov.f32 	%f1975, 0f00000000;
+	mov.f32 	%f1979, %f1838;
+	mov.f32 	%f1980, %f1837;
+	mov.f32 	%f1981, %f1836;
+	@%p26 bra 	$L__BB12_100;
+
+	// begin inline asm
+	call (%r334), _optix_read_instance_id, ();
+	// end inline asm
+	setp.ge.u32 	%p27, %r334, %r10;
+	mov.f32 	%f1979, %f1838;
+	mov.f32 	%f1980, %f1837;
+	mov.f32 	%f1981, %f1836;
+	@%p27 bra 	$L__BB12_100;
+
+	// begin inline asm
+	call (%r335), _optix_get_transform_list_size, ();
+	// end inline asm
+	setp.eq.s32 	%p28, %r335, 0;
+	mov.f32 	%f1938, 0f00000000;
+	mov.f32 	%f1937, 0f3F800000;
+	mov.f32 	%f1875, %f1937;
+	mov.f32 	%f1876, %f1938;
+	mov.f32 	%f1877, %f1938;
+	mov.f32 	%f1878, %f1938;
+	mov.f32 	%f1871, %f1938;
+	mov.f32 	%f1872, %f1937;
+	mov.f32 	%f1873, %f1938;
+	mov.f32 	%f1874, %f1938;
+	mov.f32 	%f1867, %f1938;
+	mov.f32 	%f1868, %f1938;
+	mov.f32 	%f1869, %f1937;
+	mov.f32 	%f1870, %f1938;
+	@%p28 bra 	$L__BB12_72;
+
+	// begin inline asm
+	call (%r336), _optix_get_transform_list_size, ();
+	// end inline asm
+	// begin inline asm
+	call (%f1208), _optix_get_ray_time, ();
+	// end inline asm
+	setp.lt.s32 	%p29, %r336, 1;
+	@%p29 bra 	$L__BB12_72;
+
+	add.s32 	%r644, %r336, 1;
+	mov.u32 	%r645, 1;
+
+$L__BB12_57:
 	.pragma "nounroll";
-	// inline asm
-	call (%rd295), _optix_get_transform_list_handle, (%r633);
-	// inline asm
-	// inline asm
-	call (%r329), _optix_get_transform_type_from_handle, (%rd295);
-	// inline asm
-	and.b32  	%r330, %r329, -2;
-	setp.eq.s32	%p28, %r330, 2;
-	@%p28 bra 	BB12_63;
-	bra.uni 	BB12_58;
-
-BB12_63:
-	setp.eq.s32	%p31, %r329, 2;
-	@%p31 bra 	BB12_67;
-	bra.uni 	BB12_64;
-
-BB12_67:
-	// inline asm
-	call (%rd369), _optix_get_matrix_motion_transform_from_handle, (%rd295);
-	// inline asm
-	// inline asm
-	cvta.to.global.u64 %rd371, %rd369;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r418,%r419,%r420,%r421}, [%rd371];
-	// inline asm
-	mov.b32	{%rs12, %rs13}, %r420;
-	add.s64 	%rd375, %rd369, 16;
-	// inline asm
-	cvta.to.global.u64 %rd374, %rd375;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r422,%r423,%r424,%r425}, [%rd374];
-	// inline asm
-	add.s64 	%rd378, %rd369, 32;
-	// inline asm
-	cvta.to.global.u64 %rd377, %rd378;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r426,%r427,%r428,%r429}, [%rd377];
-	// inline asm
-	add.s64 	%rd381, %rd369, 48;
-	// inline asm
-	cvta.to.global.u64 %rd380, %rd381;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r430,%r431,%r432,%r433}, [%rd380];
-	// inline asm
-	add.s64 	%rd384, %rd369, 64;
-	// inline asm
-	cvta.to.global.u64 %rd383, %rd384;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r434,%r435,%r436,%r437}, [%rd383];
-	// inline asm
-	add.s64 	%rd387, %rd369, 80;
-	// inline asm
-	cvta.to.global.u64 %rd386, %rd387;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r438,%r439,%r440,%r441}, [%rd386];
-	// inline asm
-	add.s64 	%rd390, %rd369, 96;
-	// inline asm
-	cvta.to.global.u64 %rd389, %rd390;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r442,%r443,%r444,%r445}, [%rd389];
-	// inline asm
-	add.s64 	%rd393, %rd369, 112;
-	// inline asm
-	cvta.to.global.u64 %rd392, %rd393;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r446,%r447,%r448,%r449}, [%rd392];
-	// inline asm
-	mov.b32 	 %f1303, %r421;
-	mov.b32 	 %f1304, %r422;
-	cvt.u32.u16	%r462, %rs12;
-	add.s32 	%r463, %r462, -1;
-	cvt.rn.f32.s32	%f1305, %r463;
-	sub.f32 	%f1306, %f921, %f1303;
-	mul.f32 	%f1307, %f1306, %f1305;
-	sub.f32 	%f1308, %f1304, %f1303;
-	div.rn.f32 	%f1309, %f1307, %f1308;
-	min.f32 	%f1310, %f1305, %f1309;
-	mov.f32 	%f1311, 0f00000000;
-	max.f32 	%f1312, %f1311, %f1310;
-	cvt.rmi.f32.f32	%f1313, %f1312;
-	cvt.rzi.s32.f32	%r464, %f1313;
-	cvt.s64.s32	%rd31, %r464;
-	mul.wide.s32 	%rd404, %r464, 48;
-	add.s64 	%rd396, %rd378, %rd404;
-	// inline asm
+	add.s32 	%r338, %r644, -2;
+	// begin inline asm
+	call (%rd287), _optix_get_transform_list_handle, (%r338);
+	// end inline asm
+	// begin inline asm
+	call (%r339), _optix_get_transform_type_from_handle, (%rd287);
+	// end inline asm
+	or.b32  	%r340, %r339, 1;
+	setp.eq.s32 	%p30, %r340, 3;
+	@%p30 bra 	$L__BB12_63;
+	bra.uni 	$L__BB12_58;
+
+$L__BB12_63:
+	setp.eq.s32 	%p33, %r339, 2;
+	@%p33 bra 	$L__BB12_67;
+	bra.uni 	$L__BB12_64;
+
+$L__BB12_67:
+	// begin inline asm
+	call (%rd359), _optix_get_matrix_motion_transform_from_handle, (%rd287);
+	// end inline asm
+	// begin inline asm
+	cvta.to.global.u64 %rd361, %rd359;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r428,%r429,%r430,%r431}, [%rd361];
+	// end inline asm
+	add.s64 	%rd365, %rd359, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd364, %rd365;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r432,%r433,%r434,%r435}, [%rd364];
+	// end inline asm
+	add.s64 	%rd368, %rd359, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd367, %rd368;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r436,%r437,%r438,%r439}, [%rd367];
+	// end inline asm
+	add.s64 	%rd371, %rd359, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd370, %rd371;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r440,%r441,%r442,%r443}, [%rd370];
+	// end inline asm
+	add.s64 	%rd374, %rd359, 64;
+	// begin inline asm
+	cvta.to.global.u64 %rd373, %rd374;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r444,%r445,%r446,%r447}, [%rd373];
+	// end inline asm
+	add.s64 	%rd377, %rd359, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd376, %rd377;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r448,%r449,%r450,%r451}, [%rd376];
+	// end inline asm
+	add.s64 	%rd380, %rd359, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd379, %rd380;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r452,%r453,%r454,%r455}, [%rd379];
+	// end inline asm
+	add.s64 	%rd383, %rd359, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd382, %rd383;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r456,%r457,%r458,%r459}, [%rd382];
+	// end inline asm
+	mov.b32 	%f1336, %r431;
+	mov.b32 	%f1337, %r432;
+	and.b32  	%r472, %r430, 65535;
+	add.s32 	%r473, %r472, -1;
+	cvt.rn.f32.s32 	%f1338, %r473;
+	sub.f32 	%f1339, %f1208, %f1336;
+	mul.f32 	%f1340, %f1339, %f1338;
+	sub.f32 	%f1341, %f1337, %f1336;
+	div.rn.f32 	%f1342, %f1340, %f1341;
+	min.f32 	%f1343, %f1338, %f1342;
+	mov.f32 	%f1344, 0f00000000;
+	max.f32 	%f1345, %f1344, %f1343;
+	cvt.rmi.f32.f32 	%f1346, %f1345;
+	sub.f32 	%f454, %f1345, %f1346;
+	cvt.rzi.s32.f32 	%r474, %f1346;
+	cvt.s64.s32 	%rd28, %r474;
+	mul.wide.s32 	%rd394, %r474, 48;
+	add.s64 	%rd386, %rd368, %rd394;
+	// begin inline asm
+	cvta.to.global.u64 %rd385, %rd386;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r460,%r461,%r462,%r463}, [%rd385];
+	// end inline asm
+	mov.b32 	%f1875, %r460;
+	mov.b32 	%f1876, %r461;
+	mov.b32 	%f1877, %r462;
+	mov.b32 	%f1878, %r463;
+	add.s64 	%rd389, %rd386, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd388, %rd389;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r464,%r465,%r466,%r467}, [%rd388];
+	// end inline asm
+	mov.b32 	%f1871, %r464;
+	mov.b32 	%f1872, %r465;
+	mov.b32 	%f1873, %r466;
+	mov.b32 	%f1874, %r467;
+	add.s64 	%rd392, %rd386, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd391, %rd392;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r468,%r469,%r470,%r471}, [%rd391];
+	// end inline asm
+	mov.b32 	%f1867, %r468;
+	mov.b32 	%f1868, %r469;
+	mov.b32 	%f1869, %r470;
+	mov.b32 	%f1870, %r471;
+	setp.leu.f32 	%p35, %f454, 0f00000000;
+	@%p35 bra 	$L__BB12_69;
+
+	mov.f32 	%f1347, 0f3F800000;
+	sub.f32 	%f1348, %f1347, %f454;
+	mul.lo.s64 	%rd404, %rd28, 48;
+	add.s64 	%rd405, %rd359, %rd404;
+	add.s64 	%rd396, %rd405, 80;
+	// begin inline asm
 	cvta.to.global.u64 %rd395, %rd396;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r450,%r451,%r452,%r453}, [%rd395];
-	// inline asm
-	mov.b32 	 %f1814, %r450;
-	mov.b32 	 %f1815, %r451;
-	mov.b32 	 %f1816, %r452;
-	mov.b32 	 %f1817, %r453;
-	add.s64 	%rd399, %rd396, 16;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r475,%r476,%r477,%r478}, [%rd395];
+	// end inline asm
+	mov.b32 	%f1349, %r475;
+	mov.b32 	%f1350, %r476;
+	mov.b32 	%f1351, %r477;
+	mov.b32 	%f1352, %r478;
+	mul.f32 	%f1353, %f454, %f1349;
+	mul.f32 	%f1354, %f454, %f1350;
+	mul.f32 	%f1355, %f454, %f1351;
+	mul.f32 	%f1356, %f454, %f1352;
+	fma.rn.f32 	%f1875, %f1348, %f1875, %f1353;
+	fma.rn.f32 	%f1876, %f1348, %f1876, %f1354;
+	fma.rn.f32 	%f1877, %f1348, %f1877, %f1355;
+	fma.rn.f32 	%f1878, %f1348, %f1878, %f1356;
+	add.s64 	%rd399, %rd405, 96;
+	// begin inline asm
 	cvta.to.global.u64 %rd398, %rd399;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r454,%r455,%r456,%r457}, [%rd398];
-	// inline asm
-	mov.b32 	 %f1810, %r454;
-	mov.b32 	 %f1811, %r455;
-	mov.b32 	 %f1812, %r456;
-	mov.b32 	 %f1813, %r457;
-	add.s64 	%rd402, %rd396, 32;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r479,%r480,%r481,%r482}, [%rd398];
+	// end inline asm
+	mov.b32 	%f1357, %r479;
+	mov.b32 	%f1358, %r480;
+	mov.b32 	%f1359, %r481;
+	mov.b32 	%f1360, %r482;
+	mul.f32 	%f1361, %f454, %f1357;
+	mul.f32 	%f1362, %f454, %f1358;
+	mul.f32 	%f1363, %f454, %f1359;
+	mul.f32 	%f1364, %f454, %f1360;
+	fma.rn.f32 	%f1871, %f1348, %f1871, %f1361;
+	fma.rn.f32 	%f1872, %f1348, %f1872, %f1362;
+	fma.rn.f32 	%f1873, %f1348, %f1873, %f1363;
+	fma.rn.f32 	%f1874, %f1348, %f1874, %f1364;
+	add.s64 	%rd402, %rd405, 112;
+	// begin inline asm
 	cvta.to.global.u64 %rd401, %rd402;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r458,%r459,%r460,%r461}, [%rd401];
-	// inline asm
-	sub.f32 	%f429, %f1312, %f1313;
-	mov.b32 	 %f1806, %r458;
-	mov.b32 	 %f1807, %r459;
-	mov.b32 	 %f1808, %r460;
-	mov.b32 	 %f1809, %r461;
-	setp.leu.f32	%p33, %f429, 0f00000000;
-	@%p33 bra 	BB12_69;
-
-	mul.lo.s64 	%rd414, %rd31, 48;
-	add.s64 	%rd415, %rd369, %rd414;
-	add.s64 	%rd406, %rd415, 80;
-	// inline asm
-	cvta.to.global.u64 %rd405, %rd406;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r465,%r466,%r467,%r468}, [%rd405];
-	// inline asm
-	mov.b32 	 %f1314, %r465;
-	mov.b32 	 %f1315, %r466;
-	mov.b32 	 %f1316, %r467;
-	mov.b32 	 %f1317, %r468;
-	add.s64 	%rd409, %rd415, 96;
-	// inline asm
-	cvta.to.global.u64 %rd408, %rd409;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r469,%r470,%r471,%r472}, [%rd408];
-	// inline asm
-	mov.b32 	 %f1318, %r469;
-	mov.b32 	 %f1319, %r470;
-	mov.b32 	 %f1320, %r471;
-	mov.b32 	 %f1321, %r472;
-	add.s64 	%rd412, %rd415, 112;
-	// inline asm
-	cvta.to.global.u64 %rd411, %rd412;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r473,%r474,%r475,%r476}, [%rd411];
-	// inline asm
-	mov.f32 	%f1322, 0f3F800000;
-	sub.f32 	%f1323, %f1322, %f429;
-	mul.f32 	%f1324, %f429, %f1314;
-	mul.f32 	%f1325, %f429, %f1315;
-	mul.f32 	%f1326, %f429, %f1316;
-	mul.f32 	%f1327, %f429, %f1317;
-	fma.rn.f32 	%f1814, %f1323, %f1814, %f1324;
-	fma.rn.f32 	%f1815, %f1323, %f1815, %f1325;
-	fma.rn.f32 	%f1816, %f1323, %f1816, %f1326;
-	fma.rn.f32 	%f1817, %f1323, %f1817, %f1327;
-	mul.f32 	%f1328, %f429, %f1318;
-	mul.f32 	%f1329, %f429, %f1319;
-	mul.f32 	%f1330, %f429, %f1320;
-	mul.f32 	%f1331, %f429, %f1321;
-	fma.rn.f32 	%f1810, %f1323, %f1810, %f1328;
-	fma.rn.f32 	%f1811, %f1323, %f1811, %f1329;
-	fma.rn.f32 	%f1812, %f1323, %f1812, %f1330;
-	fma.rn.f32 	%f1813, %f1323, %f1813, %f1331;
-	mov.b32 	 %f1332, %r473;
-	mov.b32 	 %f1333, %r474;
-	mov.b32 	 %f1334, %r475;
-	mov.b32 	 %f1335, %r476;
-	mul.f32 	%f1336, %f429, %f1332;
-	mul.f32 	%f1337, %f429, %f1333;
-	mul.f32 	%f1338, %f429, %f1334;
-	mul.f32 	%f1339, %f429, %f1335;
-	fma.rn.f32 	%f1806, %f1323, %f1806, %f1336;
-	fma.rn.f32 	%f1807, %f1323, %f1807, %f1337;
-	fma.rn.f32 	%f1808, %f1323, %f1808, %f1338;
-	fma.rn.f32 	%f1809, %f1323, %f1809, %f1339;
-	bra.uni 	BB12_69;
-
-BB12_58:
-	mov.f32 	%f1806, 0f00000000;
-	mov.f32 	%f1808, 0f3F800000;
-	setp.eq.s32	%p29, %r329, 4;
-	@%p29 bra 	BB12_61;
-	bra.uni 	BB12_59;
-
-BB12_61:
-	// inline asm
-	call (%rd645), _optix_get_instance_transform_from_handle, (%rd295);
-	// inline asm
-	bra.uni 	BB12_62;
-
-BB12_64:
-	// inline asm
-	call (%rd310), _optix_get_srt_motion_transform_from_handle, (%rd295);
-	// inline asm
-	// inline asm
-	cvta.to.global.u64 %rd312, %rd310;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r343,%r344,%r345,%r346}, [%rd312];
-	// inline asm
-	mov.b32	{%rs10, %rs11}, %r345;
-	add.s64 	%rd316, %rd310, 16;
-	// inline asm
-	cvta.to.global.u64 %rd315, %rd316;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r347,%r348,%r349,%r350}, [%rd315];
-	// inline asm
-	add.s64 	%rd319, %rd310, 32;
-	// inline asm
-	cvta.to.global.u64 %rd318, %rd319;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r351,%r352,%r353,%r354}, [%rd318];
-	// inline asm
-	add.s64 	%rd322, %rd310, 48;
-	// inline asm
-	cvta.to.global.u64 %rd321, %rd322;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r355,%r356,%r357,%r358}, [%rd321];
-	// inline asm
-	add.s64 	%rd325, %rd310, 64;
-	// inline asm
-	cvta.to.global.u64 %rd324, %rd325;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r359,%r360,%r361,%r362}, [%rd324];
-	// inline asm
-	add.s64 	%rd328, %rd310, 80;
-	// inline asm
-	cvta.to.global.u64 %rd327, %rd328;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r363,%r364,%r365,%r366}, [%rd327];
-	// inline asm
-	add.s64 	%rd331, %rd310, 96;
-	// inline asm
-	cvta.to.global.u64 %rd330, %rd331;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r367,%r368,%r369,%r370}, [%rd330];
-	// inline asm
-	add.s64 	%rd334, %rd310, 112;
-	// inline asm
-	cvta.to.global.u64 %rd333, %rd334;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r371,%r372,%r373,%r374}, [%rd333];
-	// inline asm
-	add.s64 	%rd337, %rd310, 128;
-	// inline asm
-	cvta.to.global.u64 %rd336, %rd337;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r375,%r376,%r377,%r378}, [%rd336];
-	// inline asm
-	add.s64 	%rd340, %rd310, 144;
-	// inline asm
-	cvta.to.global.u64 %rd339, %rd340;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r379,%r380,%r381,%r382}, [%rd339];
-	// inline asm
-	mov.b32 	 %f1190, %r346;
-	mov.b32 	 %f1191, %r347;
-	cvt.u32.u16	%r399, %rs10;
-	add.s32 	%r400, %r399, -1;
-	cvt.rn.f32.s32	%f1192, %r400;
-	sub.f32 	%f1193, %f921, %f1190;
-	mul.f32 	%f1194, %f1193, %f1192;
-	sub.f32 	%f1195, %f1191, %f1190;
-	div.rn.f32 	%f1196, %f1194, %f1195;
-	min.f32 	%f1197, %f1192, %f1196;
-	mov.f32 	%f1198, 0f00000000;
-	max.f32 	%f1199, %f1198, %f1197;
-	cvt.rmi.f32.f32	%f1200, %f1199;
-	cvt.rzi.s32.f32	%r401, %f1200;
-	cvt.s64.s32	%rd29, %r401;
-	mul.wide.s32 	%rd354, %r401, 64;
-	add.s64 	%rd343, %rd319, %rd354;
-	// inline asm
-	cvta.to.global.u64 %rd342, %rd343;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r383,%r384,%r385,%r386}, [%rd342];
-	// inline asm
-	mov.b32 	 %f1790, %r383;
-	mov.b32 	 %f1791, %r384;
-	mov.b32 	 %f1792, %r385;
-	mov.b32 	 %f1793, %r386;
-	add.s64 	%rd346, %rd343, 16;
-	// inline asm
-	cvta.to.global.u64 %rd345, %rd346;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r387,%r388,%r389,%r390}, [%rd345];
-	// inline asm
-	mov.b32 	 %f1794, %r387;
-	mov.b32 	 %f1795, %r388;
-	mov.b32 	 %f1796, %r389;
-	mov.b32 	 %f1797, %r390;
-	add.s64 	%rd349, %rd343, 32;
-	// inline asm
-	cvta.to.global.u64 %rd348, %rd349;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r391,%r392,%r393,%r394}, [%rd348];
-	// inline asm
-	sub.f32 	%f368, %f1199, %f1200;
-	mov.b32 	 %f1798, %r391;
-	mov.b32 	 %f1799, %r392;
-	mov.b32 	 %f1800, %r393;
-	mov.b32 	 %f1801, %r394;
-	add.s64 	%rd352, %rd343, 48;
-	// inline asm
-	cvta.to.global.u64 %rd351, %rd352;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r395,%r396,%r397,%r398}, [%rd351];
-	// inline asm
-	mov.b32 	 %f1802, %r395;
-	mov.b32 	 %f1803, %r396;
-	mov.b32 	 %f1804, %r397;
-	mov.b32 	 %f1805, %r398;
-	setp.leu.f32	%p32, %f368, 0f00000000;
-	@%p32 bra 	BB12_66;
-
-	shl.b64 	%rd367, %rd29, 6;
-	add.s64 	%rd368, %rd367, %rd310;
-	add.s64 	%rd356, %rd368, 96;
-	// inline asm
-	cvta.to.global.u64 %rd355, %rd356;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r402,%r403,%r404,%r405}, [%rd355];
-	// inline asm
-	mov.b32 	 %f1201, %r402;
-	mov.b32 	 %f1202, %r403;
-	mov.b32 	 %f1203, %r404;
-	mov.b32 	 %f1204, %r405;
-	add.s64 	%rd359, %rd368, 112;
-	// inline asm
-	cvta.to.global.u64 %rd358, %rd359;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r406,%r407,%r408,%r409}, [%rd358];
-	// inline asm
-	mov.b32 	 %f1205, %r406;
-	mov.b32 	 %f1206, %r407;
-	mov.b32 	 %f1207, %r408;
-	mov.b32 	 %f1208, %r409;
-	add.s64 	%rd362, %rd368, 128;
-	// inline asm
-	cvta.to.global.u64 %rd361, %rd362;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r410,%r411,%r412,%r413}, [%rd361];
-	// inline asm
-	mov.b32 	 %f1209, %r410;
-	mov.b32 	 %f1210, %r411;
-	mov.b32 	 %f1211, %r412;
-	mov.b32 	 %f1212, %r413;
-	add.s64 	%rd365, %rd368, 144;
-	// inline asm
-	cvta.to.global.u64 %rd364, %rd365;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r414,%r415,%r416,%r417}, [%rd364];
-	// inline asm
-	mov.f32 	%f1213, 0f3F800000;
-	sub.f32 	%f1214, %f1213, %f368;
-	mul.f32 	%f1215, %f368, %f1201;
-	mul.f32 	%f1216, %f368, %f1202;
-	mul.f32 	%f1217, %f368, %f1203;
-	mul.f32 	%f1218, %f368, %f1204;
-	fma.rn.f32 	%f1790, %f1214, %f1790, %f1215;
-	fma.rn.f32 	%f1791, %f1214, %f1791, %f1216;
-	fma.rn.f32 	%f1792, %f1214, %f1792, %f1217;
-	fma.rn.f32 	%f1793, %f1214, %f1793, %f1218;
-	mul.f32 	%f1219, %f368, %f1205;
-	mul.f32 	%f1220, %f368, %f1206;
-	mul.f32 	%f1221, %f368, %f1207;
-	mul.f32 	%f1222, %f368, %f1208;
-	fma.rn.f32 	%f1794, %f1214, %f1794, %f1219;
-	fma.rn.f32 	%f1795, %f1214, %f1795, %f1220;
-	fma.rn.f32 	%f1796, %f1214, %f1796, %f1221;
-	fma.rn.f32 	%f1797, %f1214, %f1797, %f1222;
-	mul.f32 	%f1223, %f368, %f1209;
-	mul.f32 	%f1224, %f368, %f1210;
-	mul.f32 	%f1225, %f368, %f1211;
-	mul.f32 	%f1226, %f368, %f1212;
-	fma.rn.f32 	%f1798, %f1214, %f1798, %f1223;
-	fma.rn.f32 	%f1227, %f1214, %f1799, %f1224;
-	fma.rn.f32 	%f1228, %f1214, %f1800, %f1225;
-	fma.rn.f32 	%f1229, %f1214, %f1801, %f1226;
-	mov.b32 	 %f1230, %r414;
-	mov.b32 	 %f1231, %r415;
-	mov.b32 	 %f1232, %r416;
-	mov.b32 	 %f1233, %r417;
-	mul.f32 	%f1234, %f368, %f1230;
-	mul.f32 	%f1235, %f368, %f1231;
-	mul.f32 	%f1236, %f368, %f1232;
-	mul.f32 	%f1237, %f368, %f1233;
-	fma.rn.f32 	%f1238, %f1214, %f1802, %f1234;
-	fma.rn.f32 	%f1803, %f1214, %f1803, %f1235;
-	fma.rn.f32 	%f1804, %f1214, %f1804, %f1236;
-	fma.rn.f32 	%f1805, %f1214, %f1805, %f1237;
-	mul.f32 	%f1239, %f1228, %f1228;
-	fma.rn.f32 	%f1240, %f1227, %f1227, %f1239;
-	fma.rn.f32 	%f1241, %f1229, %f1229, %f1240;
-	fma.rn.f32 	%f1242, %f1238, %f1238, %f1241;
-	sqrt.rn.f32 	%f1243, %f1242;
-	rcp.rn.f32 	%f1244, %f1243;
-	mul.f32 	%f1799, %f1227, %f1244;
-	mul.f32 	%f1800, %f1228, %f1244;
-	mul.f32 	%f1801, %f1229, %f1244;
-	mul.f32 	%f1802, %f1238, %f1244;
-
-BB12_66:
-	mul.f32 	%f1245, %f1800, %f1800;
-	fma.rn.f32 	%f1246, %f1799, %f1799, %f1245;
-	fma.rn.f32 	%f1247, %f1801, %f1801, %f1246;
-	fma.rn.f32 	%f1248, %f1802, %f1802, %f1247;
-	rcp.rn.f32 	%f1249, %f1248;
-	mul.f32 	%f1250, %f1799, %f1249;
-	mul.f32 	%f1251, %f1800, %f1249;
-	mul.f32 	%f1252, %f1801, %f1249;
-	mul.f32 	%f1253, %f1802, %f1249;
-	mul.f32 	%f1254, %f1799, %f1250;
-	mul.f32 	%f1255, %f1800, %f1251;
-	mul.f32 	%f1256, %f1801, %f1252;
-	mul.f32 	%f1257, %f1799, %f1251;
-	mul.f32 	%f1258, %f1801, %f1253;
-	mul.f32 	%f1259, %f1799, %f1252;
-	mul.f32 	%f1260, %f1800, %f1253;
-	mul.f32 	%f1261, %f1800, %f1252;
-	mul.f32 	%f1262, %f1799, %f1253;
-	sub.f32 	%f1263, %f1254, %f1255;
-	sub.f32 	%f1264, %f1263, %f1256;
-	fma.rn.f32 	%f1265, %f1802, %f1253, %f1264;
-	sub.f32 	%f1266, %f1257, %f1258;
-	add.f32 	%f1267, %f1266, %f1266;
-	add.f32 	%f1268, %f1259, %f1260;
-	add.f32 	%f1269, %f1268, %f1268;
-	add.f32 	%f1270, %f1257, %f1258;
-	add.f32 	%f1271, %f1270, %f1270;
-	sub.f32 	%f1272, %f1255, %f1254;
-	sub.f32 	%f1273, %f1272, %f1256;
-	fma.rn.f32 	%f1274, %f1802, %f1253, %f1273;
-	sub.f32 	%f1275, %f1261, %f1262;
-	add.f32 	%f1276, %f1275, %f1275;
-	sub.f32 	%f1277, %f1259, %f1260;
-	add.f32 	%f1278, %f1277, %f1277;
-	add.f32 	%f1279, %f1261, %f1262;
-	add.f32 	%f1280, %f1279, %f1279;
-	neg.f32 	%f1281, %f1254;
-	sub.f32 	%f1282, %f1281, %f1255;
-	add.f32 	%f1283, %f1256, %f1282;
-	fma.rn.f32 	%f1284, %f1802, %f1253, %f1283;
-	mul.f32 	%f1285, %f1793, %f1265;
-	fma.rn.f32 	%f1286, %f1796, %f1267, %f1285;
-	fma.rn.f32 	%f1287, %f1798, %f1269, %f1286;
-	sub.f32 	%f1817, %f1803, %f1287;
-	mul.f32 	%f1288, %f1796, %f1274;
-	fma.rn.f32 	%f1289, %f1793, %f1271, %f1288;
-	fma.rn.f32 	%f1290, %f1798, %f1276, %f1289;
-	sub.f32 	%f1813, %f1804, %f1290;
-	mul.f32 	%f1291, %f1796, %f1280;
-	fma.rn.f32 	%f1292, %f1793, %f1278, %f1291;
-	fma.rn.f32 	%f1293, %f1798, %f1284, %f1292;
-	sub.f32 	%f1809, %f1805, %f1293;
-	mul.f32 	%f1294, %f1792, %f1265;
-	fma.rn.f32 	%f1295, %f1795, %f1267, %f1294;
-	fma.rn.f32 	%f1816, %f1797, %f1269, %f1295;
-	mul.f32 	%f1296, %f1795, %f1274;
-	fma.rn.f32 	%f1297, %f1792, %f1271, %f1296;
-	fma.rn.f32 	%f1812, %f1797, %f1276, %f1297;
-	mul.f32 	%f1298, %f1795, %f1280;
-	fma.rn.f32 	%f1299, %f1792, %f1278, %f1298;
-	fma.rn.f32 	%f1808, %f1797, %f1284, %f1299;
-	mul.f32 	%f1300, %f1791, %f1265;
-	fma.rn.f32 	%f1815, %f1794, %f1267, %f1300;
-	mul.f32 	%f1301, %f1794, %f1274;
-	fma.rn.f32 	%f1811, %f1791, %f1271, %f1301;
-	mul.f32 	%f1302, %f1794, %f1280;
-	fma.rn.f32 	%f1807, %f1791, %f1278, %f1302;
-	mul.f32 	%f1814, %f1790, %f1265;
-	mul.f32 	%f1810, %f1790, %f1271;
-	mul.f32 	%f1806, %f1790, %f1278;
-	bra.uni 	BB12_69;
-
-BB12_59:
-	setp.ne.s32	%p30, %r329, 1;
-	mov.f32 	%f1807, %f1806;
-	mov.f32 	%f1809, %f1806;
-	mov.f32 	%f1810, %f1806;
-	mov.f32 	%f1811, %f1808;
-	mov.f32 	%f1812, %f1806;
-	mov.f32 	%f1813, %f1806;
-	mov.f32 	%f1814, %f1808;
-	mov.f32 	%f1815, %f1806;
-	mov.f32 	%f1816, %f1806;
-	mov.f32 	%f1817, %f1806;
-	@%p30 bra 	BB12_69;
-
-	// inline asm
-	call (%rd297), _optix_get_static_transform_from_handle, (%rd295);
-	// inline asm
-	add.s64 	%rd645, %rd297, 16;
-
-BB12_62:
-	// inline asm
-	cvta.to.global.u64 %rd301, %rd645;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r331,%r332,%r333,%r334}, [%rd301];
-	// inline asm
-	mov.b32 	 %f1814, %r331;
-	mov.b32 	 %f1815, %r332;
-	mov.b32 	 %f1816, %r333;
-	mov.b32 	 %f1817, %r334;
-	add.s64 	%rd305, %rd645, 16;
-	// inline asm
-	cvta.to.global.u64 %rd304, %rd305;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r335,%r336,%r337,%r338}, [%rd304];
-	// inline asm
-	mov.b32 	 %f1810, %r335;
-	mov.b32 	 %f1811, %r336;
-	mov.b32 	 %f1812, %r337;
-	mov.b32 	 %f1813, %r338;
-	add.s64 	%rd308, %rd645, 32;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r483,%r484,%r485,%r486}, [%rd401];
+	// end inline asm
+	mov.b32 	%f1365, %r483;
+	mov.b32 	%f1366, %r484;
+	mov.b32 	%f1367, %r485;
+	mov.b32 	%f1368, %r486;
+	mul.f32 	%f1369, %f454, %f1365;
+	mul.f32 	%f1370, %f454, %f1366;
+	mul.f32 	%f1371, %f454, %f1367;
+	mul.f32 	%f1372, %f454, %f1368;
+	fma.rn.f32 	%f1867, %f1348, %f1867, %f1369;
+	fma.rn.f32 	%f1868, %f1348, %f1868, %f1370;
+	fma.rn.f32 	%f1869, %f1348, %f1869, %f1371;
+	fma.rn.f32 	%f1870, %f1348, %f1870, %f1372;
+	bra.uni 	$L__BB12_69;
+
+$L__BB12_58:
+	mov.f32 	%f1867, 0f00000000;
+	mov.f32 	%f1869, 0f3F800000;
+	setp.eq.s32 	%p31, %r339, 4;
+	@%p31 bra 	$L__BB12_61;
+
+	setp.ne.s32 	%p32, %r339, 1;
+	mov.f32 	%f1868, %f1867;
+	mov.f32 	%f1870, %f1867;
+	mov.f32 	%f1871, %f1867;
+	mov.f32 	%f1872, %f1869;
+	mov.f32 	%f1873, %f1867;
+	mov.f32 	%f1874, %f1867;
+	mov.f32 	%f1875, %f1869;
+	mov.f32 	%f1876, %f1867;
+	mov.f32 	%f1877, %f1867;
+	mov.f32 	%f1878, %f1867;
+	@%p32 bra 	$L__BB12_69;
+
+	// begin inline asm
+	call (%rd289), _optix_get_static_transform_from_handle, (%rd287);
+	// end inline asm
+	add.s64 	%rd632, %rd289, 16;
+	bra.uni 	$L__BB12_62;
+
+$L__BB12_64:
+	// begin inline asm
+	call (%rd302), _optix_get_srt_motion_transform_from_handle, (%rd287);
+	// end inline asm
+	// begin inline asm
+	cvta.to.global.u64 %rd304, %rd302;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r353,%r354,%r355,%r356}, [%rd304];
+	// end inline asm
+	add.s64 	%rd308, %rd302, 16;
+	// begin inline asm
 	cvta.to.global.u64 %rd307, %rd308;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r339,%r340,%r341,%r342}, [%rd307];
-	// inline asm
-	mov.b32 	 %f1806, %r339;
-	mov.b32 	 %f1807, %r340;
-	mov.b32 	 %f1808, %r341;
-	mov.b32 	 %f1809, %r342;
-
-BB12_69:
-	add.s32 	%r14, %r633, 1;
-	setp.eq.s32	%p34, %r14, %r26;
-	@%p34 bra 	BB12_70;
-	bra.uni 	BB12_71;
-
-BB12_70:
-	mov.f32 	%f1789, %f1806;
-	mov.f32 	%f1788, %f1807;
-	mov.f32 	%f1787, %f1808;
-	mov.f32 	%f1786, %f1809;
-	mov.f32 	%f1785, %f1810;
-	mov.f32 	%f1784, %f1811;
-	mov.f32 	%f1783, %f1812;
-	mov.f32 	%f1782, %f1813;
-	mov.f32 	%f1781, %f1814;
-	mov.f32 	%f1780, %f1815;
-	mov.f32 	%f1779, %f1816;
-	mov.f32 	%f1778, %f1817;
-	bra.uni 	BB12_72;
-
-BB12_71:
-	mul.f32 	%f1340, %f1785, %f1815;
-	fma.rn.f32 	%f1341, %f1781, %f1814, %f1340;
-	fma.rn.f32 	%f458, %f1789, %f1816, %f1341;
-	mul.f32 	%f1342, %f1784, %f1815;
-	fma.rn.f32 	%f1343, %f1780, %f1814, %f1342;
-	fma.rn.f32 	%f459, %f1788, %f1816, %f1343;
-	mul.f32 	%f1344, %f1783, %f1815;
-	fma.rn.f32 	%f1345, %f1779, %f1814, %f1344;
-	fma.rn.f32 	%f460, %f1787, %f1816, %f1345;
-	mul.f32 	%f1346, %f1782, %f1815;
-	fma.rn.f32 	%f1347, %f1778, %f1814, %f1346;
-	fma.rn.f32 	%f1348, %f1786, %f1816, %f1347;
-	add.f32 	%f461, %f1817, %f1348;
-	mul.f32 	%f1349, %f1785, %f1811;
-	fma.rn.f32 	%f1350, %f1781, %f1810, %f1349;
-	fma.rn.f32 	%f462, %f1789, %f1812, %f1350;
-	mul.f32 	%f1351, %f1784, %f1811;
-	fma.rn.f32 	%f1352, %f1780, %f1810, %f1351;
-	fma.rn.f32 	%f463, %f1788, %f1812, %f1352;
-	mul.f32 	%f1353, %f1783, %f1811;
-	fma.rn.f32 	%f1354, %f1779, %f1810, %f1353;
-	fma.rn.f32 	%f464, %f1787, %f1812, %f1354;
-	mul.f32 	%f1355, %f1782, %f1811;
-	fma.rn.f32 	%f1356, %f1778, %f1810, %f1355;
-	fma.rn.f32 	%f1357, %f1786, %f1812, %f1356;
-	add.f32 	%f465, %f1813, %f1357;
-	mul.f32 	%f1358, %f1785, %f1807;
-	fma.rn.f32 	%f1359, %f1781, %f1806, %f1358;
-	fma.rn.f32 	%f1789, %f1789, %f1808, %f1359;
-	mul.f32 	%f1360, %f1784, %f1807;
-	fma.rn.f32 	%f1361, %f1780, %f1806, %f1360;
-	fma.rn.f32 	%f1788, %f1788, %f1808, %f1361;
-	mul.f32 	%f1362, %f1783, %f1807;
-	fma.rn.f32 	%f1363, %f1779, %f1806, %f1362;
-	fma.rn.f32 	%f1787, %f1787, %f1808, %f1363;
-	mul.f32 	%f1364, %f1782, %f1807;
-	fma.rn.f32 	%f1365, %f1778, %f1806, %f1364;
-	fma.rn.f32 	%f1366, %f1786, %f1808, %f1365;
-	add.f32 	%f1786, %f1809, %f1366;
-	mov.f32 	%f1785, %f462;
-	mov.f32 	%f1784, %f463;
-	mov.f32 	%f1783, %f464;
-	mov.f32 	%f1782, %f465;
-	mov.f32 	%f1781, %f458;
-	mov.f32 	%f1780, %f459;
-	mov.f32 	%f1779, %f460;
-	mov.f32 	%f1778, %f461;
-
-BB12_72:
-	add.s32 	%r633, %r14, -2;
-	setp.gt.s32	%p35, %r633, -1;
-	@%p35 bra 	BB12_57;
-
-BB12_73:
-	mov.u32 	%r634, 0;
-	mov.f32 	%f1842, %f1843;
-	mov.f32 	%f1847, %f1843;
-	mov.f32 	%f1846, %f1844;
-	mov.f32 	%f1845, %f1843;
-	mov.f32 	%f1850, %f1843;
-	mov.f32 	%f1849, %f1843;
-	mov.f32 	%f1848, %f1844;
-	@%p2 bra 	BB12_91;
-
-BB12_74:
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r357,%r358,%r359,%r360}, [%rd307];
+	// end inline asm
+	add.s64 	%rd311, %rd302, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd310, %rd311;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r361,%r362,%r363,%r364}, [%rd310];
+	// end inline asm
+	add.s64 	%rd314, %rd302, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd313, %rd314;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r365,%r366,%r367,%r368}, [%rd313];
+	// end inline asm
+	add.s64 	%rd317, %rd302, 64;
+	// begin inline asm
+	cvta.to.global.u64 %rd316, %rd317;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r369,%r370,%r371,%r372}, [%rd316];
+	// end inline asm
+	add.s64 	%rd320, %rd302, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd319, %rd320;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r373,%r374,%r375,%r376}, [%rd319];
+	// end inline asm
+	add.s64 	%rd323, %rd302, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd322, %rd323;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r377,%r378,%r379,%r380}, [%rd322];
+	// end inline asm
+	add.s64 	%rd326, %rd302, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd325, %rd326;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r381,%r382,%r383,%r384}, [%rd325];
+	// end inline asm
+	add.s64 	%rd329, %rd302, 128;
+	// begin inline asm
+	cvta.to.global.u64 %rd328, %rd329;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r385,%r386,%r387,%r388}, [%rd328];
+	// end inline asm
+	add.s64 	%rd332, %rd302, 144;
+	// begin inline asm
+	cvta.to.global.u64 %rd331, %rd332;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r389,%r390,%r391,%r392}, [%rd331];
+	// end inline asm
+	mov.b32 	%f1223, %r356;
+	mov.b32 	%f1224, %r357;
+	and.b32  	%r409, %r355, 65535;
+	add.s32 	%r410, %r409, -1;
+	cvt.rn.f32.s32 	%f1225, %r410;
+	sub.f32 	%f1226, %f1208, %f1223;
+	mul.f32 	%f1227, %f1226, %f1225;
+	sub.f32 	%f1228, %f1224, %f1223;
+	div.rn.f32 	%f1229, %f1227, %f1228;
+	min.f32 	%f1230, %f1225, %f1229;
+	mov.f32 	%f1231, 0f00000000;
+	max.f32 	%f1232, %f1231, %f1230;
+	cvt.rmi.f32.f32 	%f1233, %f1232;
+	sub.f32 	%f393, %f1232, %f1233;
+	cvt.rzi.s32.f32 	%r411, %f1233;
+	mul.wide.s32 	%rd346, %r411, 64;
+	add.s64 	%rd335, %rd311, %rd346;
+	// begin inline asm
+	cvta.to.global.u64 %rd334, %rd335;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r393,%r394,%r395,%r396}, [%rd334];
+	// end inline asm
+	mov.b32 	%f1851, %r393;
+	mov.b32 	%f1852, %r394;
+	mov.b32 	%f1853, %r395;
+	mov.b32 	%f1854, %r396;
+	add.s64 	%rd338, %rd335, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd337, %rd338;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r397,%r398,%r399,%r400}, [%rd337];
+	// end inline asm
+	mov.b32 	%f1855, %r397;
+	mov.b32 	%f1856, %r398;
+	mov.b32 	%f1857, %r399;
+	mov.b32 	%f1858, %r400;
+	add.s64 	%rd341, %rd335, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd340, %rd341;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r401,%r402,%r403,%r404}, [%rd340];
+	// end inline asm
+	mov.b32 	%f1859, %r401;
+	mov.b32 	%f1860, %r402;
+	mov.b32 	%f1861, %r403;
+	mov.b32 	%f1862, %r404;
+	add.s64 	%rd344, %rd335, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd343, %rd344;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r405,%r406,%r407,%r408}, [%rd343];
+	// end inline asm
+	mov.b32 	%f1863, %r405;
+	mov.b32 	%f1864, %r406;
+	mov.b32 	%f1865, %r407;
+	mov.b32 	%f1866, %r408;
+	setp.leu.f32 	%p34, %f393, 0f00000000;
+	@%p34 bra 	$L__BB12_66;
+
+	mov.f32 	%f1234, 0f3F800000;
+	sub.f32 	%f1235, %f1234, %f393;
+	add.s64 	%rd348, %rd335, 64;
+	// begin inline asm
+	cvta.to.global.u64 %rd347, %rd348;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r412,%r413,%r414,%r415}, [%rd347];
+	// end inline asm
+	mov.b32 	%f1236, %r412;
+	mov.b32 	%f1237, %r413;
+	mov.b32 	%f1238, %r414;
+	mov.b32 	%f1239, %r415;
+	mul.f32 	%f1240, %f393, %f1236;
+	mul.f32 	%f1241, %f393, %f1237;
+	mul.f32 	%f1242, %f393, %f1238;
+	mul.f32 	%f1243, %f393, %f1239;
+	fma.rn.f32 	%f1851, %f1235, %f1851, %f1240;
+	fma.rn.f32 	%f1852, %f1235, %f1852, %f1241;
+	fma.rn.f32 	%f1853, %f1235, %f1853, %f1242;
+	fma.rn.f32 	%f1854, %f1235, %f1854, %f1243;
+	add.s64 	%rd351, %rd335, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd350, %rd351;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r416,%r417,%r418,%r419}, [%rd350];
+	// end inline asm
+	mov.b32 	%f1244, %r416;
+	mov.b32 	%f1245, %r417;
+	mov.b32 	%f1246, %r418;
+	mov.b32 	%f1247, %r419;
+	mul.f32 	%f1248, %f393, %f1244;
+	mul.f32 	%f1249, %f393, %f1245;
+	mul.f32 	%f1250, %f393, %f1246;
+	mul.f32 	%f1251, %f393, %f1247;
+	fma.rn.f32 	%f1855, %f1235, %f1855, %f1248;
+	fma.rn.f32 	%f1856, %f1235, %f1856, %f1249;
+	fma.rn.f32 	%f1857, %f1235, %f1857, %f1250;
+	fma.rn.f32 	%f1858, %f1235, %f1858, %f1251;
+	add.s64 	%rd354, %rd335, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd353, %rd354;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r420,%r421,%r422,%r423}, [%rd353];
+	// end inline asm
+	mov.b32 	%f1252, %r420;
+	mov.b32 	%f1253, %r421;
+	mov.b32 	%f1254, %r422;
+	mov.b32 	%f1255, %r423;
+	mul.f32 	%f1256, %f393, %f1252;
+	mul.f32 	%f1257, %f393, %f1253;
+	mul.f32 	%f1258, %f393, %f1254;
+	mul.f32 	%f1259, %f393, %f1255;
+	fma.rn.f32 	%f1859, %f1235, %f1859, %f1256;
+	fma.rn.f32 	%f1260, %f1235, %f1860, %f1257;
+	fma.rn.f32 	%f1261, %f1235, %f1861, %f1258;
+	fma.rn.f32 	%f1262, %f1235, %f1862, %f1259;
+	add.s64 	%rd357, %rd335, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd356, %rd357;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r424,%r425,%r426,%r427}, [%rd356];
+	// end inline asm
+	mov.b32 	%f1263, %r424;
+	mov.b32 	%f1264, %r425;
+	mov.b32 	%f1265, %r426;
+	mov.b32 	%f1266, %r427;
+	mul.f32 	%f1267, %f393, %f1263;
+	mul.f32 	%f1268, %f393, %f1264;
+	mul.f32 	%f1269, %f393, %f1265;
+	mul.f32 	%f1270, %f393, %f1266;
+	fma.rn.f32 	%f1271, %f1235, %f1863, %f1267;
+	fma.rn.f32 	%f1864, %f1235, %f1864, %f1268;
+	fma.rn.f32 	%f1865, %f1235, %f1865, %f1269;
+	fma.rn.f32 	%f1866, %f1235, %f1866, %f1270;
+	mul.f32 	%f1272, %f1261, %f1261;
+	fma.rn.f32 	%f1273, %f1260, %f1260, %f1272;
+	fma.rn.f32 	%f1274, %f1262, %f1262, %f1273;
+	fma.rn.f32 	%f1275, %f1271, %f1271, %f1274;
+	sqrt.rn.f32 	%f1276, %f1275;
+	rcp.rn.f32 	%f1277, %f1276;
+	mul.f32 	%f1860, %f1260, %f1277;
+	mul.f32 	%f1861, %f1261, %f1277;
+	mul.f32 	%f1862, %f1262, %f1277;
+	mul.f32 	%f1863, %f1277, %f1271;
+
+$L__BB12_66:
+	mul.f32 	%f1278, %f1861, %f1861;
+	fma.rn.f32 	%f1279, %f1860, %f1860, %f1278;
+	fma.rn.f32 	%f1280, %f1862, %f1862, %f1279;
+	fma.rn.f32 	%f1281, %f1863, %f1863, %f1280;
+	rcp.rn.f32 	%f1282, %f1281;
+	mul.f32 	%f1283, %f1860, %f1282;
+	mul.f32 	%f1284, %f1861, %f1282;
+	mul.f32 	%f1285, %f1862, %f1282;
+	mul.f32 	%f1286, %f1863, %f1282;
+	mul.f32 	%f1287, %f1860, %f1283;
+	mul.f32 	%f1288, %f1861, %f1284;
+	mul.f32 	%f1289, %f1862, %f1285;
+	mul.f32 	%f1290, %f1860, %f1284;
+	mul.f32 	%f1291, %f1862, %f1286;
+	mul.f32 	%f1292, %f1860, %f1285;
+	mul.f32 	%f1293, %f1861, %f1286;
+	mul.f32 	%f1294, %f1861, %f1285;
+	mul.f32 	%f1295, %f1860, %f1286;
+	sub.f32 	%f1296, %f1287, %f1288;
+	sub.f32 	%f1297, %f1296, %f1289;
+	fma.rn.f32 	%f1298, %f1863, %f1286, %f1297;
+	sub.f32 	%f1299, %f1290, %f1291;
+	add.f32 	%f1300, %f1299, %f1299;
+	add.f32 	%f1301, %f1292, %f1293;
+	add.f32 	%f1302, %f1301, %f1301;
+	add.f32 	%f1303, %f1290, %f1291;
+	add.f32 	%f1304, %f1303, %f1303;
+	sub.f32 	%f1305, %f1288, %f1287;
+	sub.f32 	%f1306, %f1305, %f1289;
+	fma.rn.f32 	%f1307, %f1863, %f1286, %f1306;
+	sub.f32 	%f1308, %f1294, %f1295;
+	add.f32 	%f1309, %f1308, %f1308;
+	sub.f32 	%f1310, %f1292, %f1293;
+	add.f32 	%f1311, %f1310, %f1310;
+	add.f32 	%f1312, %f1294, %f1295;
+	add.f32 	%f1313, %f1312, %f1312;
+	neg.f32 	%f1314, %f1287;
+	sub.f32 	%f1315, %f1314, %f1288;
+	add.f32 	%f1316, %f1289, %f1315;
+	fma.rn.f32 	%f1317, %f1863, %f1286, %f1316;
+	mul.f32 	%f1318, %f1854, %f1298;
+	fma.rn.f32 	%f1319, %f1857, %f1300, %f1318;
+	fma.rn.f32 	%f1320, %f1859, %f1302, %f1319;
+	sub.f32 	%f1878, %f1864, %f1320;
+	mul.f32 	%f1321, %f1857, %f1307;
+	fma.rn.f32 	%f1322, %f1854, %f1304, %f1321;
+	fma.rn.f32 	%f1323, %f1859, %f1309, %f1322;
+	sub.f32 	%f1874, %f1865, %f1323;
+	mul.f32 	%f1324, %f1857, %f1313;
+	fma.rn.f32 	%f1325, %f1854, %f1311, %f1324;
+	fma.rn.f32 	%f1326, %f1859, %f1317, %f1325;
+	sub.f32 	%f1870, %f1866, %f1326;
+	mul.f32 	%f1327, %f1853, %f1298;
+	fma.rn.f32 	%f1328, %f1856, %f1300, %f1327;
+	fma.rn.f32 	%f1877, %f1858, %f1302, %f1328;
+	mul.f32 	%f1329, %f1856, %f1307;
+	fma.rn.f32 	%f1330, %f1853, %f1304, %f1329;
+	fma.rn.f32 	%f1873, %f1858, %f1309, %f1330;
+	mul.f32 	%f1331, %f1856, %f1313;
+	fma.rn.f32 	%f1332, %f1853, %f1311, %f1331;
+	fma.rn.f32 	%f1869, %f1858, %f1317, %f1332;
+	mul.f32 	%f1333, %f1852, %f1298;
+	fma.rn.f32 	%f1876, %f1855, %f1300, %f1333;
+	mul.f32 	%f1334, %f1855, %f1307;
+	fma.rn.f32 	%f1872, %f1852, %f1304, %f1334;
+	mul.f32 	%f1335, %f1855, %f1313;
+	fma.rn.f32 	%f1868, %f1852, %f1311, %f1335;
+	mul.f32 	%f1875, %f1851, %f1298;
+	mul.f32 	%f1871, %f1851, %f1304;
+	mul.f32 	%f1867, %f1851, %f1311;
+	bra.uni 	$L__BB12_69;
+
+$L__BB12_61:
+	// begin inline asm
+	call (%rd632), _optix_get_instance_transform_from_handle, (%rd287);
+	// end inline asm
+
+$L__BB12_62:
+	// begin inline asm
+	cvta.to.global.u64 %rd293, %rd632;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r341,%r342,%r343,%r344}, [%rd293];
+	// end inline asm
+	mov.b32 	%f1875, %r341;
+	mov.b32 	%f1876, %r342;
+	mov.b32 	%f1877, %r343;
+	mov.b32 	%f1878, %r344;
+	add.s64 	%rd297, %rd632, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd296, %rd297;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r345,%r346,%r347,%r348}, [%rd296];
+	// end inline asm
+	mov.b32 	%f1871, %r345;
+	mov.b32 	%f1872, %r346;
+	mov.b32 	%f1873, %r347;
+	mov.b32 	%f1874, %r348;
+	add.s64 	%rd300, %rd632, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd299, %rd300;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r349,%r350,%r351,%r352}, [%rd299];
+	// end inline asm
+	mov.b32 	%f1867, %r349;
+	mov.b32 	%f1868, %r350;
+	mov.b32 	%f1869, %r351;
+	mov.b32 	%f1870, %r352;
+
+$L__BB12_69:
+	setp.eq.s32 	%p36, %r645, 1;
+	@%p36 bra 	$L__BB12_71;
+
+	mul.f32 	%f1373, %f1846, %f1876;
+	fma.rn.f32 	%f1374, %f1842, %f1875, %f1373;
+	fma.rn.f32 	%f491, %f1850, %f1877, %f1374;
+	mul.f32 	%f1375, %f1845, %f1876;
+	fma.rn.f32 	%f1376, %f1841, %f1875, %f1375;
+	fma.rn.f32 	%f492, %f1849, %f1877, %f1376;
+	mul.f32 	%f1377, %f1844, %f1876;
+	fma.rn.f32 	%f1378, %f1840, %f1875, %f1377;
+	fma.rn.f32 	%f493, %f1848, %f1877, %f1378;
+	mul.f32 	%f1379, %f1843, %f1876;
+	fma.rn.f32 	%f1380, %f1839, %f1875, %f1379;
+	fma.rn.f32 	%f1381, %f1847, %f1877, %f1380;
+	add.f32 	%f1878, %f1878, %f1381;
+	mul.f32 	%f1382, %f1846, %f1872;
+	fma.rn.f32 	%f1383, %f1842, %f1871, %f1382;
+	fma.rn.f32 	%f495, %f1850, %f1873, %f1383;
+	mul.f32 	%f1384, %f1845, %f1872;
+	fma.rn.f32 	%f1385, %f1841, %f1871, %f1384;
+	fma.rn.f32 	%f496, %f1849, %f1873, %f1385;
+	mul.f32 	%f1386, %f1844, %f1872;
+	fma.rn.f32 	%f1387, %f1840, %f1871, %f1386;
+	fma.rn.f32 	%f497, %f1848, %f1873, %f1387;
+	mul.f32 	%f1388, %f1843, %f1872;
+	fma.rn.f32 	%f1389, %f1839, %f1871, %f1388;
+	fma.rn.f32 	%f1390, %f1847, %f1873, %f1389;
+	add.f32 	%f1874, %f1874, %f1390;
+	mul.f32 	%f1391, %f1846, %f1868;
+	fma.rn.f32 	%f1392, %f1842, %f1867, %f1391;
+	fma.rn.f32 	%f499, %f1850, %f1869, %f1392;
+	mul.f32 	%f1393, %f1845, %f1868;
+	fma.rn.f32 	%f1394, %f1841, %f1867, %f1393;
+	fma.rn.f32 	%f500, %f1849, %f1869, %f1394;
+	mul.f32 	%f1395, %f1844, %f1868;
+	fma.rn.f32 	%f1396, %f1840, %f1867, %f1395;
+	fma.rn.f32 	%f501, %f1848, %f1869, %f1396;
+	mul.f32 	%f1397, %f1843, %f1868;
+	fma.rn.f32 	%f1398, %f1839, %f1867, %f1397;
+	fma.rn.f32 	%f1399, %f1847, %f1869, %f1398;
+	add.f32 	%f1870, %f1870, %f1399;
+	mov.f32 	%f1867, %f499;
+	mov.f32 	%f1868, %f500;
+	mov.f32 	%f1869, %f501;
+	mov.f32 	%f1871, %f495;
+	mov.f32 	%f1872, %f496;
+	mov.f32 	%f1873, %f497;
+	mov.f32 	%f1875, %f491;
+	mov.f32 	%f1876, %f492;
+	mov.f32 	%f1877, %f493;
+
+$L__BB12_71:
+	add.s32 	%r645, %r645, -1;
+	add.s32 	%r644, %r644, -1;
+	setp.gt.s32 	%p37, %r644, 1;
+	mov.f32 	%f1839, %f1878;
+	mov.f32 	%f1840, %f1877;
+	mov.f32 	%f1841, %f1876;
+	mov.f32 	%f1842, %f1875;
+	mov.f32 	%f1843, %f1874;
+	mov.f32 	%f1844, %f1873;
+	mov.f32 	%f1845, %f1872;
+	mov.f32 	%f1846, %f1871;
+	mov.f32 	%f1847, %f1870;
+	mov.f32 	%f1848, %f1869;
+	mov.f32 	%f1849, %f1868;
+	mov.f32 	%f1850, %f1867;
+	@%p37 bra 	$L__BB12_57;
+
+$L__BB12_72:
+	// begin inline asm
+	call (%r487), _optix_get_transform_list_size, ();
+	// end inline asm
+	setp.eq.s32 	%p38, %r487, 0;
+	mov.f32 	%f1939, %f1938;
+	mov.f32 	%f1934, %f1938;
+	mov.f32 	%f1935, %f1937;
+	mov.f32 	%f1936, %f1938;
+	mov.f32 	%f1931, %f1938;
+	mov.f32 	%f1932, %f1938;
+	mov.f32 	%f1933, %f1937;
+	@%p38 bra 	$L__BB12_91;
+
+	// begin inline asm
+	call (%r488), _optix_get_transform_list_size, ();
+	// end inline asm
+	// begin inline asm
+	call (%f1409), _optix_get_ray_time, ();
+	// end inline asm
+	setp.eq.s32 	%p39, %r488, 0;
+	@%p39 bra 	$L__BB12_91;
+
+	mov.u32 	%r646, 0;
+
+$L__BB12_75:
 	.pragma "nounroll";
-	// inline asm
-	call (%rd416), _optix_get_transform_list_handle, (%r634);
-	// inline asm
-	// inline asm
-	call (%r479), _optix_get_transform_type_from_handle, (%rd416);
-	// inline asm
-	and.b32  	%r480, %r479, -2;
-	setp.eq.s32	%p37, %r480, 2;
-	@%p37 bra 	BB12_80;
-	bra.uni 	BB12_75;
-
-BB12_80:
-	setp.eq.s32	%p40, %r479, 2;
-	@%p40 bra 	BB12_84;
-	bra.uni 	BB12_81;
-
-BB12_84:
-	// inline asm
-	call (%rd490), _optix_get_matrix_motion_transform_from_handle, (%rd416);
-	// inline asm
-	// inline asm
-	cvta.to.global.u64 %rd492, %rd490;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r568,%r569,%r570,%r571}, [%rd492];
-	// inline asm
-	mov.b32	{%rs16, %rs17}, %r570;
-	add.s64 	%rd496, %rd490, 16;
-	// inline asm
+	// begin inline asm
+	call (%rd406), _optix_get_transform_list_handle, (%r646);
+	// end inline asm
+	// begin inline asm
+	call (%r491), _optix_get_transform_type_from_handle, (%rd406);
+	// end inline asm
+	or.b32  	%r492, %r491, 1;
+	setp.eq.s32 	%p40, %r492, 3;
+	@%p40 bra 	$L__BB12_81;
+	bra.uni 	$L__BB12_76;
+
+$L__BB12_81:
+	setp.eq.s32 	%p43, %r491, 2;
+	@%p43 bra 	$L__BB12_85;
+	bra.uni 	$L__BB12_82;
+
+$L__BB12_85:
+	// begin inline asm
+	call (%rd478), _optix_get_matrix_motion_transform_from_handle, (%rd406);
+	// end inline asm
+	// begin inline asm
+	cvta.to.global.u64 %rd480, %rd478;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r580,%r581,%r582,%r583}, [%rd480];
+	// end inline asm
+	add.s64 	%rd484, %rd478, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd483, %rd484;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r584,%r585,%r586,%r587}, [%rd483];
+	// end inline asm
+	add.s64 	%rd487, %rd478, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd486, %rd487;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r588,%r589,%r590,%r591}, [%rd486];
+	// end inline asm
+	add.s64 	%rd490, %rd478, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd489, %rd490;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r592,%r593,%r594,%r595}, [%rd489];
+	// end inline asm
+	add.s64 	%rd493, %rd478, 64;
+	// begin inline asm
+	cvta.to.global.u64 %rd492, %rd493;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r596,%r597,%r598,%r599}, [%rd492];
+	// end inline asm
+	add.s64 	%rd496, %rd478, 80;
+	// begin inline asm
 	cvta.to.global.u64 %rd495, %rd496;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r572,%r573,%r574,%r575}, [%rd495];
-	// inline asm
-	add.s64 	%rd499, %rd490, 32;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r600,%r601,%r602,%r603}, [%rd495];
+	// end inline asm
+	add.s64 	%rd499, %rd478, 96;
+	// begin inline asm
 	cvta.to.global.u64 %rd498, %rd499;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r576,%r577,%r578,%r579}, [%rd498];
-	// inline asm
-	add.s64 	%rd502, %rd490, 48;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r604,%r605,%r606,%r607}, [%rd498];
+	// end inline asm
+	add.s64 	%rd502, %rd478, 112;
+	// begin inline asm
 	cvta.to.global.u64 %rd501, %rd502;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r580,%r581,%r582,%r583}, [%rd501];
-	// inline asm
-	add.s64 	%rd505, %rd490, 64;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r608,%r609,%r610,%r611}, [%rd501];
+	// end inline asm
+	mov.b32 	%f1513, %r583;
+	mov.b32 	%f1514, %r584;
+	and.b32  	%r624, %r582, 65535;
+	add.s32 	%r625, %r624, -1;
+	cvt.rn.f32.s32 	%f1515, %r625;
+	sub.f32 	%f1516, %f1409, %f1513;
+	mul.f32 	%f1517, %f1516, %f1515;
+	sub.f32 	%f1518, %f1514, %f1513;
+	div.rn.f32 	%f1519, %f1517, %f1518;
+	min.f32 	%f1520, %f1515, %f1519;
+	mov.f32 	%f1521, 0f00000000;
+	max.f32 	%f1522, %f1521, %f1520;
+	cvt.rmi.f32.f32 	%f1523, %f1522;
+	sub.f32 	%f586, %f1522, %f1523;
+	cvt.rzi.s32.f32 	%r626, %f1523;
+	cvt.s64.s32 	%rd35, %r626;
+	mul.wide.s32 	%rd513, %r626, 48;
+	add.s64 	%rd505, %rd487, %rd513;
+	// begin inline asm
 	cvta.to.global.u64 %rd504, %rd505;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r584,%r585,%r586,%r587}, [%rd504];
-	// inline asm
-	add.s64 	%rd508, %rd490, 80;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r612,%r613,%r614,%r615}, [%rd504];
+	// end inline asm
+	mov.b32 	%f1928, %r612;
+	mov.b32 	%f1929, %r613;
+	mov.b32 	%f1930, %r614;
+	add.s64 	%rd508, %rd505, 16;
+	// begin inline asm
 	cvta.to.global.u64 %rd507, %rd508;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r588,%r589,%r590,%r591}, [%rd507];
-	// inline asm
-	add.s64 	%rd511, %rd490, 96;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r616,%r617,%r618,%r619}, [%rd507];
+	// end inline asm
+	mov.b32 	%f1925, %r616;
+	mov.b32 	%f1926, %r617;
+	mov.b32 	%f1927, %r618;
+	add.s64 	%rd511, %rd505, 32;
+	// begin inline asm
 	cvta.to.global.u64 %rd510, %rd511;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r592,%r593,%r594,%r595}, [%rd510];
-	// inline asm
-	add.s64 	%rd514, %rd490, 112;
-	// inline asm
-	cvta.to.global.u64 %rd513, %rd514;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r596,%r597,%r598,%r599}, [%rd513];
-	// inline asm
-	mov.b32 	 %f1478, %r571;
-	mov.b32 	 %f1479, %r572;
-	cvt.u32.u16	%r612, %rs16;
-	add.s32 	%r613, %r612, -1;
-	cvt.rn.f32.s32	%f1480, %r613;
-	sub.f32 	%f1481, %f921, %f1478;
-	mul.f32 	%f1482, %f1481, %f1480;
-	sub.f32 	%f1483, %f1479, %f1478;
-	div.rn.f32 	%f1484, %f1482, %f1483;
-	min.f32 	%f1485, %f1480, %f1484;
-	mov.f32 	%f1486, 0f00000000;
-	max.f32 	%f1487, %f1486, %f1485;
-	cvt.rmi.f32.f32	%f1488, %f1487;
-	cvt.rzi.s32.f32	%r614, %f1488;
-	cvt.s64.s32	%rd39, %r614;
-	mul.wide.s32 	%rd525, %r614, 48;
-	add.s64 	%rd517, %rd499, %rd525;
-	// inline asm
-	cvta.to.global.u64 %rd516, %rd517;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r600,%r601,%r602,%r603}, [%rd516];
-	// inline asm
-	mov.b32 	 %f1867, %r600;
-	mov.b32 	 %f1868, %r601;
-	mov.b32 	 %f1869, %r602;
-	add.s64 	%rd520, %rd517, 16;
-	// inline asm
-	cvta.to.global.u64 %rd519, %rd520;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r604,%r605,%r606,%r607}, [%rd519];
-	// inline asm
-	mov.b32 	 %f1864, %r604;
-	mov.b32 	 %f1865, %r605;
-	mov.b32 	 %f1866, %r606;
-	add.s64 	%rd523, %rd517, 32;
-	// inline asm
-	cvta.to.global.u64 %rd522, %rd523;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r608,%r609,%r610,%r611}, [%rd522];
-	// inline asm
-	sub.f32 	%f558, %f1487, %f1488;
-	mov.b32 	 %f1861, %r608;
-	mov.b32 	 %f1862, %r609;
-	mov.b32 	 %f1863, %r610;
-	setp.leu.f32	%p42, %f558, 0f00000000;
-	@%p42 bra 	BB12_86;
-
-	mul.lo.s64 	%rd535, %rd39, 48;
-	add.s64 	%rd536, %rd490, %rd535;
-	add.s64 	%rd527, %rd536, 80;
-	// inline asm
-	cvta.to.global.u64 %rd526, %rd527;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r615,%r616,%r617,%r618}, [%rd526];
-	// inline asm
-	mov.b32 	 %f1489, %r615;
-	mov.b32 	 %f1490, %r616;
-	mov.b32 	 %f1491, %r617;
-	add.s64 	%rd530, %rd536, 96;
-	// inline asm
-	cvta.to.global.u64 %rd529, %rd530;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r619,%r620,%r621,%r622}, [%rd529];
-	// inline asm
-	mov.b32 	 %f1492, %r619;
-	mov.b32 	 %f1493, %r620;
-	mov.b32 	 %f1494, %r621;
-	add.s64 	%rd533, %rd536, 112;
-	// inline asm
-	cvta.to.global.u64 %rd532, %rd533;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r623,%r624,%r625,%r626}, [%rd532];
-	// inline asm
-	mov.f32 	%f1495, 0f3F800000;
-	sub.f32 	%f1496, %f1495, %f558;
-	mul.f32 	%f1497, %f558, %f1489;
-	mul.f32 	%f1498, %f558, %f1490;
-	mul.f32 	%f1499, %f558, %f1491;
-	fma.rn.f32 	%f1867, %f1496, %f1867, %f1497;
-	fma.rn.f32 	%f1868, %f1496, %f1868, %f1498;
-	fma.rn.f32 	%f1869, %f1496, %f1869, %f1499;
-	mul.f32 	%f1500, %f558, %f1492;
-	mul.f32 	%f1501, %f558, %f1493;
-	mul.f32 	%f1502, %f558, %f1494;
-	fma.rn.f32 	%f1864, %f1496, %f1864, %f1500;
-	fma.rn.f32 	%f1865, %f1496, %f1865, %f1501;
-	fma.rn.f32 	%f1866, %f1496, %f1866, %f1502;
-	mov.b32 	 %f1503, %r623;
-	mov.b32 	 %f1504, %r624;
-	mov.b32 	 %f1505, %r625;
-	mul.f32 	%f1506, %f558, %f1503;
-	mul.f32 	%f1507, %f558, %f1504;
-	mul.f32 	%f1508, %f558, %f1505;
-	fma.rn.f32 	%f1861, %f1496, %f1861, %f1506;
-	fma.rn.f32 	%f1862, %f1496, %f1862, %f1507;
-	fma.rn.f32 	%f1863, %f1496, %f1863, %f1508;
-	bra.uni 	BB12_86;
-
-BB12_75:
-	mov.f32 	%f1870, 0f00000000;
-	mov.f32 	%f1872, 0f3F800000;
-	setp.eq.s32	%p38, %r479, 4;
-	@%p38 bra 	BB12_78;
-	bra.uni 	BB12_76;
-
-BB12_78:
-	// inline asm
-	call (%rd646), _optix_get_instance_inverse_transform_from_handle, (%rd416);
-	// inline asm
-	bra.uni 	BB12_79;
-
-BB12_81:
-	// inline asm
-	call (%rd431), _optix_get_srt_motion_transform_from_handle, (%rd416);
-	// inline asm
-	// inline asm
-	cvta.to.global.u64 %rd433, %rd431;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r493,%r494,%r495,%r496}, [%rd433];
-	// inline asm
-	mov.b32	{%rs14, %rs15}, %r495;
-	add.s64 	%rd437, %rd431, 16;
-	// inline asm
-	cvta.to.global.u64 %rd436, %rd437;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r497,%r498,%r499,%r500}, [%rd436];
-	// inline asm
-	add.s64 	%rd440, %rd431, 32;
-	// inline asm
-	cvta.to.global.u64 %rd439, %rd440;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r501,%r502,%r503,%r504}, [%rd439];
-	// inline asm
-	add.s64 	%rd443, %rd431, 48;
-	// inline asm
-	cvta.to.global.u64 %rd442, %rd443;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r505,%r506,%r507,%r508}, [%rd442];
-	// inline asm
-	add.s64 	%rd446, %rd431, 64;
-	// inline asm
-	cvta.to.global.u64 %rd445, %rd446;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r509,%r510,%r511,%r512}, [%rd445];
-	// inline asm
-	add.s64 	%rd449, %rd431, 80;
-	// inline asm
-	cvta.to.global.u64 %rd448, %rd449;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r513,%r514,%r515,%r516}, [%rd448];
-	// inline asm
-	add.s64 	%rd452, %rd431, 96;
-	// inline asm
-	cvta.to.global.u64 %rd451, %rd452;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r517,%r518,%r519,%r520}, [%rd451];
-	// inline asm
-	add.s64 	%rd455, %rd431, 112;
-	// inline asm
-	cvta.to.global.u64 %rd454, %rd455;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r521,%r522,%r523,%r524}, [%rd454];
-	// inline asm
-	add.s64 	%rd458, %rd431, 128;
-	// inline asm
-	cvta.to.global.u64 %rd457, %rd458;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r525,%r526,%r527,%r528}, [%rd457];
-	// inline asm
-	add.s64 	%rd461, %rd431, 144;
-	// inline asm
-	cvta.to.global.u64 %rd460, %rd461;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r529,%r530,%r531,%r532}, [%rd460];
-	// inline asm
-	mov.b32 	 %f1386, %r496;
-	mov.b32 	 %f1387, %r497;
-	cvt.u32.u16	%r549, %rs14;
-	add.s32 	%r550, %r549, -1;
-	cvt.rn.f32.s32	%f1388, %r550;
-	sub.f32 	%f1389, %f921, %f1386;
-	mul.f32 	%f1390, %f1389, %f1388;
-	sub.f32 	%f1391, %f1387, %f1386;
-	div.rn.f32 	%f1392, %f1390, %f1391;
-	min.f32 	%f1393, %f1388, %f1392;
-	mov.f32 	%f1394, 0f00000000;
-	max.f32 	%f1395, %f1394, %f1393;
-	cvt.rmi.f32.f32	%f1396, %f1395;
-	cvt.rzi.s32.f32	%r551, %f1396;
-	cvt.s64.s32	%rd37, %r551;
-	mul.wide.s32 	%rd475, %r551, 64;
-	add.s64 	%rd464, %rd440, %rd475;
-	// inline asm
-	cvta.to.global.u64 %rd463, %rd464;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r533,%r534,%r535,%r536}, [%rd463];
-	// inline asm
-	mov.b32 	 %f1851, %r533;
-	mov.b32 	 %f1852, %r534;
-	mov.b32 	 %f1853, %r535;
-	add.s64 	%rd467, %rd464, 16;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r620,%r621,%r622,%r623}, [%rd510];
+	// end inline asm
+	mov.b32 	%f1922, %r620;
+	mov.b32 	%f1923, %r621;
+	mov.b32 	%f1924, %r622;
+	setp.leu.f32 	%p45, %f586, 0f00000000;
+	@%p45 bra 	$L__BB12_87;
+
+	mov.f32 	%f1524, 0f3F800000;
+	sub.f32 	%f1525, %f1524, %f586;
+	mul.lo.s64 	%rd523, %rd35, 48;
+	add.s64 	%rd524, %rd478, %rd523;
+	add.s64 	%rd515, %rd524, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd514, %rd515;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r627,%r628,%r629,%r630}, [%rd514];
+	// end inline asm
+	mov.b32 	%f1526, %r627;
+	mov.b32 	%f1527, %r628;
+	mov.b32 	%f1528, %r629;
+	mul.f32 	%f1529, %f586, %f1526;
+	mul.f32 	%f1530, %f586, %f1527;
+	mul.f32 	%f1531, %f586, %f1528;
+	fma.rn.f32 	%f1928, %f1525, %f1928, %f1529;
+	fma.rn.f32 	%f1929, %f1525, %f1929, %f1530;
+	fma.rn.f32 	%f1930, %f1525, %f1930, %f1531;
+	add.s64 	%rd518, %rd524, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd517, %rd518;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r631,%r632,%r633,%r634}, [%rd517];
+	// end inline asm
+	mov.b32 	%f1532, %r631;
+	mov.b32 	%f1533, %r632;
+	mov.b32 	%f1534, %r633;
+	mul.f32 	%f1535, %f586, %f1532;
+	mul.f32 	%f1536, %f586, %f1533;
+	mul.f32 	%f1537, %f586, %f1534;
+	fma.rn.f32 	%f1925, %f1525, %f1925, %f1535;
+	fma.rn.f32 	%f1926, %f1525, %f1926, %f1536;
+	fma.rn.f32 	%f1927, %f1525, %f1927, %f1537;
+	add.s64 	%rd521, %rd524, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd520, %rd521;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r635,%r636,%r637,%r638}, [%rd520];
+	// end inline asm
+	mov.b32 	%f1538, %r635;
+	mov.b32 	%f1539, %r636;
+	mov.b32 	%f1540, %r637;
+	mul.f32 	%f1541, %f586, %f1538;
+	mul.f32 	%f1542, %f586, %f1539;
+	mul.f32 	%f1543, %f586, %f1540;
+	fma.rn.f32 	%f1922, %f1525, %f1922, %f1541;
+	fma.rn.f32 	%f1923, %f1525, %f1923, %f1542;
+	fma.rn.f32 	%f1924, %f1525, %f1924, %f1543;
+	bra.uni 	$L__BB12_87;
+
+$L__BB12_76:
+	mov.f32 	%f1931, 0f00000000;
+	mov.f32 	%f1933, 0f3F800000;
+	setp.eq.s32 	%p41, %r491, 4;
+	@%p41 bra 	$L__BB12_79;
+
+	setp.ne.s32 	%p42, %r491, 1;
+	mov.f32 	%f1932, %f1931;
+	mov.f32 	%f1934, %f1931;
+	mov.f32 	%f1935, %f1933;
+	mov.f32 	%f1936, %f1931;
+	mov.f32 	%f1937, %f1933;
+	mov.f32 	%f1938, %f1931;
+	mov.f32 	%f1939, %f1931;
+	@%p42 bra 	$L__BB12_88;
+
+	// begin inline asm
+	call (%rd408), _optix_get_static_transform_from_handle, (%rd406);
+	// end inline asm
+	add.s64 	%rd633, %rd408, 64;
+	bra.uni 	$L__BB12_80;
+
+$L__BB12_82:
+	// begin inline asm
+	call (%rd421), _optix_get_srt_motion_transform_from_handle, (%rd406);
+	// end inline asm
+	// begin inline asm
+	cvta.to.global.u64 %rd423, %rd421;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r505,%r506,%r507,%r508}, [%rd423];
+	// end inline asm
+	add.s64 	%rd427, %rd421, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd426, %rd427;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r509,%r510,%r511,%r512}, [%rd426];
+	// end inline asm
+	add.s64 	%rd430, %rd421, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd429, %rd430;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r513,%r514,%r515,%r516}, [%rd429];
+	// end inline asm
+	add.s64 	%rd433, %rd421, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd432, %rd433;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r517,%r518,%r519,%r520}, [%rd432];
+	// end inline asm
+	add.s64 	%rd436, %rd421, 64;
+	// begin inline asm
+	cvta.to.global.u64 %rd435, %rd436;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r521,%r522,%r523,%r524}, [%rd435];
+	// end inline asm
+	add.s64 	%rd439, %rd421, 80;
+	// begin inline asm
+	cvta.to.global.u64 %rd438, %rd439;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r525,%r526,%r527,%r528}, [%rd438];
+	// end inline asm
+	add.s64 	%rd442, %rd421, 96;
+	// begin inline asm
+	cvta.to.global.u64 %rd441, %rd442;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r529,%r530,%r531,%r532}, [%rd441];
+	// end inline asm
+	add.s64 	%rd445, %rd421, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd444, %rd445;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r533,%r534,%r535,%r536}, [%rd444];
+	// end inline asm
+	add.s64 	%rd448, %rd421, 128;
+	// begin inline asm
+	cvta.to.global.u64 %rd447, %rd448;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r537,%r538,%r539,%r540}, [%rd447];
+	// end inline asm
+	add.s64 	%rd451, %rd421, 144;
+	// begin inline asm
+	cvta.to.global.u64 %rd450, %rd451;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r541,%r542,%r543,%r544}, [%rd450];
+	// end inline asm
+	mov.b32 	%f1421, %r508;
+	mov.b32 	%f1422, %r509;
+	and.b32  	%r561, %r507, 65535;
+	add.s32 	%r562, %r561, -1;
+	cvt.rn.f32.s32 	%f1423, %r562;
+	sub.f32 	%f1424, %f1409, %f1421;
+	mul.f32 	%f1425, %f1424, %f1423;
+	sub.f32 	%f1426, %f1422, %f1421;
+	div.rn.f32 	%f1427, %f1425, %f1426;
+	min.f32 	%f1428, %f1423, %f1427;
+	mov.f32 	%f1429, 0f00000000;
+	max.f32 	%f1430, %f1429, %f1428;
+	cvt.rmi.f32.f32 	%f1431, %f1430;
+	sub.f32 	%f546, %f1430, %f1431;
+	cvt.rzi.s32.f32 	%r563, %f1431;
+	mul.wide.s32 	%rd465, %r563, 64;
+	add.s64 	%rd454, %rd430, %rd465;
+	// begin inline asm
+	cvta.to.global.u64 %rd453, %rd454;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r545,%r546,%r547,%r548}, [%rd453];
+	// end inline asm
+	mov.b32 	%f1912, %r545;
+	mov.b32 	%f1913, %r546;
+	mov.b32 	%f1914, %r547;
+	add.s64 	%rd457, %rd454, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd456, %rd457;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r549,%r550,%r551,%r552}, [%rd456];
+	// end inline asm
+	mov.b32 	%f1915, %r549;
+	mov.b32 	%f1916, %r550;
+	mov.b32 	%f1917, %r552;
+	add.s64 	%rd460, %rd454, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd459, %rd460;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r553,%r554,%r555,%r556}, [%rd459];
+	// end inline asm
+	mov.b32 	%f1918, %r554;
+	mov.b32 	%f1919, %r555;
+	mov.b32 	%f1920, %r556;
+	add.s64 	%rd463, %rd454, 48;
+	// begin inline asm
+	cvta.to.global.u64 %rd462, %rd463;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r557,%r558,%r559,%r560}, [%rd462];
+	// end inline asm
+	mov.b32 	%f1921, %r557;
+	setp.leu.f32 	%p44, %f546, 0f00000000;
+	@%p44 bra 	$L__BB12_84;
+
+	mov.f32 	%f1432, 0f3F800000;
+	sub.f32 	%f1433, %f1432, %f546;
+	add.s64 	%rd467, %rd454, 64;
+	// begin inline asm
 	cvta.to.global.u64 %rd466, %rd467;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r537,%r538,%r539,%r540}, [%rd466];
-	// inline asm
-	mov.b32 	 %f1854, %r537;
-	mov.b32 	 %f1855, %r538;
-	mov.b32 	 %f1856, %r540;
-	add.s64 	%rd470, %rd464, 32;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r564,%r565,%r566,%r567}, [%rd466];
+	// end inline asm
+	mov.b32 	%f1434, %r564;
+	mov.b32 	%f1435, %r565;
+	mov.b32 	%f1436, %r566;
+	mul.f32 	%f1437, %f546, %f1434;
+	mul.f32 	%f1438, %f546, %f1435;
+	mul.f32 	%f1439, %f546, %f1436;
+	fma.rn.f32 	%f1912, %f1433, %f1912, %f1437;
+	fma.rn.f32 	%f1913, %f1433, %f1913, %f1438;
+	fma.rn.f32 	%f1914, %f1433, %f1914, %f1439;
+	add.s64 	%rd470, %rd454, 80;
+	// begin inline asm
 	cvta.to.global.u64 %rd469, %rd470;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r541,%r542,%r543,%r544}, [%rd469];
-	// inline asm
-	sub.f32 	%f518, %f1395, %f1396;
-	mov.b32 	 %f1857, %r542;
-	mov.b32 	 %f1858, %r543;
-	mov.b32 	 %f1859, %r544;
-	add.s64 	%rd473, %rd464, 48;
-	// inline asm
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r568,%r569,%r570,%r571}, [%rd469];
+	// end inline asm
+	mov.b32 	%f1440, %r568;
+	mov.b32 	%f1441, %r569;
+	mov.b32 	%f1442, %r571;
+	mul.f32 	%f1443, %f546, %f1440;
+	mul.f32 	%f1444, %f546, %f1441;
+	mul.f32 	%f1445, %f546, %f1442;
+	fma.rn.f32 	%f1915, %f1433, %f1915, %f1443;
+	fma.rn.f32 	%f1916, %f1433, %f1916, %f1444;
+	fma.rn.f32 	%f1917, %f1433, %f1917, %f1445;
+	add.s64 	%rd473, %rd454, 96;
+	// begin inline asm
 	cvta.to.global.u64 %rd472, %rd473;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r545,%r546,%r547,%r548}, [%rd472];
-	// inline asm
-	mov.b32 	 %f1860, %r545;
-	setp.leu.f32	%p41, %f518, 0f00000000;
-	@%p41 bra 	BB12_83;
-
-	shl.b64 	%rd488, %rd37, 6;
-	add.s64 	%rd489, %rd488, %rd431;
-	add.s64 	%rd477, %rd489, 96;
-	// inline asm
-	cvta.to.global.u64 %rd476, %rd477;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r552,%r553,%r554,%r555}, [%rd476];
-	// inline asm
-	mov.b32 	 %f1397, %r552;
-	mov.b32 	 %f1398, %r553;
-	mov.b32 	 %f1399, %r554;
-	add.s64 	%rd480, %rd489, 112;
-	// inline asm
-	cvta.to.global.u64 %rd479, %rd480;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r556,%r557,%r558,%r559}, [%rd479];
-	// inline asm
-	mov.b32 	 %f1400, %r556;
-	mov.b32 	 %f1401, %r557;
-	mov.b32 	 %f1402, %r559;
-	add.s64 	%rd483, %rd489, 128;
-	// inline asm
-	cvta.to.global.u64 %rd482, %rd483;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r560,%r561,%r562,%r563}, [%rd482];
-	// inline asm
-	mov.b32 	 %f1403, %r561;
-	mov.b32 	 %f1404, %r562;
-	mov.b32 	 %f1405, %r563;
-	add.s64 	%rd486, %rd489, 144;
-	// inline asm
-	cvta.to.global.u64 %rd485, %rd486;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r564,%r565,%r566,%r567}, [%rd485];
-	// inline asm
-	mov.f32 	%f1406, 0f3F800000;
-	sub.f32 	%f1407, %f1406, %f518;
-	mul.f32 	%f1408, %f518, %f1397;
-	mul.f32 	%f1409, %f518, %f1398;
-	mul.f32 	%f1410, %f518, %f1399;
-	fma.rn.f32 	%f1851, %f1407, %f1851, %f1408;
-	fma.rn.f32 	%f1852, %f1407, %f1852, %f1409;
-	fma.rn.f32 	%f1853, %f1407, %f1853, %f1410;
-	mul.f32 	%f1411, %f518, %f1400;
-	mul.f32 	%f1412, %f518, %f1401;
-	mul.f32 	%f1413, %f518, %f1402;
-	fma.rn.f32 	%f1854, %f1407, %f1854, %f1411;
-	fma.rn.f32 	%f1855, %f1407, %f1855, %f1412;
-	fma.rn.f32 	%f1856, %f1407, %f1856, %f1413;
-	mul.f32 	%f1414, %f518, %f1403;
-	mul.f32 	%f1415, %f518, %f1404;
-	mul.f32 	%f1416, %f518, %f1405;
-	fma.rn.f32 	%f1417, %f1407, %f1857, %f1414;
-	fma.rn.f32 	%f1418, %f1407, %f1858, %f1415;
-	fma.rn.f32 	%f1419, %f1407, %f1859, %f1416;
-	mov.b32 	 %f1420, %r564;
-	mul.f32 	%f1421, %f518, %f1420;
-	fma.rn.f32 	%f1422, %f1407, %f1860, %f1421;
-	mul.f32 	%f1423, %f1418, %f1418;
-	fma.rn.f32 	%f1424, %f1417, %f1417, %f1423;
-	fma.rn.f32 	%f1425, %f1419, %f1419, %f1424;
-	fma.rn.f32 	%f1426, %f1422, %f1422, %f1425;
-	sqrt.rn.f32 	%f1427, %f1426;
-	rcp.rn.f32 	%f1428, %f1427;
-	mul.f32 	%f1857, %f1417, %f1428;
-	mul.f32 	%f1858, %f1418, %f1428;
-	mul.f32 	%f1859, %f1419, %f1428;
-	mul.f32 	%f1860, %f1422, %f1428;
-
-BB12_83:
-	mul.f32 	%f1429, %f1858, %f1858;
-	fma.rn.f32 	%f1430, %f1857, %f1857, %f1429;
-	fma.rn.f32 	%f1431, %f1859, %f1859, %f1430;
-	fma.rn.f32 	%f1432, %f1860, %f1860, %f1431;
-	rcp.rn.f32 	%f1433, %f1432;
-	mul.f32 	%f1434, %f1857, %f1433;
-	mul.f32 	%f1435, %f1858, %f1433;
-	mul.f32 	%f1436, %f1859, %f1433;
-	mul.f32 	%f1437, %f1860, %f1433;
-	mul.f32 	%f1438, %f1857, %f1434;
-	mul.f32 	%f1439, %f1858, %f1435;
-	mul.f32 	%f1440, %f1859, %f1436;
-	mul.f32 	%f1441, %f1857, %f1435;
-	mul.f32 	%f1442, %f1859, %f1437;
-	mul.f32 	%f1443, %f1857, %f1436;
-	mul.f32 	%f1444, %f1858, %f1437;
-	mul.f32 	%f1445, %f1858, %f1436;
-	mul.f32 	%f1446, %f1857, %f1437;
-	sub.f32 	%f1447, %f1438, %f1439;
-	sub.f32 	%f1448, %f1447, %f1440;
-	fma.rn.f32 	%f1449, %f1860, %f1437, %f1448;
-	sub.f32 	%f1450, %f1441, %f1442;
-	add.f32 	%f1451, %f1450, %f1450;
-	add.f32 	%f1452, %f1443, %f1444;
-	add.f32 	%f1453, %f1452, %f1452;
-	add.f32 	%f1454, %f1441, %f1442;
-	add.f32 	%f1455, %f1454, %f1454;
-	sub.f32 	%f1456, %f1439, %f1438;
-	sub.f32 	%f1457, %f1456, %f1440;
-	fma.rn.f32 	%f1458, %f1860, %f1437, %f1457;
-	sub.f32 	%f1459, %f1445, %f1446;
-	add.f32 	%f1460, %f1459, %f1459;
-	sub.f32 	%f1461, %f1443, %f1444;
-	add.f32 	%f1462, %f1461, %f1461;
-	add.f32 	%f1463, %f1445, %f1446;
-	add.f32 	%f1464, %f1463, %f1463;
-	neg.f32 	%f1465, %f1438;
-	sub.f32 	%f1466, %f1465, %f1439;
-	add.f32 	%f1467, %f1440, %f1466;
-	fma.rn.f32 	%f1468, %f1860, %f1437, %f1467;
-	mul.f32 	%f1469, %f1853, %f1449;
-	fma.rn.f32 	%f1470, %f1855, %f1451, %f1469;
-	fma.rn.f32 	%f1869, %f1856, %f1453, %f1470;
-	mul.f32 	%f1471, %f1855, %f1458;
-	fma.rn.f32 	%f1472, %f1853, %f1455, %f1471;
-	fma.rn.f32 	%f1866, %f1856, %f1460, %f1472;
-	mul.f32 	%f1473, %f1855, %f1464;
-	fma.rn.f32 	%f1474, %f1853, %f1462, %f1473;
-	fma.rn.f32 	%f1863, %f1856, %f1468, %f1474;
-	mul.f32 	%f1475, %f1852, %f1449;
-	fma.rn.f32 	%f1868, %f1854, %f1451, %f1475;
-	mul.f32 	%f1476, %f1854, %f1458;
-	fma.rn.f32 	%f1865, %f1852, %f1455, %f1476;
-	mul.f32 	%f1477, %f1854, %f1464;
-	fma.rn.f32 	%f1862, %f1852, %f1462, %f1477;
-	mul.f32 	%f1867, %f1851, %f1449;
-	mul.f32 	%f1864, %f1851, %f1455;
-	mul.f32 	%f1861, %f1851, %f1462;
-
-BB12_86:
-	mul.f32 	%f1509, %f1862, %f1866;
-	mul.f32 	%f1510, %f1863, %f1865;
-	sub.f32 	%f1511, %f1510, %f1509;
-	mul.f32 	%f1512, %f1867, %f1511;
-	mul.f32 	%f1513, %f1861, %f1866;
-	mul.f32 	%f1514, %f1863, %f1864;
-	sub.f32 	%f1515, %f1514, %f1513;
-	mul.f32 	%f1516, %f1515, %f1868;
-	sub.f32 	%f1517, %f1512, %f1516;
-	mul.f32 	%f1518, %f1861, %f1865;
-	mul.f32 	%f1519, %f1862, %f1864;
-	sub.f32 	%f1520, %f1519, %f1518;
-	fma.rn.f32 	%f1521, %f1520, %f1869, %f1517;
-	rcp.rn.f32 	%f1522, %f1521;
-	mul.f32 	%f1876, %f1511, %f1522;
-	mul.f32 	%f1523, %f1863, %f1868;
-	mul.f32 	%f1524, %f1862, %f1869;
-	sub.f32 	%f1525, %f1524, %f1523;
-	mul.f32 	%f1877, %f1522, %f1525;
-	mul.f32 	%f1526, %f1865, %f1869;
-	mul.f32 	%f1527, %f1866, %f1868;
-	sub.f32 	%f1528, %f1527, %f1526;
-	mul.f32 	%f1878, %f1522, %f1528;
-	sub.f32 	%f1529, %f1513, %f1514;
-	mul.f32 	%f1873, %f1529, %f1522;
-	mul.f32 	%f1530, %f1861, %f1869;
-	mul.f32 	%f1531, %f1863, %f1867;
-	sub.f32 	%f1532, %f1531, %f1530;
-	mul.f32 	%f1874, %f1522, %f1532;
-	mul.f32 	%f1533, %f1866, %f1867;
-	mul.f32 	%f1534, %f1864, %f1869;
-	sub.f32 	%f1535, %f1534, %f1533;
-	mul.f32 	%f1875, %f1522, %f1535;
-	mul.f32 	%f1870, %f1520, %f1522;
-	mul.f32 	%f1536, %f1862, %f1867;
-	mul.f32 	%f1537, %f1861, %f1868;
-	sub.f32 	%f1538, %f1537, %f1536;
-	mul.f32 	%f1871, %f1538, %f1522;
-	mul.f32 	%f1539, %f1864, %f1868;
-	mul.f32 	%f1540, %f1865, %f1867;
-	sub.f32 	%f1541, %f1540, %f1539;
-	mul.f32 	%f1872, %f1541, %f1522;
-	bra.uni 	BB12_87;
-
-BB12_76:
-	setp.ne.s32	%p39, %r479, 1;
-	mov.f32 	%f1871, %f1870;
-	mov.f32 	%f1873, %f1870;
-	mov.f32 	%f1874, %f1872;
-	mov.f32 	%f1875, %f1870;
-	mov.f32 	%f1876, %f1872;
-	mov.f32 	%f1877, %f1870;
-	mov.f32 	%f1878, %f1870;
-	@%p39 bra 	BB12_87;
-
-	// inline asm
-	call (%rd418), _optix_get_static_transform_from_handle, (%rd416);
-	// inline asm
-	add.s64 	%rd646, %rd418, 64;
-
-BB12_79:
-	// inline asm
-	cvta.to.global.u64 %rd422, %rd646;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r481,%r482,%r483,%r484}, [%rd422];
-	// inline asm
-	mov.b32 	 %f1876, %r481;
-	mov.b32 	 %f1877, %r482;
-	mov.b32 	 %f1878, %r483;
-	add.s64 	%rd426, %rd646, 16;
-	// inline asm
-	cvta.to.global.u64 %rd425, %rd426;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r485,%r486,%r487,%r488}, [%rd425];
-	// inline asm
-	mov.b32 	 %f1873, %r485;
-	mov.b32 	 %f1874, %r486;
-	mov.b32 	 %f1875, %r487;
-	add.s64 	%rd429, %rd646, 32;
-	// inline asm
-	cvta.to.global.u64 %rd428, %rd429;
-	// inline asm
-	// inline asm
-	ld.global.v4.u32 {%r489,%r490,%r491,%r492}, [%rd428];
-	// inline asm
-	mov.b32 	 %f1870, %r489;
-	mov.b32 	 %f1871, %r490;
-	mov.b32 	 %f1872, %r491;
-
-BB12_87:
-	setp.eq.s32	%p43, %r634, 0;
-	@%p43 bra 	BB12_88;
-	bra.uni 	BB12_89;
-
-BB12_88:
-	mov.f32 	%f1850, %f1870;
-	mov.f32 	%f1849, %f1871;
-	mov.f32 	%f1848, %f1872;
-	mov.f32 	%f1847, %f1873;
-	mov.f32 	%f1846, %f1874;
-	mov.f32 	%f1845, %f1875;
-	mov.f32 	%f1844, %f1876;
-	mov.f32 	%f1843, %f1877;
-	mov.f32 	%f1842, %f1878;
-	bra.uni 	BB12_90;
-
-BB12_89:
-	mul.f32 	%f1542, %f1847, %f1877;
-	fma.rn.f32 	%f1543, %f1844, %f1876, %f1542;
-	fma.rn.f32 	%f598, %f1850, %f1878, %f1543;
-	mul.f32 	%f1544, %f1846, %f1877;
-	fma.rn.f32 	%f1545, %f1843, %f1876, %f1544;
-	fma.rn.f32 	%f599, %f1849, %f1878, %f1545;
-	mul.f32 	%f1546, %f1845, %f1877;
-	fma.rn.f32 	%f1547, %f1842, %f1876, %f1546;
-	fma.rn.f32 	%f600, %f1848, %f1878, %f1547;
-	mul.f32 	%f1548, %f1847, %f1874;
-	fma.rn.f32 	%f1549, %f1844, %f1873, %f1548;
-	fma.rn.f32 	%f601, %f1850, %f1875, %f1549;
-	mul.f32 	%f1550, %f1846, %f1874;
-	fma.rn.f32 	%f1551, %f1843, %f1873, %f1550;
-	fma.rn.f32 	%f602, %f1849, %f1875, %f1551;
-	mul.f32 	%f1552, %f1845, %f1874;
-	fma.rn.f32 	%f1553, %f1842, %f1873, %f1552;
-	fma.rn.f32 	%f603, %f1848, %f1875, %f1553;
-	mul.f32 	%f1554, %f1847, %f1871;
-	fma.rn.f32 	%f1555, %f1844, %f1870, %f1554;
-	fma.rn.f32 	%f1850, %f1850, %f1872, %f1555;
-	mul.f32 	%f1556, %f1846, %f1871;
-	fma.rn.f32 	%f1557, %f1843, %f1870, %f1556;
-	fma.rn.f32 	%f1849, %f1849, %f1872, %f1557;
-	mul.f32 	%f1558, %f1845, %f1871;
-	fma.rn.f32 	%f1559, %f1842, %f1870, %f1558;
-	fma.rn.f32 	%f1848, %f1848, %f1872, %f1559;
-	mov.f32 	%f1847, %f601;
-	mov.f32 	%f1846, %f602;
-	mov.f32 	%f1845, %f603;
-	mov.f32 	%f1844, %f598;
-	mov.f32 	%f1843, %f599;
-	mov.f32 	%f1842, %f600;
-
-BB12_90:
-	add.s32 	%r634, %r634, 1;
-	setp.lt.u32	%p44, %r634, %r26;
-	@%p44 bra 	BB12_74;
-
-BB12_91:
-	fma.rn.f32 	%f1560, %f1921, %f1781, %f1778;
-	fma.rn.f32 	%f1561, %f1922, %f1780, %f1560;
-	fma.rn.f32 	%f1562, %f1921, %f1785, %f1782;
-	fma.rn.f32 	%f1563, %f1922, %f1784, %f1562;
-	fma.rn.f32 	%f1564, %f1921, %f1789, %f1786;
-	fma.rn.f32 	%f1565, %f1922, %f1788, %f1564;
-	fma.rn.f32 	%f1921, %f1923, %f1779, %f1561;
-	fma.rn.f32 	%f1922, %f1923, %f1783, %f1563;
-	fma.rn.f32 	%f1923, %f1923, %f1787, %f1565;
-	ld.const.u64 	%rd537, [params+112];
-	setp.eq.s64	%p45, %rd537, 0;
-	mov.f32 	%f1915, %f1775;
-	mov.f32 	%f1916, %f1776;
-	mov.f32 	%f1917, %f1777;
-	@%p45 bra 	BB12_93;
-
-	mul.f32 	%f1566, %f1775, %f1844;
-	fma.rn.f32 	%f1567, %f1776, %f1847, %f1566;
-	mul.f32 	%f1568, %f1775, %f1843;
-	fma.rn.f32 	%f1569, %f1776, %f1846, %f1568;
-	mul.f32 	%f1570, %f1775, %f1842;
-	fma.rn.f32 	%f1571, %f1776, %f1845, %f1570;
-	fma.rn.f32 	%f1572, %f1777, %f1850, %f1567;
-	fma.rn.f32 	%f1573, %f1777, %f1849, %f1569;
-	fma.rn.f32 	%f1574, %f1777, %f1848, %f1571;
-	mul.f32 	%f1575, %f1572, %f1572;
-	fma.rn.f32 	%f1576, %f1573, %f1573, %f1575;
-	fma.rn.f32 	%f1577, %f1574, %f1574, %f1576;
-	sqrt.rn.f32 	%f1578, %f1577;
-	div.rn.f32 	%f1915, %f1572, %f1578;
-	div.rn.f32 	%f1916, %f1573, %f1578;
-	div.rn.f32 	%f1917, %f1574, %f1578;
-
-BB12_93:
-	ld.const.u64 	%rd538, [params+136];
-	setp.eq.s64	%p46, %rd538, 0;
-	@%p46 bra 	BB12_95;
-
-	mul.f32 	%f1579, %f1775, %f1844;
-	fma.rn.f32 	%f1580, %f1776, %f1847, %f1579;
-	mul.f32 	%f1581, %f1775, %f1843;
-	fma.rn.f32 	%f1582, %f1776, %f1846, %f1581;
-	mul.f32 	%f1583, %f1775, %f1842;
-	fma.rn.f32 	%f1584, %f1776, %f1845, %f1583;
-	fma.rn.f32 	%f1585, %f1777, %f1850, %f1580;
-	fma.rn.f32 	%f1586, %f1777, %f1849, %f1582;
-	fma.rn.f32 	%f1587, %f1777, %f1848, %f1584;
-	mul.f32 	%f1588, %f1585, %f1585;
-	fma.rn.f32 	%f1589, %f1586, %f1586, %f1588;
-	fma.rn.f32 	%f1590, %f1587, %f1587, %f1589;
-	sqrt.rn.f32 	%f1591, %f1590;
-	div.rn.f32 	%f1775, %f1585, %f1591;
-	div.rn.f32 	%f1776, %f1586, %f1591;
-	div.rn.f32 	%f1777, %f1587, %f1591;
-
-BB12_95:
-	ld.const.u64 	%rd539, [params+184];
-	setp.eq.s64	%p47, %rd539, 0;
-	mov.f32 	%f1909, 0f00000000;
-	mov.f32 	%f1912, %f1909;
-	mov.f32 	%f1913, %f1909;
-	mov.f32 	%f1914, %f1909;
-	@%p47 bra 	BB12_97;
-
-	mul.f32 	%f1595, %f1781, 0f00000000;
-	mov.f32 	%f1596, 0f00000000;
-	fma.rn.f32 	%f1597, %f1596, %f1780, %f1595;
-	mul.f32 	%f1598, %f1785, 0f00000000;
-	fma.rn.f32 	%f1599, %f1596, %f1784, %f1598;
-	mul.f32 	%f1600, %f1789, 0f00000000;
-	fma.rn.f32 	%f1601, %f1596, %f1788, %f1600;
-	fma.rn.f32 	%f1912, %f1596, %f1779, %f1597;
-	fma.rn.f32 	%f1913, %f1596, %f1783, %f1599;
-	fma.rn.f32 	%f1914, %f1596, %f1787, %f1601;
-
-BB12_97:
-	ld.const.u64 	%rd540, [params+280];
-	ld.const.u64 	%rd541, [params+232];
-	or.b64  	%rd542, %rd540, %rd541;
-	setp.eq.s64	%p48, %rd542, 0;
-	mov.f32 	%f1910, %f1909;
-	mov.f32 	%f1911, %f1909;
-	@%p48 bra 	BB12_99;
-
-	mul.f32 	%f1605, %f1775, %f1781;
-	fma.rn.f32 	%f1606, %f1776, %f1785, %f1605;
-	mul.f32 	%f1607, %f1775, %f1780;
-	fma.rn.f32 	%f1608, %f1776, %f1784, %f1607;
-	mul.f32 	%f1609, %f1775, %f1779;
-	fma.rn.f32 	%f1610, %f1776, %f1783, %f1609;
-	fma.rn.f32 	%f1611, %f1777, %f1789, %f1606;
-	fma.rn.f32 	%f1612, %f1777, %f1788, %f1608;
-	fma.rn.f32 	%f1613, %f1777, %f1787, %f1610;
-	mul.f32 	%f1614, %f1611, %f1611;
-	fma.rn.f32 	%f1615, %f1612, %f1612, %f1614;
-	fma.rn.f32 	%f1616, %f1613, %f1613, %f1615;
-	sqrt.rn.f32 	%f1617, %f1616;
-	div.rn.f32 	%f1618, %f1611, %f1617;
-	div.rn.f32 	%f1619, %f1612, %f1617;
-	div.rn.f32 	%f1620, %f1613, %f1617;
-	mul.f32 	%f1621, %f1618, %f1844;
-	mul.f32 	%f1622, %f1618, %f1843;
-	mul.f32 	%f1623, %f1618, %f1842;
-	fma.rn.f32 	%f1624, %f1619, %f1847, %f1621;
-	fma.rn.f32 	%f1625, %f1619, %f1846, %f1622;
-	fma.rn.f32 	%f1626, %f1619, %f1845, %f1623;
-	fma.rn.f32 	%f1627, %f1620, %f1850, %f1624;
-	fma.rn.f32 	%f1628, %f1620, %f1849, %f1625;
-	fma.rn.f32 	%f1629, %f1620, %f1848, %f1626;
-	mul.f32 	%f1630, %f1627, %f1627;
-	fma.rn.f32 	%f1631, %f1628, %f1628, %f1630;
-	fma.rn.f32 	%f1632, %f1629, %f1629, %f1631;
-	sqrt.rn.f32 	%f1633, %f1632;
-	rcp.rn.f32 	%f1634, %f1633;
-	mul.f32 	%f1635, %f1634, %f1627;
-	mul.f32 	%f1636, %f1634, %f1628;
-	mul.f32 	%f1637, %f1634, %f1629;
-	mul.f32 	%f1638, %f1844, 0f00000000;
-	mov.f32 	%f1639, 0f00000000;
-	fma.rn.f32 	%f1640, %f1639, %f1847, %f1638;
-	mul.f32 	%f1641, %f1843, 0f00000000;
-	fma.rn.f32 	%f1642, %f1639, %f1846, %f1641;
-	mul.f32 	%f1643, %f1842, 0f00000000;
-	fma.rn.f32 	%f1644, %f1639, %f1845, %f1643;
-	fma.rn.f32 	%f1645, %f1639, %f1850, %f1640;
-	fma.rn.f32 	%f1646, %f1639, %f1849, %f1642;
-	fma.rn.f32 	%f1647, %f1639, %f1848, %f1644;
-	mul.f32 	%f1648, %f1645, %f1634;
-	mul.f32 	%f1649, %f1646, %f1634;
-	mul.f32 	%f1650, %f1647, %f1634;
-	mul.f32 	%f1651, %f1635, %f1648;
-	fma.rn.f32 	%f1652, %f1636, %f1649, %f1651;
-	fma.rn.f32 	%f1653, %f1637, %f1650, %f1652;
-	mul.f32 	%f1654, %f1635, %f1653;
-	mul.f32 	%f1655, %f1636, %f1653;
-	mul.f32 	%f1656, %f1637, %f1653;
-	sub.f32 	%f1909, %f1648, %f1654;
-	sub.f32 	%f1910, %f1649, %f1655;
-	sub.f32 	%f1911, %f1650, %f1656;
-
-BB12_99:
-	st.global.u32 	[%rd23], %r327;
-	bra.uni 	BB12_100;
-
-BB12_53:
-	mov.f32 	%f1910, %f1909;
-	mov.f32 	%f1911, %f1909;
-	mov.f32 	%f1912, %f1909;
-	mov.f32 	%f1913, %f1909;
-	mov.f32 	%f1914, %f1909;
-	mov.f32 	%f1915, %f1775;
-	mov.f32 	%f1916, %f1776;
-	mov.f32 	%f1917, %f1777;
-
-BB12_100:
-	ld.const.u64 	%rd543, [params+328];
-	cvta.to.global.u64 	%rd544, %rd543;
-	shl.b64 	%rd545, %rd22, 3;
-	add.s64 	%rd546, %rd544, %rd545;
-	st.global.u64 	[%rd546], %rd21;
-	ld.const.u64 	%rd547, [params+336];
-	cvta.to.global.u64 	%rd548, %rd547;
-	shl.b64 	%rd549, %rd22, 2;
-	add.s64 	%rd550, %rd548, %rd549;
-	mov.u32 	%r627, 0;
-	st.global.u32 	[%rd550], %r627;
-	ld.const.u64 	%rd551, [params+160];
-	cvta.to.global.u64 	%rd552, %rd551;
-	add.s64 	%rd553, %rd552, %rd549;
-	st.global.f32 	[%rd553], %f1921;
-	ld.const.u64 	%rd554, [params+168];
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r572,%r573,%r574,%r575}, [%rd472];
+	// end inline asm
+	mov.b32 	%f1446, %r573;
+	mov.b32 	%f1447, %r574;
+	mov.b32 	%f1448, %r575;
+	mul.f32 	%f1449, %f546, %f1446;
+	mul.f32 	%f1450, %f546, %f1447;
+	mul.f32 	%f1451, %f546, %f1448;
+	fma.rn.f32 	%f1452, %f1433, %f1918, %f1449;
+	fma.rn.f32 	%f1453, %f1433, %f1919, %f1450;
+	fma.rn.f32 	%f1454, %f1433, %f1920, %f1451;
+	add.s64 	%rd476, %rd454, 112;
+	// begin inline asm
+	cvta.to.global.u64 %rd475, %rd476;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r576,%r577,%r578,%r579}, [%rd475];
+	// end inline asm
+	mov.b32 	%f1455, %r576;
+	mul.f32 	%f1456, %f546, %f1455;
+	fma.rn.f32 	%f1457, %f1433, %f1921, %f1456;
+	mul.f32 	%f1458, %f1453, %f1453;
+	fma.rn.f32 	%f1459, %f1452, %f1452, %f1458;
+	fma.rn.f32 	%f1460, %f1454, %f1454, %f1459;
+	fma.rn.f32 	%f1461, %f1457, %f1457, %f1460;
+	sqrt.rn.f32 	%f1462, %f1461;
+	rcp.rn.f32 	%f1463, %f1462;
+	mul.f32 	%f1918, %f1452, %f1463;
+	mul.f32 	%f1919, %f1453, %f1463;
+	mul.f32 	%f1920, %f1454, %f1463;
+	mul.f32 	%f1921, %f1463, %f1457;
+
+$L__BB12_84:
+	mul.f32 	%f1464, %f1919, %f1919;
+	fma.rn.f32 	%f1465, %f1918, %f1918, %f1464;
+	fma.rn.f32 	%f1466, %f1920, %f1920, %f1465;
+	fma.rn.f32 	%f1467, %f1921, %f1921, %f1466;
+	rcp.rn.f32 	%f1468, %f1467;
+	mul.f32 	%f1469, %f1918, %f1468;
+	mul.f32 	%f1470, %f1919, %f1468;
+	mul.f32 	%f1471, %f1920, %f1468;
+	mul.f32 	%f1472, %f1921, %f1468;
+	mul.f32 	%f1473, %f1918, %f1469;
+	mul.f32 	%f1474, %f1919, %f1470;
+	mul.f32 	%f1475, %f1920, %f1471;
+	mul.f32 	%f1476, %f1918, %f1470;
+	mul.f32 	%f1477, %f1920, %f1472;
+	mul.f32 	%f1478, %f1918, %f1471;
+	mul.f32 	%f1479, %f1919, %f1472;
+	mul.f32 	%f1480, %f1919, %f1471;
+	mul.f32 	%f1481, %f1918, %f1472;
+	sub.f32 	%f1482, %f1473, %f1474;
+	sub.f32 	%f1483, %f1482, %f1475;
+	fma.rn.f32 	%f1484, %f1921, %f1472, %f1483;
+	sub.f32 	%f1485, %f1476, %f1477;
+	add.f32 	%f1486, %f1485, %f1485;
+	add.f32 	%f1487, %f1478, %f1479;
+	add.f32 	%f1488, %f1487, %f1487;
+	add.f32 	%f1489, %f1476, %f1477;
+	add.f32 	%f1490, %f1489, %f1489;
+	sub.f32 	%f1491, %f1474, %f1473;
+	sub.f32 	%f1492, %f1491, %f1475;
+	fma.rn.f32 	%f1493, %f1921, %f1472, %f1492;
+	sub.f32 	%f1494, %f1480, %f1481;
+	add.f32 	%f1495, %f1494, %f1494;
+	sub.f32 	%f1496, %f1478, %f1479;
+	add.f32 	%f1497, %f1496, %f1496;
+	add.f32 	%f1498, %f1480, %f1481;
+	add.f32 	%f1499, %f1498, %f1498;
+	neg.f32 	%f1500, %f1473;
+	sub.f32 	%f1501, %f1500, %f1474;
+	add.f32 	%f1502, %f1475, %f1501;
+	fma.rn.f32 	%f1503, %f1921, %f1472, %f1502;
+	mul.f32 	%f1504, %f1914, %f1484;
+	fma.rn.f32 	%f1505, %f1916, %f1486, %f1504;
+	fma.rn.f32 	%f1930, %f1917, %f1488, %f1505;
+	mul.f32 	%f1506, %f1916, %f1493;
+	fma.rn.f32 	%f1507, %f1914, %f1490, %f1506;
+	fma.rn.f32 	%f1927, %f1917, %f1495, %f1507;
+	mul.f32 	%f1508, %f1916, %f1499;
+	fma.rn.f32 	%f1509, %f1914, %f1497, %f1508;
+	fma.rn.f32 	%f1924, %f1917, %f1503, %f1509;
+	mul.f32 	%f1510, %f1913, %f1484;
+	fma.rn.f32 	%f1929, %f1915, %f1486, %f1510;
+	mul.f32 	%f1511, %f1915, %f1493;
+	fma.rn.f32 	%f1926, %f1913, %f1490, %f1511;
+	mul.f32 	%f1512, %f1915, %f1499;
+	fma.rn.f32 	%f1923, %f1913, %f1497, %f1512;
+	mul.f32 	%f1928, %f1912, %f1484;
+	mul.f32 	%f1925, %f1912, %f1490;
+	mul.f32 	%f1922, %f1912, %f1497;
+
+$L__BB12_87:
+	mul.f32 	%f1544, %f1923, %f1927;
+	mul.f32 	%f1545, %f1924, %f1926;
+	sub.f32 	%f1546, %f1545, %f1544;
+	mul.f32 	%f1547, %f1928, %f1546;
+	mul.f32 	%f1548, %f1922, %f1927;
+	mul.f32 	%f1549, %f1924, %f1925;
+	sub.f32 	%f1550, %f1549, %f1548;
+	mul.f32 	%f1551, %f1550, %f1929;
+	sub.f32 	%f1552, %f1547, %f1551;
+	mul.f32 	%f1553, %f1922, %f1926;
+	mul.f32 	%f1554, %f1923, %f1925;
+	sub.f32 	%f1555, %f1554, %f1553;
+	fma.rn.f32 	%f1556, %f1555, %f1930, %f1552;
+	rcp.rn.f32 	%f1557, %f1556;
+	mul.f32 	%f1937, %f1546, %f1557;
+	mul.f32 	%f1558, %f1924, %f1929;
+	mul.f32 	%f1559, %f1923, %f1930;
+	sub.f32 	%f1560, %f1559, %f1558;
+	mul.f32 	%f1938, %f1560, %f1557;
+	mul.f32 	%f1561, %f1926, %f1930;
+	mul.f32 	%f1562, %f1927, %f1929;
+	sub.f32 	%f1563, %f1562, %f1561;
+	mul.f32 	%f1939, %f1563, %f1557;
+	sub.f32 	%f1564, %f1548, %f1549;
+	mul.f32 	%f1934, %f1564, %f1557;
+	mul.f32 	%f1565, %f1922, %f1930;
+	mul.f32 	%f1566, %f1924, %f1928;
+	sub.f32 	%f1567, %f1566, %f1565;
+	mul.f32 	%f1935, %f1567, %f1557;
+	mul.f32 	%f1568, %f1927, %f1928;
+	mul.f32 	%f1569, %f1925, %f1930;
+	sub.f32 	%f1570, %f1569, %f1568;
+	mul.f32 	%f1936, %f1570, %f1557;
+	mul.f32 	%f1931, %f1555, %f1557;
+	mul.f32 	%f1571, %f1923, %f1928;
+	mul.f32 	%f1572, %f1922, %f1929;
+	sub.f32 	%f1573, %f1572, %f1571;
+	mul.f32 	%f1932, %f1573, %f1557;
+	mul.f32 	%f1574, %f1925, %f1929;
+	mul.f32 	%f1575, %f1926, %f1928;
+	sub.f32 	%f1576, %f1575, %f1574;
+	mul.f32 	%f1933, %f1576, %f1557;
+	bra.uni 	$L__BB12_88;
+
+$L__BB12_79:
+	// begin inline asm
+	call (%rd633), _optix_get_instance_inverse_transform_from_handle, (%rd406);
+	// end inline asm
+
+$L__BB12_80:
+	// begin inline asm
+	cvta.to.global.u64 %rd412, %rd633;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r493,%r494,%r495,%r496}, [%rd412];
+	// end inline asm
+	mov.b32 	%f1937, %r493;
+	mov.b32 	%f1938, %r494;
+	mov.b32 	%f1939, %r495;
+	add.s64 	%rd416, %rd633, 16;
+	// begin inline asm
+	cvta.to.global.u64 %rd415, %rd416;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r497,%r498,%r499,%r500}, [%rd415];
+	// end inline asm
+	mov.b32 	%f1934, %r497;
+	mov.b32 	%f1935, %r498;
+	mov.b32 	%f1936, %r499;
+	add.s64 	%rd419, %rd633, 32;
+	// begin inline asm
+	cvta.to.global.u64 %rd418, %rd419;
+	// end inline asm
+	// begin inline asm
+	ld.global.v4.u32 {%r501,%r502,%r503,%r504}, [%rd418];
+	// end inline asm
+	mov.b32 	%f1931, %r501;
+	mov.b32 	%f1932, %r502;
+	mov.b32 	%f1933, %r503;
+
+$L__BB12_88:
+	setp.eq.s32 	%p46, %r646, 0;
+	@%p46 bra 	$L__BB12_90;
+
+	mul.f32 	%f1577, %f1908, %f1938;
+	fma.rn.f32 	%f1578, %f1905, %f1937, %f1577;
+	fma.rn.f32 	%f632, %f1911, %f1939, %f1578;
+	mul.f32 	%f1579, %f1907, %f1938;
+	fma.rn.f32 	%f1580, %f1904, %f1937, %f1579;
+	fma.rn.f32 	%f633, %f1910, %f1939, %f1580;
+	mul.f32 	%f1581, %f1906, %f1938;
+	fma.rn.f32 	%f1582, %f1903, %f1937, %f1581;
+	fma.rn.f32 	%f1939, %f1909, %f1939, %f1582;
+	mul.f32 	%f1583, %f1908, %f1935;
+	fma.rn.f32 	%f1584, %f1905, %f1934, %f1583;
+	fma.rn.f32 	%f635, %f1911, %f1936, %f1584;
+	mul.f32 	%f1585, %f1907, %f1935;
+	fma.rn.f32 	%f1586, %f1904, %f1934, %f1585;
+	fma.rn.f32 	%f636, %f1910, %f1936, %f1586;
+	mul.f32 	%f1587, %f1906, %f1935;
+	fma.rn.f32 	%f1588, %f1903, %f1934, %f1587;
+	fma.rn.f32 	%f1936, %f1909, %f1936, %f1588;
+	mul.f32 	%f1589, %f1908, %f1932;
+	fma.rn.f32 	%f1590, %f1905, %f1931, %f1589;
+	fma.rn.f32 	%f638, %f1911, %f1933, %f1590;
+	mul.f32 	%f1591, %f1907, %f1932;
+	fma.rn.f32 	%f1592, %f1904, %f1931, %f1591;
+	fma.rn.f32 	%f639, %f1910, %f1933, %f1592;
+	mul.f32 	%f1593, %f1906, %f1932;
+	fma.rn.f32 	%f1594, %f1903, %f1931, %f1593;
+	fma.rn.f32 	%f1933, %f1909, %f1933, %f1594;
+	mov.f32 	%f1931, %f638;
+	mov.f32 	%f1932, %f639;
+	mov.f32 	%f1934, %f635;
+	mov.f32 	%f1935, %f636;
+	mov.f32 	%f1937, %f632;
+	mov.f32 	%f1938, %f633;
+
+$L__BB12_90:
+	add.s32 	%r646, %r646, 1;
+	setp.lt.u32 	%p47, %r646, %r488;
+	mov.f32 	%f1903, %f1939;
+	mov.f32 	%f1904, %f1938;
+	mov.f32 	%f1905, %f1937;
+	mov.f32 	%f1906, %f1936;
+	mov.f32 	%f1907, %f1935;
+	mov.f32 	%f1908, %f1934;
+	mov.f32 	%f1909, %f1933;
+	mov.f32 	%f1910, %f1932;
+	mov.f32 	%f1911, %f1931;
+	@%p47 bra 	$L__BB12_75;
+
+$L__BB12_91:
+	fma.rn.f32 	%f1595, %f1982, %f1875, %f1878;
+	fma.rn.f32 	%f1596, %f1983, %f1876, %f1595;
+	fma.rn.f32 	%f1597, %f1982, %f1871, %f1874;
+	fma.rn.f32 	%f1598, %f1983, %f1872, %f1597;
+	fma.rn.f32 	%f1599, %f1982, %f1867, %f1870;
+	fma.rn.f32 	%f1600, %f1983, %f1868, %f1599;
+	fma.rn.f32 	%f1982, %f1984, %f1877, %f1596;
+	fma.rn.f32 	%f1983, %f1984, %f1873, %f1598;
+	fma.rn.f32 	%f1984, %f1984, %f1869, %f1600;
+	ld.const.u64 	%rd525, [params+112];
+	setp.eq.s64 	%p48, %rd525, 0;
+	mov.f32 	%f1958, %f1838;
+	mov.f32 	%f1959, %f1837;
+	mov.f32 	%f1960, %f1836;
+	@%p48 bra 	$L__BB12_93;
+
+	mul.f32 	%f1601, %f1838, %f1937;
+	fma.rn.f32 	%f1602, %f1837, %f1934, %f1601;
+	mul.f32 	%f1603, %f1838, %f1938;
+	fma.rn.f32 	%f1604, %f1837, %f1935, %f1603;
+	mul.f32 	%f1605, %f1838, %f1939;
+	fma.rn.f32 	%f1606, %f1837, %f1936, %f1605;
+	fma.rn.f32 	%f1607, %f1836, %f1931, %f1602;
+	fma.rn.f32 	%f1608, %f1836, %f1932, %f1604;
+	fma.rn.f32 	%f1609, %f1836, %f1933, %f1606;
+	mul.f32 	%f1610, %f1607, %f1607;
+	fma.rn.f32 	%f1611, %f1608, %f1608, %f1610;
+	fma.rn.f32 	%f1612, %f1609, %f1609, %f1611;
+	sqrt.rn.f32 	%f1613, %f1612;
+	div.rn.f32 	%f1958, %f1607, %f1613;
+	div.rn.f32 	%f1959, %f1608, %f1613;
+	div.rn.f32 	%f1960, %f1609, %f1613;
+
+$L__BB12_93:
+	ld.const.u64 	%rd526, [params+136];
+	setp.eq.s64 	%p49, %rd526, 0;
+	@%p49 bra 	$L__BB12_95;
+
+	mul.f32 	%f1614, %f1838, %f1937;
+	fma.rn.f32 	%f1615, %f1837, %f1934, %f1614;
+	mul.f32 	%f1616, %f1838, %f1938;
+	fma.rn.f32 	%f1617, %f1837, %f1935, %f1616;
+	mul.f32 	%f1618, %f1838, %f1939;
+	fma.rn.f32 	%f1619, %f1837, %f1936, %f1618;
+	fma.rn.f32 	%f1620, %f1836, %f1931, %f1615;
+	fma.rn.f32 	%f1621, %f1836, %f1932, %f1617;
+	fma.rn.f32 	%f1622, %f1836, %f1933, %f1619;
+	mul.f32 	%f1623, %f1620, %f1620;
+	fma.rn.f32 	%f1624, %f1621, %f1621, %f1623;
+	fma.rn.f32 	%f1625, %f1622, %f1622, %f1624;
+	sqrt.rn.f32 	%f1626, %f1625;
+	div.rn.f32 	%f1838, %f1620, %f1626;
+	div.rn.f32 	%f1837, %f1621, %f1626;
+	div.rn.f32 	%f1836, %f1622, %f1626;
+
+$L__BB12_95:
+	mov.f32 	%f1981, %f1836;
+	mov.f32 	%f1980, %f1837;
+	mov.f32 	%f1979, %f1838;
+	ld.const.u64 	%rd527, [params+184];
+	setp.eq.s64 	%p50, %rd527, 0;
+	mov.f32 	%f1970, 0f00000000;
+	mov.f32 	%f1973, %f1970;
+	mov.f32 	%f1974, %f1970;
+	mov.f32 	%f1975, %f1970;
+	@%p50 bra 	$L__BB12_97;
+
+	mul.f32 	%f1630, %f1875, 0f00000000;
+	mov.f32 	%f1631, 0f00000000;
+	fma.rn.f32 	%f1632, %f1631, %f1876, %f1630;
+	mul.f32 	%f1633, %f1871, 0f00000000;
+	fma.rn.f32 	%f1634, %f1631, %f1872, %f1633;
+	mul.f32 	%f1635, %f1867, 0f00000000;
+	fma.rn.f32 	%f1636, %f1631, %f1868, %f1635;
+	fma.rn.f32 	%f1973, %f1631, %f1877, %f1632;
+	fma.rn.f32 	%f1974, %f1631, %f1873, %f1634;
+	fma.rn.f32 	%f1975, %f1631, %f1869, %f1636;
+
+$L__BB12_97:
+	ld.const.u64 	%rd528, [params+232];
+	ld.const.u64 	%rd529, [params+280];
+	or.b64  	%rd530, %rd528, %rd529;
+	setp.eq.s64 	%p51, %rd530, 0;
+	mov.f32 	%f1971, %f1970;
+	mov.f32 	%f1972, %f1970;
+	@%p51 bra 	$L__BB12_99;
+
+	mul.f32 	%f1640, %f1979, %f1875;
+	fma.rn.f32 	%f1641, %f1980, %f1871, %f1640;
+	mul.f32 	%f1642, %f1979, %f1876;
+	fma.rn.f32 	%f1643, %f1980, %f1872, %f1642;
+	mul.f32 	%f1644, %f1979, %f1877;
+	fma.rn.f32 	%f1645, %f1980, %f1873, %f1644;
+	fma.rn.f32 	%f1646, %f1981, %f1867, %f1641;
+	fma.rn.f32 	%f1647, %f1981, %f1868, %f1643;
+	fma.rn.f32 	%f1648, %f1981, %f1869, %f1645;
+	mul.f32 	%f1649, %f1646, %f1646;
+	fma.rn.f32 	%f1650, %f1647, %f1647, %f1649;
+	fma.rn.f32 	%f1651, %f1648, %f1648, %f1650;
+	sqrt.rn.f32 	%f1652, %f1651;
+	div.rn.f32 	%f1653, %f1646, %f1652;
+	div.rn.f32 	%f1654, %f1647, %f1652;
+	div.rn.f32 	%f1655, %f1648, %f1652;
+	mul.f32 	%f1656, %f1653, %f1937;
+	mul.f32 	%f1657, %f1653, %f1938;
+	mul.f32 	%f1658, %f1653, %f1939;
+	fma.rn.f32 	%f1659, %f1654, %f1934, %f1656;
+	fma.rn.f32 	%f1660, %f1654, %f1935, %f1657;
+	fma.rn.f32 	%f1661, %f1654, %f1936, %f1658;
+	fma.rn.f32 	%f1662, %f1655, %f1931, %f1659;
+	fma.rn.f32 	%f1663, %f1655, %f1932, %f1660;
+	fma.rn.f32 	%f1664, %f1655, %f1933, %f1661;
+	mul.f32 	%f1665, %f1662, %f1662;
+	fma.rn.f32 	%f1666, %f1663, %f1663, %f1665;
+	fma.rn.f32 	%f1667, %f1664, %f1664, %f1666;
+	sqrt.rn.f32 	%f1668, %f1667;
+	rcp.rn.f32 	%f1669, %f1668;
+	mul.f32 	%f1670, %f1669, %f1662;
+	mul.f32 	%f1671, %f1669, %f1663;
+	mul.f32 	%f1672, %f1669, %f1664;
+	mul.f32 	%f1673, %f1937, 0f00000000;
+	mov.f32 	%f1674, 0f00000000;
+	fma.rn.f32 	%f1675, %f1674, %f1934, %f1673;
+	mul.f32 	%f1676, %f1938, 0f00000000;
+	fma.rn.f32 	%f1677, %f1674, %f1935, %f1676;
+	mul.f32 	%f1678, %f1939, 0f00000000;
+	fma.rn.f32 	%f1679, %f1674, %f1936, %f1678;
+	fma.rn.f32 	%f1680, %f1674, %f1931, %f1675;
+	fma.rn.f32 	%f1681, %f1674, %f1932, %f1677;
+	fma.rn.f32 	%f1682, %f1674, %f1933, %f1679;
+	mul.f32 	%f1683, %f1680, %f1669;
+	mul.f32 	%f1684, %f1681, %f1669;
+	mul.f32 	%f1685, %f1682, %f1669;
+	mul.f32 	%f1686, %f1670, %f1683;
+	fma.rn.f32 	%f1687, %f1671, %f1684, %f1686;
+	fma.rn.f32 	%f1688, %f1672, %f1685, %f1687;
+	mul.f32 	%f1689, %f1670, %f1688;
+	mul.f32 	%f1690, %f1671, %f1688;
+	mul.f32 	%f1691, %f1672, %f1688;
+	sub.f32 	%f1970, %f1683, %f1689;
+	sub.f32 	%f1971, %f1684, %f1690;
+	sub.f32 	%f1972, %f1685, %f1691;
+
+$L__BB12_99:
+	st.global.u32 	[%rd21], %r334;
+	mov.f32 	%f1838, %f1958;
+	mov.f32 	%f1837, %f1959;
+	mov.f32 	%f1836, %f1960;
+
+$L__BB12_100:
+	ld.const.u64 	%rd531, [params+328];
+	cvta.to.global.u64 	%rd532, %rd531;
+	shl.b64 	%rd533, %rd20, 3;
+	add.s64 	%rd534, %rd532, %rd533;
+	st.global.u64 	[%rd534], %rd19;
+	ld.const.u64 	%rd535, [params+336];
+	cvta.to.global.u64 	%rd536, %rd535;
+	shl.b64 	%rd537, %rd20, 2;
+	add.s64 	%rd538, %rd536, %rd537;
+	mov.u32 	%r639, 0;
+	st.global.u32 	[%rd538], %r639;
+	ld.const.u64 	%rd539, [params+160];
+	cvta.to.global.u64 	%rd540, %rd539;
+	add.s64 	%rd541, %rd540, %rd537;
+	st.global.f32 	[%rd541], %f1982;
+	ld.const.u64 	%rd542, [params+168];
+	cvta.to.global.u64 	%rd543, %rd542;
+	add.s64 	%rd544, %rd543, %rd537;
+	st.global.f32 	[%rd544], %f1983;
+	ld.const.u64 	%rd545, [params+176];
+	cvta.to.global.u64 	%rd546, %rd545;
+	add.s64 	%rd547, %rd546, %rd537;
+	st.global.f32 	[%rd547], %f1984;
+	ld.const.u64 	%rd548, [params+72];
+	cvta.to.global.u64 	%rd549, %rd548;
+	add.s64 	%rd550, %rd549, %rd537;
+	st.global.f32 	[%rd550], %f1149;
+	ld.const.u64 	%rd36, [params+96];
+	setp.eq.s64 	%p52, %rd36, 0;
+	@%p52 bra 	$L__BB12_102;
+
+	cvta.to.global.u64 	%rd551, %rd36;
+	add.s64 	%rd553, %rd551, %rd537;
+	st.global.u32 	[%rd553], %r639;
+	ld.const.u64 	%rd554, [params+104];
 	cvta.to.global.u64 	%rd555, %rd554;
-	add.s64 	%rd556, %rd555, %rd549;
-	st.global.f32 	[%rd556], %f1922;
-	ld.const.u64 	%rd557, [params+176];
-	cvta.to.global.u64 	%rd558, %rd557;
-	add.s64 	%rd559, %rd558, %rd549;
-	st.global.f32 	[%rd559], %f1923;
-	ld.const.u64 	%rd560, [params+72];
+	add.s64 	%rd556, %rd555, %rd537;
+	st.global.u32 	[%rd556], %r639;
+
+$L__BB12_102:
+	ld.const.u64 	%rd37, [params+112];
+	setp.eq.s64 	%p53, %rd37, 0;
+	@%p53 bra 	$L__BB12_104;
+
+	cvta.to.global.u64 	%rd557, %rd37;
+	add.s64 	%rd559, %rd557, %rd537;
+	st.global.f32 	[%rd559], %f1838;
+	ld.const.u64 	%rd560, [params+120];
 	cvta.to.global.u64 	%rd561, %rd560;
-	add.s64 	%rd562, %rd561, %rd549;
-	st.global.f32 	[%rd562], %f1113;
-	ld.const.u64 	%rd40, [params+96];
-	setp.eq.s64	%p49, %rd40, 0;
-	@%p49 bra 	BB12_102;
-
-	cvta.to.global.u64 	%rd563, %rd40;
-	add.s64 	%rd565, %rd563, %rd549;
-	st.global.u32 	[%rd565], %r627;
-	ld.const.u64 	%rd566, [params+104];
-	cvta.to.global.u64 	%rd567, %rd566;
-	add.s64 	%rd568, %rd567, %rd549;
-	st.global.u32 	[%rd568], %r627;
-
-BB12_102:
-	ld.const.u64 	%rd41, [params+112];
-	setp.eq.s64	%p50, %rd41, 0;
-	@%p50 bra 	BB12_104;
-
-	cvta.to.global.u64 	%rd569, %rd41;
-	add.s64 	%rd571, %rd569, %rd549;
-	st.global.f32 	[%rd571], %f1915;
-	ld.const.u64 	%rd572, [params+120];
+	add.s64 	%rd562, %rd561, %rd537;
+	st.global.f32 	[%rd562], %f1837;
+	ld.const.u64 	%rd563, [params+128];
+	cvta.to.global.u64 	%rd564, %rd563;
+	add.s64 	%rd565, %rd564, %rd537;
+	st.global.f32 	[%rd565], %f1836;
+
+$L__BB12_104:
+	ld.const.u64 	%rd38, [params+136];
+	setp.eq.s64 	%p54, %rd38, 0;
+	@%p54 bra 	$L__BB12_106;
+
+	cvta.to.global.u64 	%rd566, %rd38;
+	add.s64 	%rd568, %rd566, %rd537;
+	st.global.f32 	[%rd568], %f1979;
+	ld.const.u64 	%rd569, [params+144];
+	cvta.to.global.u64 	%rd570, %rd569;
+	add.s64 	%rd571, %rd570, %rd537;
+	st.global.f32 	[%rd571], %f1980;
+	ld.const.u64 	%rd572, [params+152];
 	cvta.to.global.u64 	%rd573, %rd572;
-	add.s64 	%rd574, %rd573, %rd549;
-	st.global.f32 	[%rd574], %f1916;
-	ld.const.u64 	%rd575, [params+128];
-	cvta.to.global.u64 	%rd576, %rd575;
-	add.s64 	%rd577, %rd576, %rd549;
-	st.global.f32 	[%rd577], %f1917;
-
-BB12_104:
-	ld.const.u64 	%rd42, [params+136];
-	setp.eq.s64	%p51, %rd42, 0;
-	@%p51 bra 	BB12_106;
-
-	cvta.to.global.u64 	%rd578, %rd42;
-	add.s64 	%rd580, %rd578, %rd549;
-	st.global.f32 	[%rd580], %f1775;
-	ld.const.u64 	%rd581, [params+144];
+	add.s64 	%rd574, %rd573, %rd537;
+	st.global.f32 	[%rd574], %f1981;
+
+$L__BB12_106:
+	ld.const.u64 	%rd39, [params+184];
+	setp.eq.s64 	%p55, %rd39, 0;
+	@%p55 bra 	$L__BB12_108;
+
+	cvta.to.global.u64 	%rd575, %rd39;
+	add.s64 	%rd577, %rd575, %rd537;
+	st.global.f32 	[%rd577], %f1973;
+	ld.const.u64 	%rd578, [params+192];
+	cvta.to.global.u64 	%rd579, %rd578;
+	add.s64 	%rd580, %rd579, %rd537;
+	st.global.f32 	[%rd580], %f1974;
+	ld.const.u64 	%rd581, [params+200];
 	cvta.to.global.u64 	%rd582, %rd581;
-	add.s64 	%rd583, %rd582, %rd549;
-	st.global.f32 	[%rd583], %f1776;
-	ld.const.u64 	%rd584, [params+152];
+	add.s64 	%rd583, %rd582, %rd537;
+	st.global.f32 	[%rd583], %f1975;
+	ld.const.u64 	%rd584, [params+208];
 	cvta.to.global.u64 	%rd585, %rd584;
-	add.s64 	%rd586, %rd585, %rd549;
-	st.global.f32 	[%rd586], %f1777;
-
-BB12_106:
-	ld.const.u64 	%rd43, [params+184];
-	setp.eq.s64	%p52, %rd43, 0;
-	@%p52 bra 	BB12_108;
-
-	cvta.to.global.u64 	%rd587, %rd43;
-	add.s64 	%rd589, %rd587, %rd549;
-	st.global.f32 	[%rd589], %f1912;
-	ld.const.u64 	%rd590, [params+192];
+	add.s64 	%rd586, %rd585, %rd537;
+	st.global.f32 	[%rd586], %f1973;
+	ld.const.u64 	%rd587, [params+216];
+	cvta.to.global.u64 	%rd588, %rd587;
+	add.s64 	%rd589, %rd588, %rd537;
+	st.global.f32 	[%rd589], %f1974;
+	ld.const.u64 	%rd590, [params+224];
 	cvta.to.global.u64 	%rd591, %rd590;
-	add.s64 	%rd592, %rd591, %rd549;
-	st.global.f32 	[%rd592], %f1913;
-	ld.const.u64 	%rd593, [params+200];
-	cvta.to.global.u64 	%rd594, %rd593;
-	add.s64 	%rd595, %rd594, %rd549;
-	st.global.f32 	[%rd595], %f1914;
-	ld.const.u64 	%rd596, [params+208];
+	add.s64 	%rd592, %rd591, %rd537;
+	st.global.f32 	[%rd592], %f1975;
+
+$L__BB12_108:
+	ld.const.u64 	%rd40, [params+232];
+	setp.eq.s64 	%p56, %rd40, 0;
+	@%p56 bra 	$L__BB12_110;
+
+	cvta.to.global.u64 	%rd593, %rd40;
+	add.s64 	%rd595, %rd593, %rd537;
+	st.global.f32 	[%rd595], %f1970;
+	ld.const.u64 	%rd596, [params+240];
 	cvta.to.global.u64 	%rd597, %rd596;
-	add.s64 	%rd598, %rd597, %rd549;
-	st.global.f32 	[%rd598], %f1912;
-	ld.const.u64 	%rd599, [params+216];
+	add.s64 	%rd598, %rd597, %rd537;
+	st.global.f32 	[%rd598], %f1971;
+	ld.const.u64 	%rd599, [params+248];
 	cvta.to.global.u64 	%rd600, %rd599;
-	add.s64 	%rd601, %rd600, %rd549;
-	st.global.f32 	[%rd601], %f1913;
-	ld.const.u64 	%rd602, [params+224];
+	add.s64 	%rd601, %rd600, %rd537;
+	st.global.f32 	[%rd601], %f1972;
+	ld.const.u64 	%rd602, [params+256];
 	cvta.to.global.u64 	%rd603, %rd602;
-	add.s64 	%rd604, %rd603, %rd549;
-	st.global.f32 	[%rd604], %f1914;
-
-BB12_108:
-	ld.const.u64 	%rd44, [params+232];
-	setp.eq.s64	%p53, %rd44, 0;
-	@%p53 bra 	BB12_110;
-
-	cvta.to.global.u64 	%rd605, %rd44;
-	add.s64 	%rd607, %rd605, %rd549;
-	st.global.f32 	[%rd607], %f1909;
-	ld.const.u64 	%rd608, [params+240];
+	add.s64 	%rd604, %rd603, %rd537;
+	st.global.f32 	[%rd604], %f1970;
+	ld.const.u64 	%rd605, [params+264];
+	cvta.to.global.u64 	%rd606, %rd605;
+	add.s64 	%rd607, %rd606, %rd537;
+	st.global.f32 	[%rd607], %f1971;
+	ld.const.u64 	%rd608, [params+272];
 	cvta.to.global.u64 	%rd609, %rd608;
-	add.s64 	%rd610, %rd609, %rd549;
-	st.global.f32 	[%rd610], %f1910;
-	ld.const.u64 	%rd611, [params+248];
-	cvta.to.global.u64 	%rd612, %rd611;
-	add.s64 	%rd613, %rd612, %rd549;
-	st.global.f32 	[%rd613], %f1911;
-	ld.const.u64 	%rd614, [params+256];
+	add.s64 	%rd610, %rd609, %rd537;
+	st.global.f32 	[%rd610], %f1972;
+
+$L__BB12_110:
+	ld.const.u64 	%rd41, [params+280];
+	setp.eq.s64 	%p57, %rd41, 0;
+	@%p57 bra 	$L__BB12_112;
+
+	cvta.to.global.u64 	%rd611, %rd41;
+	add.s64 	%rd613, %rd611, %rd537;
+	st.global.f32 	[%rd613], %f1970;
+	ld.const.u64 	%rd614, [params+288];
 	cvta.to.global.u64 	%rd615, %rd614;
-	add.s64 	%rd616, %rd615, %rd549;
-	st.global.f32 	[%rd616], %f1909;
-	ld.const.u64 	%rd617, [params+264];
+	add.s64 	%rd616, %rd615, %rd537;
+	st.global.f32 	[%rd616], %f1971;
+	ld.const.u64 	%rd617, [params+296];
 	cvta.to.global.u64 	%rd618, %rd617;
-	add.s64 	%rd619, %rd618, %rd549;
-	st.global.f32 	[%rd619], %f1910;
-	ld.const.u64 	%rd620, [params+272];
+	add.s64 	%rd619, %rd618, %rd537;
+	st.global.f32 	[%rd619], %f1972;
+	ld.const.u64 	%rd620, [params+304];
 	cvta.to.global.u64 	%rd621, %rd620;
-	add.s64 	%rd622, %rd621, %rd549;
-	st.global.f32 	[%rd622], %f1911;
-
-BB12_110:
-	ld.const.u64 	%rd45, [params+280];
-	setp.eq.s64	%p54, %rd45, 0;
-	@%p54 bra 	BB12_112;
-
-	cvta.to.global.u64 	%rd623, %rd45;
-	add.s64 	%rd625, %rd623, %rd549;
-	st.global.f32 	[%rd625], %f1909;
-	ld.const.u64 	%rd626, [params+288];
+	add.s64 	%rd622, %rd621, %rd537;
+	st.global.f32 	[%rd622], %f1970;
+	ld.const.u64 	%rd623, [params+312];
+	cvta.to.global.u64 	%rd624, %rd623;
+	add.s64 	%rd625, %rd624, %rd537;
+	st.global.f32 	[%rd625], %f1971;
+	ld.const.u64 	%rd626, [params+320];
 	cvta.to.global.u64 	%rd627, %rd626;
-	add.s64 	%rd628, %rd627, %rd549;
-	st.global.f32 	[%rd628], %f1910;
-	ld.const.u64 	%rd629, [params+296];
-	cvta.to.global.u64 	%rd630, %rd629;
-	add.s64 	%rd631, %rd630, %rd549;
-	st.global.f32 	[%rd631], %f1911;
-	ld.const.u64 	%rd632, [params+304];
-	cvta.to.global.u64 	%rd633, %rd632;
-	add.s64 	%rd634, %rd633, %rd549;
-	st.global.f32 	[%rd634], %f1909;
-	ld.const.u64 	%rd635, [params+312];
-	cvta.to.global.u64 	%rd636, %rd635;
-	add.s64 	%rd637, %rd636, %rd549;
-	st.global.f32 	[%rd637], %f1910;
-	ld.const.u64 	%rd638, [params+320];
-	cvta.to.global.u64 	%rd639, %rd638;
-	add.s64 	%rd640, %rd639, %rd549;
-	st.global.f32 	[%rd640], %f1911;
+	add.s64 	%rd628, %rd627, %rd537;
+	st.global.f32 	[%rd628], %f1972;
 
-BB12_112:
+$L__BB12_112:
 	ret;
 
-BB12_45:
-	fma.rn.f32 	%f1115, %f1113, %f1772, %f1725;
-	ld.v4.f32 	{%f1116, %f1117, %f1118, %f1119}, [%rd3+288];
-	sub.f32 	%f316, %f1115, %f1116;
-	fma.rn.f32 	%f1121, %f1113, %f1773, %f1724;
-	sub.f32 	%f317, %f1121, %f1117;
-	sub.f32 	%f1124, %f1923, %f1118;
-	mul.f32 	%f1125, %f1124, 0f00000000;
-	mul.f32 	%f1126, %f316, %f316;
-	fma.rn.f32 	%f318, %f317, %f317, %f1126;
-	fma.rn.f32 	%f1127, %f1125, %f1125, %f318;
-	sqrt.rn.f32 	%f1128, %f1127;
-	ld.v4.f32 	{%f1129, %f1130, %f1131, %f1132}, [%rd3+304];
-	fma.rn.f32 	%f1135, %f1131, 0f3F000000, %f1129;
-	setp.gt.f32	%p22, %f1128, %f1135;
-	mul.f32 	%f319, %f1125, 0f00000000;
-	@%p22 bra 	BB12_47;
-	bra.uni 	BB12_46;
-
-BB12_47:
-	fma.rn.f32 	%f1142, %f319, %f319, %f318;
-	sqrt.rn.f32 	%f1143, %f1142;
-	div.rn.f32 	%f1775, %f316, %f1143;
-	div.rn.f32 	%f1776, %f317, %f1143;
-	div.rn.f32 	%f1777, %f319, %f1143;
-	bra.uni 	BB12_52;
-
-BB12_46:
-	neg.f32 	%f1136, %f316;
-	neg.f32 	%f1138, %f317;
-	fma.rn.f32 	%f1139, %f1138, %f1138, %f1126;
-	fma.rn.f32 	%f1140, %f319, %f319, %f1139;
-	sqrt.rn.f32 	%f1141, %f1140;
-	div.rn.f32 	%f1775, %f1136, %f1141;
-	div.rn.f32 	%f1776, %f1138, %f1141;
-	div.rn.f32 	%f1777, %f319, %f1141;
-	bra.uni 	BB12_52;
-}
+$L__BB12_46:
+	fma.rn.f32 	%f1152, %f1149, %f1833, %f1775;
+	ld.f32 	%f1153, [%rd18+-32];
+	sub.f32 	%f347, %f1152, %f1153;
+	ld.f32 	%f1154, [%rd18+-28];
+	fma.rn.f32 	%f1155, %f1149, %f1834, %f1776;
+	sub.f32 	%f348, %f1155, %f1154;
+	ld.f32 	%f1156, [%rd18+-24];
+	sub.f32 	%f1157, %f1984, %f1156;
+	mul.f32 	%f1158, %f1157, 0f00000000;
+	mul.f32 	%f349, %f347, %f347;
+	fma.rn.f32 	%f350, %f348, %f348, %f349;
+	fma.rn.f32 	%f1159, %f1158, %f1158, %f350;
+	sqrt.rn.f32 	%f1160, %f1159;
+	ld.v4.f32 	{%f1161, %f1162, %f1163, %f1164}, [%rd18+-16];
+	fma.rn.f32 	%f1167, %f1163, 0f3F000000, %f1161;
+	setp.gt.f32 	%p24, %f1160, %f1167;
+	mul.f32 	%f351, %f1158, 0f00000000;
+	@%p24 bra 	$L__BB12_48;
+	bra.uni 	$L__BB12_47;
+
+$L__BB12_48:
+	fma.rn.f32 	%f1173, %f351, %f351, %f350;
+	sqrt.rn.f32 	%f1174, %f1173;
+	div.rn.f32 	%f1838, %f347, %f1174;
+	div.rn.f32 	%f1837, %f348, %f1174;
+	div.rn.f32 	%f1836, %f351, %f1174;
+	bra.uni 	$L__BB12_52;
+
+$L__BB12_47:
+	neg.f32 	%f1168, %f347;
+	neg.f32 	%f1169, %f348;
+	fma.rn.f32 	%f1170, %f1169, %f1169, %f349;
+	fma.rn.f32 	%f1171, %f351, %f351, %f1170;
+	sqrt.rn.f32 	%f1172, %f1171;
+	div.rn.f32 	%f1838, %f1168, %f1172;
+	div.rn.f32 	%f1837, %f1169, %f1172;
+	div.rn.f32 	%f1836, %f351, %f1172;
+	bra.uni 	$L__BB12_52;
 
+}
 	// .globl	__raygen__rg
-.visible .entry __raygen__rg(
-
-)
+.visible .entry __raygen__rg()
 {
 	.reg .pred 	%p<5>;
 	.reg .b16 	%rs<5>;
@@ -35818,26 +31866,26 @@ BB12_46:
 	.reg .b64 	%rd<44>;
 
 
-	// inline asm
+	// begin inline asm
 	call (%r1), _optix_get_launch_dimension_x, ();
-	// inline asm
-	// inline asm
+	// end inline asm
+	// begin inline asm
 	call (%r2), _optix_get_launch_dimension_y, ();
-	// inline asm
-	// inline asm
+	// end inline asm
+	// begin inline asm
 	call (%r4), _optix_get_launch_index_x, ();
-	// inline asm
-	// inline asm
+	// end inline asm
+	// begin inline asm
 	call (%r5), _optix_get_launch_index_y, ();
-	// inline asm
-	// inline asm
+	// end inline asm
+	// begin inline asm
 	call (%r6), _optix_get_launch_index_z, ();
-	// inline asm
+	// end inline asm
 	mad.lo.s32 	%r7, %r6, %r2, %r5;
 	mad.lo.s32 	%r8, %r7, %r1, %r4;
 	ld.const.u64 	%rd3, [params+8];
 	cvta.to.global.u64 	%rd4, %rd3;
-	cvt.u64.u32	%rd1, %r8;
+	cvt.u64.u32 	%rd1, %r8;
 	mul.wide.u32 	%rd5, %r8, 4;
 	add.s64 	%rd6, %rd4, %rd5;
 	ld.global.f32 	%f1, [%rd6];
@@ -35869,56 +31917,56 @@ BB12_46:
 	cvta.to.global.u64 	%rd26, %rd25;
 	add.s64 	%rd27, %rd26, %rd5;
 	ld.global.f32 	%f9, [%rd27];
-	setp.eq.f32	%p1, %f9, 0f7F800000;
-	selp.f32	%f8, 0f7F7FFFFF, %f9, %p1;
+	setp.eq.f32 	%p1, %f9, 0f7F800000;
+	selp.f32 	%f8, 0f7F7FFFFF, %f9, %p1;
 	ld.const.u64 	%rd2, [params+352];
-	setp.eq.s64	%p2, %rd2, 0;
+	setp.eq.s64 	%p2, %rd2, 0;
 	ld.const.u64 	%rd28, [params];
 	cvta.to.global.u64 	%rd29, %rd28;
 	add.s64 	%rd30, %rd29, %rd1;
 	ld.global.u8 	%rs1, [%rd30];
-	@%p2 bra 	BB13_4;
+	@%p2 bra 	$L__BB13_4;
 
-	setp.eq.s16	%p3, %rs1, 0;
-	@%p3 bra 	BB13_3;
+	setp.eq.s16 	%p3, %rs1, 0;
+	@%p3 bra 	$L__BB13_3;
 
 	ld.const.u64 	%rd31, [params+360];
+	mov.f32 	%f18, 0f00000000;
 	mov.u32 	%r10, 4;
 	mov.u32 	%r12, 1;
 	mov.u32 	%r13, 0;
-	mov.f32 	%f18, 0f00000000;
-	// inline asm
+	// begin inline asm
 	call _optix_trace_0, (%rd31, %f1, %f2, %f3, %f4, %f5, %f6, %f7, %f8, %f18, %r12, %r10, %r13, %r12, %r13);
-	// inline asm
-	bra.uni 	BB13_7;
+	// end inline asm
+	bra.uni 	$L__BB13_7;
 
-BB13_4:
-	setp.eq.s16	%p4, %rs1, 0;
-	@%p4 bra 	BB13_6;
+$L__BB13_4:
+	setp.eq.s16 	%p4, %rs1, 0;
+	@%p4 bra 	$L__BB13_6;
 
 	ld.const.u64 	%rd34, [params+360];
+	mov.f32 	%f27, 0f00000000;
 	mov.u32 	%r17, 1;
 	mov.u32 	%r18, 0;
-	mov.f32 	%f27, 0f00000000;
-	// inline asm
+	// begin inline asm
 	call _optix_trace_0, (%rd34, %f1, %f2, %f3, %f4, %f5, %f6, %f7, %f8, %f27, %r17, %r18, %r18, %r17, %r18);
-	// inline asm
-	bra.uni 	BB13_7;
+	// end inline asm
+	bra.uni 	$L__BB13_7;
 
-BB13_3:
+$L__BB13_3:
 	cvta.to.global.u64 	%rd32, %rd2;
 	add.s64 	%rd33, %rd32, %rd1;
 	mov.u16 	%rs3, 0;
 	st.global.u8 	[%rd33], %rs3;
-	bra.uni 	BB13_7;
-
-BB13_6:
-	ld.const.u64 	%rd35, [params+328];
-	cvta.to.global.u64 	%rd36, %rd35;
-	shl.b64 	%rd37, %rd1, 3;
-	add.s64 	%rd38, %rd36, %rd37;
-	mov.u64 	%rd39, 0;
-	st.global.u64 	[%rd38], %rd39;
+	bra.uni 	$L__BB13_7;
+
+$L__BB13_6:
+	mov.u64 	%rd35, 0;
+	ld.const.u64 	%rd36, [params+328];
+	cvta.to.global.u64 	%rd37, %rd36;
+	shl.b64 	%rd38, %rd1, 3;
+	add.s64 	%rd39, %rd37, %rd38;
+	st.global.u64 	[%rd39], %rd35;
 	ld.const.u64 	%rd40, [params+72];
 	cvta.to.global.u64 	%rd41, %rd40;
 	shl.b64 	%rd42, %rd1, 2;
@@ -35926,14 +31974,12 @@ BB13_6:
 	mov.u32 	%r19, 2139095040;
 	st.global.u32 	[%rd43], %r19;
 
-BB13_7:
+$L__BB13_7:
 	ret;
-}
 
+}
 	// .globl	__miss__ms
-.visible .entry __miss__ms(
-
-)
+.visible .entry __miss__ms()
 {
 	.reg .pred 	%p<2>;
 	.reg .b16 	%rs<2>;
@@ -35941,41 +31987,41 @@ BB13_7:
 	.reg .b64 	%rd<14>;
 
 
-	// inline asm
+	// begin inline asm
 	call (%r1), _optix_get_launch_dimension_x, ();
-	// inline asm
-	// inline asm
+	// end inline asm
+	// begin inline asm
 	call (%r2), _optix_get_launch_dimension_y, ();
-	// inline asm
-	// inline asm
+	// end inline asm
+	// begin inline asm
 	call (%r4), _optix_get_launch_index_x, ();
-	// inline asm
-	// inline asm
+	// end inline asm
+	// begin inline asm
 	call (%r5), _optix_get_launch_index_y, ();
-	// inline asm
-	// inline asm
+	// end inline asm
+	// begin inline asm
 	call (%r6), _optix_get_launch_index_z, ();
-	// inline asm
+	// end inline asm
 	mad.lo.s32 	%r7, %r6, %r2, %r5;
 	mad.lo.s32 	%r8, %r7, %r1, %r4;
 	ld.const.u64 	%rd1, [params+352];
-	setp.eq.s64	%p1, %rd1, 0;
-	cvt.u64.u32	%rd2, %r8;
-	@%p1 bra 	BB14_2;
+	setp.eq.s64 	%p1, %rd1, 0;
+	cvt.u64.u32 	%rd2, %r8;
+	@%p1 bra 	$L__BB14_2;
 
 	cvta.to.global.u64 	%rd3, %rd1;
 	add.s64 	%rd4, %rd3, %rd2;
 	mov.u16 	%rs1, 0;
 	st.global.u8 	[%rd4], %rs1;
-	bra.uni 	BB14_3;
-
-BB14_2:
-	ld.const.u64 	%rd5, [params+328];
-	cvta.to.global.u64 	%rd6, %rd5;
-	shl.b64 	%rd7, %rd2, 3;
-	add.s64 	%rd8, %rd6, %rd7;
-	mov.u64 	%rd9, 0;
-	st.global.u64 	[%rd8], %rd9;
+	bra.uni 	$L__BB14_3;
+
+$L__BB14_2:
+	mov.u64 	%rd5, 0;
+	ld.const.u64 	%rd6, [params+328];
+	cvta.to.global.u64 	%rd7, %rd6;
+	shl.b64 	%rd8, %rd2, 3;
+	add.s64 	%rd9, %rd7, %rd8;
+	st.global.u64 	[%rd9], %rd5;
 	ld.const.u64 	%rd10, [params+72];
 	cvta.to.global.u64 	%rd11, %rd10;
 	shl.b64 	%rd12, %rd2, 2;
@@ -35983,14 +32029,12 @@ BB14_2:
 	mov.u32 	%r9, 2139095040;
 	st.global.u32 	[%rd13], %r9;
 
-BB14_3:
+$L__BB14_3:
 	ret;
-}
 
+}
 	// .globl	__exception__err
-.visible .entry __exception__err(
-
-)
+.visible .entry __exception__err()
 {
 	.local .align 16 .b8 	__local_depot15[16];
 	.reg .b64 	%SP;
@@ -36001,27 +32045,25 @@ BB14_3:
 
 	mov.u64 	%SPL, __local_depot15;
 	cvta.local.u64 	%SP, %SPL;
-	// inline asm
+	add.u64 	%rd1, %SP, 0;
+	add.u64 	%rd2, %SPL, 0;
+	// begin inline asm
 	call (%r1), _optix_get_exception_code, ();
-	// inline asm
-	mul.wide.s32 	%rd1, %r1, 16;
-	mov.u64 	%rd2, exceptions;
-	add.s64 	%rd3, %rd2, %rd1;
-	ld.const.u64 	%rd4, [%rd3+8];
-	add.u64 	%rd5, %SP, 0;
-	add.u64 	%rd6, %SPL, 0;
-	st.local.u32 	[%rd6], %r1;
-	st.local.u64 	[%rd6+8], %rd4;
-	mov.u64 	%rd7, $str6;
+	// end inline asm
+	mul.wide.s32 	%rd3, %r1, 16;
+	mov.u64 	%rd4, exceptions;
+	add.s64 	%rd5, %rd4, %rd3;
+	ld.const.u64 	%rd6, [%rd5+8];
+	st.local.u32 	[%rd2], %r1;
+	st.local.u64 	[%rd2+8], %rd6;
+	mov.u64 	%rd7, $str$6;
 	cvta.global.u64 	%rd8, %rd7;
-	// Callseq Start 25
-	{
+	{ // callseq 0, 0
 	.reg .b32 temp_param_reg;
-	// <end>}
 	.param .b64 param0;
-	st.param.b64	[param0+0], %rd8;
+	st.param.b64 	[param0+0], %rd8;
 	.param .b64 param1;
-	st.param.b64	[param1+0], %rd5;
+	st.param.b64 	[param1+0], %rd1;
 	.param .b32 retval0;
 	call.uni (retval0), 
 	vprintf, 
@@ -36029,282 +32071,9 @@ BB14_3:
 	param0, 
 	param1
 	);
-	ld.param.b32	%r2, [retval0+0];
-	
-	//{
-	}// Callseq End 25
+	ld.param.b32 	%r2, [retval0+0];
+	} // callseq 0
 	ret;
-}
 
-.func  (.param .b64 func_retval0) __internal_accurate_pow(
-	.param .b64 __internal_accurate_pow_param_0
-)
-{
-	.reg .pred 	%p<9>;
-	.reg .f32 	%f<3>;
-	.reg .b32 	%r<53>;
-	.reg .f64 	%fd<138>;
-
-
-	ld.param.f64 	%fd12, [__internal_accurate_pow_param_0];
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r50}, %fd12;
-	}
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r49, %temp}, %fd12;
-	}
-	shr.u32 	%r51, %r50, 20;
-	setp.ne.s32	%p1, %r51, 0;
-	@%p1 bra 	BB16_2;
-
-	mul.f64 	%fd13, %fd12, 0d4350000000000000;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r50}, %fd13;
-	}
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r49, %temp}, %fd13;
-	}
-	shr.u32 	%r16, %r50, 20;
-	add.s32 	%r51, %r16, -54;
-
-BB16_2:
-	add.s32 	%r52, %r51, -1023;
-	and.b32  	%r17, %r50, -2146435073;
-	or.b32  	%r18, %r17, 1072693248;
-	mov.b64 	%fd135, {%r49, %r18};
-	setp.lt.u32	%p2, %r18, 1073127583;
-	@%p2 bra 	BB16_4;
-
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r19, %temp}, %fd135;
-	}
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r20}, %fd135;
-	}
-	add.s32 	%r21, %r20, -1048576;
-	mov.b64 	%fd135, {%r19, %r21};
-	add.s32 	%r52, %r51, -1022;
-
-BB16_4:
-	add.f64 	%fd14, %fd135, 0d3FF0000000000000;
-	rcp.approx.ftz.f64 	%fd15, %fd14;
-	neg.f64 	%fd16, %fd14;
-	mov.f64 	%fd17, 0d3FF0000000000000;
-	fma.rn.f64 	%fd18, %fd16, %fd15, %fd17;
-	fma.rn.f64 	%fd19, %fd18, %fd18, %fd18;
-	fma.rn.f64 	%fd20, %fd19, %fd15, %fd15;
-	add.f64 	%fd21, %fd135, 0dBFF0000000000000;
-	mul.f64 	%fd22, %fd21, %fd20;
-	fma.rn.f64 	%fd23, %fd21, %fd20, %fd22;
-	mul.f64 	%fd24, %fd23, %fd23;
-	mov.f64 	%fd25, 0d3ED0F5D241AD3B5A;
-	mov.f64 	%fd26, 0d3EB0F5FF7D2CAFE2;
-	fma.rn.f64 	%fd27, %fd26, %fd24, %fd25;
-	mov.f64 	%fd28, 0d3EF3B20A75488A3F;
-	fma.rn.f64 	%fd29, %fd27, %fd24, %fd28;
-	mov.f64 	%fd30, 0d3F1745CDE4FAECD5;
-	fma.rn.f64 	%fd31, %fd29, %fd24, %fd30;
-	mov.f64 	%fd32, 0d3F3C71C7258A578B;
-	fma.rn.f64 	%fd33, %fd31, %fd24, %fd32;
-	mov.f64 	%fd34, 0d3F6249249242B910;
-	fma.rn.f64 	%fd35, %fd33, %fd24, %fd34;
-	mov.f64 	%fd36, 0d3F89999999999DFB;
-	fma.rn.f64 	%fd37, %fd35, %fd24, %fd36;
-	sub.f64 	%fd38, %fd21, %fd23;
-	add.f64 	%fd39, %fd38, %fd38;
-	neg.f64 	%fd40, %fd23;
-	fma.rn.f64 	%fd41, %fd40, %fd21, %fd39;
-	mul.f64 	%fd42, %fd20, %fd41;
-	fma.rn.f64 	%fd43, %fd24, %fd37, 0d3FB5555555555555;
-	mov.f64 	%fd44, 0d3FB5555555555555;
-	sub.f64 	%fd45, %fd44, %fd43;
-	fma.rn.f64 	%fd46, %fd24, %fd37, %fd45;
-	add.f64 	%fd47, %fd46, 0d0000000000000000;
-	add.f64 	%fd48, %fd47, 0dBC46A4CB00B9E7B0;
-	add.f64 	%fd49, %fd43, %fd48;
-	sub.f64 	%fd50, %fd43, %fd49;
-	add.f64 	%fd51, %fd48, %fd50;
-	mul.rn.f64 	%fd52, %fd23, %fd23;
-	neg.f64 	%fd53, %fd52;
-	fma.rn.f64 	%fd54, %fd23, %fd23, %fd53;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r22, %temp}, %fd42;
-	}
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r23}, %fd42;
-	}
-	add.s32 	%r24, %r23, 1048576;
-	mov.b64 	%fd55, {%r22, %r24};
-	fma.rn.f64 	%fd56, %fd23, %fd55, %fd54;
-	mul.rn.f64 	%fd57, %fd52, %fd23;
-	neg.f64 	%fd58, %fd57;
-	fma.rn.f64 	%fd59, %fd52, %fd23, %fd58;
-	fma.rn.f64 	%fd60, %fd52, %fd42, %fd59;
-	fma.rn.f64 	%fd61, %fd56, %fd23, %fd60;
-	mul.rn.f64 	%fd62, %fd49, %fd57;
-	neg.f64 	%fd63, %fd62;
-	fma.rn.f64 	%fd64, %fd49, %fd57, %fd63;
-	fma.rn.f64 	%fd65, %fd49, %fd61, %fd64;
-	fma.rn.f64 	%fd66, %fd51, %fd57, %fd65;
-	add.f64 	%fd67, %fd62, %fd66;
-	sub.f64 	%fd68, %fd62, %fd67;
-	add.f64 	%fd69, %fd66, %fd68;
-	add.f64 	%fd70, %fd23, %fd67;
-	sub.f64 	%fd71, %fd23, %fd70;
-	add.f64 	%fd72, %fd67, %fd71;
-	add.f64 	%fd73, %fd69, %fd72;
-	add.f64 	%fd74, %fd42, %fd73;
-	add.f64 	%fd75, %fd70, %fd74;
-	sub.f64 	%fd76, %fd70, %fd75;
-	add.f64 	%fd77, %fd74, %fd76;
-	xor.b32  	%r25, %r52, -2147483648;
-	mov.u32 	%r26, -2147483648;
-	mov.u32 	%r27, 1127219200;
-	mov.b64 	%fd78, {%r25, %r27};
-	mov.b64 	%fd79, {%r26, %r27};
-	sub.f64 	%fd80, %fd78, %fd79;
-	mov.f64 	%fd81, 0d3FE62E42FEFA39EF;
-	fma.rn.f64 	%fd82, %fd80, %fd81, %fd75;
-	neg.f64 	%fd83, %fd80;
-	fma.rn.f64 	%fd84, %fd83, %fd81, %fd82;
-	sub.f64 	%fd85, %fd84, %fd75;
-	sub.f64 	%fd86, %fd77, %fd85;
-	mov.f64 	%fd87, 0d3C7ABC9E3B39803F;
-	fma.rn.f64 	%fd88, %fd80, %fd87, %fd86;
-	add.f64 	%fd89, %fd82, %fd88;
-	sub.f64 	%fd90, %fd82, %fd89;
-	add.f64 	%fd91, %fd88, %fd90;
-	mov.f64 	%fd92, 0d4000000000000000;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r28}, %fd92;
-	}
-	add.s32 	%r29, %r28, %r28;
-	setp.gt.u32	%p3, %r29, -33554433;
-	and.b32  	%r30, %r28, -15728641;
-	selp.b32	%r31, %r30, %r28, %p3;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r32, %temp}, %fd92;
-	}
-	mov.b64 	%fd93, {%r32, %r31};
-	mul.rn.f64 	%fd94, %fd89, %fd93;
-	neg.f64 	%fd95, %fd94;
-	fma.rn.f64 	%fd96, %fd89, %fd93, %fd95;
-	fma.rn.f64 	%fd97, %fd91, %fd93, %fd96;
-	add.f64 	%fd4, %fd94, %fd97;
-	sub.f64 	%fd98, %fd94, %fd4;
-	add.f64 	%fd5, %fd97, %fd98;
-	mov.f64 	%fd99, 0d4338000000000000;
-	mov.f64 	%fd100, 0d3FF71547652B82FE;
-	fma.rn.f64 	%fd101, %fd4, %fd100, %fd99;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r13, %temp}, %fd101;
-	}
-	mov.f64 	%fd102, 0dC338000000000000;
-	add.rn.f64 	%fd103, %fd101, %fd102;
-	mov.f64 	%fd104, 0dBFE62E42FEFA39EF;
-	fma.rn.f64 	%fd105, %fd103, %fd104, %fd4;
-	mov.f64 	%fd106, 0dBC7ABC9E3B39803F;
-	fma.rn.f64 	%fd107, %fd103, %fd106, %fd105;
-	mov.f64 	%fd108, 0d3E928AF3FCA213EA;
-	mov.f64 	%fd109, 0d3E5ADE1569CE2BDF;
-	fma.rn.f64 	%fd110, %fd109, %fd107, %fd108;
-	mov.f64 	%fd111, 0d3EC71DEE62401315;
-	fma.rn.f64 	%fd112, %fd110, %fd107, %fd111;
-	mov.f64 	%fd113, 0d3EFA01997C89EB71;
-	fma.rn.f64 	%fd114, %fd112, %fd107, %fd113;
-	mov.f64 	%fd115, 0d3F2A01A014761F65;
-	fma.rn.f64 	%fd116, %fd114, %fd107, %fd115;
-	mov.f64 	%fd117, 0d3F56C16C1852B7AF;
-	fma.rn.f64 	%fd118, %fd116, %fd107, %fd117;
-	mov.f64 	%fd119, 0d3F81111111122322;
-	fma.rn.f64 	%fd120, %fd118, %fd107, %fd119;
-	mov.f64 	%fd121, 0d3FA55555555502A1;
-	fma.rn.f64 	%fd122, %fd120, %fd107, %fd121;
-	mov.f64 	%fd123, 0d3FC5555555555511;
-	fma.rn.f64 	%fd124, %fd122, %fd107, %fd123;
-	mov.f64 	%fd125, 0d3FE000000000000B;
-	fma.rn.f64 	%fd126, %fd124, %fd107, %fd125;
-	fma.rn.f64 	%fd127, %fd126, %fd107, %fd17;
-	fma.rn.f64 	%fd128, %fd127, %fd107, %fd17;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r14, %temp}, %fd128;
-	}
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r15}, %fd128;
-	}
-	shl.b32 	%r33, %r13, 20;
-	add.s32 	%r34, %r15, %r33;
-	mov.b64 	%fd136, {%r14, %r34};
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r35}, %fd4;
-	}
-	mov.b32 	 %f2, %r35;
-	abs.f32 	%f1, %f2;
-	setp.lt.f32	%p4, %f1, 0f4086232B;
-	@%p4 bra 	BB16_7;
-
-	setp.lt.f64	%p5, %fd4, 0d0000000000000000;
-	add.f64 	%fd129, %fd4, 0d7FF0000000000000;
-	selp.f64	%fd136, 0d0000000000000000, %fd129, %p5;
-	setp.geu.f32	%p6, %f1, 0f40874800;
-	@%p6 bra 	BB16_7;
-
-	mov.f64 	%fd134, 0d4338000000000000;
-	mov.f64 	%fd133, 0d3FF71547652B82FE;
-	fma.rn.f64 	%fd132, %fd4, %fd133, %fd134;
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r48, %temp}, %fd132;
-	}
-	shr.u32 	%r36, %r48, 31;
-	add.s32 	%r37, %r48, %r36;
-	shr.s32 	%r38, %r37, 1;
-	shl.b32 	%r39, %r38, 20;
-	add.s32 	%r40, %r39, %r15;
-	mov.b64 	%fd130, {%r14, %r40};
-	sub.s32 	%r41, %r48, %r38;
-	shl.b32 	%r42, %r41, 20;
-	add.s32 	%r43, %r42, 1072693248;
-	mov.u32 	%r44, 0;
-	mov.b64 	%fd131, {%r44, %r43};
-	mul.f64 	%fd136, %fd130, %fd131;
-
-BB16_7:
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%temp, %r45}, %fd136;
-	}
-	and.b32  	%r46, %r45, 2147483647;
-	setp.ne.s32	%p7, %r46, 2146435072;
-	@%p7 bra 	BB16_9;
-
-	{
-	.reg .b32 %temp; 
-	mov.b64 	{%r47, %temp}, %fd136;
-	}
-	setp.eq.s32	%p8, %r47, 0;
-	@%p8 bra 	BB16_10;
-
-BB16_9:
-	fma.rn.f64 	%fd136, %fd136, %fd5, %fd136;
-
-BB16_10:
-	st.param.f64	[func_retval0+0], %fd136;
-	ret;
 }
 
-
diff --git a/src/shapes/aspheric_surf.cpp b/src/shapes/aspheric_surf.cpp
index ec25024a..dcc397fe 100644
--- a/src/shapes/aspheric_surf.cpp
+++ b/src/shapes/aspheric_surf.cpp
@@ -34,15 +34,15 @@ NAMESPACE_BEGIN(mitsuba)
                 using Double3 = Vector<Double, 3>;
 
             AsphSurf(const Properties &props) : Base(props) {
-                /// Are the sphere normals pointing inwards? default: no
+                /// Are the normals pointing inwards relative to the sphere? default: yes for negative curvature, no for positive curvature
+                /// This means that the normals are always pointing in the negative z direction by default, i.e. the inside is towards the right halfspace along the z-axis
                 m_flip_normals = props.bool_("flip_normals", false);
 
-                // Flip curvature?
+                // Flip curvature? Can also be specified through negative radius/curvature, note that these combine like -1*-1 = 1
                 m_flip = props.bool_("flip", false);
 
-                // Update the to_world transform if radius and center are also provided
+                // Update the to_world transform if center is also provided
                 m_to_world = m_to_world * ScalarTransform4f::translate(props.point3f("center", 0.f));
-                m_to_world = m_to_world * ScalarTransform4f::scale(props.float_("radius", 1.f));
 
                 // h limit is common to both
                 m_h_lim = props.float_("limit", 0.0f);
@@ -55,7 +55,8 @@ NAMESPACE_BEGIN(mitsuba)
                 update();
 
                 // AsphSurfes' z limit
-                m_z_lim = ((pow(m_h_lim, 2.0f) * m_p) / (1 + sqrt(1 - (1 + m_k) * pow(m_h_lim*m_p,2.0f))));
+                ScalarFloat p = (ScalarFloat)m_p * (ScalarFloat)(m_flip ? -1.l: 1.l);
+                m_z_lim = ((sqr(m_h_lim) * p) / (1 + sqrt(1 - (1 + m_k) * sqr(m_h_lim*p))));
 
                 // How far into z plane?
                 fprintf(stdout, "AsphSurf using flip=%s inv_norm=%s kappa=%.2f radius=%.2f (rho=%f) hlim=%.2f zlim=%.2f\n",
@@ -76,28 +77,21 @@ NAMESPACE_BEGIN(mitsuba)
             void update() {
 
                 std::cerr << "Update\n";
-                // Extract center and radius from to_world matrix (25 iterations for numerical accuracy)
+                // Extract center from to_world matrix (25 iterations for numerical accuracy)
                 auto [S, Q, T] = transform_decompose(m_to_world.matrix, 25);
 
                 if (abs(S[0][1]) > 1e-6f || abs(S[0][2]) > 1e-6f || abs(S[1][0]) > 1e-6f ||
-                    abs(S[1][2]) > 1e-6f || abs(S[2][0]) > 1e-6f || abs(S[2][1]) > 1e-6f)
-                    Log(Warn, "'to_world' transform shouldn't contain any shearing!");
-
-                if (!(abs(S[0][0] - S[1][1]) < 1e-6f && abs(S[0][0] - S[2][2]) < 1e-6f))
-                    Log(Warn, "'to_world' transform shouldn't contain non-uniform scaling!");
+                    abs(S[1][2]) > 1e-6f || abs(S[2][0]) > 1e-6f || abs(S[2][1]) > 1e-6f ||
+                    abs(S[0][0]-1.0f) > 1e-6f || abs(S[1][1]-1.0f) > 1e-6f || abs(S[2][2]-1.0f) > 1e-6f)
+                    Log(Warn, "'to_world' transform shouldn't contain any scaling or shearing!");
 
                 m_center = T;
-                m_radius = S[0][0];
-
-                if (m_radius <= 0.f) {
-                    Log(Error, "Radius must be > 0");
-                }
 
-                // Reconstruct the to_world transform with uniform scaling and no shear
-                m_to_world = transform_compose(ScalarMatrix3f(m_radius), Q, T);
+                // Reconstruct the to_world transform with no scaling / shear
+                m_to_world = transform_compose(S, Q, T);
                 m_to_object = m_to_world.inverse();
 
-                m_inv_surface_area = rcp(surface_area());
+                m_inv_surface_area = 1.0f;
             }
 
 
@@ -113,7 +107,7 @@ NAMESPACE_BEGIN(mitsuba)
 #if 1
             ScalarFloat surface_area() const override {
                 std::cerr << "surface_area\n";
-                return 1000 * 4.f * math::Pi<ScalarFloat> * m_radius * m_radius;
+                return 1.0f;
             }
 #endif
 
@@ -133,14 +127,14 @@ NAMESPACE_BEGIN(mitsuba)
                 Point3f local = warp::square_to_uniform_sphere(sample);
 
                 PositionSample3f ps;
-                ps.p = fmadd(local, m_radius, m_center);
+                ps.p = local + m_center;
                 ps.n = local;
 
                 if (m_flip_normals)
                     ps.n = -ps.n;
 
                 ps.time = time;
-                ps.delta = m_radius == 0.f;
+                ps.delta = false;
                 ps.pdf = m_inv_surface_area;
 
                 return ps;
@@ -168,12 +162,10 @@ NAMESPACE_BEGIN(mitsuba)
                 Vector3f dc_v = m_center - it.p;
                 Float dc_2 = squared_norm(dc_v);
 
-                Float radius_adj = m_radius * (m_flip_normals ? (1.f + math::RayEpsilon<Float>) :
-                                               (1.f - math::RayEpsilon<Float>));
-                Mask outside_mask = active && dc_2 > sqr(radius_adj);
+                Mask outside_mask = active && dc_2 > 1.0f;
                 if (likely(any(outside_mask))) {
                     Float inv_dc            = rsqrt(dc_2),
-                          sin_theta_max     = m_radius * inv_dc,
+                          sin_theta_max     = inv_dc,
                           sin_theta_max_2   = sqr(sin_theta_max),
                           inv_sin_theta_max = rcp(sin_theta_max),
                           cos_theta_max     = safe_sqrt(1.f - sin_theta_max_2);
@@ -198,7 +190,7 @@ NAMESPACE_BEGIN(mitsuba)
                                                                            cos_alpha));
 
                     DirectionSample3f ds = zero<DirectionSample3f>();
-                    ds.p        = fmadd(d, m_radius, m_center);
+                    ds.p        = d + m_center;
                     ds.n        = d;
                     ds.d        = ds.p - it.p;
 
@@ -215,7 +207,7 @@ NAMESPACE_BEGIN(mitsuba)
                 if (unlikely(any(inside_mask))) {
                     Vector3f d = warp::square_to_uniform_sphere(sample);
                     DirectionSample3f ds = zero<DirectionSample3f>();
-                    ds.p        = fmadd(d, m_radius, m_center);
+                    ds.p        = d + m_center;
                     ds.n        = d;
                     ds.d        = ds.p - it.p;
 
@@ -228,7 +220,7 @@ NAMESPACE_BEGIN(mitsuba)
                 }
 
                 result.time = it.time;
-                result.delta = m_radius == 0.f;
+                result.delta = .0f;
 
                 if (m_flip_normals)
                     result.n = -result.n;
@@ -243,7 +235,7 @@ NAMESPACE_BEGIN(mitsuba)
                 std::cout << "pdf_direction\n";
 
                 // Sine of the angle of the cone containing the sphere as seen from 'it.p'.
-                Float sin_alpha = m_radius * rcp(norm(m_center - it.p)),
+                Float sin_alpha = rcp(norm(m_center - it.p)),
                       cos_alpha = enoki::safe_sqrt(1.f - sin_alpha * sin_alpha);
 
                 return select(sin_alpha < math::OneMinusEpsilon<Float>,
@@ -255,7 +247,7 @@ NAMESPACE_BEGIN(mitsuba)
 
             Mask find_intersections( Double &near_t_, Double &far_t_,
                                      Double3 center,
-                                     scalar_t<Double> m_p, scalar_t<Double> m_k,
+                                     scalar_t<Double> p, scalar_t<Double> k,
                                      const Ray3f &ray) const{
 
                 // Unit vector
@@ -272,11 +264,11 @@ NAMESPACE_BEGIN(mitsuba)
 
                 Double x0 = c[0], y0 = c[1], z0 = c[2];
 
-                Double g = -1 * ( 1 + m_k );
+                Double g = -1 * ( 1 + k );
 
-                Double A = -1 * g * pow(dz, 2.0) + pow(dx,2.0) + pow(dy,2.0);
-                Double B = -1 * g * 2 * oz * dz + 2 * g * z0 * dz + 2 * ox * dx - 2 * x0 * dx + 2 * oy * dy - 2 * y0 * dy - 2 * dz / m_p;
-                Double C = -1 * g * pow(oz, 2.0) + g * 2 * z0 * oz - g * pow(-1*z0,2.0) + pow(ox,2.0) - 2 * x0 * ox + pow(-1*x0,2.0) + pow(oy,2.0) - 2 * y0 * oy + pow(-1*y0,2.0) - 2 * oz / m_p - 2 * -1*z0 / m_p;
+                Double A = -1 * g * sqr(dz) + sqr(dx) + sqr(dy);
+                Double B = -1 * g * 2 * oz * dz + 2 * g * z0 * dz + 2 * ox * dx - 2 * x0 * dx + 2 * oy * dy - 2 * y0 * dy - 2 * dz / p;
+                Double C = -1 * g * sqr(oz) + g * 2 * z0 * oz - g * sqr(-1*z0) + sqr(ox) - 2 * x0 * ox + sqr(-1*x0) + sqr(oy) - 2 * y0 * oy + sqr(-1*y0) - 2 * oz / p - 2 * -1*z0 / p;
 
                 auto [solution_found, near_t, far_t] = math::solve_quadratic(A, B, C);
 
@@ -286,21 +278,21 @@ NAMESPACE_BEGIN(mitsuba)
                 return solution_found;
             }
 
-            Mask point_valid( Double3 t0, Double3 center,
-                              scalar_t<Double> z_lim) const {
+            Mask point_on_lens_surface( Double3 point, Double3 center,
+                                        scalar_t<Double> z_lim) const {
 
                 Double3 delta0;
                 Double hyp0;
 
-                delta0 = t0 - center;
+                delta0 = point - center;
 
-                hyp0 = sqrt( pow( delta0[0], 2.0) + pow(delta0[1], 2.0) + pow(delta0[2], 2.0) );
+                hyp0 = sqrt( sqr(delta0[0]) + sqr(delta0[1]) + sqr(delta0[2]) );
 
                 Double limit;
 
                 Double w = (Double) z_lim;
 
-                limit = sqrt( (pow( (Double) m_h_lim, 2.0)) + pow(w, 2.0) );
+                limit = sqrt( sqr( (scalar_t<Double>)m_h_lim) + sqr(w) );
 
                 return (hyp0 <= limit);
             }
@@ -321,58 +313,58 @@ NAMESPACE_BEGIN(mitsuba)
 
                 //std::cout << " mint " << mint << " maxt " << maxt << "\n";
 
+
                 // Point-solutions for each sphere
                 Double near_t0, far_t0;
 
                 near_t0 = 0.0;
                 far_t0  = 0.0;
 
-                Mask solution;
+                scalar_t<Double> p = (scalar_t<Double>)m_p * (scalar_t<Double>)(m_flip ? -1. : 1.);
+                Mask intersected = find_intersections( near_t0, far_t0,
+                                        m_center,
+                                        p, (scalar_t<Double>) m_k,
+                                        ray);
 
-                if( m_flip ){
+                // Is any hit on the sphere surface which is limited by lens height & depth?
+                Mask valid_near = point_on_lens_surface( ray(near_t0),
+                                                     m_center,
+                                                     (scalar_t<Double>) m_z_lim );
 
-                    solution = find_intersections( near_t0, far_t0,
-                                           m_center - Double3(0,0,m_r*2.f),
-                                           (scalar_t<Double>) m_p, (scalar_t<Double>) m_k,
-                                           ray);
+                valid_near = valid_near && (near_t0 >= mint && near_t0 < maxt);
+
+
+                Mask valid_far = point_on_lens_surface( ray(far_t0),
+                                                     m_center,
+                                                     (scalar_t<Double>) m_z_lim );
 
-                    // Works as long as origin ray "comes from the right
-                    // direction" - needs fix.
-                    near_t0 = far_t0;
-                }
-                else{
-                    solution = find_intersections( near_t0, far_t0,
-                                           m_center,
-                                           (scalar_t<Double>) m_p, (scalar_t<Double>) m_k,
-                                           ray);
-                }
 
-                // Where on the sphere plane is that?
-                Mask valid0 = point_valid( ray(near_t0),
-                                           m_center,
-                                           (scalar_t<Double>) m_z_lim );
+                valid_far = valid_far && (far_t0 >= mint && far_t0 < maxt);
 
-                valid0 = valid0 && solution && (near_t0 >= mint && near_t0 < maxt);
+
+                Double chosen_t0 = select(valid_near, near_t0, far_t0);
+
+                Mask valid = intersected && (valid_near || valid_far);
 
                 /*
                  * Build the resulting ray.
                  * */
                 PreliminaryIntersection3f pi = zero<PreliminaryIntersection3f>();
 
-                pi.t = select( valid0, near_t0, math::Infinity<Float> );
+                pi.t = select( valid, chosen_t0, (scalar_t<Double>)math::Infinity<Float> );
 
                 // Remember to set active mask
-                active &= valid0;
+                active &= valid;
 
                 Ray3f out_ray;
                 out_ray.o = ray( pi.t );
 
 #if 1
-                if( m_flip ){
+                if( p < 0 ){
 
                     if( 0 || ( ++dbg > 100000 ) ){
 
-                        if( any(  valid0 ) ) {
+                        if( any(  valid ) ) {
 
                             std::cerr << "point1," << out_ray.o[0] << "," << out_ray.o[1] << "," << out_ray.o[2] << "\n";
                             std::cerr << "vec1," << ray.o[0] << "," << ray.o[1] << "," << ray.o[2] << "," << ray.d[0] << "," << ray.d[1] << "," << ray.d[2]  << "\n";
@@ -389,7 +381,7 @@ NAMESPACE_BEGIN(mitsuba)
                 else{ // !m_flip
 
                     if( 0 || ( ++dbg2 > 100000 ) ){
-                        if( any( valid0 ) ) {
+                        if( any( valid ) ) {
 
                             std::cerr << "point1," << out_ray.o[0] << "," << out_ray.o[1] << "," << out_ray.o[2] << "\n";
                             std::cerr << "vec1," << ray.o[0] << "," << ray.o[1] << "," << ray.o[2] << "," << ray.d[0] << "," << ray.d[1] << "," << ray.d[2]  << "\n";
@@ -474,21 +466,12 @@ NAMESPACE_BEGIN(mitsuba)
                  * Now compute the unit vector
                  * */
                 Double fx, fy, fz;
-                Double p(m_p);
-                Double k(m_k);
+                Double p((scalar_t<Double>)(m_p * (m_flip ? -1.f : 1.f)));
+                Double k((scalar_t<Double>)(m_k));
 
-                if( m_flip ){
-
-                    fx = ( point[0] * p ) / sqrt( 1 - (1+k) * (pow(point[0], 2) + pow(point[1], 2)) * pow(p, 2));
-                    fy = ( point[1] * p ) / sqrt( 1 - (1+k) * (pow(point[0], 2) + pow(point[1], 2)) * pow(p, 2));
-                    fz = 1.0;
-                }
-                else{
-
-                    fx = ( point[0] * p ) / sqrt( 1 - (1+k) * (pow(point[0], 2) + pow(point[1], 2)) * pow(p, 2));
-                    fy = ( point[1] * p ) / sqrt( 1 - (1+k) * (pow(point[0], 2) + pow(point[1], 2)) * pow(p, 2));
-                    fz = -1.0;
-                }
+                fx = ( point[0] * p ) / sqrt( 1 - (1+k) * (sqr(point[0]) + sqr(point[1])) * sqr(p));
+                fy = ( point[1] * p ) / sqrt( 1 - (1+k) * (sqr(point[0]) + sqr(point[1])) * sqr(p));
+                fz = -1.0;
 
                 if( ! m_flip_normals )
                     si.sh_frame.n = normalize( Double3( fx, fy, fz ) );
@@ -501,7 +484,7 @@ NAMESPACE_BEGIN(mitsuba)
 
 #if 0
                 if( 0 || ( ++dbg > 100000 ) ){
-                    if(m_flip){
+                    if(p < 0){
                         std::cerr << "point3," << si.p[0] << "," << si.p[1] << "," << si.p[2] << "\n";
                         std::cerr << "vec3," << ray.o[0] << "," << ray.o[1] << "," << ray.o[2] << "," << ray.d[0] << "," << ray.d[1] << "," << ray.d[2]  << "\n";
                         std::cerr << "vec4," << si.p[0] << "," << si.p[1] << "," << si.p[2] << "," << si.sh_frame.n[0] << "," << si.sh_frame.n[1] << "," << si.sh_frame.n[2] << "\n";
@@ -574,8 +557,10 @@ NAMESPACE_BEGIN(mitsuba)
                     if (!m_optix_data_ptr)
                         m_optix_data_ptr = cuda_malloc(sizeof(OptixAsphSurfData));
 
+                    ScalarFloat p = m_p * (m_flip? -1 : 1);
+
                     OptixAsphSurfData data = { bbox(), m_to_world, m_to_object,
-                        m_center, m_radius, m_k, m_p, m_r, m_h_lim, m_flip, m_z_lim,
+                        m_center, m_k, p, m_r, m_h_lim, m_z_lim,
                         m_flip_normals };
 
                     cuda_memcpy_to_device(m_optix_data_ptr, &data, sizeof(OptixAsphSurfData));
@@ -588,7 +573,6 @@ NAMESPACE_BEGIN(mitsuba)
                 oss << "AsphSurf[" << std::endl
                     << "  to_world = " << string::indent(m_to_world, 13) << "," << std::endl
                     << "  center = "  << m_center << "," << std::endl
-                    << "  radius = "  << m_radius << "," << std::endl
                     << "  surface_area = " << surface_area() << "," << std::endl
                     << "  " << string::indent(get_children_string()) << std::endl
                     << "]";
@@ -599,8 +583,6 @@ NAMESPACE_BEGIN(mitsuba)
         private:
                 /// Center in world-space
                 ScalarPoint3f m_center;
-                /// Radius in world-space
-                ScalarFloat m_radius;
                 /// kappa
                 ScalarFloat m_k;
                 /// curvature
@@ -608,13 +590,13 @@ NAMESPACE_BEGIN(mitsuba)
                 /// radius
                 ScalarFloat m_r;
 
-                /// limit of h
+                /// limit of height
                 ScalarFloat m_h_lim;
 
-                /// flip curvature?
+                /// flip curvature? Can also be specified through negative radius/curvature, note that these combine like -1*-1 = 1
                 bool m_flip;
 
-                /// how far into the "z plane" the surface reaches
+                /// how far into the z-dimension the surface reaches
                 /// -- it is a function of m_h_lim
                 ScalarFloat m_z_lim;
 
diff --git a/src/shapes/optix/aspheric_surf.cuh b/src/shapes/optix/aspheric_surf.cuh
index 8c2ef283..2bf9d654 100644
--- a/src/shapes/optix/aspheric_surf.cuh
+++ b/src/shapes/optix/aspheric_surf.cuh
@@ -10,13 +10,11 @@ struct OptixAsphSurfData {
     optix::Transform4f to_world;
     optix::Transform4f to_object;
     optix::Vector3f center;
-    float radius;
 
     float k;
     float p;
     float r;
     float h_lim;
-    bool flip;
     float z_lim;
 
     bool flip_normals;
@@ -24,27 +22,27 @@ struct OptixAsphSurfData {
 
 #ifdef __CUDACC__
 
-bool __device__ point_valid( Vector3f t0, Vector3f center, float z_lim, float h_lim) {
+bool __device__ point_on_lens_surface( Vector3f point, Vector3f center, float z_lim, float h_lim) {
 
     Vector3f delta0;
     float hyp0;
 
-    delta0 = t0 - center;
+    delta0 = point - center;
 
-    hyp0 = sqrt( pow( delta0[0], 2.0) + pow(delta0[1], 2.0) + pow(delta0[2], 2.0) );
+    hyp0 = sqrt( sqr(delta0[0]) + sqr(delta0[1]) + sqr(delta0[2]) );
 
     float limit;
 
     float w = (float) z_lim;
 
-    limit = sqrt( (pow( (float) h_lim, 2.0)) + pow(w, 2.0) );
+    limit = sqrt( sqr(h_lim) + sqr(w) );
 
     return (hyp0 <= limit);
 }
 
 bool __device__ find_intersections0( float &near_t, float &far_t,
                           Vector3f center,
-                          float m_p, float m_k,
+                          float p, float k,
                           const Ray3f &ray){
 
     // Unit vector
@@ -61,11 +59,11 @@ bool __device__ find_intersections0( float &near_t, float &far_t,
 
     float x0 = c[0], y0 = c[1], z0 = c[2];
 
-    float g = -1 * ( 1 + m_k );
+    float g = -1 * ( 1 + k );
 
-    float A = -1 * g * pow(dz, 2.0) + pow(dx,2.0) + pow(dy,2.0);
-    float B = -1 * g * 2 * oz * dz + 2 * g * z0 * dz + 2 * ox * dx - 2 * x0 * dx + 2 * oy * dy - 2 * y0 * dy - 2 * dz / m_p;
-    float C = -1 * g * pow(oz, 2.0) + g * 2 * z0 * oz - g * pow(-1*z0,2.0) + pow(ox,2.0) - 2 * x0 * ox + pow(-1*x0,2.0) + pow(oy,2.0) - 2 * y0 * oy + pow(-1*y0,2.0) - 2 * oz / m_p - 2 * -1*z0 / m_p;
+    float A = -1 * g * sqr(dz) + sqr(dx) + sqr(dy);
+    float B = -1 * g * 2 * oz * dz + 2 * g * z0 * dz + 2 * ox * dx - 2 * x0 * dx + 2 * oy * dy - 2 * y0 * dy - 2 * dz / p;
+    float C = -1 * g * sqr(oz) + g * 2 * z0 * oz - g * sqr(-1*z0) + sqr(ox) - 2 * x0 * ox + sqr(-1*x0) + sqr(oy) - 2 * y0 * oy + sqr(-1*y0) - 2 * oz / p - 2 * -1*z0 / p;
 
     bool solution_found = solve_quadratic(A, B, C, near_t, far_t);
 
@@ -80,36 +78,41 @@ extern "C" __global__ void __intersection__asphsurf() {
     Ray3f ray = get_ray();
 
     float near_t0, far_t0;
+    bool solution = find_intersections0( near_t0, far_t0,
+                                        asurf->center,
+                                        asurf->p, asurf->k,
+                                        ray);
 
-    bool solution0;
-    bool valid0;
-
-    if( asurf->flip ){
-        solution0 = find_intersections0( near_t0, far_t0,
-                                         asurf->center - Vector3f(0,0, asurf->r * 2.f),
-                                         asurf->p, asurf->k,
-                                         ray);
-
-        near_t0 = far_t0; // hack hack
+    if(!solution) {
+        return;
     }
-    else{
-        solution0 = find_intersections0( near_t0, far_t0,
-                                         asurf->center,
-                                         asurf->p, asurf->k,
-                                         ray);
-    }
-
-    // Where on the sphere plane is that?
 
-    valid0 = point_valid( ray(near_t0),
+    // Is one or both hits on the sphere surface which is limited by lens height & depth?
+    bool valid_near = point_on_lens_surface( ray(near_t0),
                     asurf->center,
                     asurf->z_lim, asurf->h_lim );
 
-    valid0 = valid0 && solution0 && (near_t0 >= ray.mint && near_t0 < ray.maxt);
+    valid_near = valid_near && (near_t0 >= ray.mint && near_t0 < ray.maxt);
+
+    bool valid_far = point_on_lens_surface( ray(far_t0),
+                     asurf->center,
+                     asurf->z_lim, asurf->h_lim );
+
+    valid_far = valid_far && (far_t0 >= ray.mint && far_t0 < ray.maxt);
+
+    if(!(valid_far || valid_near)) {
+        return;
+    }
 
-    if( valid0 ){
-        optixReportIntersection( near_t0, OPTIX_HIT_KIND_TRIANGLE_FRONT_FACE );
+    float golden_t0;
+    if(valid_near) {
+        golden_t0 = near_t0;
+    }
+    else {
+        golden_t0 = far_t0;
     }
+
+    optixReportIntersection( golden_t0, OPTIX_HIT_KIND_TRIANGLE_FRONT_FACE );
 }
 
 
@@ -148,18 +151,9 @@ extern "C" __global__ void __closesthit__asphsurf() {
 
         Vector3f ns;
 
-        if( asurf->flip ){
-
-            fx = ( point[0] * asurf->p ) / sqrt( 1 - (1+asurf->k) * (pow(point[0], 2) + pow(point[1], 2)) * pow(asurf->p, 2) );
-            fy = ( point[1] * asurf->p ) / sqrt( 1 - (1+asurf->k) * (pow(point[0], 2) + pow(point[1], 2)) * pow(asurf->p, 2) );
-            fz = 1.0;
-        }
-        else{
-
-            fx = ( point[0] * asurf->p ) / sqrt( 1 - (1+asurf->k) * (pow(point[0], 2) + pow(point[1], 2)) * pow(asurf->p, 2) );
-            fy = ( point[1] * asurf->p ) / sqrt( 1 - (1+asurf->k) * (pow(point[0], 2) + pow(point[1], 2)) * pow(asurf->p, 2) );
-            fz = -1.0;
-        }
+        fx = ( point[0] * asurf->p ) / sqrt( 1 - (1+asurf->k) * (sqr(point[0]) + sqr(point[1])) * sqr(asurf->p) );
+        fy = ( point[1] * asurf->p ) / sqrt( 1 - (1+asurf->k) * (sqr(point[0]) + sqr(point[1])) * sqr(asurf->p) );
+        fz = -1.0;
 
         if( ! asurf->flip_normals )
             ns = normalize( Vector3f( fx, fy, fz ) );
